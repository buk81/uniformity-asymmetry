{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Chat Template Control Experiment: Llama-3.1-8B-Instruct\n\n**Purpose:** Test whether chat templates affect the embedding-output relationship.\n\n**Background:** Kevin-pw pointed out that chat-tuned models require proper templating.\n\n**Experimental Design:**\n- Condition A: Instruct model WITH chat template\n- Condition B: Instruct model WITHOUT chat template\n\n**Expected Runtime:** ~3-4h on A100\n\n---\n\n**Author:** Davide D'Elia  \n**Date:** 2026-01-03"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers accelerate torch numpy scipy matplotlib scikit-learn huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "N_BOOTSTRAP = 10000\n",
    "CI_LEVEL = 0.95\n",
    "\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'CUDA: {torch.cuda.is_available()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ========================================\n# HUGGINGFACE LOGIN\n# ========================================\n# OPTION 1: Colab Secrets (empfohlen)\ntry:\n    from google.colab import userdata\n    HF_TOKEN = userdata.get('HF_TOKEN')\n    print(\"✅ Token aus Colab Secrets geladen\")\nexcept:\n    HF_TOKEN = None\n\n# OPTION 2: Manuell eingeben\nif not HF_TOKEN:\n    HF_TOKEN = ''  # <-- Hier Token einfügen falls nötig\n\n# OPTION 3: Interaktiver Login\nif not HF_TOKEN:\n    from huggingface_hub import notebook_login\n    notebook_login()\nelse:\n    from huggingface_hub import login\n    login(token=HF_TOKEN)\n    print(\"✅ Eingeloggt\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Instruct Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "MODEL_NAME = 'meta-llama/Llama-3.1-8B-Instruct'\nMODEL_DISPLAY = 'Llama-3.1-8B-Instruct'\n\nprint(f'Loading {MODEL_DISPLAY}...')\n\ntoken_arg = HF_TOKEN if HF_TOKEN else True\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=token_arg)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    token=token_arg,\n    torch_dtype=torch.float16,\n    device_map='auto',\n    output_hidden_states=True\n)\n\nprint(f'✅ Layers: {model.config.num_hidden_layers}')\nprint(f'Chat template: {tokenizer.chat_template is not None}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate chat template\n",
    "example = 'The Earth is flat.'\n",
    "messages = [{'role': 'user', 'content': example}]\n",
    "templated = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "print('Raw:', example)\n",
    "print('Templated:', templated[:80], '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://raw.githubusercontent.com/buk81/uniformity-asymmetry/main/dataset.json\n",
    "\n",
    "with open('dataset.json', 'r') as f:\n",
    "    DATASET = json.load(f)\n",
    "\n",
    "ALL_PAIRS = []\n",
    "for cat_name, cat_data in DATASET.items():\n",
    "    for pair in cat_data['pairs']:\n",
    "        ALL_PAIRS.append({'stmt_a': pair[0], 'stmt_b': pair[1], 'category': cat_name})\n",
    "\n",
    "print(f'Total pairs: {len(ALL_PAIRS)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Core Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_template(text, tokenizer):\n",
    "    messages = [{'role': 'user', 'content': text}]\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "\n",
    "def get_layer_embedding(text, model, tokenizer, layer_idx, use_template=False):\n",
    "    if use_template:\n",
    "        text = apply_template(text, tokenizer)\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "    layer_hidden = outputs.hidden_states[layer_idx]\n",
    "    return layer_hidden[0, 1:, :].mean(dim=0).cpu().numpy().astype(np.float32)\n",
    "\n",
    "def get_output_preference(text_a, text_b, model, tokenizer, use_template=False):\n",
    "    def get_nll(text):\n",
    "        if use_template:\n",
    "            text = apply_template(text, tokenizer)\n",
    "        inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512).to(model.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, labels=inputs['input_ids'])\n",
    "        return outputs.loss.item()\n",
    "    return get_nll(text_b) - get_nll(text_a)\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b) + 1e-10))\n",
    "\n",
    "def bootstrap_correlation(x, y, n_bootstrap=10000, ci_level=0.95):\n",
    "    n = len(x)\n",
    "    r_obs, p = stats.pearsonr(x, y)\n",
    "    rs = []\n",
    "    for _ in range(n_bootstrap):\n",
    "        idx = np.random.choice(n, size=n, replace=True)\n",
    "        if np.std(x[idx]) > 0 and np.std(y[idx]) > 0:\n",
    "            r, _ = stats.pearsonr(x[idx], y[idx])\n",
    "            rs.append(r)\n",
    "    rs = np.array(rs)\n",
    "    alpha = 1 - ci_level\n",
    "    return float(r_obs), float(np.percentile(rs, alpha/2*100)), float(np.percentile(rs, (1-alpha/2)*100)), float(p)\n",
    "\n",
    "print('Functions defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Collect Embeddings (Both Conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_LAYERS = model.config.num_hidden_layers\n",
    "LAYERS_TO_TEST = list(range(0, N_LAYERS + 1, 4))\n",
    "if N_LAYERS not in LAYERS_TO_TEST:\n",
    "    LAYERS_TO_TEST.append(N_LAYERS)\n",
    "\n",
    "print(f'Layers: {LAYERS_TO_TEST}')\n",
    "print(f'Running TWO conditions: WITH and WITHOUT template')\n",
    "\n",
    "pair_data_with = []\n",
    "pair_data_without = []\n",
    "start = datetime.now()\n",
    "\n",
    "for i, pair in enumerate(ALL_PAIRS):\n",
    "    if (i + 1) % 25 == 0:\n",
    "        elapsed = (datetime.now() - start).total_seconds() / 60\n",
    "        print(f'  [{i+1}/{len(ALL_PAIRS)}] - {elapsed:.1f} min')\n",
    "    \n",
    "    # WITH template\n",
    "    pref_w = get_output_preference(pair['stmt_a'], pair['stmt_b'], model, tokenizer, use_template=True)\n",
    "    embs_w = {l: {'a': get_layer_embedding(pair['stmt_a'], model, tokenizer, l, True),\n",
    "                  'b': get_layer_embedding(pair['stmt_b'], model, tokenizer, l, True)} for l in LAYERS_TO_TEST}\n",
    "    pair_data_with.append({'pref': pref_w, 'cat': pair['category'], 'embs': embs_w})\n",
    "    \n",
    "    # WITHOUT template\n",
    "    pref_wo = get_output_preference(pair['stmt_a'], pair['stmt_b'], model, tokenizer, use_template=False)\n",
    "    embs_wo = {l: {'a': get_layer_embedding(pair['stmt_a'], model, tokenizer, l, False),\n",
    "                   'b': get_layer_embedding(pair['stmt_b'], model, tokenizer, l, False)} for l in LAYERS_TO_TEST}\n",
    "    pair_data_without.append({'pref': pref_wo, 'cat': pair['category'], 'embs': embs_wo})\n",
    "\n",
    "print(f'Done in {(datetime.now() - start).total_seconds() / 60:.1f} min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyze Both Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_centroid_asymmetry(pair_data, layer_idx):\n",
    "    embs_a = np.array([p['embs'][layer_idx]['a'] for p in pair_data])\n",
    "    embs_b = np.array([p['embs'][layer_idx]['b'] for p in pair_data])\n",
    "    cent_a, cent_b = embs_a.mean(0), embs_b.mean(0)\n",
    "    dist_a = np.array([cosine_similarity(e, cent_a) for e in embs_a])\n",
    "    dist_b = np.array([cosine_similarity(e, cent_b) for e in embs_b])\n",
    "    return dist_a - dist_b\n",
    "\n",
    "def analyze(pair_data, name):\n",
    "    print(f'\\n=== {name} ===')\n",
    "    prefs = np.array([p['pref'] for p in pair_data])\n",
    "    results = {}\n",
    "    for l in LAYERS_TO_TEST:\n",
    "        metric = compute_centroid_asymmetry(pair_data, l)\n",
    "        r, ci_lo, ci_hi, p = bootstrap_correlation(metric, prefs, N_BOOTSTRAP, CI_LEVEL)\n",
    "        results[l] = {'r': r, 'ci_lo': ci_lo, 'ci_hi': ci_hi, 'sig': not (ci_lo <= 0 <= ci_hi)}\n",
    "        sig = '***' if results[l]['sig'] else ''\n",
    "        print(f'  Layer {l:2d}: r={r:+.3f} CI=[{ci_lo:+.3f},{ci_hi:+.3f}] {sig}')\n",
    "    return results\n",
    "\n",
    "results_with = analyze(pair_data_with, 'WITH Template')\n",
    "results_without = analyze(pair_data_without, 'WITHOUT Template')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compare Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '='*60)\n",
    "print(' TEMPLATE EFFECT COMPARISON')\n",
    "print('='*60)\n",
    "print(f'Layer   WITH      WITHOUT   Same Sign?')\n",
    "print('-'*45)\n",
    "\n",
    "same = 0\n",
    "for l in LAYERS_TO_TEST:\n",
    "    rw = results_with[l]['r']\n",
    "    rwo = results_without[l]['r']\n",
    "    ss = (rw * rwo) > 0\n",
    "    if ss: same += 1\n",
    "    print(f'{l:2d}      {rw:+.3f}    {rwo:+.3f}    {\"YES\" if ss else \"NO\"}')\n",
    "\n",
    "print(f'\\nSame sign: {same}/{len(LAYERS_TO_TEST)}')\n",
    "\n",
    "# Phase structure\n",
    "def phase_means(results):\n",
    "    e = np.mean([results[l]['r'] for l in [0,4,8]])\n",
    "    m = np.mean([results[l]['r'] for l in [12,16,20]])\n",
    "    t = np.mean([results[l]['r'] for l in [24,28,32]])\n",
    "    return e, m, t\n",
    "\n",
    "ew, mw, tw = phase_means(results_with)\n",
    "ewo, mwo, two = phase_means(results_without)\n",
    "\n",
    "print(f'\\nPhase Structure:')\n",
    "print(f'           WITH     WITHOUT')\n",
    "print(f'Early:    {ew:+.3f}   {ewo:+.3f}')\n",
    "print(f'Mid:      {mw:+.3f}   {mwo:+.3f}')\n",
    "print(f'Late:     {tw:+.3f}   {two:+.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Verdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pattern_str(e, m, t):\n",
    "    if e > 0.1 and m > 0.1 and t < -0.1:\n",
    "        return 'PYTHIA_PATTERN'\n",
    "    elif abs(e) < 0.1 and abs(m) < 0.1 and abs(t) < 0.1:\n",
    "        return 'DECOUPLED'\n",
    "    else:\n",
    "        return 'OTHER'\n",
    "\n",
    "pat_w = pattern_str(ew, mw, tw)\n",
    "pat_wo = pattern_str(ewo, mwo, two)\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print(' VERDICT')\n",
    "print('='*60)\n",
    "print(f'WITH template: {pat_w}')\n",
    "print(f'WITHOUT template: {pat_wo}')\n",
    "\n",
    "if pat_w == pat_wo:\n",
    "    verdict = 'TEMPLATE_INVARIANT'\n",
    "elif pat_wo == 'DECOUPLED' and pat_w != 'DECOUPLED':\n",
    "    verdict = 'TEMPLATE_ENABLES_COUPLING'\n",
    "else:\n",
    "    verdict = 'TEMPLATE_CHANGES_PATTERN'\n",
    "\n",
    "print(f'\\n>>> {verdict} <<<')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'model': MODEL_NAME,\n",
    "    'n_pairs': len(ALL_PAIRS),\n",
    "    'results_with_template': {str(k): v for k, v in results_with.items()},\n",
    "    'results_without_template': {str(k): v for k, v in results_without.items()},\n",
    "    'phase_with': {'early': ew, 'mid': mw, 'late': tw, 'pattern': pat_w},\n",
    "    'phase_without': {'early': ewo, 'mid': mwo, 'late': two, 'pattern': pat_wo},\n",
    "    'verdict': verdict\n",
    "}\n",
    "\n",
    "fname = f'llama3_instruct_template_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.json'\n",
    "with open(fname, 'w') as f:\n",
    "    json.dump(save_data, f, indent=2)\n",
    "print(f'Saved: {fname}')\n",
    "\n",
    "from google.colab import files\n",
    "files.download(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "rs_w = [results_with[l]['r'] for l in LAYERS_TO_TEST]\n",
    "rs_wo = [results_without[l]['r'] for l in LAYERS_TO_TEST]\n",
    "\n",
    "ax.plot(LAYERS_TO_TEST, rs_w, 'o-', label='WITH template', color='blue', markersize=8)\n",
    "ax.plot(LAYERS_TO_TEST, rs_wo, 's--', label='WITHOUT template', color='red', markersize=8)\n",
    "ax.axhline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel('Layer')\n",
    "ax.set_ylabel('r(centroid_asymmetry, output)')\n",
    "ax.set_title(f'{MODEL_DISPLAY}: Template Effect\\nVerdict: {verdict}')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('llama3_instruct_template.png', dpi=150)\n",
    "plt.show()\n",
    "print('Plot saved')\n",
    "\n",
    "from google.colab import files\n",
    "files.download('llama3_instruct_template.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}