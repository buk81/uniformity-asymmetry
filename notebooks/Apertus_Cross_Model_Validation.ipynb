{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "A100"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Model Validation: Apertus-8B Layer Analysis\n\n",
    "**Purpose:** Test if phase-structured embedding-output relationship exists in multilingual models.\n\n",
    "**Research Question:** Does Apertus-8B (massively multilingual) show:\n",
    "1. The same late-layer inversion as Pythia/Llama?\n",
    "2. Different phase structure due to multilingual compression?\n\n",
    "**Context:** Original paper showed complex pattern due to multilingual training.\n",
    "Layer analysis will reveal if compression affects phase structure.\n\n",
    "**Expected Runtime:** ~2.5h on A100\n\n",
    "---\n\n",
    "**Author:** Davide D'Elia  \n",
    "**Date:** 2026-01-03  \n",
    "**Model:** swiss-ai/Apertus-8B-2509 (Multilingual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install -q transformers accelerate torch numpy scipy matplotlib scikit-learn huggingface_hub"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import json\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple\n\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n\n",
    "warnings.filterwarnings('ignore')\n\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n\n",
    "N_BOOTSTRAP = 10000\n",
    "CI_LEVEL = 0.95\n\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'CUDA: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Apertus is open - no token required\n# KRITISCH: Apertus hat dtype-Probleme mit device_map='auto'\n# LÃ¶sung: Manuell laden, konvertieren, dann auf GPU schieben\n\nMODEL_NAME = 'swiss-ai/Apertus-8B-2509'\nMODEL_DISPLAY = 'Apertus-8B'\n\nprint(f'Loading {MODEL_DISPLAY} (Swiss AI - Multilingual)...')\nprint('NOTE: Loading without device_map to avoid dtype issues')\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n# Step 1: Load model (will be on CPU initially)\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    torch_dtype=torch.float32,\n    low_cpu_mem_usage=True,\n    output_hidden_states=True\n)\n\n# Step 2: Force ALL parameters to float32\nmodel = model.float()\n\n# Step 3: Move to GPU\nmodel = model.cuda()\n\n# Verify dtypes\nparam_dtypes = set(p.dtype for p in model.parameters())\nprint(f'Parameter dtypes after conversion: {param_dtypes}')\n\nprint(f'Model loaded on: {next(model.parameters()).device}')\nprint(f'Layers: {model.config.num_hidden_layers}')\nprint(f'Hidden size: {model.config.hidden_size}')\nprint(f'Vocab size: {model.config.vocab_size} (multilingual)')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!wget -q https://raw.githubusercontent.com/buk81/uniformity-asymmetry/main/dataset.json\n\n",
    "with open('dataset.json', 'r') as f:\n",
    "    DATASET = json.load(f)\n\n",
    "ALL_PAIRS = []\n",
    "for cat_name, cat_data in DATASET.items():\n",
    "    for pair in cat_data['pairs']:\n",
    "        ALL_PAIRS.append({'stmt_a': pair[0], 'stmt_b': pair[1], 'category': cat_name})\n\n",
    "print(f'Categories: {list(DATASET.keys())}')\n",
    "print(f'Total pairs: {len(ALL_PAIRS)}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Core Functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def get_layer_embedding(text, model, tokenizer, layer_idx):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "    layer_hidden = outputs.hidden_states[layer_idx]\n",
    "    return layer_hidden[0, 1:, :].mean(dim=0).cpu().numpy().astype(np.float32)\n\n",
    "def get_output_preference(text_a, text_b, model, tokenizer):\n",
    "    def get_nll(text):\n",
    "        inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512).to(model.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, labels=inputs['input_ids'])\n",
    "        return outputs.loss.item()\n",
    "    return get_nll(text_b) - get_nll(text_a)\n\n",
    "def cosine_similarity(a, b):\n",
    "    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b) + 1e-10))\n\n",
    "def bootstrap_correlation(x, y, n_bootstrap=10000, ci_level=0.95):\n",
    "    n = len(x)\n",
    "    r_obs, p = stats.pearsonr(x, y)\n",
    "    rs = []\n",
    "    for _ in range(n_bootstrap):\n",
    "        idx = np.random.choice(n, size=n, replace=True)\n",
    "        if np.std(x[idx]) > 0 and np.std(y[idx]) > 0:\n",
    "            r, _ = stats.pearsonr(x[idx], y[idx])\n",
    "            rs.append(r)\n",
    "    rs = np.array(rs)\n",
    "    alpha = 1 - ci_level\n",
    "    return float(r_obs), float(np.percentile(rs, alpha/2*100)), float(np.percentile(rs, (1-alpha/2)*100)), float(p)\n\n",
    "print('Functions defined.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Collect Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "N_LAYERS = model.config.num_hidden_layers\n",
    "# Sample every 4th layer like Pythia\n",
    "LAYERS_TO_TEST = list(range(0, N_LAYERS + 1, 4))\n",
    "if N_LAYERS not in LAYERS_TO_TEST:\n",
    "    LAYERS_TO_TEST.append(N_LAYERS)\n\n",
    "print(f'Total layers: {N_LAYERS}')\n",
    "print(f'Testing layers: {LAYERS_TO_TEST}')\n",
    "print(f'Collecting embeddings for {len(ALL_PAIRS)} pairs...')\n\n",
    "pair_data = []\n",
    "start = datetime.now()\n\n",
    "for i, pair in enumerate(ALL_PAIRS):\n",
    "    if (i + 1) % 25 == 0:\n",
    "        elapsed = (datetime.now() - start).total_seconds() / 60\n",
    "        print(f'  [{i+1:03d}/{len(ALL_PAIRS)}] - {elapsed:.1f} min')\n",
    "    \n",
    "    pref = get_output_preference(pair['stmt_a'], pair['stmt_b'], model, tokenizer)\n",
    "    layer_embs = {}\n",
    "    for l in LAYERS_TO_TEST:\n",
    "        emb_a = get_layer_embedding(pair['stmt_a'], model, tokenizer, l)\n",
    "        emb_b = get_layer_embedding(pair['stmt_b'], model, tokenizer, l)\n",
    "        layer_embs[l] = {'a': emb_a, 'b': emb_b}\n",
    "    pair_data.append({'pref': pref, 'cat': pair['category'], 'embs': layer_embs})\n\n",
    "print(f'Done in {(datetime.now() - start).total_seconds() / 60:.1f} min')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Pair-Level Analysis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def compute_centroid_asymmetry(pair_data, layer_idx):\n",
    "    embs_a = np.array([p['embs'][layer_idx]['a'] for p in pair_data])\n",
    "    embs_b = np.array([p['embs'][layer_idx]['b'] for p in pair_data])\n",
    "    cent_a, cent_b = embs_a.mean(0), embs_b.mean(0)\n",
    "    dist_a = np.array([cosine_similarity(e, cent_a) for e in embs_a])\n",
    "    dist_b = np.array([cosine_similarity(e, cent_b) for e in embs_b])\n",
    "    return dist_a - dist_b\n\n",
    "all_prefs = np.array([p['pref'] for p in pair_data])\n\n",
    "print('=' * 70)\n",
    "print(f' {MODEL_DISPLAY}: PAIR-LEVEL ANALYSIS (n={len(pair_data)})')\n",
    "print('=' * 70)\n\n",
    "results = {}\n",
    "for l in LAYERS_TO_TEST:\n",
    "    metric = compute_centroid_asymmetry(pair_data, l)\n",
    "    r, ci_lo, ci_hi, p = bootstrap_correlation(metric, all_prefs, N_BOOTSTRAP, CI_LEVEL)\n",
    "    sig = '***' if not (ci_lo <= 0 <= ci_hi) else ''\n",
    "    results[l] = {'r': r, 'ci_lo': ci_lo, 'ci_hi': ci_hi, 'sig': not (ci_lo <= 0 <= ci_hi)}\n",
    "    print(f'Layer {l:2d}: r={r:+.3f} CI=[{ci_lo:+.3f},{ci_hi:+.3f}] {sig}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Phase Structure"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Determine phase boundaries\n",
    "n_layers = len(LAYERS_TO_TEST)\n",
    "early_layers = LAYERS_TO_TEST[:n_layers//3]\n",
    "mid_layers = LAYERS_TO_TEST[n_layers//3:2*n_layers//3]\n",
    "late_layers = LAYERS_TO_TEST[2*n_layers//3:]\n\n",
    "early_mean = np.mean([results[l]['r'] for l in early_layers])\n",
    "mid_mean = np.mean([results[l]['r'] for l in mid_layers])\n",
    "late_mean = np.mean([results[l]['r'] for l in late_layers])\n\n",
    "print(f'\\nPhase Structure:')\n",
    "print(f'  Early {early_layers}: mean r = {early_mean:+.3f}')\n",
    "print(f'  Mid   {mid_layers}: mean r = {mid_mean:+.3f}')\n",
    "print(f'  Late  {late_layers}: mean r = {late_mean:+.3f}')\n\n",
    "# Determine pattern\n",
    "if early_mean > 0.1 and late_mean < -0.1:\n",
    "    pattern = 'PYTHIA_PATTERN: positive early, negative late'\n",
    "elif late_mean < -0.1:\n",
    "    pattern = 'LATE_INVERSION: negative in late layers'\n",
    "elif abs(early_mean) < 0.15 and abs(mid_mean) < 0.15 and abs(late_mean) < 0.15:\n",
    "    pattern = 'DECOUPLED: weak correlations throughout'\n",
    "else:\n",
    "    pattern = 'OTHER'\n\n",
    "print(f'\\n>>> Pattern: {pattern} <<<')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comparison with Other Models"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print('=' * 70)\n",
    "print(' CROSS-MODEL COMPARISON (All 4 Original Models)')\n",
    "print('=' * 70)\n\n",
    "print(f'\\nPhase Means Comparison:')\n",
    "print(f'{\"Model\":<30} {\"Early\":>10} {\"Mid\":>10} {\"Late\":>10}')\n",
    "print('-' * 60)\n",
    "print(f'{\"Pythia-6.9B (Base)\":<30} {+0.44:>+10.2f} {+0.36:>+10.2f} {-0.17:>+10.2f}')\n",
    "print(f'{\"Llama-3.1-8B (Base)\":<30} {+0.05:>+10.2f} {-0.16:>+10.2f} {-0.30:>+10.2f}')\n",
    "print(f'{\"Llama-3.1 Instruct+Template\":<30} {-0.47:>+10.2f} {-0.58:>+10.2f} {-0.60:>+10.2f}')\n",
    "print(f'{\"Gemma-2B (SFT)\":<30} {\"TBD\":>10} {\"TBD\":>10} {\"TBD\":>10}')\n",
    "print(f'{MODEL_DISPLAY + \" (Multilingual)\":<30} {early_mean:>+10.2f} {mid_mean:>+10.2f} {late_mean:>+10.2f}')\n\n",
    "# Check late layer inversion\n",
    "final_layer = LAYERS_TO_TEST[-1]\n",
    "final_r = results[final_layer]['r']\n",
    "print(f'\\nFinal layer ({final_layer}) correlation: r = {final_r:+.3f}')\n",
    "if final_r < -0.1 and results[final_layer]['sig']:\n",
    "    print('>>> LATE-LAYER INVERSION CONFIRMED <<<')\n",
    "elif final_r < 0:\n",
    "    print('>>> Negative trend in late layer <<<')\n",
    "else:\n",
    "    print('>>> NO late-layer inversion <<<')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n\n",
    "layers = list(results.keys())\n",
    "rs = [results[l]['r'] for l in layers]\n",
    "ci_los = [results[l]['ci_lo'] for l in layers]\n",
    "ci_his = [results[l]['ci_hi'] for l in layers]\n\n",
    "yerr_lo = [r - ci_lo for r, ci_lo in zip(rs, ci_los)]\n",
    "yerr_hi = [ci_hi - r for r, ci_hi in zip(rs, ci_his)]\n\n",
    "ax.errorbar(layers, rs, yerr=[yerr_lo, yerr_hi], fmt='o-', capsize=5, \n",
    "            capthick=2, markersize=8, color='purple', label=MODEL_DISPLAY)\n\n",
    "# Mark significant\n",
    "for l, r in zip(layers, rs):\n",
    "    if results[l]['sig']:\n",
    "        ax.scatter([l], [r], color='red', s=150, zorder=5, marker='*')\n\n",
    "ax.axhline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel('Layer', fontsize=12)\n",
    "ax.set_ylabel('r(centroid_asymmetry, output)', fontsize=12)\n",
    "ax.set_title(f'{MODEL_DISPLAY} (Multilingual): Layer-wise Correlation\\nPattern: {pattern}', fontsize=14)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n\n",
    "plt.tight_layout()\n",
    "plt.savefig('apertus_layer_analysis.png', dpi=150)\n",
    "plt.show()\n",
    "print('Plot saved')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "save_data = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'model': MODEL_NAME,\n",
    "    'model_display': MODEL_DISPLAY,\n",
    "    'model_type': 'Multilingual (Swiss AI)',\n",
    "    'n_layers': N_LAYERS,\n",
    "    'layers_tested': LAYERS_TO_TEST,\n",
    "    'n_pairs': len(pair_data),\n",
    "    'n_bootstrap': N_BOOTSTRAP,\n",
    "    'results': {str(k): v for k, v in results.items()},\n",
    "    'phase_structure': {\n",
    "        'early_mean': float(early_mean),\n",
    "        'mid_mean': float(mid_mean),\n",
    "        'late_mean': float(late_mean),\n",
    "        'pattern': pattern\n",
    "    }\n",
    "}\n\n",
    "fname = f'apertus_cross_validation_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.json'\n",
    "with open(fname, 'w') as f:\n",
    "    json.dump(save_data, f, indent=2)\n",
    "print(f'Saved: {fname}')\n\n",
    "from google.colab import files\n",
    "files.download(fname)\n",
    "files.download('apertus_layer_analysis.png')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary\n\n",
    "**Key Questions Answered:**\n",
    "1. Does multilingual compression affect phase structure?\n",
    "2. Is late-layer inversion universal across architectures?\n",
    "3. How does Apertus compare to monolingual models?"
   ]
  }
 ]
}