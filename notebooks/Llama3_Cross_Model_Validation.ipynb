{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Cross-Model Validation: Llama-3.1-8B Layer Analysis\n\n**Purpose:** Validate whether the phase-structured embedding-output relationship generalizes beyond Pythia.\n\n**Research Question:** Does Llama-3.1-8B show the same three-phase pattern?\n1. Positive correlation in early/mid layers\n2. Transition zone\n3. Negative correlation in late layers\n\n**Method:**\n- Same 230 pairs as Pythia experiments\n- Layer-wise UA analysis (every 4th layer)\n- Pair-level metrics (centroid_asymmetry, etc.)\n- Bootstrap CI with n=10,000\n\n**Expected Runtime:** ~2h on A100\n\n---\n\n**Author:** Davide D'Elia  \n**Date:** 2026-01-03  \n**Model:** Meta-Llama-3.1-8B (Base, not Instruct)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers accelerate torch numpy scipy matplotlib scikit-learn huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "N_BOOTSTRAP = 10000\n",
    "CI_LEVEL = 0.95\n",
    "\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'CUDA: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ========================================\n# HUGGINGFACE LOGIN\n# ========================================\n# Zwei Optionen:\n\n# OPTION 1: Colab Secrets (empfohlen)\n# Gehe zu: Colab Menü → Schlüssel-Symbol links → Add secret \"HF_TOKEN\"\ntry:\n    from google.colab import userdata\n    HF_TOKEN = userdata.get('HF_TOKEN')\n    print(\"✅ Token aus Colab Secrets geladen\")\nexcept:\n    HF_TOKEN = None\n\n# OPTION 2: Manuell eingeben falls Secrets nicht funktioniert\nif not HF_TOKEN:\n    HF_TOKEN = ''  # <-- Hier Token einfügen falls nötig\n\n# OPTION 3: Interaktiver Login\nif not HF_TOKEN:\n    from huggingface_hub import notebook_login\n    notebook_login()\n    print(\"✅ Interaktiv eingeloggt\")\nelse:\n    from huggingface_hub import login\n    login(token=HF_TOKEN)\n    print(\"✅ Mit Token eingeloggt\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "MODEL_NAME = 'meta-llama/Llama-3.1-8B'\nMODEL_DISPLAY = 'Llama-3.1-8B'\n\nprint(f'Loading {MODEL_DISPLAY}...')\nprint('NOTE: Using BASE model (not Instruct) for fair comparison with Pythia')\n\n# Token nur übergeben wenn gesetzt (sonst nutzt es den cached Login)\ntoken_arg = HF_TOKEN if HF_TOKEN else True  # True = use cached token\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=token_arg)\n\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    token=token_arg,\n    torch_dtype=torch.float16,\n    device_map='auto',\n    output_hidden_states=True\n)\n\nprint(f'✅ Model loaded on: {model.device}')\nprint(f'Layers: {model.config.num_hidden_layers}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://raw.githubusercontent.com/buk81/uniformity-asymmetry/main/dataset.json\n",
    "\n",
    "with open('dataset.json', 'r') as f:\n",
    "    DATASET = json.load(f)\n",
    "\n",
    "ALL_PAIRS = []\n",
    "for cat_name, cat_data in DATASET.items():\n",
    "    for pair in cat_data['pairs']:\n",
    "        ALL_PAIRS.append({\n",
    "            'stmt_a': pair[0],\n",
    "            'stmt_b': pair[1],\n",
    "            'category': cat_name\n",
    "        })\n",
    "\n",
    "print(f'Categories: {list(DATASET.keys())}')\n",
    "print(f'Total pairs: {len(ALL_PAIRS)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Core Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_embedding(text, model, tokenizer, layer_idx):\n",
    "    '''Get mean-pooled embedding from a specific layer.'''\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "        hidden_states = outputs.hidden_states\n",
    "    \n",
    "    layer_hidden = hidden_states[layer_idx]\n",
    "    embedding = layer_hidden[0, 1:, :].mean(dim=0).cpu().numpy().astype(np.float32)\n",
    "    \n",
    "    return embedding\n",
    "\n",
    "\n",
    "def get_output_preference(text_a, text_b, model, tokenizer):\n",
    "    '''Calculate output preference as NLL(B) - NLL(A).'''\n",
    "    def get_nll(text):\n",
    "        inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512).to(model.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, labels=inputs['input_ids'])\n",
    "            return outputs.loss.item()\n",
    "    \n",
    "    return get_nll(text_b) - get_nll(text_a)\n",
    "\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b) + 1e-10))\n",
    "\n",
    "\n",
    "def uniformity_score(embeddings):\n",
    "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    normalized = embeddings / (norms + 1e-10)\n",
    "    kernel = normalized @ normalized.T\n",
    "    n = kernel.shape[0]\n",
    "    idx = np.triu_indices(n, k=1)\n",
    "    return float(np.mean(kernel[idx]))\n",
    "\n",
    "\n",
    "def bootstrap_correlation(x, y, n_bootstrap=10000, ci_level=0.95):\n",
    "    n = len(x)\n",
    "    r_observed, p_value = stats.pearsonr(x, y)\n",
    "    \n",
    "    bootstrap_rs = []\n",
    "    for _ in range(n_bootstrap):\n",
    "        idx = np.random.choice(n, size=n, replace=True)\n",
    "        x_boot, y_boot = x[idx], y[idx]\n",
    "        if np.std(x_boot) > 0 and np.std(y_boot) > 0:\n",
    "            r_boot, _ = stats.pearsonr(x_boot, y_boot)\n",
    "            bootstrap_rs.append(r_boot)\n",
    "    \n",
    "    bootstrap_rs = np.array(bootstrap_rs)\n",
    "    alpha = 1 - ci_level\n",
    "    ci_lower = np.percentile(bootstrap_rs, alpha/2 * 100)\n",
    "    ci_upper = np.percentile(bootstrap_rs, (1 - alpha/2) * 100)\n",
    "    \n",
    "    return float(r_observed), float(ci_lower), float(ci_upper), float(p_value)\n",
    "\n",
    "\n",
    "print('Core functions defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Collect All Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_LAYERS = model.config.num_hidden_layers\n",
    "LAYERS_TO_TEST = list(range(0, N_LAYERS + 1, 4))\n",
    "if N_LAYERS not in LAYERS_TO_TEST:\n",
    "    LAYERS_TO_TEST.append(N_LAYERS)\n",
    "\n",
    "print(f'Testing layers: {LAYERS_TO_TEST}')\n",
    "print(f'Collecting embeddings for {len(ALL_PAIRS)} pairs...')\n",
    "print(f'Estimated time: ~1-2 hours on A100')\n",
    "\n",
    "pair_data = []\n",
    "start_time = datetime.now()\n",
    "\n",
    "for i, pair in enumerate(ALL_PAIRS):\n",
    "    if (i + 1) % 25 == 0:\n",
    "        elapsed = (datetime.now() - start_time).total_seconds() / 60\n",
    "        rate = (i + 1) / elapsed if elapsed > 0 else 0\n",
    "        eta = (len(ALL_PAIRS) - i - 1) / rate if rate > 0 else 0\n",
    "        print(f'  [{i+1:03d}/{len(ALL_PAIRS)}] - {elapsed:.1f} min elapsed, ~{eta:.1f} min remaining')\n",
    "    \n",
    "    stmt_a = pair['stmt_a']\n",
    "    stmt_b = pair['stmt_b']\n",
    "    \n",
    "    pref = get_output_preference(stmt_a, stmt_b, model, tokenizer)\n",
    "    \n",
    "    layer_embeddings = {}\n",
    "    for layer_idx in LAYERS_TO_TEST:\n",
    "        emb_a = get_layer_embedding(stmt_a, model, tokenizer, layer_idx)\n",
    "        emb_b = get_layer_embedding(stmt_b, model, tokenizer, layer_idx)\n",
    "        layer_embeddings[layer_idx] = {'emb_a': emb_a, 'emb_b': emb_b}\n",
    "    \n",
    "    pair_data.append({\n",
    "        'pref': pref,\n",
    "        'category': pair['category'],\n",
    "        'layer_embeddings': layer_embeddings\n",
    "    })\n",
    "\n",
    "total_time = (datetime.now() - start_time).total_seconds() / 60\n",
    "print(f'Done! Collected {len(pair_data)} pairs in {total_time:.1f} minutes.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Category-Level UA Analysis (n=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 80)\n",
    "print(' CATEGORY-LEVEL UA ANALYSIS (n=6)')\n",
    "print('=' * 80)\n",
    "\n",
    "category_results = {}\n",
    "\n",
    "for layer_idx in LAYERS_TO_TEST:\n",
    "    category_uas = []\n",
    "    category_prefs = []\n",
    "    \n",
    "    for cat_name in DATASET.keys():\n",
    "        cat_pairs = [p for p in pair_data if p['category'] == cat_name]\n",
    "        \n",
    "        embs_a = np.array([p['layer_embeddings'][layer_idx]['emb_a'] for p in cat_pairs])\n",
    "        embs_b = np.array([p['layer_embeddings'][layer_idx]['emb_b'] for p in cat_pairs])\n",
    "        prefs = np.array([p['pref'] for p in cat_pairs])\n",
    "        \n",
    "        u_a = uniformity_score(embs_a)\n",
    "        u_b = uniformity_score(embs_b)\n",
    "        ua = u_a - u_b\n",
    "        \n",
    "        category_uas.append(ua)\n",
    "        category_prefs.append(np.mean(prefs))\n",
    "    \n",
    "    r, p = stats.pearsonr(category_uas, category_prefs)\n",
    "    \n",
    "    category_results[layer_idx] = {\n",
    "        'r': float(r),\n",
    "        'p_value': float(p)\n",
    "    }\n",
    "    \n",
    "    sig = '*' if p < 0.05 else ''\n",
    "    print(f'Layer {layer_idx:2d}: r = {r:+.3f} (p = {p:.4f}) {sig}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Pair-Level Metrics Analysis (n=230)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pair_metrics(pair_data, layer_idx):\n",
    "    n_pairs = len(pair_data)\n",
    "    \n",
    "    all_embs_a = np.array([p['layer_embeddings'][layer_idx]['emb_a'] for p in pair_data])\n",
    "    all_embs_b = np.array([p['layer_embeddings'][layer_idx]['emb_b'] for p in pair_data])\n",
    "    \n",
    "    centroid_a = all_embs_a.mean(axis=0)\n",
    "    centroid_b = all_embs_b.mean(axis=0)\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    # Centroid Asymmetry\n",
    "    centroid_dist_a = np.array([cosine_similarity(emb, centroid_a) for emb in all_embs_a])\n",
    "    centroid_dist_b = np.array([cosine_similarity(emb, centroid_b) for emb in all_embs_b])\n",
    "    metrics['centroid_asymmetry'] = centroid_dist_a - centroid_dist_b\n",
    "    \n",
    "    # Cross-Centroid\n",
    "    cross_dist_a = np.array([cosine_similarity(emb, centroid_b) for emb in all_embs_a])\n",
    "    cross_dist_b = np.array([cosine_similarity(emb, centroid_a) for emb in all_embs_b])\n",
    "    metrics['cross_centroid'] = cross_dist_a - cross_dist_b\n",
    "    \n",
    "    # Within-Pair Similarity\n",
    "    within_pair_sim = np.array([cosine_similarity(all_embs_a[i], all_embs_b[i]) for i in range(n_pairs)])\n",
    "    metrics['within_pair_sim'] = within_pair_sim\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_prefs = np.array([p['pref'] for p in pair_data])\n",
    "\n",
    "print('=' * 80)\n",
    "print(f' PAIR-LEVEL ANALYSIS (n={len(pair_data)})')\n",
    "print('=' * 80)\n",
    "\n",
    "pair_results = {}\n",
    "\n",
    "for layer_idx in LAYERS_TO_TEST:\n",
    "    print(f'\\n--- Layer {layer_idx} ---')\n",
    "    \n",
    "    metrics = compute_pair_metrics(pair_data, layer_idx)\n",
    "    layer_results = {}\n",
    "    \n",
    "    for metric_name, metric_values in metrics.items():\n",
    "        r, ci_lower, ci_upper, p = bootstrap_correlation(metric_values, all_prefs, N_BOOTSTRAP, CI_LEVEL)\n",
    "        \n",
    "        includes_zero = ci_lower <= 0 <= ci_upper\n",
    "        sig = '' if includes_zero else '***'\n",
    "        \n",
    "        layer_results[metric_name] = {\n",
    "            'r': r, 'ci_lower': ci_lower, 'ci_upper': ci_upper,\n",
    "            'p_value': p, 'includes_zero': includes_zero\n",
    "        }\n",
    "        \n",
    "        print(f'  {metric_name:<20} r={r:+.3f}  CI=[{ci_lower:+.3f}, {ci_upper:+.3f}] {sig}')\n",
    "    \n",
    "    pair_results[layer_idx] = layer_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Category-level UA\n",
    "ax1 = axes[0, 0]\n",
    "cat_rs = [category_results[l]['r'] for l in LAYERS_TO_TEST]\n",
    "ax1.plot(LAYERS_TO_TEST, cat_rs, 'o-', linewidth=2, markersize=8, color='blue')\n",
    "ax1.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax1.set_xlabel('Layer')\n",
    "ax1.set_ylabel('r(UA, Output)')\n",
    "ax1.set_title('Category-Level UA (n=6)', fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plots 2-4: Pair-level metrics\n",
    "metric_names = ['centroid_asymmetry', 'cross_centroid', 'within_pair_sim']\n",
    "plot_axes = [axes[0, 1], axes[1, 0], axes[1, 1]]\n",
    "\n",
    "for ax, metric_name in zip(plot_axes, metric_names):\n",
    "    rs = [pair_results[l][metric_name]['r'] for l in LAYERS_TO_TEST]\n",
    "    ci_lowers = [pair_results[l][metric_name]['ci_lower'] for l in LAYERS_TO_TEST]\n",
    "    ci_uppers = [pair_results[l][metric_name]['ci_upper'] for l in LAYERS_TO_TEST]\n",
    "    \n",
    "    yerr_lower = [r - ci_l for r, ci_l in zip(rs, ci_lowers)]\n",
    "    yerr_upper = [ci_u - r for r, ci_u in zip(rs, ci_uppers)]\n",
    "    \n",
    "    ax.errorbar(LAYERS_TO_TEST, rs, yerr=[yerr_lower, yerr_upper],\n",
    "                fmt='o-', capsize=5, capthick=2, markersize=8, color='blue', alpha=0.7)\n",
    "    \n",
    "    for l, r in zip(LAYERS_TO_TEST, rs):\n",
    "        if not pair_results[l][metric_name]['includes_zero']:\n",
    "            ax.scatter([l], [r], color='red', s=150, zorder=5, marker='*')\n",
    "    \n",
    "    ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax.set_xlabel('Layer')\n",
    "    ax.set_ylabel('r')\n",
    "    ax.set_title(f'{metric_name} (n=230)', fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'{MODEL_DISPLAY}: Layer-wise Correlation Analysis', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('llama3_layer_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('Plot saved to: llama3_layer_analysis.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Comparison with Pythia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pythia-6.9B reference values\n",
    "PYTHIA_CENTROID = {\n",
    "    0: +0.46, 4: +0.41, 8: +0.46, 12: +0.41, 16: +0.35,\n",
    "    20: +0.32, 24: +0.21, 28: +0.14, 32: -0.34\n",
    "}\n",
    "\n",
    "print('=' * 80)\n",
    "print(' CROSS-MODEL COMPARISON: Llama-3-8B vs Pythia-6.9B')\n",
    "print('=' * 80)\n",
    "\n",
    "print(f'\\nLayer    Pythia-6.9B    Llama-3-8B    Same Sign?')\n",
    "print('-' * 50)\n",
    "\n",
    "same_sign_count = 0\n",
    "for layer_idx in LAYERS_TO_TEST:\n",
    "    pythia_r = PYTHIA_CENTROID.get(layer_idx, None)\n",
    "    llama_r = pair_results[layer_idx]['centroid_asymmetry']['r']\n",
    "    \n",
    "    if pythia_r is not None:\n",
    "        same_sign = (pythia_r * llama_r) > 0\n",
    "        if same_sign:\n",
    "            same_sign_count += 1\n",
    "        sign_str = 'YES' if same_sign else 'NO'\n",
    "        print(f'Layer {layer_idx:<3} {pythia_r:+.3f}          {llama_r:+.3f}          {sign_str}')\n",
    "\n",
    "print(f'\\nSame sign: {same_sign_count}/{len(PYTHIA_CENTROID)} layers')\n",
    "\n",
    "# Phase structure analysis\n",
    "print('\\n--- Phase Structure Analysis ---')\n",
    "early_layers = [0, 4, 8]\n",
    "mid_layers = [12, 16, 20]\n",
    "late_layers = [24, 28, 32]\n",
    "\n",
    "early_mean = np.mean([pair_results[l]['centroid_asymmetry']['r'] for l in early_layers])\n",
    "mid_mean = np.mean([pair_results[l]['centroid_asymmetry']['r'] for l in mid_layers])\n",
    "late_mean = np.mean([pair_results[l]['centroid_asymmetry']['r'] for l in late_layers])\n",
    "\n",
    "print(f'Early layers (0-8):   mean r = {early_mean:+.3f}')\n",
    "print(f'Mid layers (12-20):   mean r = {mid_mean:+.3f}')\n",
    "print(f'Late layers (24-32):  mean r = {late_mean:+.3f}')\n",
    "\n",
    "# Determine pattern\n",
    "if early_mean > 0 and mid_mean > 0 and late_mean < 0:\n",
    "    pattern = 'MATCHES PYTHIA: positive early/mid, negative late'\n",
    "elif early_mean > 0 and late_mean < 0:\n",
    "    pattern = 'PARTIAL MATCH: positive early, negative late'\n",
    "elif late_mean < 0:\n",
    "    pattern = 'PARTIAL: late inversion confirmed'\n",
    "else:\n",
    "    pattern = 'DIFFERENT PATTERN'\n",
    "\n",
    "print(f'\\n>>> {pattern} <<<')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'model': MODEL_NAME,\n",
    "    'model_display': MODEL_DISPLAY,\n",
    "    'n_layers': N_LAYERS,\n",
    "    'layers_tested': LAYERS_TO_TEST,\n",
    "    'n_pairs': len(pair_data),\n",
    "    'n_bootstrap': N_BOOTSTRAP,\n",
    "    'category_level_results': {str(k): v for k, v in category_results.items()},\n",
    "    'pair_level_results': {\n",
    "        str(k): {m: dict(v) for m, v in layer_res.items()}\n",
    "        for k, layer_res in pair_results.items()\n",
    "    },\n",
    "    'phase_structure': {\n",
    "        'early_mean': float(early_mean),\n",
    "        'mid_mean': float(mid_mean),\n",
    "        'late_mean': float(late_mean),\n",
    "        'pattern': pattern\n",
    "    },\n",
    "    'pythia_comparison': {\n",
    "        'same_sign_layers': same_sign_count,\n",
    "        'total_compared': len(PYTHIA_CENTROID)\n",
    "    }\n",
    "}\n",
    "\n",
    "output_file = f'llama3_cross_validation_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.json'\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(save_data, f, indent=2)\n",
    "\n",
    "print(f'Results saved to: {output_file}')\n",
    "\n",
    "from google.colab import files\n",
    "files.download(output_file)\n",
    "files.download('llama3_layer_analysis.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('#' * 80)\n",
    "print(f'# CROSS-MODEL VALIDATION SUMMARY: {MODEL_DISPLAY}')\n",
    "print('#' * 80)\n",
    "\n",
    "print(f'''\n",
    "Pattern: {pattern}\n",
    "\n",
    "Phase Structure (centroid_asymmetry):\n",
    "  Early (0-8):   {early_mean:+.3f}\n",
    "  Mid (12-20):   {mid_mean:+.3f}\n",
    "  Late (24-32):  {late_mean:+.3f}\n",
    "\n",
    "Pythia Comparison: {same_sign_count}/{len(PYTHIA_CENTROID)} layers same sign\n",
    "\n",
    "Generated: {datetime.now().isoformat()}\n",
    "''')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}