% References for Paper #2: Phase-Structured Embedding-Output Dynamics

@article{delia2025ua,
  title={Uniformity Asymmetry: An Exploratory Metric for Detecting Representational Preferences in {LLM} Embeddings},
  author={D'Elia, Davide},
  journal={Zenodo preprint doi:10.5281/zenodo.18110161},
  year={2025}
}

@article{belinkov2022probing,
  title={Probing Classifiers: Promises, Shortcomings, and Advances},
  author={Belinkov, Yonatan},
  journal={Computational Linguistics},
  volume={48},
  number={1},
  pages={207--219},
  year={2022},
  publisher={MIT Press}
}

@article{noroozizadeh2025geometric,
  title={Deep sequence models tend to memorize geometrically; it is unclear why},
  author={Noroozizadeh, Shahriar and Nagarajan, Vaishnavh and Rosenfeld, Elan and Kumar, Sanjiv},
  journal={arXiv preprint arXiv:2510.26745},
  year={2025}
}

@article{ganesh2025narrative,
  title={Whose Narrative is it Anyway? {A} {KV} Cache Manipulation Attack},
  author={Ganesh, Mukkesh and Iyer, Kaushik and Ananthan, Arun Baalaaji Sankar},
  journal={arXiv preprint arXiv:2511.12752},
  year={2025}
}

@article{biderman2023pythia,
  title={Pythia: A suite for analyzing large language models across training and scaling},
  author={Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin Gregory and others},
  journal={arXiv preprint arXiv:2304.01373},
  year={2023}
}

@article{touvron2023llama,
  title={Llama 2: Open Foundation and Fine-Tuned Chat Models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{team2024gemma,
  title={Gemma: Open models based on Gemini research and technology},
  author={Team, Gemma and Mesnard, Thomas and Hardin, Cassidy and others},
  journal={arXiv preprint arXiv:2403.08295},
  year={2024}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{elhage2021mathematical,
  title={A mathematical framework for transformer circuits},
  author={Elhage, Nelson and Nanda, Neel and Olsson, Catherine and others},
  journal={Transformer Circuits Thread},
  year={2021}
}

@article{geva2023dissecting,
  title={Dissecting recall of factual associations in auto-regressive language models},
  author={Geva, Mor and Bastings, Jasmijn and Filippova, Katja and Globerson, Amir},
  journal={arXiv preprint arXiv:2304.14767},
  year={2023}
}

@inproceedings{tenney2019bert,
  title={BERT Rediscovers the Classical NLP Pipeline},
  author={Tenney, Ian and Das, Dipanjan and Pavlick, Ellie},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={4593--4601},
  year={2019}
}

@article{huh2024platonic,
  title={The Platonic Representation Hypothesis},
  author={Huh, Minyoung and Cheung, Brian and Wang, Tongzhou and Isola, Phillip},
  journal={arXiv preprint arXiv:2405.07987},
  year={2024}
}

@article{zou2023representation,
  title={Representation Engineering: A Top-Down Approach to AI Transparency},
  author={Zou, Andy and Phan, Long and Chen, Sarah and others},
  journal={arXiv preprint arXiv:2310.01405},
  year={2023}
}

