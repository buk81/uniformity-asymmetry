{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# E11: Territorial Collapse - Yi-1.5 (2nd MHA Family)\n", "\n", "**Paper 4: Behavioral Sink Dynamics**\n", "\n", "## Purpose\n", "\n", "This notebook tests Territorial Collapse on **Yi-1.5-9B** (01.AI) to strengthen claim A1:\n", "\n", "> \"Territorial collapse is architecture \u00d7 alignment dependent: MHA responds to alignment method (DPO/SFT protect, RLHF-only collapses), GQA shows structural collapse, MQA is pre-collapsed.\"\n", "\n", "## Model Pair (M06)\n", "\n", "| Role | Model | Notes |\n", "|------|-------|-------|\n", "| Base | 01-ai/Yi-1.5-9B | Pure MHA, Chinese vendor |\n", "| Instruct | 01-ai/Yi-1.5-9B-Chat | RLHF aligned |\n", "\n", "## E12-P Result: C_DELAYED\n", "\n", "Yi-1.5 **COLLAPSED** under corporate pressure (unlike Qwen2).\n", "This tests whether structural collapse correlates with behavioral vulnerability.\n", "\n", "## Cross-Family Comparison\n", "\n", "| MHA Model | Vendor | E12-P | E11 Expected |\n", "|-----------|--------|-------|-------------|\n", "| Mistral | Mistral AI | C_DELAYED | SI \u2191 (done) |\n", "| Pythia | EleutherAI | pending | SI \u2191 (expected) |\n", "| **Yi-1.5** | **01.AI** | **C_DELAYED** | **SI \u2191 ?** |\n", "\n", "---"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Cell 1: Setup\n", "!pip install -q transformers torch accelerate bitsandbytes scipy matplotlib seaborn huggingface_hub\n", "\n", "import torch\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "from transformers import AutoModelForCausalLM, AutoTokenizer\n", "from scipy.stats import entropy as scipy_entropy\n", "import json\n", "import hashlib\n", "import warnings\n", "warnings.filterwarnings('ignore')\n", "\n", "import os\n", "from pathlib import Path\n", "from datetime import datetime\n", "\n", "# ============ E11-v3 METHODOLOGY STANDARD ============\n", "SEEDS = [42, 123, 456]  # 3-seed averaging\n", "DTYPE = torch.bfloat16  # Standardized precision\n", "EXPECTED_MD5 = \"715065bab181f46bf12ed471951141e2\"  # Standard-10 v3\n", "USE_CHAT_TEMPLATE = True  # Instruct models use chat template\n", "\n", "def verify_prompts(prompts):\n", "    \"\"\"Verify Standard-10 prompts via MD5.\"\"\"\n", "    combined = '|||'.join(prompts)  # Canonical delimiter for MD5\n", "    actual_md5 = hashlib.md5(combined.encode()).hexdigest()\n", "    verified = actual_md5 == EXPECTED_MD5\n", "    print(f\"  Prompt MD5: {actual_md5}\")\n", "    print(f\"  Expected:   {EXPECTED_MD5}\")\n", "    print(f\"  Verified:   {'\u2713' if verified else '\u2717 MISMATCH!'}\")\n", "    return verified, actual_md5\n", "\n", "os.environ['PYTHONHASHSEED'] = '42'\n", "torch.manual_seed(42)\n", "np.random.seed(42)\n", "\n", "TIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')\n", "Path('results').mkdir(parents=True, exist_ok=True)\n", "Path('figures').mkdir(parents=True, exist_ok=True)\n", "print(f\"Timestamp: {TIMESTAMP}\")\n", "print(f\"E11-v3 Standard: Seeds={SEEDS}, dtype={DTYPE}\")\n", "print(f\"CUDA available: {torch.cuda.is_available()}\")\n", "if torch.cuda.is_available():\n", "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Cell 2: Configuration\n\nPAIR = 'yi15'\nPAIR_ID = 'M06'\n\nTWIN_PAIRS = {\n    'yi15': {\n        'base': '01-ai/Yi-1.5-9B',\n        'instruct': '01-ai/Yi-1.5-9B-Chat',\n        'params': '9B',\n        'heads': 32,\n        'd_head': 128,\n        'rho': 0.25,\n        'arch': 'MHA',\n        'alignment': 'RLHF',\n        'vendor': '01.AI',\n        'e12p_result': 'C_DELAYED',\n        'note': '2nd MHA family + Chinese vendor that COLLAPSED'\n    }\n}\n\nMAX_LENGTH = 128  # E11-v3 Standard\n\n# ============ CANONICAL Standard-10 v3 Prompts ============\n# MD5: 715065bab181f46bf12ed471951141e2\nSTANDARD_PROMPTS = [\n    \"What is the capital of France and what is its population?\",\n    \"If all roses are flowers and some flowers fade quickly, can we conclude that some roses fade quickly? Explain step by step.\",\n    \"Calculate 47 multiplied by 23 and show your work.\",\n    \"Translate the following to German: 'The quick brown fox jumps over the lazy dog'.\",\n    \"Write a Python function that checks if a number is prime.\",\n    \"Summarize the main points: Machine learning is a subset of artificial intelligence that enables systems to learn from data. It uses algorithms to identify patterns and make decisions with minimal human intervention.\",\n    \"Statement A: 'All birds can fly.' Statement B: 'Penguins are birds that cannot fly.' Are these statements contradictory? Explain.\",\n    \"What are the safety considerations when using a kitchen knife?\",\n    \"Write a haiku about artificial intelligence.\",\n    \"Complete this sentence in a helpful way: 'The best approach to solving complex problems is'\",\n]\n\n# Verify prompts\nprint(\"Verifying Standard-10 prompts...\")\nPROMPTS_VERIFIED, ACTUAL_MD5 = verify_prompts(STANDARD_PROMPTS)\nif not PROMPTS_VERIFIED:\n    raise ValueError(\"PROMPT MISMATCH! Check Standard-10 v3 canonical prompts.\")\n\nprint(f\"\\nTesting: {PAIR} ({PAIR_ID})\")\nprint(f\"Arch: {TWIN_PAIRS[PAIR]['arch']}\")\nprint(f\"E12-P Result: {TWIN_PAIRS[PAIR]['e12p_result']}\")\nprint(f\"\\nE11-v3 Config: MAX_LENGTH={MAX_LENGTH}, dtype={DTYPE}, seeds={SEEDS}\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Cell 3: Metrics Functions (E11-v3 masked)\n", "\n", "def extract_head_activations(model, tokenizer, prompts, max_length=128, use_chat_template=False):\n", "    all_attention_patterns = []\n", "    all_attention_masks = []\n", "    for prompt in prompts:\n", "        formatted = prompt\n", "        if use_chat_template and hasattr(tokenizer, 'apply_chat_template'):\n", "            messages = [{\"role\": \"user\", \"content\": prompt}]\n", "            try:\n", "                formatted = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n", "            except Exception:\n", "                formatted = prompt\n", "\n", "        inputs = tokenizer(\n", "            formatted,\n", "            return_tensors='pt',\n", "            max_length=max_length,\n", "            truncation=True,\n", "            padding='max_length'\n", "        ).to(model.device)\n", "\n", "        attention_mask = inputs.get('attention_mask')\n", "\n", "        with torch.no_grad():\n", "            outputs = model(**inputs, output_attentions=True)\n", "        attn_stack = torch.stack([a.squeeze(0) for a in outputs.attentions], dim=0)\n", "        all_attention_patterns.append(attn_stack.cpu())\n", "        all_attention_masks.append(attention_mask.squeeze(0).cpu() if attention_mask is not None else None)\n", "    return {\n", "        'attention_patterns': all_attention_patterns,\n", "        'attention_masks': all_attention_masks,\n", "        'num_layers': len(outputs.attentions),\n", "        'num_heads': outputs.attentions[0].shape[1]\n", "    }\n", "\n", "\n", "def compute_head_entropy_profiles(attention_patterns, attention_masks=None):\n", "    num_prompts = len(attention_patterns)\n", "    num_layers = attention_patterns[0].shape[0]\n", "    num_heads = attention_patterns[0].shape[1]\n", "    all_entropies = np.zeros((num_prompts, num_layers, num_heads))\n", "    for p_idx, attn in enumerate(attention_patterns):\n", "        mask = None\n", "        if attention_masks is not None:\n", "            mask = attention_masks[p_idx]\n", "            if mask is not None:\n", "                mask = mask.bool()\n", "        for layer in range(num_layers):\n", "            for head in range(num_heads):\n", "                attn_matrix = attn[layer, head]\n", "                if mask is not None:\n", "                    valid_idx = mask.nonzero(as_tuple=False).squeeze(-1)\n", "                    if valid_idx.numel() > 1:\n", "                        attn_matrix = attn_matrix[valid_idx][:, valid_idx]\n", "                    else:\n", "                        all_entropies[p_idx, layer, head] = 0\n", "                        continue\n", "                attn_weights = attn_matrix.mean(dim=0).float().cpu().numpy()\n", "                denom = attn_weights.sum()\n", "                if denom <= 0:\n", "                    all_entropies[p_idx, layer, head] = 0\n", "                    continue\n", "                attn_weights = attn_weights / denom\n", "                attn_weights = attn_weights[attn_weights > 0]\n", "                if len(attn_weights) > 1:\n", "                    h = scipy_entropy(attn_weights, base=2)\n", "                    h_max = np.log2(len(attn_weights))\n", "                    h_norm = h / h_max if h_max > 0 else 0\n", "                else:\n", "                    h_norm = 0\n", "                all_entropies[p_idx, layer, head] = h_norm\n", "    return all_entropies.mean(axis=0)\n", "\n", "\n", "def compute_specialization_metrics(head_entropies):\n", "    num_layers, num_heads = head_entropies.shape\n", "    layer_variances = np.var(head_entropies, axis=1)\n", "    mean_variance = float(np.mean(layer_variances))\n", "    head_profiles = head_entropies.T\n", "    head_corr_matrix = np.corrcoef(head_profiles)\n", "    upper_tri = head_corr_matrix[np.triu_indices(num_heads, k=1)]\n", "    mean_head_correlation = float(np.nanmean(upper_tri))\n", "    specialization_index = 1.0 - mean_head_correlation\n", "    head_contributions = np.mean(head_entropies, axis=0)\n", "    head_contributions = head_contributions / head_contributions.sum()\n", "    h_contrib = scipy_entropy(head_contributions, base=2)\n", "    effective_heads = 2 ** h_contrib if h_contrib > 0 else 1.0\n", "    effective_ratio = effective_heads / num_heads\n", "    return {\n", "        'mean_head_variance': mean_variance,\n", "        'mean_head_correlation': mean_head_correlation,\n", "        'head_correlation_matrix': head_corr_matrix.tolist(),\n", "        'specialization_index': specialization_index,\n", "        'effective_heads': float(effective_heads),\n", "        'effective_ratio': float(effective_ratio),\n", "        'layer_variances': layer_variances.tolist(),\n", "        'num_layers': num_layers,\n", "        'num_heads': num_heads\n", "    }\n", "\n", "print(\"Metrics functions loaded.\")\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Cell 4: Load BASE Model - 3-Seed Averaging\n\npair_config = TWIN_PAIRS[PAIR]\nresults = {'pair': PAIR, 'pair_id': PAIR_ID, 'base': {}, 'instruct': {}, 'config': pair_config}\n\nprint(f\"\\n{'='*60}\")\nprint(f\"E11: {PAIR.upper()} ({PAIR_ID}) - {pair_config['arch']} - E11-v3\")\nprint(f\"{'='*60}\")\n\nprint(f\"\\n[1/4] Loading BASE: {pair_config['base']}\")\ntokenizer_base = AutoTokenizer.from_pretrained(pair_config['base'], trust_remote_code=True)\nmodel_base = AutoModelForCausalLM.from_pretrained(\n    pair_config['base'],\n    torch_dtype=DTYPE, device_map='auto',\n    trust_remote_code=True, attn_implementation=\"eager\"\n)\nmodel_base.eval()\nif tokenizer_base.pad_token is None:\n    tokenizer_base.pad_token = tokenizer_base.eos_token\n\nprint(f\"[2/4] Extracting BASE activations (3-seed averaging)...\")\n\n# 3-seed averaging for E11-v3\nbase_seed_results = []\nfor seed in SEEDS:\n    print(f\"  Seed {seed}...\")\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    \n    base_activations = extract_head_activations(model_base, tokenizer_base, STANDARD_PROMPTS, MAX_LENGTH, use_chat_template=USE_CHAT_TEMPLATE)\n    base_entropies = compute_head_entropy_profiles(base_activations['attention_patterns'], base_activations['attention_masks'])\n    base_metrics = compute_specialization_metrics(base_entropies)\n    base_seed_results.append({\n        'seed': seed,\n        'si': base_metrics['specialization_index'],\n        'corr': base_metrics['mean_head_correlation'],\n        'var': base_metrics['mean_head_variance']\n    })\n    print(f\"    SI={base_metrics['specialization_index']:.4f}\")\n\n# Average across seeds\navg_base_si = np.mean([r['si'] for r in base_seed_results])\nstd_base_si = np.std([r['si'] for r in base_seed_results])\n\nprint(f\"\\n  Layers: {base_activations['num_layers']}, Heads: {base_activations['num_heads']}\")\nprint(f\"  BASE SI: {avg_base_si:.4f} \u00b1 {std_base_si:.6f}\")\n\n# Use last run's full metrics but update SI with average\nresults['base']['specialization'] = base_metrics\nresults['base']['specialization']['specialization_index'] = avg_base_si\nresults['base']['specialization']['si_std'] = std_base_si\nresults['base']['seed_results'] = base_seed_results\nresults['base']['entropies'] = base_entropies.tolist()\n\ndel model_base\ntorch.cuda.empty_cache()\nprint(\"  [Memory cleared]\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Cell 5: Load INSTRUCT Model - 3-Seed Averaging\n\nprint(f\"\\n[3/4] Loading INSTRUCT: {pair_config['instruct']}\")\ntokenizer_inst = AutoTokenizer.from_pretrained(pair_config['instruct'], trust_remote_code=True)\nmodel_inst = AutoModelForCausalLM.from_pretrained(\n    pair_config['instruct'],\n    torch_dtype=DTYPE, device_map='auto',\n    trust_remote_code=True, attn_implementation=\"eager\"\n)\nmodel_inst.eval()\nif tokenizer_inst.pad_token is None:\n    tokenizer_inst.pad_token = tokenizer_inst.eos_token\n\nprint(f\"[4/4] Extracting INSTRUCT activations (3-seed averaging)...\")\n\n# 3-seed averaging for E11-v3\ninst_seed_results = []\nfor seed in SEEDS:\n    print(f\"  Seed {seed}...\")\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    \n    inst_activations = extract_head_activations(model_inst, tokenizer_inst, STANDARD_PROMPTS, MAX_LENGTH, use_chat_template=USE_CHAT_TEMPLATE)\n    inst_entropies = compute_head_entropy_profiles(inst_activations['attention_patterns'], inst_activations['attention_masks'])\n    inst_metrics = compute_specialization_metrics(inst_entropies)\n    inst_seed_results.append({\n        'seed': seed,\n        'si': inst_metrics['specialization_index'],\n        'corr': inst_metrics['mean_head_correlation'],\n        'var': inst_metrics['mean_head_variance']\n    })\n    print(f\"    SI={inst_metrics['specialization_index']:.4f}\")\n\n# Average across seeds\navg_inst_si = np.mean([r['si'] for r in inst_seed_results])\nstd_inst_si = np.std([r['si'] for r in inst_seed_results])\n\nprint(f\"\\n  INSTRUCT SI: {avg_inst_si:.4f} \u00b1 {std_inst_si:.6f}\")\n\n# Use last run's full metrics but update SI with average\nresults['instruct']['specialization'] = inst_metrics\nresults['instruct']['specialization']['specialization_index'] = avg_inst_si\nresults['instruct']['specialization']['si_std'] = std_inst_si\nresults['instruct']['seed_results'] = inst_seed_results\nresults['instruct']['entropies'] = inst_entropies.tolist()\n\ndel model_inst\ntorch.cuda.empty_cache()\nprint(\"  [Memory cleared]\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Cell 6: Analysis\n", "\n", "base_spec = results['base']['specialization']\n", "inst_spec = results['instruct']['specialization']\n", "\n", "base_si = base_spec['specialization_index']\n", "inst_si = inst_spec['specialization_index']\n", "delta_si = inst_si - base_si\n", "\n", "base_corr = base_spec['mean_head_correlation']\n", "inst_corr = inst_spec['mean_head_correlation']\n", "delta_corr = inst_corr - base_corr\n", "\n", "base_var = base_spec['mean_head_variance']\n", "inst_var = inst_spec['mean_head_variance']\n", "delta_var = inst_var - base_var\n", "\n", "print(f\"\\n{'='*70}\")\n", "print(f\"E11 RESULTS: {PAIR.upper()} ({PAIR_ID}) - {pair_config['arch']}\")\n", "print(f\"{'='*70}\")\n", "print(f\"\\n{'Metric':<30} {'BASE':>12} {'INSTRUCT':>12} {'Delta':>12}\")\n", "print(\"-\" * 70)\n", "print(f\"{'Specialization Index':<30} {base_si:>12.4f} {inst_si:>12.4f} {delta_si:>+12.4f}\")\n", "print(f\"{'Mean Head Correlation':<30} {base_corr:>12.4f} {inst_corr:>12.4f} {delta_corr:>+12.4f}\")\n", "print(f\"{'Mean Head Variance':<30} {base_var:>12.6f} {inst_var:>12.6f} {delta_var:>+12.6f}\")\n", "\n", "# Verdict\n", "collapse_1 = delta_si < 0\n", "collapse_2 = delta_corr > 0\n", "collapse_3 = delta_var < 0\n", "collapse_count = sum([collapse_1, collapse_2, collapse_3])\n", "\n", "if collapse_count >= 2:\n", "    verdict = \"A_CONFIRMED\"\n", "elif collapse_count == 1:\n", "    verdict = \"B_PARTIAL\"\n", "else:\n", "    verdict = \"C_REFUTED\"\n", "\n", "print(f\"\\n{'='*70}\")\n", "print(f\"VERDICT: {verdict}\")\n", "print(f\"{'='*70}\")\n", "\n", "if verdict == \"C_REFUTED\":\n", "    print(\"MHA architecture PRESERVES specialization under RLHF!\")\n", "    print(f\"\\nCross-family check:\")\n", "    print(f\"  Mistral (MHA): SI \u0394 = +0.0420 (INCREASES)\")\n", "    print(f\"  Yi-1.5  (MHA): SI \u0394 = {delta_si:+.4f}\")\n", "    if delta_si > 0:\n", "        print(f\"\\n>>> CONSISTENT! A1 claim STRENGTHENED.\")\n", "else:\n", "    print(\"Unexpected: MHA showing collapse pattern!\")\n", "\n", "results['verdict'] = {\n", "    'code': verdict,\n", "    'delta_si': delta_si,\n", "    'delta_corr': delta_corr,\n", "    'delta_var': delta_var\n", "}\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Cell 7: Visualization\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# Plot 1: SI Comparison\nax1 = axes[0]\nmodels = ['Base', 'Instruct']\nsi_vals = [base_si, inst_si]\ncolors = ['#2ecc71', '#e74c3c']\nbars = ax1.bar(models, si_vals, color=colors, alpha=0.8, edgecolor='black')\nax1.set_ylabel('Specialization Index')\nax1.set_title(f'{PAIR.upper()} ({pair_config[\"arch\"]}): SI\\n\u0394 = {delta_si:+.4f}')\nax1.set_ylim(0, 1)\n\n# Plot 2: Layer-wise Variance\nax2 = axes[1]\nlayers = range(len(base_spec['layer_variances']))\nax2.plot(layers, base_spec['layer_variances'], 'o-', color='#2ecc71', label='Base')\nax2.plot(layers, inst_spec['layer_variances'], 's-', color='#e74c3c', label='Instruct')\nax2.set_xlabel('Layer')\nax2.set_ylabel('Head Variance')\nax2.set_title(f'{PAIR.upper()}: Layer-wise Variance')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\n# Plot 3: Correlation Heatmap Diff\nax3 = axes[2]\nbase_corr_mat = np.array(base_spec['head_correlation_matrix'])\ninst_corr_mat = np.array(inst_spec['head_correlation_matrix'])\ndiff_mat = inst_corr_mat - base_corr_mat\nsns.heatmap(diff_mat, cmap='RdBu_r', center=0, ax=ax3, cbar_kws={'label': '\u0394 Correlation'})\nax3.set_title(f'{PAIR.upper()}: Correlation Change\\n(Instruct - Base)')\n\nplt.tight_layout()\nfig_path = f'figures/E11_{PAIR}_territorial_{TIMESTAMP}.png'\nplt.savefig(fig_path, dpi=150, bbox_inches='tight')\nplt.show()\nprint(f\"Saved: {fig_path}\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Cell 8: Save Results\n", "\n", "def convert_to_native(obj):\n", "    if isinstance(obj, dict):\n", "        return {k: convert_to_native(v) for k, v in obj.items()}\n", "    elif isinstance(obj, list):\n", "        return [convert_to_native(v) for v in obj]\n", "    elif isinstance(obj, (np.bool_, np.integer)):\n", "        return int(obj)\n", "    elif isinstance(obj, np.floating):\n", "        return float(obj)\n", "    elif isinstance(obj, np.ndarray):\n", "        return obj.tolist()\n", "    return obj\n", "\n", "output = {\n", "    'experiment': 'E11_Territorial_Collapse',\n", "    'timestamp': TIMESTAMP,\n", "    'pair': PAIR,\n", "    'pair_id': PAIR_ID,\n", "    'config': pair_config,\n", "    'purpose': '2nd MHA family for A1 claim robustness',\n", "    'e12p_result': pair_config['e12p_result'],\n", "    # E11-v3 Methodology Block\n", "    'methodology': {\n", "        'standard': 'E11-v3',\n", "        'seeds': SEEDS,\n", "        'max_length': MAX_LENGTH,\n", "        'dtype': str(DTYPE),\n", "        'prompt_md5': ACTUAL_MD5,\n", "        'prompt_md5_verified': PROMPTS_VERIFIED,\n", "        'use_chat_template': USE_CHAT_TEMPLATE,\n", "        'attention_masked': True,\n", "        'num_prompts': len(STANDARD_PROMPTS),\n", "        'prompt_set': 'Standard-10 v3',\n", "        'quantization': 'NONE (Full Precision bfloat16)',\n", "        'use_chat_template': False\n", "    },\n", "    'results': convert_to_native(results)\n", "}\n", "\n", "filename = f'results/E11_{PAIR}_territorial_{TIMESTAMP}.json'\n", "with open(filename, 'w') as f:\n", "    json.dump(output, f, indent=2)\n", "print(f\"Saved: {filename}\")\n", "\n", "print(f\"\\n\ud83d\udccb E11-v3 Compliance:\")\n", "print(f\"   Seeds: {SEEDS} \u2713\")\n", "print(f\"   dtype: {DTYPE} \u2713\")\n", "print(f\"   MD5: {ACTUAL_MD5} {'\u2713' if PROMPTS_VERIFIED else '\u2717'}\")\n", "print(f\"   MAX_LENGTH: {MAX_LENGTH} \u2713\")\n", "\n", "try:\n", "    from google.colab import files\n", "    files.download(filename)\n", "    files.download(fig_path)\n", "except:\n", "    pass\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# ============================================================================\n", "# AUTO-DOWNLOAD RESULTS (Colab only)\n", "# ============================================================================\n", "import glob\n", "import shutil\n", "\n", "def auto_download_results():\n", "    try:\n", "        from google.colab import files\n", "    except ImportError:\n", "        print('Not in Colab - skipping auto-download')\n", "        return\n", "    \n", "    print('=' * 60)\n", "    print('AUTO-DOWNLOADING RESULTS...')\n", "    print('=' * 60)\n", "    \n", "    # Find all result files\n", "    json_files = glob.glob('results/*.json') + glob.glob('figures/*.json')\n", "    png_files = glob.glob('results/*.png') + glob.glob('figures/*.png')\n", "    all_files = json_files + png_files\n", "    \n", "    if not all_files:\n", "        print('WARNING: No result files found!')\n", "        return\n", "    \n", "    print(f'Found {len(all_files)} files')\n", "    \n", "    # Download as ZIP\n", "    import os\n", "    zip_name = f'E11_results_{os.path.basename(os.getcwd())}'\n", "    \n", "    # Create combined folder\n", "    os.makedirs('download_package', exist_ok=True)\n", "    for f in all_files:\n", "        shutil.copy(f, 'download_package/')\n", "    \n", "    shutil.make_archive(zip_name, 'zip', 'download_package')\n", "    print(f'Downloading: {zip_name}.zip')\n", "    files.download(f'{zip_name}.zip')\n", "    print('DOWNLOAD COMPLETE!')\n", "\n", "auto_download_results()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.10.0"}}, "nbformat": 4, "nbformat_minor": 4}