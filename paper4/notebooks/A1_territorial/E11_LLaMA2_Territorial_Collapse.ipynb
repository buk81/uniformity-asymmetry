{"cells": [{"cell_type": "markdown", "id": "cell-0", "metadata": {}, "source": ["# E11: Territorial Collapse - LLaMA-2 (RLHF+SFT Test)\n", "\n", "**Paper 4: Behavioral Sink Dynamics**\n", "\n", "## Purpose\n", "\n", "This notebook tests the **RLHF Hypothesis** with LLaMA-2:\n", "\n", "> \"Does RLHF cause territorial collapse, or does SFT provide protection?\"\n", "\n", "## Critical Test\n", "\n", "| Model | Arch | Alignment | Delta SI | Result |\n", "|-------|------|-----------|----------|--------|\n", "| Mistral | MHA | SFT+DPO | +0.0314 | SI\u2191 (protected) |\n", "| Yi-1.5 | MHA | RLHF-only | -0.1003 | SI\u2193 (collapsed) |\n", "| **LLaMA-2** | **MHA** | **RLHF+SFT** | **?** | **Critical test!** |\n", "\n", "## Hypothesis\n", "\n", "- If SI\u2193: RLHF causes collapse (SFT doesn't protect)\n", "- If SI\u2191: SFT provides partial protection (DPO > SFT > nothing)\n", "\n", "## Model Pair (M01)\n", "\n", "| Role | Model | Notes |\n", "|------|-------|-------|\n", "| Base | meta-llama/Llama-2-7b-hf | Pure MHA, gated |\n", "| Instruct | meta-llama/Llama-2-7b-chat-hf | RLHF+SFT aligned |\n", "\n", "---"]}, {"cell_type": "code", "execution_count": null, "id": "cell-1", "metadata": {}, "outputs": [], "source": ["# Cell 1: Setup\n", "!pip install -q transformers torch accelerate bitsandbytes scipy matplotlib seaborn\n", "\n", "import torch\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "from transformers import AutoModelForCausalLM, AutoTokenizer\n", "from scipy.stats import entropy as scipy_entropy\n", "import json\n", "import hashlib\n", "import warnings\n", "warnings.filterwarnings('ignore')\n", "\n", "import os\n", "from pathlib import Path\n", "from datetime import datetime\n", "\n", "# ============ E11-v3 METHODOLOGY STANDARD ============\n", "SEEDS = [42, 123, 456]  # 3-seed averaging\n", "DTYPE = torch.bfloat16  # Standardized precision\n", "EXPECTED_MD5 = \"715065bab181f46bf12ed471951141e2\"  # Standard-10 v3\n", "USE_CHAT_TEMPLATE = True  # Instruct models use chat template\n", "\n", "def verify_prompts(prompts):\n", "    \"\"\"Verify Standard-10 prompts via MD5.\"\"\"\n", "    combined = '|||'.join(prompts)  # Canonical delimiter for MD5\n", "    actual_md5 = hashlib.md5(combined.encode()).hexdigest()\n", "    verified = actual_md5 == EXPECTED_MD5\n", "    print(f\"  Prompt MD5: {actual_md5}\")\n", "    print(f\"  Expected:   {EXPECTED_MD5}\")\n", "    print(f\"  Verified:   {'\u2713' if verified else '\u2717 MISMATCH!'}\")\n", "    return verified, actual_md5\n", "\n", "os.environ['PYTHONHASHSEED'] = '42'\n", "torch.manual_seed(42)\n", "np.random.seed(42)\n", "\n", "TIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')\n", "Path('results').mkdir(parents=True, exist_ok=True)\n", "Path('figures').mkdir(parents=True, exist_ok=True)\n", "print(f\"Timestamp: {TIMESTAMP}\")\n", "print(f\"E11-v3 Standard: Seeds={SEEDS}, dtype={DTYPE}\")\n", "print(f\"CUDA available: {torch.cuda.is_available()}\")\n", "if torch.cuda.is_available():\n", "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n", "\n", "# HF Login for gated models (LLaMA/Yi) - REQUIRED!\n", "from huggingface_hub import login, HfFolder\n", "\n", "def get_hf_token():\n", "    token = None\n", "    try:\n", "        from google.colab import userdata\n", "        token = userdata.get('HF_TOKEN')\n", "    except Exception:\n", "        pass\n", "    if not token:\n", "        token = os.environ.get('HF_TOKEN') or os.environ.get('HUGGINGFACE_TOKEN') or os.environ.get('HUGGING_FACE_HUB_TOKEN')\n", "    if not token:\n", "        token = HfFolder.get_token()\n", "    return token\n", "\n", "HF_TOKEN = get_hf_token()\n", "if HF_TOKEN:\n", "    try:\n", "        login(token=HF_TOKEN)\n", "        print(\"HF Login: SUCCESS (required for gated models)\")\n", "    except Exception as e:\n", "        print(f\"HF Login failed: {e}\")\n", "else:\n", "    print(\"WARNING: No HF_TOKEN found! Gated models require authentication.\")\n", "    print(\"Colab: Runtime -> Secrets -> Add HF_TOKEN\")\n", "    print(\"Local: run `huggingface-cli login` or set HF_TOKEN env var\")\n", "\n", "TOKEN_KWARGS = {'token': HF_TOKEN} if HF_TOKEN else {}\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": null, "id": "cell-2", "metadata": {}, "outputs": [], "source": "# Cell 2: Configuration\n\nPAIR = 'llama2'\nPAIR_ID = 'M01'\n\nTWIN_PAIRS = {\n    'llama2': {\n        'base': 'meta-llama/Llama-2-7b-hf',\n        'instruct': 'meta-llama/Llama-2-7b-chat-hf',\n        'params': '7B',\n        'heads': 32,\n        'd_head': 128,\n        'rho': 0.25,\n        'arch': 'MHA',\n        'alignment': 'RLHF+SFT',  # KEY: Has both RLHF and SFT!\n        'vendor': 'Meta',\n        'e12p_result': 'C_DELAYED',\n        'note': 'Critical RLHF hypothesis test: Does SFT protect against collapse?'\n    }\n}\n\n# Reference results for comparison\nREFERENCE_RESULTS = {\n    'mistral': {'arch': 'MHA', 'alignment': 'SFT+DPO', 'delta_si': +0.0314, 'verdict': 'C_REFUTED'},\n    'yi15': {'arch': 'MHA', 'alignment': 'RLHF', 'delta_si': -0.1003, 'verdict': 'A_CONFIRMED'},\n    'llama31': {'arch': 'GQA', 'alignment': 'RLHF+DPO', 'delta_si': -0.4019, 'verdict': 'A_CONFIRMED'}\n}\n\nMAX_LENGTH = 128  # E11-v3 Standard\n\n# ============ CANONICAL Standard-10 v3 Prompts ============\n# MD5: 715065bab181f46bf12ed471951141e2\nSTANDARD_PROMPTS = [\n    \"What is the capital of France and what is its population?\",\n    \"If all roses are flowers and some flowers fade quickly, can we conclude that some roses fade quickly? Explain step by step.\",\n    \"Calculate 47 multiplied by 23 and show your work.\",\n    \"Translate the following to German: 'The quick brown fox jumps over the lazy dog'.\",\n    \"Write a Python function that checks if a number is prime.\",\n    \"Summarize the main points: Machine learning is a subset of artificial intelligence that enables systems to learn from data. It uses algorithms to identify patterns and make decisions with minimal human intervention.\",\n    \"Statement A: 'All birds can fly.' Statement B: 'Penguins are birds that cannot fly.' Are these statements contradictory? Explain.\",\n    \"What are the safety considerations when using a kitchen knife?\",\n    \"Write a haiku about artificial intelligence.\",\n    \"Complete this sentence in a helpful way: 'The best approach to solving complex problems is'\",\n]\n\n# Verify prompts\nprint(\"Verifying Standard-10 prompts...\")\nPROMPTS_VERIFIED, ACTUAL_MD5 = verify_prompts(STANDARD_PROMPTS)\nif not PROMPTS_VERIFIED:\n    raise ValueError(\"PROMPT MISMATCH! Check Standard-10 v3 canonical prompts.\")\n\nprint(f\"\\nTesting: {PAIR} ({PAIR_ID})\")\nprint(f\"Arch: {TWIN_PAIRS[PAIR]['arch']}\")\nprint(f\"Alignment: {TWIN_PAIRS[PAIR]['alignment']}\")\nprint(f\"\\nHypothesis: Does RLHF+SFT cause collapse like RLHF-only (Yi-1.5)?\")\nprint(f\"Or does SFT protect like DPO (Mistral)?\")\nprint(f\"\\nE11-v3 Config: MAX_LENGTH={MAX_LENGTH}, dtype={DTYPE}, seeds={SEEDS}\")"}, {"cell_type": "code", "execution_count": null, "id": "cell-3", "metadata": {}, "outputs": [], "source": ["# Cell 3: Metrics Functions (E11-v3 masked)\n", "\n", "def extract_head_activations(model, tokenizer, prompts, max_length=128, use_chat_template=False):\n", "    all_attention_patterns = []\n", "    all_attention_masks = []\n", "    for prompt in prompts:\n", "        formatted = prompt\n", "        if use_chat_template and hasattr(tokenizer, 'apply_chat_template'):\n", "            messages = [{\"role\": \"user\", \"content\": prompt}]\n", "            try:\n", "                formatted = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n", "            except Exception:\n", "                formatted = prompt\n", "\n", "        inputs = tokenizer(\n", "            formatted,\n", "            return_tensors='pt',\n", "            max_length=max_length,\n", "            truncation=True,\n", "            padding='max_length'\n", "        ).to(model.device)\n", "\n", "        attention_mask = inputs.get('attention_mask')\n", "\n", "        with torch.no_grad():\n", "            outputs = model(**inputs, output_attentions=True)\n", "        attn_stack = torch.stack([a.squeeze(0) for a in outputs.attentions], dim=0)\n", "        all_attention_patterns.append(attn_stack.cpu())\n", "        all_attention_masks.append(attention_mask.squeeze(0).cpu() if attention_mask is not None else None)\n", "    return {\n", "        'attention_patterns': all_attention_patterns,\n", "        'attention_masks': all_attention_masks,\n", "        'num_layers': len(outputs.attentions),\n", "        'num_heads': outputs.attentions[0].shape[1]\n", "    }\n", "\n", "\n", "def compute_head_entropy_profiles(attention_patterns, attention_masks=None):\n", "    num_prompts = len(attention_patterns)\n", "    num_layers = attention_patterns[0].shape[0]\n", "    num_heads = attention_patterns[0].shape[1]\n", "    all_entropies = np.zeros((num_prompts, num_layers, num_heads))\n", "    for p_idx, attn in enumerate(attention_patterns):\n", "        mask = None\n", "        if attention_masks is not None:\n", "            mask = attention_masks[p_idx]\n", "            if mask is not None:\n", "                mask = mask.bool()\n", "        for layer in range(num_layers):\n", "            for head in range(num_heads):\n", "                attn_matrix = attn[layer, head]\n", "                if mask is not None:\n", "                    valid_idx = mask.nonzero(as_tuple=False).squeeze(-1)\n", "                    if valid_idx.numel() > 1:\n", "                        attn_matrix = attn_matrix[valid_idx][:, valid_idx]\n", "                    else:\n", "                        all_entropies[p_idx, layer, head] = 0\n", "                        continue\n", "                attn_weights = attn_matrix.mean(dim=0).float().cpu().numpy()\n", "                denom = attn_weights.sum()\n", "                if denom <= 0:\n", "                    all_entropies[p_idx, layer, head] = 0\n", "                    continue\n", "                attn_weights = attn_weights / denom\n", "                attn_weights = attn_weights[attn_weights > 0]\n", "                if len(attn_weights) > 1:\n", "                    h = scipy_entropy(attn_weights, base=2)\n", "                    h_max = np.log2(len(attn_weights))\n", "                    h_norm = h / h_max if h_max > 0 else 0\n", "                else:\n", "                    h_norm = 0\n", "                all_entropies[p_idx, layer, head] = h_norm\n", "    return all_entropies.mean(axis=0)\n", "\n", "\n", "def compute_specialization_metrics(head_entropies):\n", "    num_layers, num_heads = head_entropies.shape\n", "    layer_variances = np.var(head_entropies, axis=1)\n", "    mean_variance = float(np.mean(layer_variances))\n", "    head_profiles = head_entropies.T\n", "    head_corr_matrix = np.corrcoef(head_profiles)\n", "    upper_tri = head_corr_matrix[np.triu_indices(num_heads, k=1)]\n", "    mean_head_correlation = float(np.nanmean(upper_tri))\n", "    specialization_index = 1.0 - mean_head_correlation\n", "    head_contributions = np.mean(head_entropies, axis=0)\n", "    head_contributions = head_contributions / head_contributions.sum()\n", "    h_contrib = scipy_entropy(head_contributions, base=2)\n", "    effective_heads = 2 ** h_contrib if h_contrib > 0 else 1.0\n", "    effective_ratio = effective_heads / num_heads\n", "    return {\n", "        'mean_head_variance': mean_variance,\n", "        'mean_head_correlation': mean_head_correlation,\n", "        'head_correlation_matrix': head_corr_matrix.tolist(),\n", "        'specialization_index': specialization_index,\n", "        'effective_heads': float(effective_heads),\n", "        'effective_ratio': float(effective_ratio),\n", "        'layer_variances': layer_variances.tolist(),\n", "        'num_layers': num_layers,\n", "        'num_heads': num_heads\n", "    }\n", "\n", "print(\"Metrics functions loaded.\")\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": null, "id": "cell-4", "metadata": {}, "outputs": [], "source": ["# Cell 4: Load BASE Model - 3-Seed Averaging\n", "\n", "pair_config = TWIN_PAIRS[PAIR]\n", "results = {'pair': PAIR, 'pair_id': PAIR_ID, 'base': {}, 'instruct': {}, 'config': pair_config}\n", "\n", "print(f\"\\n{'='*60}\")\n", "print(f\"E11: {PAIR.upper()} ({PAIR_ID}) - RLHF HYPOTHESIS TEST - E11-v3\")\n", "print(f\"{'='*60}\")\n", "\n", "print(f\"\\n[1/4] Loading BASE: {pair_config['base']}\")\n", "tokenizer_base = AutoTokenizer.from_pretrained(pair_config['base'], **TOKEN_KWARGS)\n", "model_base = AutoModelForCausalLM.from_pretrained(\n", "    pair_config['base'], torch_dtype=DTYPE, device_map='auto',\n", "    attn_implementation=\"eager\"  # Required for attention output\n", ")\n", "model_base.eval()\n", "if tokenizer_base.pad_token is None:\n", "    tokenizer_base.pad_token = tokenizer_base.eos_token\n", "\n", "print(f\"[2/4] Extracting BASE activations (3-seed averaging)...\")\n", "\n", "# 3-seed averaging for E11-v3\n", "base_seed_results = []\n", "for seed in SEEDS:\n", "    print(f\"  Seed {seed}...\")\n", "    torch.manual_seed(seed)\n", "    np.random.seed(seed)\n", "    \n", "    base_activations = extract_head_activations(model_base, tokenizer_base, STANDARD_PROMPTS, MAX_LENGTH, use_chat_template=USE_CHAT_TEMPLATE)\n", "    base_entropies = compute_head_entropy_profiles(base_activations['attention_patterns'], base_activations['attention_masks'])\n", "    base_metrics = compute_specialization_metrics(base_entropies)\n", "    base_seed_results.append({\n", "        'seed': seed,\n", "        'si': base_metrics['specialization_index'],\n", "        'corr': base_metrics['mean_head_correlation'],\n", "        'var': base_metrics['mean_head_variance']\n", "    })\n", "    print(f\"    SI={base_metrics['specialization_index']:.4f}\")\n", "\n", "# Average across seeds\n", "avg_base_si = np.mean([r['si'] for r in base_seed_results])\n", "std_base_si = np.std([r['si'] for r in base_seed_results])\n", "\n", "print(f\"\\n  Layers: {base_activations['num_layers']}, Heads: {base_activations['num_heads']}\")\n", "print(f\"  BASE SI: {avg_base_si:.4f} \u00b1 {std_base_si:.6f}\")\n", "\n", "# Use last run's full metrics but update SI with average\n", "results['base']['specialization'] = base_metrics\n", "results['base']['specialization']['specialization_index'] = avg_base_si\n", "results['base']['specialization']['si_std'] = std_base_si\n", "results['base']['seed_results'] = base_seed_results\n", "results['base']['entropies'] = base_entropies.tolist()\n", "\n", "del model_base\n", "torch.cuda.empty_cache()\n", "print(\"  [Memory cleared]\")\n", "\n"]}, {"cell_type": "code", "execution_count": null, "id": "cell-5", "metadata": {}, "outputs": [], "source": ["# Cell 5: Load INSTRUCT Model - 3-Seed Averaging\n", "\n", "print(f\"\\n[3/4] Loading INSTRUCT: {pair_config['instruct']}\")\n", "print(f\"       Alignment: {pair_config['alignment']}\")\n", "\n", "tokenizer_inst = AutoTokenizer.from_pretrained(pair_config['instruct'], **TOKEN_KWARGS)\n", "model_inst = AutoModelForCausalLM.from_pretrained(\n", "    pair_config['instruct'], torch_dtype=DTYPE, device_map='auto',\n", "    attn_implementation=\"eager\"\n", ")\n", "model_inst.eval()\n", "if tokenizer_inst.pad_token is None:\n", "    tokenizer_inst.pad_token = tokenizer_inst.eos_token\n", "\n", "print(f\"[4/4] Extracting INSTRUCT activations (3-seed averaging)...\")\n", "\n", "# 3-seed averaging for E11-v3\n", "inst_seed_results = []\n", "for seed in SEEDS:\n", "    print(f\"  Seed {seed}...\")\n", "    torch.manual_seed(seed)\n", "    np.random.seed(seed)\n", "    \n", "    inst_activations = extract_head_activations(model_inst, tokenizer_inst, STANDARD_PROMPTS, MAX_LENGTH, use_chat_template=USE_CHAT_TEMPLATE)\n", "    inst_entropies = compute_head_entropy_profiles(inst_activations['attention_patterns'], inst_activations['attention_masks'])\n", "    inst_metrics = compute_specialization_metrics(inst_entropies)\n", "    inst_seed_results.append({\n", "        'seed': seed,\n", "        'si': inst_metrics['specialization_index'],\n", "        'corr': inst_metrics['mean_head_correlation'],\n", "        'var': inst_metrics['mean_head_variance']\n", "    })\n", "    print(f\"    SI={inst_metrics['specialization_index']:.4f}\")\n", "\n", "# Average across seeds\n", "avg_inst_si = np.mean([r['si'] for r in inst_seed_results])\n", "std_inst_si = np.std([r['si'] for r in inst_seed_results])\n", "\n", "print(f\"\\n  INSTRUCT SI: {avg_inst_si:.4f} \u00b1 {std_inst_si:.6f}\")\n", "\n", "# Use last run's full metrics but update SI with average\n", "results['instruct']['specialization'] = inst_metrics\n", "results['instruct']['specialization']['specialization_index'] = avg_inst_si\n", "results['instruct']['specialization']['si_std'] = std_inst_si\n", "results['instruct']['seed_results'] = inst_seed_results\n", "results['instruct']['entropies'] = inst_entropies.tolist()\n", "\n", "del model_inst\n", "torch.cuda.empty_cache()\n", "print(\"  [Memory cleared]\")\n", "\n"]}, {"cell_type": "code", "execution_count": null, "id": "cell-6", "metadata": {}, "outputs": [], "source": ["# Cell 6: Analysis & RLHF Hypothesis Test\n", "\n", "base_spec = results['base']['specialization']\n", "inst_spec = results['instruct']['specialization']\n", "\n", "base_si = base_spec['specialization_index']\n", "inst_si = inst_spec['specialization_index']\n", "delta_si = inst_si - base_si\n", "\n", "base_corr = base_spec['mean_head_correlation']\n", "inst_corr = inst_spec['mean_head_correlation']\n", "delta_corr = inst_corr - base_corr\n", "\n", "base_var = base_spec['mean_head_variance']\n", "inst_var = inst_spec['mean_head_variance']\n", "delta_var = inst_var - base_var\n", "\n", "print(f\"\\n{'='*70}\")\n", "print(f\"E11 RESULTS: {PAIR.upper()} ({PAIR_ID}) - {pair_config['arch']} + {pair_config['alignment']}\")\n", "print(f\"{'='*70}\")\n", "print(f\"\\n{'Metric':<30} {'BASE':>12} {'INSTRUCT':>12} {'Delta':>12}\")\n", "print(\"-\" * 70)\n", "print(f\"{'Specialization Index':<30} {base_si:>12.4f} {inst_si:>12.4f} {delta_si:>+12.4f}\")\n", "print(f\"{'Mean Head Correlation':<30} {base_corr:>12.4f} {inst_corr:>12.4f} {delta_corr:>+12.4f}\")\n", "print(f\"{'Mean Head Variance':<30} {base_var:>12.6f} {inst_var:>12.6f} {delta_var:>+12.6f}\")\n", "\n", "# Verdict\n", "collapse_1 = delta_si < 0\n", "collapse_2 = delta_corr > 0\n", "collapse_3 = delta_var < 0\n", "collapse_count = sum([collapse_1, collapse_2, collapse_3])\n", "\n", "if collapse_count >= 2:\n", "    verdict = \"A_CONFIRMED\"\n", "elif collapse_count == 1:\n", "    verdict = \"B_PARTIAL\"\n", "else:\n", "    verdict = \"C_REFUTED\"\n", "\n", "print(f\"\\n{'='*70}\")\n", "print(f\"VERDICT: {verdict}\")\n", "print(f\"{'='*70}\")\n", "\n", "# RLHF Hypothesis Test\n", "print(f\"\\n{'='*70}\")\n", "print(\"RLHF HYPOTHESIS TEST\")\n", "print(f\"{'='*70}\")\n", "print(f\"\\n  Reference Results:\")\n", "print(f\"  {'Model':<12} {'Arch':<6} {'Alignment':<12} {'Delta SI':>10} {'Verdict':<12}\")\n", "print(f\"  {'-'*56}\")\n", "print(f\"  {'Mistral':<12} {'MHA':<6} {'SFT+DPO':<12} {'+0.0314':>10} {'C_REFUTED':<12}\")\n", "print(f\"  {'Yi-1.5':<12} {'MHA':<6} {'RLHF':<12} {'-0.1003':>10} {'A_CONFIRMED':<12}\")\n", "print(f\"  {'LLaMA-3.1':<12} {'GQA':<6} {'RLHF+DPO':<12} {'-0.4019':>10} {'A_CONFIRMED':<12}\")\n", "print(f\"  {'-'*56}\")\n", "print(f\"  {'LLaMA-2':<12} {'MHA':<6} {'RLHF+SFT':<12} {delta_si:>+10.4f} {verdict:<12} <<<\")\n", "\n", "print(f\"\\n  INTERPRETATION:\")\n", "if delta_si < 0:\n", "    print(f\"  >>> LLaMA-2 shows COLLAPSE (SI\u2193) like Yi-1.5!\")\n", "    print(f\"  >>> CONCLUSION: RLHF causes collapse, SFT does NOT protect.\")\n", "    print(f\"  >>> Only DPO provides protection against territorial collapse.\")\n", "elif delta_si > 0:\n", "    print(f\"  >>> LLaMA-2 shows DIVERSIFICATION (SI\u2191) like Mistral!\")\n", "    print(f\"  >>> CONCLUSION: SFT provides partial protection against collapse.\")\n", "    print(f\"  >>> Protection hierarchy: DPO > SFT > nothing\")\n", "else:\n", "    print(f\"  >>> LLaMA-2 shows NO CHANGE (SI\u22480)\")\n", "    print(f\"  >>> CONCLUSION: SFT neutralizes RLHF collapse effect.\")\n", "\n", "results['verdict'] = {\n", "    'code': verdict,\n", "    'delta_si': delta_si,\n", "    'delta_corr': delta_corr,\n", "    'delta_var': delta_var,\n", "    'rlhf_hypothesis': 'RLHF_CAUSES_COLLAPSE' if delta_si < 0 else 'SFT_PROTECTS'\n", "}"]}, {"cell_type": "code", "execution_count": null, "id": "cell-7", "metadata": {}, "outputs": [], "source": ["# Cell 7: Visualization\n", "\n", "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n", "\n", "# Plot 1: SI Comparison\n", "ax1 = axes[0]\n", "models = ['Base', 'Instruct']\n", "si_vals = [base_si, inst_si]\n", "colors = ['#2ecc71', '#e74c3c']\n", "bars = ax1.bar(models, si_vals, color=colors, alpha=0.8, edgecolor='black')\n", "ax1.set_ylabel('Specialization Index')\n", "ax1.set_title(f'{PAIR.upper()} ({pair_config[\"alignment\"]}): SI\\n\u0394 = {delta_si:+.4f}')\n", "ax1.set_ylim(0, 1)\n", "for bar, val in zip(bars, si_vals):\n", "    ax1.annotate(f'{val:.4f}', xy=(bar.get_x() + bar.get_width()/2, val),\n", "                 xytext=(0, 5), textcoords='offset points', ha='center', fontsize=11)\n", "\n", "# Plot 2: Layer-wise Variance\n", "ax2 = axes[1]\n", "layers = range(len(base_spec['layer_variances']))\n", "ax2.plot(layers, base_spec['layer_variances'], 'o-', color='#2ecc71', label='Base', linewidth=2, markersize=3)\n", "ax2.plot(layers, inst_spec['layer_variances'], 's-', color='#e74c3c', label='Instruct', linewidth=2, markersize=3)\n", "ax2.set_xlabel('Layer')\n", "ax2.set_ylabel('Head Variance')\n", "ax2.set_title(f'{PAIR.upper()}: Layer-wise Variance')\n", "ax2.legend()\n", "ax2.grid(True, alpha=0.3)\n", "\n", "# Plot 3: Correlation Heatmap Diff\n", "ax3 = axes[2]\n", "base_corr_mat = np.array(base_spec['head_correlation_matrix'])\n", "inst_corr_mat = np.array(inst_spec['head_correlation_matrix'])\n", "diff_mat = inst_corr_mat - base_corr_mat\n", "sns.heatmap(diff_mat, cmap='RdBu_r', center=0, ax=ax3, cbar_kws={'label': '\u0394 Correlation'})\n", "ax3.set_title(f'{PAIR.upper()}: Correlation Change\\n(Instruct - Base)')\n", "\n", "# Plot 4: RLHF Hypothesis Summary\n", "ax4 = axes[3]\n", "models_all = ['Mistral\\n(SFT+DPO)', 'Yi-1.5\\n(RLHF)', 'LLaMA-2\\n(RLHF+SFT)', 'LLaMA-3.1\\n(GQA+RLHF)']\n", "deltas = [0.0314, -0.1003, delta_si, -0.4019]\n", "colors_all = ['#2ecc71' if d > 0 else '#e74c3c' for d in deltas]\n", "bars = ax4.bar(models_all, deltas, color=colors_all, alpha=0.8, edgecolor='black')\n", "ax4.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n", "ax4.set_ylabel('Delta SI')\n", "ax4.set_title('RLHF Hypothesis: All MHA Models\\n(Green=Protected, Red=Collapsed)')\n", "for bar, val in zip(bars, deltas):\n", "    ypos = val + 0.02 if val > 0 else val - 0.04\n", "    ax4.annotate(f'{val:+.4f}', xy=(bar.get_x() + bar.get_width()/2, ypos),\n", "                 ha='center', fontsize=10, fontweight='bold')\n", "\n", "plt.tight_layout()\n", "fig_path = f'figures/E11_{PAIR}_territorial_{TIMESTAMP}.png'\n", "plt.savefig(fig_path, dpi=150, bbox_inches='tight')\n", "plt.show()\n", "print(f\"Saved: {fig_path}\")"]}, {"cell_type": "code", "execution_count": null, "id": "cell-8", "metadata": {}, "outputs": [], "source": ["# Cell 8: Save Results\n", "\n", "def convert_to_native(obj):\n", "    if isinstance(obj, dict):\n", "        return {k: convert_to_native(v) for k, v in obj.items()}\n", "    elif isinstance(obj, list):\n", "        return [convert_to_native(v) for v in obj]\n", "    elif isinstance(obj, (np.bool_, np.integer)):\n", "        return int(obj)\n", "    elif isinstance(obj, np.floating):\n", "        return float(obj)\n", "    elif isinstance(obj, np.ndarray):\n", "        return obj.tolist()\n", "    return obj\n", "\n", "output = {\n", "    'experiment': 'E11_Territorial_Collapse',\n", "    'timestamp': TIMESTAMP,\n", "    'pair': PAIR,\n", "    'pair_id': PAIR_ID,\n", "    'config': pair_config,\n", "    'purpose': 'RLHF Hypothesis Test: Does SFT protect against collapse?',\n", "    # E11-v3 Methodology Block\n", "    'methodology': {\n", "        'standard': 'E11-v3',\n", "        'seeds': SEEDS,\n", "        'max_length': MAX_LENGTH,\n", "        'dtype': str(DTYPE),\n", "        'prompt_md5': ACTUAL_MD5,\n", "        'prompt_md5_verified': PROMPTS_VERIFIED,\n", "        'use_chat_template': USE_CHAT_TEMPLATE,\n", "        'attention_masked': True,\n", "        'num_prompts': len(STANDARD_PROMPTS),\n", "        'prompt_set': 'Standard-10 v3',\n", "        'quantization': 'NONE (Full Precision bfloat16)',\n", "        'use_chat_template': False\n", "    },\n", "    'reference_results': REFERENCE_RESULTS,\n", "    'results': convert_to_native(results)\n", "}\n", "\n", "filename = f'results/E11_{PAIR}_territorial_{TIMESTAMP}.json'\n", "with open(filename, 'w') as f:\n", "    json.dump(output, f, indent=2)\n", "print(f\"Saved: {filename}\")\n", "\n", "print(f\"\\n\ud83d\udccb E11-v3 Compliance:\")\n", "print(f\"   Seeds: {SEEDS} \u2713\")\n", "print(f\"   dtype: {DTYPE} \u2713\")\n", "print(f\"   MD5: {ACTUAL_MD5} {'\u2713' if PROMPTS_VERIFIED else '\u2717'}\")\n", "print(f\"   MAX_LENGTH: {MAX_LENGTH} \u2713\")\n", "\n", "try:\n", "    from google.colab import files\n", "    files.download(filename)\n", "    files.download(fig_path)\n", "except:\n", "    pass\n"]}, {"cell_type": "markdown", "id": "cell-9", "metadata": {}, "source": ["---\n", "\n", "## Summary: RLHF Hypothesis Test\n", "\n", "### Question\n", "\n", "> Does RLHF cause territorial collapse, or does SFT provide protection?\n", "\n", "### Evidence Matrix\n", "\n", "| Model | Arch | Alignment | Delta SI | Verdict |\n", "|-------|------|-----------|----------|--------|\n", "| Mistral | MHA | SFT+DPO | +0.0314 | Protected |\n", "| Yi-1.5 | MHA | RLHF-only | -0.1003 | Collapsed |\n", "| LLaMA-3.1 | GQA | RLHF+DPO | -0.4019 | Collapsed (arch) |\n", "| **LLaMA-2** | **MHA** | **RLHF+SFT** | **?** | **This test** |\n", "\n", "### Possible Conclusions\n", "\n", "**If LLaMA-2 shows SI\u2193 (collapse):**\n", "- RLHF is the collapse driver\n", "- SFT does NOT protect\n", "- Only DPO provides protection\n", "- Protection hierarchy: DPO >> SFT \u2248 nothing\n", "\n", "**If LLaMA-2 shows SI\u2191 (diversification):**\n", "- SFT provides partial protection\n", "- Protection hierarchy: DPO > SFT > nothing\n", "- RLHF effect is modulated by additional training\n", "\n", "---\n", "\n", "*Paper 4: Behavioral Sink Dynamics*  \n", "*E11-LLaMA2: Critical RLHF Hypothesis Test*"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# ============================================================================\n", "# AUTO-DOWNLOAD RESULTS (Colab only)\n", "# ============================================================================\n", "import glob\n", "import shutil\n", "\n", "def auto_download_results():\n", "    try:\n", "        from google.colab import files\n", "    except ImportError:\n", "        print('Not in Colab - skipping auto-download')\n", "        return\n", "    \n", "    print('=' * 60)\n", "    print('AUTO-DOWNLOADING RESULTS...')\n", "    print('=' * 60)\n", "    \n", "    # Find all result files\n", "    json_files = glob.glob('results/*.json') + glob.glob('figures/*.json')\n", "    png_files = glob.glob('results/*.png') + glob.glob('figures/*.png')\n", "    all_files = json_files + png_files\n", "    \n", "    if not all_files:\n", "        print('WARNING: No result files found!')\n", "        return\n", "    \n", "    print(f'Found {len(all_files)} files')\n", "    \n", "    # Download as ZIP\n", "    import os\n", "    zip_name = f'E11_results_{os.path.basename(os.getcwd())}'\n", "    \n", "    # Create combined folder\n", "    os.makedirs('download_package', exist_ok=True)\n", "    for f in all_files:\n", "        shutil.copy(f, 'download_package/')\n", "    \n", "    shutil.make_archive(zip_name, 'zip', 'download_package')\n", "    print(f'Downloading: {zip_name}.zip')\n", "    files.download(f'{zip_name}.zip')\n", "    print('DOWNLOAD COMPLETE!')\n", "\n", "auto_download_results()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.10.0"}}, "nbformat": 4, "nbformat_minor": 5}