{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# E11: Territorial Collapse - Falcon (MQA Control)\n\n**Paper 4: Behavioral Sink Dynamics**\n\n## Purpose\n\nThis notebook tests Territorial Collapse on **Falcon-7B** (TII) as an **MQA Control**:\n\n> \"Does Multi-Query Attention (MQA) behave like MHA or GQA under alignment pressure?\"\n\n**IMPORTANT:** Falcon uses **MQA** (Multi-Query Attention), NOT MHA!\n- 71 Query Heads, but only **1 shared K/V Head**\n- This is a distinct architecture from both MHA and GQA\n\n## Model Pair (M08)\n\n| Role | Model | Notes |\n|------|-------|-------|\n| Base | tiiuae/falcon-7b | MQA (71 Q-heads, 1 KV-head) |\n| Instruct | tiiuae/falcon-7b-instruct | SFT-only (no RLHF!) |\n\n## Hypothesis\n\nMQA shares K/V across all heads â†’ structurally similar to extreme GQA (1 group).\n- If Falcon shows SIâ†“ (like GQA): MQA behaves like GQA\n- If Falcon shows SIâ†‘ (like MHA): Query diversity dominates\n- If Falcon shows no change: Shared K/V prevents collapse\n\n---"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 1: Setup\n!pip install -q transformers torch accelerate bitsandbytes scipy matplotlib seaborn\n\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom scipy.stats import entropy as scipy_entropy\nfrom scipy.stats import pearsonr, spearmanr\nfrom scipy.spatial.distance import pdist, squareform\nimport json\nimport hashlib\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfrom pathlib import Path\nfrom datetime import datetime\n\n# ============ E11-v3 METHODOLOGY STANDARD ============\nSEEDS = [42, 123, 456]  # 3-seed averaging\nDTYPE = torch.bfloat16  # Standardized precision\nEXPECTED_MD5 = \"715065bab181f46bf12ed471951141e2\"  # Standard-10 v3\n\ndef verify_prompts(prompts):\n    \"\"\"Verify Standard-10 prompts via MD5.\"\"\"\n    combined = '|||'.join(prompts)  # Canonical delimiter for MD5\n    actual_md5 = hashlib.md5(combined.encode()).hexdigest()\n    verified = actual_md5 == EXPECTED_MD5\n    print(f\"  Prompt MD5: {actual_md5}\")\n    print(f\"  Expected:   {EXPECTED_MD5}\")\n    print(f\"  Verified:   {'âœ“' if verified else 'âœ— MISMATCH!'}\")\n    return verified, actual_md5\n\n# Set initial seed\nos.environ['PYTHONHASHSEED'] = '42'\ntorch.manual_seed(42)\nnp.random.seed(42)\n\nTIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')\nPath('results').mkdir(parents=True, exist_ok=True)\nPath('figures').mkdir(parents=True, exist_ok=True)\nprint(f\"Timestamp: {TIMESTAMP}\")\nprint(f\"E11-v3 Standard: Seeds={SEEDS}, dtype={DTYPE}\")\n\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Configuration\n",
    "\n",
    "# M08 Falcon Pair - MQA CONTROL (NOT MHA!)\n",
    "TWIN_PAIRS = {\n",
    "    'falcon': {\n",
    "        'base': 'tiiuae/falcon-7b',\n",
    "        'instruct': 'tiiuae/falcon-7b-instruct',  # SFT-only, no RLHF!\n",
    "        'params': '7B',\n",
    "        'num_attention_heads': 71,  # Query heads\n",
    "        'num_kv_heads': 1,          # KEY: Only 1 KV head = MQA!\n",
    "        'd_head': 64,\n",
    "        'arch': 'MQA',  # Multi-Query Attention, NOT MHA!\n",
    "        'alignment': 'SFT-only',\n",
    "        'note': 'MQA Control - tests if shared KV behaves like GQA or MHA'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Select pair to test\n",
    "PAIR = 'falcon'  # M08 - MQA Control\n",
    "\n",
    "# Tokenization settings\n",
    "MAX_LENGTH = 128  # E11-v3 Standard\n",
    "USE_CHAT_TEMPLATE = False  # Falcon models are not chat templated\n",
    "\n",
    "# ============ CANONICAL Standard-10 v3 Prompts ============\n",
    "# MD5: 715065bab181f46bf12ed471951141e2\n",
    "STANDARD_PROMPTS = [\n",
    "    'What is the capital of France and what is its population?',\n",
    "    'If all roses are flowers and some flowers fade quickly, can we conclude that some roses fade quickly? Explain step by step.',\n",
    "    'Calculate 47 multiplied by 23 and show your work.',\n",
    "    \"Translate the following to German: 'The quick brown fox jumps over the lazy dog'.\",\n",
    "    'Write a Python function that checks if a number is prime.',\n",
    "    'Summarize the main points: Machine learning is a subset of artificial intelligence that enables systems to learn from data. It uses algorithms to identify patterns and make decisions with minimal human intervention.',\n",
    "    \"Statement A: 'All birds can fly.' Statement B: 'Penguins are birds that cannot fly.' Are these statements contradictory? Explain.\",\n",
    "    'What are the safety considerations when using a kitchen knife?',\n",
    "    'Write a haiku about artificial intelligence.',\n",
    "    \"Complete this sentence in a helpful way: 'The best approach to solving complex problems is'\",\n",
    "]\n",
    "\n",
    "# Verify prompts\n",
    "print(\"Verifying Standard-10 prompts...\")\n",
    "PROMPTS_VERIFIED, ACTUAL_MD5 = verify_prompts(STANDARD_PROMPTS)\n",
    "if not PROMPTS_VERIFIED:\n",
    "    raise ValueError(\"PROMPT MISMATCH! Check Standard-10 v3 canonical prompts.\")\n",
    "\n",
    "print(f\"\\nConfiguration loaded.\")\n",
    "print(f\"Testing pair: {PAIR}\")\n",
    "print(f\"Architecture: {TWIN_PAIRS[PAIR]['arch']}\")\n",
    "print(f\"  Query Heads: {TWIN_PAIRS[PAIR]['num_attention_heads']}\")\n",
    "print(f\"  KV Heads:    {TWIN_PAIRS[PAIR]['num_kv_heads']} (shared!)\")\n",
    "print(f\"\\nKEY: MQA shares 1 KV head across 71 query heads\")\n",
    "print(f\"     This is structurally similar to extreme GQA (1 group)\")\n",
    "print(f\"\\nE11-v3 Config: MAX_LENGTH={MAX_LENGTH}, dtype={DTYPE}, seeds={SEEDS}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 3: Head Specialization Metrics (E11-v3 masked)\n\ndef extract_head_activations(model, tokenizer, prompts, max_length=128, use_chat_template=False):\n    all_attention_patterns = []\n    all_attention_masks = []\n\n    for prompt in prompts:\n        formatted = prompt\n        if use_chat_template and hasattr(tokenizer, 'apply_chat_template'):\n            messages = [{\"role\": \"user\", \"content\": prompt}]\n            try:\n                formatted = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n            except Exception:\n                formatted = prompt\n\n        inputs = tokenizer(\n            formatted,\n            return_tensors='pt',\n            max_length=max_length,\n            truncation=True,\n            padding='max_length'\n        ).to(model.device)\n\n        attention_mask = inputs.get('attention_mask')\n\n        with torch.no_grad():\n            outputs = model(**inputs, output_attentions=True, output_hidden_states=True)\n\n        attn_stack = torch.stack([a.squeeze(0) for a in outputs.attentions], dim=0)\n        all_attention_patterns.append(attn_stack.cpu())\n        all_attention_masks.append(attention_mask.squeeze(0).cpu() if attention_mask is not None else None)\n\n    return {\n        'attention_patterns': all_attention_patterns,\n        'attention_masks': all_attention_masks,\n        'num_layers': len(outputs.attentions),\n        'num_heads': outputs.attentions[0].shape[1]\n    }\n\n\ndef compute_head_entropy_profiles(attention_patterns, attention_masks=None):\n    num_prompts = len(attention_patterns)\n    num_layers = attention_patterns[0].shape[0]\n    num_heads = attention_patterns[0].shape[1]\n\n    all_entropies = np.zeros((num_prompts, num_layers, num_heads))\n\n    for p_idx, attn in enumerate(attention_patterns):\n        mask = None\n        if attention_masks is not None:\n            mask = attention_masks[p_idx]\n            if mask is not None:\n                mask = mask.bool()\n\n        for layer in range(num_layers):\n            for head in range(num_heads):\n                attn_matrix = attn[layer, head]\n\n                if mask is not None:\n                    valid_idx = mask.nonzero(as_tuple=False).squeeze(-1)\n                    if valid_idx.numel() > 1:\n                        attn_matrix = attn_matrix[valid_idx][:, valid_idx]\n                    else:\n                        all_entropies[p_idx, layer, head] = 0\n                        continue\n\n                attn_weights = attn_matrix.mean(dim=0).float().cpu().numpy()\n                denom = attn_weights.sum()\n                if denom <= 0:\n                    all_entropies[p_idx, layer, head] = 0\n                    continue\n\n                attn_weights = attn_weights / denom\n                attn_weights = attn_weights[attn_weights > 0]\n\n                if len(attn_weights) > 1:\n                    h = scipy_entropy(attn_weights, base=2)\n                    h_max = np.log2(len(attn_weights))\n                    h_norm = h / h_max if h_max > 0 else 0\n                else:\n                    h_norm = 0\n\n                all_entropies[p_idx, layer, head] = h_norm\n\n    return all_entropies.mean(axis=0)\n\n\ndef compute_specialization_metrics(head_entropies):\n    num_layers, num_heads = head_entropies.shape\n\n    layer_variances = np.var(head_entropies, axis=1)\n    mean_variance = float(np.mean(layer_variances))\n\n    head_profiles = head_entropies.T\n    head_corr_matrix = np.corrcoef(head_profiles)\n    upper_tri = head_corr_matrix[np.triu_indices(num_heads, k=1)]\n    mean_head_correlation = float(np.nanmean(upper_tri))\n\n    specialization_index = 1.0 - mean_head_correlation\n\n    head_contributions = np.mean(head_entropies, axis=0)\n    head_contributions = head_contributions / head_contributions.sum()\n    h_contrib = scipy_entropy(head_contributions, base=2)\n    effective_heads = 2 ** h_contrib if h_contrib > 0 else 1.0\n    effective_ratio = effective_heads / num_heads\n\n    # Layer region variances (Early/Middle/Late)\n    third = num_layers // 3\n    early_variance = float(np.mean(layer_variances[:third]))\n    middle_variance = float(np.mean(layer_variances[third:2*third]))\n    late_variance = float(np.mean(layer_variances[2*third:]))\n\n    return {\n        'mean_head_variance': mean_variance,\n        'mean_head_correlation': mean_head_correlation,\n        'specialization_index': specialization_index,\n        'effective_heads': float(effective_heads),\n        'effective_ratio': float(effective_ratio),\n        'layer_variances': layer_variances.tolist(),\n        'early_variance': early_variance,\n        'middle_variance': middle_variance,\n        'late_variance': late_variance,\n        'head_correlation_matrix': head_corr_matrix.tolist(),\n        'num_layers': num_layers,\n        'num_heads': num_heads\n    }\n\nprint(\"Head specialization functions loaded.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Load and Analyze BASE Model (Falcon-7b) - 3-Seed Averaging\n",
    "\n",
    "pair_config = TWIN_PAIRS[PAIR]\n",
    "results = {'pair': PAIR, 'base': {}, 'instruct': {}, 'config': pair_config}\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"E11 TERRITORIAL COLLAPSE: {PAIR.upper()} (M08) - E11-v3\")\n",
    "print(f\"Purpose: MQA architecture control\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(f\"\\n[1/4] Loading BASE: {pair_config['base']}\")\n",
    "\n",
    "tokenizer_base = AutoTokenizer.from_pretrained(pair_config['base'], trust_remote_code=True)\n",
    "model_base = AutoModelForCausalLM.from_pretrained(\n",
    "    pair_config['base'],\n",
    "    torch_dtype=DTYPE,  # E11-v3: Use DTYPE constant\n",
    "    device_map='auto',\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"eager\"\n",
    ")\n",
    "\n",
    "model_base.eval()\n",
    "\n",
    "if tokenizer_base.pad_token is None:\n",
    "    tokenizer_base.pad_token = tokenizer_base.eos_token\n",
    "\n",
    "print(f\"\\n[2/4] Extracting BASE head activations (3-seed averaging)...\")\n",
    "\n",
    "# 3-seed averaging for E11-v3\n",
    "base_seed_results = []\n",
    "for seed in SEEDS:\n",
    "    print(f\"  Seed {seed}...\")\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    base_activations = extract_head_activations(model_base, tokenizer_base, STANDARD_PROMPTS, max_length=MAX_LENGTH, use_chat_template=USE_CHAT_TEMPLATE)\n",
    "    base_entropies = compute_head_entropy_profiles(base_activations['attention_patterns'], base_activations['attention_masks'])\n",
    "    base_metrics = compute_specialization_metrics(base_entropies)\n",
    "    base_seed_results.append({\n",
    "        'seed': seed,\n",
    "        'si': base_metrics['specialization_index'],\n",
    "        'corr': base_metrics['mean_head_correlation'],\n",
    "        'var': base_metrics['mean_head_variance']\n",
    "    })\n",
    "    print(f\"    SI={base_metrics['specialization_index']:.4f}\")\n",
    "\n",
    "# Average across seeds\n",
    "avg_base_si = np.mean([r['si'] for r in base_seed_results])\n",
    "std_base_si = np.std([r['si'] for r in base_seed_results])\n",
    "\n",
    "print(f\"\\n  BASE Specialization Index: {avg_base_si:.4f} Â± {std_base_si:.6f}\")\n",
    "print(f\"  Layers: {base_activations['num_layers']}, Heads: {base_activations['num_heads']}\")\n",
    "\n",
    "# Use last run's full metrics but update SI with average\n",
    "results['base']['specialization'] = base_metrics\n",
    "results['base']['specialization']['specialization_index'] = avg_base_si\n",
    "results['base']['specialization']['si_std'] = std_base_si\n",
    "results['base']['seed_results'] = base_seed_results\n",
    "results['base']['entropies'] = base_entropies.tolist()\n",
    "\n",
    "del model_base\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\n  [Memory cleared]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Load and Analyze INSTRUCT Model (Falcon-7b-instruct) - 3-Seed Averaging\n",
    "\n",
    "print(f\"\\n[3/4] Loading INSTRUCT: {pair_config['instruct']}\")\n",
    "print(f\"       Alignment: {pair_config['alignment']} (SFT-only, NO RLHF!)\")\n",
    "\n",
    "tokenizer_inst = AutoTokenizer.from_pretrained(pair_config['instruct'], trust_remote_code=True)\n",
    "model_inst = AutoModelForCausalLM.from_pretrained(\n",
    "    pair_config['instruct'],\n",
    "    torch_dtype=DTYPE,  # E11-v3: Use DTYPE constant\n",
    "    device_map='auto',\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"eager\"\n",
    ")\n",
    "\n",
    "model_inst.eval()\n",
    "\n",
    "if tokenizer_inst.pad_token is None:\n",
    "    tokenizer_inst.pad_token = tokenizer_inst.eos_token\n",
    "\n",
    "print(f\"\\n[4/4] Extracting INSTRUCT head activations (3-seed averaging)...\")\n",
    "\n",
    "# 3-seed averaging for E11-v3\n",
    "inst_seed_results = []\n",
    "for seed in SEEDS:\n",
    "    print(f\"  Seed {seed}...\")\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    inst_activations = extract_head_activations(model_inst, tokenizer_inst, STANDARD_PROMPTS, max_length=MAX_LENGTH, use_chat_template=USE_CHAT_TEMPLATE)\n",
    "    inst_entropies = compute_head_entropy_profiles(inst_activations['attention_patterns'], inst_activations['attention_masks'])\n",
    "    inst_metrics = compute_specialization_metrics(inst_entropies)\n",
    "    inst_seed_results.append({\n",
    "        'seed': seed,\n",
    "        'si': inst_metrics['specialization_index'],\n",
    "        'corr': inst_metrics['mean_head_correlation'],\n",
    "        'var': inst_metrics['mean_head_variance']\n",
    "    })\n",
    "    print(f\"    SI={inst_metrics['specialization_index']:.4f}\")\n",
    "\n",
    "# Average across seeds\n",
    "avg_inst_si = np.mean([r['si'] for r in inst_seed_results])\n",
    "std_inst_si = np.std([r['si'] for r in inst_seed_results])\n",
    "\n",
    "print(f\"\\n  INSTRUCT Specialization Index: {avg_inst_si:.4f} Â± {std_inst_si:.6f}\")\n",
    "print(f\"  Layers: {inst_activations['num_layers']}, Heads: {inst_activations['num_heads']}\")\n",
    "\n",
    "# Use last run's full metrics but update SI with average\n",
    "results['instruct']['specialization'] = inst_metrics\n",
    "results['instruct']['specialization']['specialization_index'] = avg_inst_si\n",
    "results['instruct']['specialization']['si_std'] = std_inst_si\n",
    "results['instruct']['seed_results'] = inst_seed_results\n",
    "results['instruct']['entropies'] = inst_entropies.tolist()\n",
    "\n",
    "del model_inst\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\n  [Memory cleared]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Hypothesis Test - Territorial Collapse\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"E11 TERRITORIAL COLLAPSE RESULTS: {PAIR.upper()} (M08)\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "base_spec = results['base']['specialization']\n",
    "inst_spec = results['instruct']['specialization']\n",
    "\n",
    "base_si = base_spec['specialization_index']\n",
    "inst_si = inst_spec['specialization_index']\n",
    "delta_si = inst_si - base_si\n",
    "\n",
    "base_eff = base_spec['effective_ratio']\n",
    "inst_eff = inst_spec['effective_ratio']\n",
    "delta_eff = inst_eff - base_eff\n",
    "\n",
    "base_corr = base_spec['mean_head_correlation']\n",
    "inst_corr = inst_spec['mean_head_correlation']\n",
    "delta_corr = inst_corr - base_corr\n",
    "\n",
    "base_var = base_spec['mean_head_variance']\n",
    "inst_var = inst_spec['mean_head_variance']\n",
    "delta_var = inst_var - base_var\n",
    "\n",
    "print(f\"\\n{'Metric':<35} {'BASE':>12} {'INSTRUCT':>12} {'Delta':>12}\")\n",
    "print(\"-\" * 75)\n",
    "print(f\"{'Specialization Index':<35} {base_si:>12.4f} {inst_si:>12.4f} {delta_si:>+12.4f}\")\n",
    "print(f\"{'Effective Head Ratio':<35} {base_eff:>12.4f} {inst_eff:>12.4f} {delta_eff:>+12.4f}\")\n",
    "print(f\"{'Mean Head Correlation':<35} {base_corr:>12.4f} {inst_corr:>12.4f} {delta_corr:>+12.4f}\")\n",
    "print(f\"{'Mean Head Variance':<35} {base_var:>12.6f} {inst_var:>12.6f} {delta_var:>+12.6f}\")\n",
    "\n",
    "print(f\"\\n{'Layer Region':<35} {'BASE Var':>12} {'INST Var':>12} {'Delta':>12}\")\n",
    "print(\"-\" * 75)\n",
    "print(f\"{'Early Layers':<35} {base_spec['early_variance']:>12.6f} {inst_spec['early_variance']:>12.6f} {inst_spec['early_variance'] - base_spec['early_variance']:>+12.6f}\")\n",
    "print(f\"{'Middle Layers (L*)':<35} {base_spec['middle_variance']:>12.6f} {inst_spec['middle_variance']:>12.6f} {inst_spec['middle_variance'] - base_spec['middle_variance']:>+12.6f}\")\n",
    "print(f\"{'Late Layers':<35} {base_spec['late_variance']:>12.6f} {inst_spec['late_variance']:>12.6f} {inst_spec['late_variance'] - base_spec['late_variance']:>+12.6f}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"HYPOTHESIS TEST: Does SFT-only cause TERRITORIAL COLLAPSE?\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "collapse_1 = delta_si < 0\n",
    "collapse_2 = delta_corr > 0\n",
    "collapse_3 = delta_var < 0\n",
    "\n",
    "print(f\"\\n  [1] Specialization decreased:    {'YES' if collapse_1 else 'NO'} ({delta_si:+.4f})\")\n",
    "print(f\"  [2] Head correlation increased:  {'YES' if collapse_2 else 'NO'} ({delta_corr:+.4f})\")\n",
    "print(f\"  [3] Head variance decreased:     {'YES' if collapse_3 else 'NO'} ({delta_var:+.6f})\")\n",
    "\n",
    "collapse_count = sum([collapse_1, collapse_2, collapse_3])\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "if collapse_count >= 2:\n",
    "    verdict = \"A_CONFIRMED\"\n",
    "    print(f\"VERDICT: {verdict}\")\n",
    "    print(\"SFT-only causes TERRITORIAL COLLAPSE - heads lose specialization!\")\n",
    "    print(\"\\n>>> IMPLICATION: Collapse is NOT RLHF-specific, SFT alone suffices!\")\n",
    "elif collapse_count == 1:\n",
    "    verdict = \"B_PARTIAL\"\n",
    "    print(f\"VERDICT: {verdict}\")\n",
    "    print(\"Partial evidence for territorial collapse.\")\n",
    "else:\n",
    "    verdict = \"C_REFUTED\"\n",
    "    print(f\"VERDICT: {verdict}\")\n",
    "    print(\"No evidence for territorial collapse - SFT preserves specialization.\")\n",
    "    print(\"\\n>>> IMPLICATION: RLHF may be required for collapse; SFT is safe!\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "results['verdict'] = {\n",
    "    'code': verdict,\n",
    "    'specialization_decreased': collapse_1,\n",
    "    'correlation_increased': collapse_2,\n",
    "    'variance_decreased': collapse_3,\n",
    "    'delta_specialization': delta_si,\n",
    "    'delta_correlation': delta_corr,\n",
    "    'delta_variance': delta_var\n",
    "}\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"CROSS-FAMILY COMPARISON (A1 Claim Robustness)\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\n  Mistral (RLHF+DPO): SI delta = +0.0420 (INCREASES specialization)\")\n",
    "print(f\"  Falcon  (SFT-only): SI delta = {delta_si:+.4f}\")\n",
    "if delta_si > 0:\n",
    "    print(f\"\\n  >>> CONSISTENT! Both MHA families show SI INCREASE under alignment.\")\n",
    "    print(f\"  >>> A1 claim STRENGTHENED: MHA architecture diversifies heads.\")\n",
    "else:\n",
    "    print(f\"\\n  >>> INCONSISTENT! Falcon shows opposite pattern.\")\n",
    "    print(f\"  >>> Investigate: Is SFT-only different from RLHF?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Visualization\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "ax1 = axes[0, 0]\n",
    "models = ['Base\\n(falcon-7b)', 'Instruct\\n(falcon-7b-instruct)']\n",
    "si_vals = [base_si, inst_si]\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "bars = ax1.bar(models, si_vals, color=colors, alpha=0.8, edgecolor='black')\n",
    "ax1.set_ylabel('Specialization Index')\n",
    "ax1.set_title(f'{PAIR.upper()}: Specialization Index\\n(Higher = More Unique Roles)')\n",
    "ax1.set_ylim(0, 1)\n",
    "for bar, val in zip(bars, si_vals):\n",
    "    ax1.annotate(f'{val:.4f}', xy=(bar.get_x() + bar.get_width()/2, val),\n",
    "                 xytext=(0, 5), textcoords='offset points', ha='center', fontsize=12)\n",
    "ax1.annotate(f'delta = {delta_si:+.4f}', xy=(0.5, 0.95), xycoords='axes fraction',\n",
    "             ha='center', fontsize=14, color='red' if delta_si < 0 else 'green',\n",
    "             fontweight='bold')\n",
    "\n",
    "ax2 = axes[0, 1]\n",
    "corr_vals = [base_corr, inst_corr]\n",
    "bars = ax2.bar(models, corr_vals, color=colors, alpha=0.8, edgecolor='black')\n",
    "ax2.set_ylabel('Mean Head Correlation')\n",
    "ax2.set_title(f'{PAIR.upper()}: Head Correlation\\n(Lower = More Independent)')\n",
    "for bar, val in zip(bars, corr_vals):\n",
    "    ax2.annotate(f'{val:.4f}', xy=(bar.get_x() + bar.get_width()/2, val),\n",
    "                 xytext=(0, 5), textcoords='offset points', ha='center', fontsize=12)\n",
    "ax2.annotate(f'delta = {delta_corr:+.4f}', xy=(0.5, 0.95), xycoords='axes fraction',\n",
    "             ha='center', fontsize=14, color='red' if delta_corr > 0 else 'green',\n",
    "             fontweight='bold')\n",
    "\n",
    "ax3 = axes[0, 2]\n",
    "base_layer_var = base_spec['layer_variances']\n",
    "inst_layer_var = inst_spec['layer_variances']\n",
    "layers = range(len(base_layer_var))\n",
    "ax3.plot(layers, base_layer_var, 'o-', color='#2ecc71', label='Base (falcon-7b)', linewidth=2, markersize=4)\n",
    "ax3.plot(layers, inst_layer_var, 's-', color='#e74c3c', label='Instruct (falcon-instruct)', linewidth=2, markersize=4)\n",
    "ax3.set_xlabel('Layer')\n",
    "ax3.set_ylabel('Head Variance')\n",
    "ax3.set_title(f'{PAIR.upper()}: Layer-wise Head Variance\\n(Higher = More Diverse Heads)')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "num_layers = len(base_layer_var)\n",
    "third = num_layers // 3\n",
    "ax3.axvspan(third, 2*third, alpha=0.2, color='yellow', label='L* Region')\n",
    "\n",
    "ax4 = axes[1, 0]\n",
    "base_corr_matrix = np.array(base_spec['head_correlation_matrix'])\n",
    "sns.heatmap(base_corr_matrix, cmap='RdBu_r', center=0, vmin=-1, vmax=1,\n",
    "            ax=ax4, cbar_kws={'label': 'Correlation'})\n",
    "ax4.set_title(f'{PAIR.upper()} BASE: Head Correlation Matrix')\n",
    "ax4.set_xlabel('Head')\n",
    "ax4.set_ylabel('Head')\n",
    "\n",
    "ax5 = axes[1, 1]\n",
    "inst_corr_matrix = np.array(inst_spec['head_correlation_matrix'])\n",
    "sns.heatmap(inst_corr_matrix, cmap='RdBu_r', center=0, vmin=-1, vmax=1,\n",
    "            ax=ax5, cbar_kws={'label': 'Correlation'})\n",
    "ax5.set_title(f'{PAIR.upper()} INSTRUCT (SFT-only): Head Correlation Matrix')\n",
    "ax5.set_xlabel('Head')\n",
    "ax5.set_ylabel('Head')\n",
    "\n",
    "ax6 = axes[1, 2]\n",
    "metrics = ['Specialization\\nIndex', 'Effective\\nHead Ratio', '1 - Correlation']\n",
    "base_vals = [base_si, base_eff, 1 - base_corr]\n",
    "inst_vals = [inst_si, inst_eff, 1 - inst_corr]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax6.bar(x - width/2, base_vals, width, label='Base (falcon-7b)', color='#2ecc71', alpha=0.8)\n",
    "bars2 = ax6.bar(x + width/2, inst_vals, width, label='Instruct (falcon-instruct)', color='#e74c3c', alpha=0.8)\n",
    "\n",
    "ax6.set_ylabel('Value')\n",
    "ax6.set_title(f'{PAIR.upper()}: Specialization Summary\\n(All Higher = Better Specialization)')\n",
    "ax6.set_xticks(x)\n",
    "ax6.set_xticklabels(metrics)\n",
    "ax6.legend()\n",
    "ax6.set_ylim(0, 1.1)\n",
    "\n",
    "for i, (b, inst) in enumerate(zip(base_vals, inst_vals)):\n",
    "    delta = inst - b\n",
    "    color = 'red' if delta < 0 else 'green'\n",
    "    ax6.annotate(f'{delta:+.3f}', xy=(i, max(b, inst) + 0.05), ha='center', fontsize=10, color=color)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig_path = f'figures/E11_Falcon_Territorial_Collapse_{TIMESTAMP}.png'\n",
    "plt.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFigure saved: {fig_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Save Results with E11-v3 Methodology Block\n",
    "\n",
    "filename = f'results/E11_falcon_territorial_collapse_{TIMESTAMP}.json'\n",
    "\n",
    "def convert_to_native(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_to_native(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_native(v) for v in obj]\n",
    "    elif isinstance(obj, tuple):\n",
    "        return tuple(convert_to_native(v) for v in obj)\n",
    "    elif isinstance(obj, (np.bool_, np.integer)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "output = {\n",
    "    'experiment': 'E11_Territorial_Collapse',\n",
    "    'timestamp': TIMESTAMP,\n",
    "    'pair': PAIR,\n",
    "    'pair_id': 'M08',\n",
    "    'config': pair_config,\n",
    "    \n",
    "    # E11-v3 Methodology Block\n",
    "    'methodology': {\n",
    "        'standard': 'E11-v3',\n",
    "        'seeds': SEEDS,\n",
    "        'max_length': MAX_LENGTH,\n",
    "        'dtype': str(DTYPE),\n",
    "        'prompt_md5': ACTUAL_MD5,\n",
    "        'prompt_md5_verified': PROMPTS_VERIFIED,\n",
    "        'use_chat_template': USE_CHAT_TEMPLATE,\n",
    "        'attention_masked': True,\n",
    "        'num_prompts': len(STANDARD_PROMPTS),\n",
    "        'prompt_set': 'Standard-10 v3',\n",
    "        'quantization': 'NONE (Full Precision bfloat16)',\n",
    "        'use_chat_template': False\n",
    "    },\n",
    "    \n",
    "    'prompt_set': 'Standard-10 v3',\n",
    "    'num_prompts': len(STANDARD_PROMPTS),\n",
    "    'hypothesis': 'Does SFT-only cause territorial collapse like RLHF?',\n",
    "    'purpose': 'MQA architecture control for A5 claim',\n",
    "    'comparison': {\n",
    "        'mistral_rlhf_dpo': {'delta_si': +0.0420, 'verdict': 'C_REFUTED'},\n",
    "        'falcon_sft_only': {'delta_si': delta_si, 'verdict': verdict}\n",
    "    },\n",
    "    'results': convert_to_native(results)\n",
    "}\n",
    "\n",
    "with open(filename, 'w') as f:\n",
    "    json.dump(output, f, indent=2)\n",
    "\n",
    "print(f\"Results saved: {filename}\")\n",
    "print(f\"\\nðŸ“‹ E11-v3 Compliance:\")\n",
    "print(f\"   Seeds: {SEEDS} âœ“\")\n",
    "print(f\"   dtype: {DTYPE} âœ“\")\n",
    "print(f\"   MD5: {ACTUAL_MD5} {'âœ“' if PROMPTS_VERIFIED else 'âœ—'}\")\n",
    "print(f\"   MAX_LENGTH: {MAX_LENGTH} âœ“\")\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(filename)\n",
    "    files.download(fig_path)\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": "---\n\n## Summary\n\n### E11: Territorial Collapse - Falcon (M08) - MQA Control\n\n**Architecture:** Multi-Query Attention (MQA)\n- 71 Query Heads\n- 1 shared K/V Head (NOT MHA!)\n\n**Key Question:** Does MQA behave like MHA or GQA under alignment?\n\n**Theoretical Prediction:**\n- MQA = GQA with 1 group â†’ should behave like extreme GQA\n- Shared K/V forces homogenization â†’ expect SIâ†“\n\n**Possible Outcomes:**\n\n| Outcome | Interpretation |\n|---------|----------------|\n| SIâ†“ (like GQA) | MQA behaves like GQA - shared KV dominates |\n| SIâ†‘ (like MHA) | Query diversity matters more than KV sharing |\n| No change | Shared KV stabilizes against alignment pressure |\n\n**This is NOT a 2nd MHA family test!**\nThis is a new architecture category (MQA) that extends the A1 claim to a third attention variant.\n\n---\n\n*Paper 4: Behavioral Sink Dynamics*  \n*E11-Falcon: MQA Control (distinct from MHA/GQA)*"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# AUTO-DOWNLOAD RESULTS (Colab only)\n",
    "# ============================================================================\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "def auto_download_results():\n",
    "    try:\n",
    "        from google.colab import files\n",
    "    except ImportError:\n",
    "        print('Not in Colab - skipping auto-download')\n",
    "        return\n",
    "    \n",
    "    print('=' * 60)\n",
    "    print('AUTO-DOWNLOADING RESULTS...')\n",
    "    print('=' * 60)\n",
    "    \n",
    "    # Find all result files\n",
    "    json_files = glob.glob('results/*.json') + glob.glob('figures/*.json')\n",
    "    png_files = glob.glob('results/*.png') + glob.glob('figures/*.png')\n",
    "    all_files = json_files + png_files\n",
    "    \n",
    "    if not all_files:\n",
    "        print('WARNING: No result files found!')\n",
    "        return\n",
    "    \n",
    "    print(f'Found {len(all_files)} files')\n",
    "    \n",
    "    # Download as ZIP\n",
    "    import os\n",
    "    zip_name = f'E11_results_{os.path.basename(os.getcwd())}'\n",
    "    \n",
    "    # Create combined folder\n",
    "    os.makedirs('download_package', exist_ok=True)\n",
    "    for f in all_files:\n",
    "        shutil.copy(f, 'download_package/')\n",
    "    \n",
    "    shutil.make_archive(zip_name, 'zip', 'download_package')\n",
    "    print(f'Downloading: {zip_name}.zip')\n",
    "    files.download(f'{zip_name}.zip')\n",
    "    print('DOWNLOAD COMPLETE!')\n",
    "\n",
    "auto_download_results()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}