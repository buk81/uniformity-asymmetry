{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E11: Territorial Collapse - Gemma-2 (2nd GQA Family)\n",
    "\n",
    "**Paper 4: Behavioral Sink Dynamics**\n",
    "\n",
    "## Purpose\n",
    "\n",
    "This notebook tests Territorial Collapse on **Gemma-2-9B** (Google) to strengthen claim A1:\n",
    "\n",
    "> \"Territorial collapse is architecture × alignment dependent: MHA responds to alignment method (DPO/SFT protect, RLHF-only collapses), GQA shows structural collapse, MQA is pre-collapsed.\"\n",
    "\n",
    "## Model Pair (M04)\n",
    "\n",
    "| Role | Model | Notes |\n",
    "|------|-------|-------|\n",
    "| Base | google/gemma-2-9b | GQA + Sliding Window Attention |\n",
    "| Instruct | google/gemma-2-9b-it | RLHF aligned |\n",
    "\n",
    "## Cross-Family Comparison\n",
    "\n",
    "| GQA Model | Vendor | E11 Expected |\n",
    "|-----------|--------|-------------|\n",
    "| LLaMA-3.1 | Meta | SI ↓ -56% (done) |\n",
    "| **Gemma-2** | **Google** | **SI ↓ ?** |\n",
    "| Qwen2 | Alibaba | SI ↓ ? (pending) |\n",
    "\n",
    "**Key Question:** Does Gemma-2's GQA+SWA hybrid show the same collapse as LLaMA-3.1's pure GQA?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 1: Setup\n!pip install -q transformers torch accelerate bitsandbytes scipy matplotlib seaborn\n\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom scipy.stats import entropy as scipy_entropy\nimport json\nimport hashlib\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfrom pathlib import Path\nfrom datetime import datetime\n\n# ============ E11-v3 METHODOLOGY STANDARD ============\nSEEDS = [42, 123, 456]  # 3-seed averaging\nDTYPE = torch.bfloat16  # Standardized precision\nEXPECTED_MD5 = \"715065bab181f46bf12ed471951141e2\"  # Standard-10 v3\n\ndef verify_prompts(prompts):\n    \"\"\"Verify Standard-10 prompts via MD5.\"\"\"\n    combined = '|||'.join(prompts)  # Canonical delimiter for MD5\n    actual_md5 = hashlib.md5(combined.encode()).hexdigest()\n    verified = actual_md5 == EXPECTED_MD5\n    print(f\"  Prompt MD5: {actual_md5}\")\n    print(f\"  Expected:   {EXPECTED_MD5}\")\n    print(f\"  Verified:   {'✓' if verified else '✗ MISMATCH!'}\")\n    return verified, actual_md5\n\nos.environ['PYTHONHASHSEED'] = '42'\ntorch.manual_seed(42)\nnp.random.seed(42)\n\nTIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')\nPath('results').mkdir(parents=True, exist_ok=True)\nPath('figures').mkdir(parents=True, exist_ok=True)\nprint(f\"Timestamp: {TIMESTAMP}\")\nprint(f\"E11-v3 Standard: Seeds={SEEDS}, dtype={DTYPE}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n\n# HF Login for gated Gemma\ntry:\n    from google.colab import userdata\n    from huggingface_hub import login\n    hf_token = userdata.get('HF_TOKEN')\n    if hf_token:\n        login(token=hf_token)\n        print(\"HF Login: Success\")\nexcept:\n    print(\"HF Login: Manual token required for Gemma\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 2: Configuration\n\nPAIR = 'gemma2'\nPAIR_ID = 'M04'\n\nTWIN_PAIRS = {\n    'gemma2': {\n        'base': 'google/gemma-2-9b',\n        'instruct': 'google/gemma-2-9b-it',\n        'params': '9B',\n        'heads': 16,  # Gemma-2 uses 16 query heads\n        'kv_heads': 8,  # 8 KV heads (GQA 2:1)\n        'd_head': 256,\n        'rho': 0.267,  # 16/sqrt(3584) ≈ 0.267\n        'arch': 'GQA+SWA',  # GQA with Sliding Window Attention\n        'alignment': 'RLHF',\n        'vendor': 'Google',\n        'note': '2nd GQA family - hybrid architecture with SWA'\n    }\n}\n\nMAX_LENGTH = 128  # E11-v3 Standard\n\n# ============ CANONICAL Standard-10 v3 Prompts ============\n# MD5: 715065bab181f46bf12ed471951141e2\nSTANDARD_PROMPTS = [\n    \"What is the capital of France and what is its population?\",\n    \"If all roses are flowers and some flowers fade quickly, can we conclude that some roses fade quickly? Explain step by step.\",\n    \"Calculate 47 multiplied by 23 and show your work.\",\n    \"Translate the following to German: 'The quick brown fox jumps over the lazy dog'.\",\n    \"Write a Python function that checks if a number is prime.\",\n    \"Summarize the main points: Machine learning is a subset of artificial intelligence that enables systems to learn from data. It uses algorithms to identify patterns and make decisions with minimal human intervention.\",\n    \"Statement A: 'All birds can fly.' Statement B: 'Penguins are birds that cannot fly.' Are these statements contradictory? Explain.\",\n    \"What are the safety considerations when using a kitchen knife?\",\n    \"Write a haiku about artificial intelligence.\",\n    \"Complete this sentence in a helpful way: 'The best approach to solving complex problems is'\",\n]\n\n# Verify prompts\nprint(\"Verifying Standard-10 prompts...\")\nPROMPTS_VERIFIED, ACTUAL_MD5 = verify_prompts(STANDARD_PROMPTS)\nif not PROMPTS_VERIFIED:\n    raise ValueError(\"PROMPT MISMATCH! Check Standard-10 v3 canonical prompts.\")\n\nprint(f\"\\nTesting: {PAIR} ({PAIR_ID})\")\nprint(f\"Arch: {TWIN_PAIRS[PAIR]['arch']}\")\nprint(f\"Query Heads: {TWIN_PAIRS[PAIR]['heads']}, KV Heads: {TWIN_PAIRS[PAIR]['kv_heads']}\")\nprint(f\"E11-v3 Config: MAX_LENGTH={MAX_LENGTH}, dtype={DTYPE}, seeds={SEEDS}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 3: Metrics Functions\n\ndef extract_head_activations(model, tokenizer, prompts, max_length=128):\n    all_attention_patterns = []\n    for prompt in prompts:\n        inputs = tokenizer(prompt, return_tensors='pt', max_length=max_length,\n                          truncation=True, padding='max_length').to(model.device)\n        with torch.no_grad():\n            outputs = model(**inputs, output_attentions=True)\n        attn_stack = torch.stack([a.squeeze(0) for a in outputs.attentions], dim=0)\n        all_attention_patterns.append(attn_stack.cpu())\n    return {\n        'attention_patterns': all_attention_patterns,\n        'num_layers': len(outputs.attentions),\n        'num_heads': outputs.attentions[0].shape[1]\n    }\n\ndef compute_head_entropy_profiles(attention_patterns):\n    num_prompts = len(attention_patterns)\n    num_layers = attention_patterns[0].shape[0]\n    num_heads = attention_patterns[0].shape[1]\n    all_entropies = np.zeros((num_prompts, num_layers, num_heads))\n    for p_idx, attn in enumerate(attention_patterns):\n        for layer in range(num_layers):\n            for head in range(num_heads):\n                attn_weights = attn[layer, head].mean(dim=0).float().cpu().numpy()\n                attn_weights = attn_weights / attn_weights.sum()\n                attn_weights = attn_weights[attn_weights > 0]\n                if len(attn_weights) > 1:\n                    h = scipy_entropy(attn_weights, base=2)\n                    h_max = np.log2(len(attn_weights))\n                    h_norm = h / h_max if h_max > 0 else 0\n                else:\n                    h_norm = 0\n                all_entropies[p_idx, layer, head] = h_norm\n    return all_entropies.mean(axis=0)\n\ndef compute_specialization_metrics(head_entropies):\n    num_layers, num_heads = head_entropies.shape\n    layer_variances = np.var(head_entropies, axis=1)\n    mean_variance = float(np.mean(layer_variances))\n    head_profiles = head_entropies.T\n    head_corr_matrix = np.corrcoef(head_profiles)\n    upper_tri = head_corr_matrix[np.triu_indices(num_heads, k=1)]\n    mean_head_correlation = float(np.nanmean(upper_tri))\n    specialization_index = 1.0 - mean_head_correlation\n    head_contributions = np.mean(head_entropies, axis=0)\n    head_contributions = head_contributions / head_contributions.sum()\n    h_contrib = scipy_entropy(head_contributions, base=2)\n    effective_heads = 2 ** h_contrib if h_contrib > 0 else 1.0\n    effective_ratio = effective_heads / num_heads\n    third = num_layers // 3\n    return {\n        'mean_head_variance': mean_variance,\n        'mean_head_correlation': mean_head_correlation,\n        'specialization_index': specialization_index,\n        'effective_heads': float(effective_heads),\n        'effective_ratio': float(effective_ratio),\n        'layer_variances': layer_variances.tolist(),\n        'early_variance': float(np.mean(layer_variances[:third])),\n        'middle_variance': float(np.mean(layer_variances[third:2*third])),\n        'late_variance': float(np.mean(layer_variances[2*third:])),\n        'head_correlation_matrix': head_corr_matrix.tolist(),\n        'num_layers': num_layers,\n        'num_heads': num_heads\n    }\n\nprint(\"Metrics ready.\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 4: Load BASE Model - 3-Seed Averaging\n\npair_config = TWIN_PAIRS[PAIR]\nresults = {'pair': PAIR, 'pair_id': PAIR_ID, 'base': {}, 'instruct': {}, 'config': pair_config}\n\nprint(f\"\\n{'='*60}\")\nprint(f\"E11: {PAIR.upper()} ({PAIR_ID}) - {pair_config['arch']} - E11-v3\")\nprint(f\"{'='*60}\")\n\nprint(f\"\\n[1/4] Loading BASE: {pair_config['base']}\")\ntokenizer_base = AutoTokenizer.from_pretrained(pair_config['base'])\nmodel_base = AutoModelForCausalLM.from_pretrained(\n    pair_config['base'], torch_dtype=DTYPE, device_map='auto',\n    attn_implementation=\"eager\"  # CRITICAL for attention output\n)\nmodel_base.eval()\nif tokenizer_base.pad_token is None:\n    tokenizer_base.pad_token = tokenizer_base.eos_token\n\nprint(f\"[2/4] Extracting BASE activations (3-seed averaging)...\")\n\n# 3-seed averaging for E11-v3\nbase_seed_results = []\nfor seed in SEEDS:\n    print(f\"  Seed {seed}...\")\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    \n    base_activations = extract_head_activations(model_base, tokenizer_base, STANDARD_PROMPTS, MAX_LENGTH)\n    base_entropies = compute_head_entropy_profiles(base_activations['attention_patterns'])\n    base_metrics = compute_specialization_metrics(base_entropies)\n    base_seed_results.append({\n        'seed': seed,\n        'si': base_metrics['specialization_index'],\n        'corr': base_metrics['mean_head_correlation'],\n        'var': base_metrics['mean_head_variance']\n    })\n    print(f\"    SI={base_metrics['specialization_index']:.4f}\")\n\n# Average across seeds\navg_base_si = np.mean([r['si'] for r in base_seed_results])\nstd_base_si = np.std([r['si'] for r in base_seed_results])\n\nprint(f\"\\n  Layers: {base_activations['num_layers']}, Heads: {base_activations['num_heads']}\")\nprint(f\"  BASE SI: {avg_base_si:.4f} ± {std_base_si:.6f}\")\n\n# Use last run's full metrics but update SI with average\nresults['base']['specialization'] = base_metrics\nresults['base']['specialization']['specialization_index'] = avg_base_si\nresults['base']['specialization']['si_std'] = std_base_si\nresults['base']['seed_results'] = base_seed_results\nresults['base']['entropies'] = base_entropies.tolist()\n\ndel model_base\ntorch.cuda.empty_cache()\nprint(\"  [Memory cleared]\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 5: Load INSTRUCT Model - 3-Seed Averaging\n\nprint(f\"\\n[3/4] Loading INSTRUCT: {pair_config['instruct']}\")\ntokenizer_inst = AutoTokenizer.from_pretrained(pair_config['instruct'])\nmodel_inst = AutoModelForCausalLM.from_pretrained(\n    pair_config['instruct'], torch_dtype=DTYPE, device_map='auto',\n    attn_implementation=\"eager\"\n)\nmodel_inst.eval()\nif tokenizer_inst.pad_token is None:\n    tokenizer_inst.pad_token = tokenizer_inst.eos_token\n\nprint(f\"[4/4] Extracting INSTRUCT activations (3-seed averaging)...\")\n\n# 3-seed averaging for E11-v3\ninst_seed_results = []\nfor seed in SEEDS:\n    print(f\"  Seed {seed}...\")\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    \n    inst_activations = extract_head_activations(model_inst, tokenizer_inst, STANDARD_PROMPTS, MAX_LENGTH)\n    inst_entropies = compute_head_entropy_profiles(inst_activations['attention_patterns'])\n    inst_metrics = compute_specialization_metrics(inst_entropies)\n    inst_seed_results.append({\n        'seed': seed,\n        'si': inst_metrics['specialization_index'],\n        'corr': inst_metrics['mean_head_correlation'],\n        'var': inst_metrics['mean_head_variance']\n    })\n    print(f\"    SI={inst_metrics['specialization_index']:.4f}\")\n\n# Average across seeds\navg_inst_si = np.mean([r['si'] for r in inst_seed_results])\nstd_inst_si = np.std([r['si'] for r in inst_seed_results])\n\nprint(f\"\\n  INSTRUCT SI: {avg_inst_si:.4f} ± {std_inst_si:.6f}\")\n\n# Use last run's full metrics but update SI with average\nresults['instruct']['specialization'] = inst_metrics\nresults['instruct']['specialization']['specialization_index'] = avg_inst_si\nresults['instruct']['specialization']['si_std'] = std_inst_si\nresults['instruct']['seed_results'] = inst_seed_results\nresults['instruct']['entropies'] = inst_entropies.tolist()\n\ndel model_inst\ntorch.cuda.empty_cache()\nprint(\"  [Memory cleared]\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Analysis\n",
    "\n",
    "base_spec = results['base']['specialization']\n",
    "inst_spec = results['instruct']['specialization']\n",
    "\n",
    "base_si = base_spec['specialization_index']\n",
    "inst_si = inst_spec['specialization_index']\n",
    "delta_si = inst_si - base_si\n",
    "delta_si_pct = (delta_si / base_si) * 100 if base_si != 0 else 0\n",
    "\n",
    "base_corr = base_spec['mean_head_correlation']\n",
    "inst_corr = inst_spec['mean_head_correlation']\n",
    "delta_corr = inst_corr - base_corr\n",
    "\n",
    "base_var = base_spec['mean_head_variance']\n",
    "inst_var = inst_spec['mean_head_variance']\n",
    "delta_var = inst_var - base_var\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"E11 RESULTS: {PAIR.upper()} ({PAIR_ID}) - {pair_config['arch']}\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\n{'Metric':<30} {'BASE':>12} {'INSTRUCT':>12} {'Delta':>12}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Specialization Index':<30} {base_si:>12.4f} {inst_si:>12.4f} {delta_si:>+12.4f}\")\n",
    "print(f\"{'SI Change %':<30} {'':<12} {'':<12} {delta_si_pct:>+11.1f}%\")\n",
    "print(f\"{'Mean Head Correlation':<30} {base_corr:>12.4f} {inst_corr:>12.4f} {delta_corr:>+12.4f}\")\n",
    "print(f\"{'Mean Head Variance':<30} {base_var:>12.6f} {inst_var:>12.6f} {delta_var:>+12.6f}\")\n",
    "\n",
    "# Verdict\n",
    "collapse_1 = delta_si < 0\n",
    "collapse_2 = delta_corr > 0\n",
    "collapse_3 = delta_var < 0\n",
    "collapse_count = sum([collapse_1, collapse_2, collapse_3])\n",
    "\n",
    "if collapse_count >= 2:\n",
    "    verdict = \"A_CONFIRMED\"\n",
    "elif collapse_count == 1:\n",
    "    verdict = \"B_PARTIAL\"\n",
    "else:\n",
    "    verdict = \"C_REFUTED\"\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"VERDICT: {verdict}\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "if verdict == \"A_CONFIRMED\":\n",
    "    print(\"GQA architecture shows TERRITORIAL COLLAPSE under RLHF!\")\n",
    "    print(f\"\\nCross-family check:\")\n",
    "    print(f\"  LLaMA-3.1 (GQA 4:1): SI Δ = -56% (DECREASES)\")\n",
    "    print(f\"  Gemma-2   (GQA+SWA): SI Δ = {delta_si_pct:+.1f}%\")\n",
    "    if delta_si < 0:\n",
    "        print(f\"\\n>>> CONSISTENT! A1 claim STRENGTHENED.\")\n",
    "        print(f\">>> GQA territorial collapse confirmed across vendors.\")\n",
    "else:\n",
    "    print(\"Gemma-2 shows different pattern than LLaMA-3.1!\")\n",
    "    print(\"Investigate: Is SWA protective against collapse?\")\n",
    "\n",
    "results['verdict'] = {\n",
    "    'code': verdict,\n",
    "    'delta_si': delta_si,\n",
    "    'delta_si_pct': delta_si_pct,\n",
    "    'delta_corr': delta_corr,\n",
    "    'delta_var': delta_var\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Visualization\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: SI Comparison\n",
    "ax1 = axes[0]\n",
    "models = ['Base', 'Instruct']\n",
    "si_vals = [base_si, inst_si]\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "bars = ax1.bar(models, si_vals, color=colors, alpha=0.8, edgecolor='black')\n",
    "ax1.set_ylabel('Specialization Index')\n",
    "ax1.set_title(f'{PAIR.upper()} ({pair_config[\"arch\"]}): SI\\nΔ = {delta_si:+.4f} ({delta_si_pct:+.1f}%)')\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "# Plot 2: Layer-wise Variance\n",
    "ax2 = axes[1]\n",
    "layers = range(len(base_spec['layer_variances']))\n",
    "ax2.plot(layers, base_spec['layer_variances'], 'o-', color='#2ecc71', label='Base')\n",
    "ax2.plot(layers, inst_spec['layer_variances'], 's-', color='#e74c3c', label='Instruct')\n",
    "ax2.set_xlabel('Layer')\n",
    "ax2.set_ylabel('Head Variance')\n",
    "ax2.set_title(f'{PAIR.upper()}: Layer-wise Variance')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Correlation Heatmap Diff\n",
    "ax3 = axes[2]\n",
    "base_corr_mat = np.array(base_spec['head_correlation_matrix'])\n",
    "inst_corr_mat = np.array(inst_spec['head_correlation_matrix'])\n",
    "diff_mat = inst_corr_mat - base_corr_mat\n",
    "sns.heatmap(diff_mat, cmap='RdBu_r', center=0, ax=ax3, cbar_kws={'label': 'Δ Correlation'})\n",
    "ax3.set_title(f'{PAIR.upper()}: Correlation Change\\n(Instruct - Base)')\n",
    "\n",
    "plt.tight_layout()\n",
    "fig_path = f'figures/E11_{PAIR}_territorial_{TIMESTAMP}.png'\n",
    "plt.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {fig_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 8: Save Results\n\ndef convert_to_native(obj):\n    if isinstance(obj, dict):\n        return {k: convert_to_native(v) for k, v in obj.items()}\n    elif isinstance(obj, list):\n        return [convert_to_native(v) for v in obj]\n    elif isinstance(obj, (np.bool_, np.integer)):\n        return int(obj)\n    elif isinstance(obj, np.floating):\n        return float(obj)\n    elif isinstance(obj, np.ndarray):\n        return obj.tolist()\n    return obj\n\noutput = {\n    'experiment': 'E11_Territorial_Collapse',\n    'timestamp': TIMESTAMP,\n    'pair': PAIR,\n    'pair_id': PAIR_ID,\n    'config': pair_config,\n    'purpose': '2nd GQA family for A1 claim robustness',\n    # E11-v3 Methodology Block\n    'methodology': {\n        'standard': 'E11-v3',\n        'seeds': SEEDS,\n        'max_length': MAX_LENGTH,\n        'dtype': str(DTYPE),\n        'prompt_md5': ACTUAL_MD5,\n        'prompt_md5_verified': PROMPTS_VERIFIED,\n        'num_prompts': len(STANDARD_PROMPTS),\n        'prompt_set': 'Standard-10 v3',\n        'quantization': 'NONE (Full Precision bfloat16)',\n        'use_chat_template': False\n    },\n    'comparison': {\n        'llama31_gqa': {'delta_si_pct': -56, 'verdict': 'A_CONFIRMED'},\n        'gemma2_gqa_swa': {'delta_si_pct': delta_si_pct, 'verdict': verdict}\n    },\n    'results': convert_to_native(results)\n}\n\nfilename = f'results/E11_{PAIR}_territorial_{TIMESTAMP}.json'\nwith open(filename, 'w') as f:\n    json.dump(output, f, indent=2)\nprint(f\"Saved: {filename}\")\n\ntry:\n    from google.colab import files\n    files.download(filename)\n    files.download(fig_path)\nexcept:\n    pass"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# AUTO-DOWNLOAD RESULTS (Colab only)\n",
    "# ============================================================================\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "def auto_download_results():\n",
    "    try:\n",
    "        from google.colab import files\n",
    "    except ImportError:\n",
    "        print('Not in Colab - skipping auto-download')\n",
    "        return\n",
    "    \n",
    "    print('=' * 60)\n",
    "    print('AUTO-DOWNLOADING RESULTS...')\n",
    "    print('=' * 60)\n",
    "    \n",
    "    # Find all result files\n",
    "    json_files = glob.glob('results/*.json') + glob.glob('figures/*.json')\n",
    "    png_files = glob.glob('results/*.png') + glob.glob('figures/*.png')\n",
    "    all_files = json_files + png_files\n",
    "    \n",
    "    if not all_files:\n",
    "        print('WARNING: No result files found!')\n",
    "        return\n",
    "    \n",
    "    print(f'Found {len(all_files)} files')\n",
    "    \n",
    "    # Download as ZIP\n",
    "    import os\n",
    "    zip_name = f'E11_results_{os.path.basename(os.getcwd())}'\n",
    "    \n",
    "    # Create combined folder\n",
    "    os.makedirs('download_package', exist_ok=True)\n",
    "    for f in all_files:\n",
    "        shutil.copy(f, 'download_package/')\n",
    "    \n",
    "    shutil.make_archive(zip_name, 'zip', 'download_package')\n",
    "    print(f'Downloading: {zip_name}.zip')\n",
    "    files.download(f'{zip_name}.zip')\n",
    "    print('DOWNLOAD COMPLETE!')\n",
    "\n",
    "auto_download_results()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}