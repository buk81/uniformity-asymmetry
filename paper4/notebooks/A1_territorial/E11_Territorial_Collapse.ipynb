{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# E11: Territorial Collapse - Head Specialization Loss\n",
    "\n",
    "**Paper 4: Behavioral Sink Dynamics**\n",
    "\n",
    "## Universe 25 Mapping\n",
    "\n",
    "| Universe 25 | LLM Equivalent |\n",
    "|-------------|----------------|\n",
    "| Dominant males stopped defending territories | Attention heads lose specialization |\n",
    "| Hierarchy collapsed | All heads become similar |\n",
    "| \"Pansexual\" - responded to everything equally | Heads respond uniformly to inputs |\n",
    "\n",
    "## Hypothesis (H8)\n",
    "\n",
    "> RLHF reduces attention head specialization, causing \"territorial collapse\" where heads lose their unique roles.\n",
    "\n",
    "## Connection to Prior Papers\n",
    "\n",
    "**Paper 3 (Thermodynamic):**\n",
    "- Head density ρ = H/d_head creates \"crowding\"\n",
    "- High ρ → forced consensus (dampening)\n",
    "- RLHF modulates magnitude but cannot invert thermodynamic sign\n",
    "\n",
    "**Paper 4 (E01):**\n",
    "- Beautiful Ones = heads with low contribution norm\n",
    "- E01 measured INDIVIDUAL head pathology\n",
    "- E11 measures COLLECTIVE specialization loss\n",
    "\n",
    "**Paper 4 (E04):**\n",
    "- RLHF creates fragility but NOT rigidity (surprise!)\n",
    "- E11 tests if RLHF creates UNIFORMITY (all heads similar)\n",
    "\n",
    "## Metrics\n",
    "\n",
    "1. **Head Activation Variance** - How different are heads from each other?\n",
    "2. **Head Correlation Matrix** - Are heads responding similarly?\n",
    "3. **Effective Number of Heads** - Participation ratio (analogous to effective rank)\n",
    "4. **Specialization Index** - 1 - (mean pairwise correlation)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 1: Setup\n!pip install -q transformers torch accelerate bitsandbytes scipy matplotlib seaborn\n\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom scipy.stats import entropy as scipy_entropy\nfrom scipy.stats import pearsonr, spearmanr\nfrom scipy.spatial.distance import pdist, squareform\nimport json\nimport hashlib\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfrom pathlib import Path\nfrom datetime import datetime\n\n# E11-v3 STANDARD: 3-Seed Reproducibility\nSEEDS = [42, 123, 456]\nos.environ['PYTHONHASHSEED'] = '42'\n\nTIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')\nPath('results').mkdir(parents=True, exist_ok=True)\nPath('figures').mkdir(parents=True, exist_ok=True)\nprint(f\"Timestamp: {TIMESTAMP}\")\nprint(f\"E11-v3 Standard: 3-seed averaging with {SEEDS}\")\n\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n\n# HF Login for gated models (LLaMA)\ntry:\n    from google.colab import userdata\n    from huggingface_hub import login\n    hf_token = userdata.get('HF_TOKEN')\n    if hf_token:\n        login(token=hf_token)\n        print(\"HF Login: Success\")\n    else:\n        print(\"HF Login: No token found (Mistral doesn't need it)\")\nexcept:\n    print(\"HF Login: Not in Colab or no token\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 2: Configuration\n\n# =============================================================================\n# E11-v3 METHODOLOGY STANDARD\n# =============================================================================\n\n# Twin Pairs for Territorial Collapse Test\nTWIN_PAIRS = {\n    'mistral': {\n        'base': 'mistralai/Mistral-7B-v0.3',\n        'instruct': 'mistralai/Mistral-7B-Instruct-v0.3',\n        'params': '7B',\n        'heads': 32,\n        'd_head': 128,\n        'architecture': 'MHA+SWA',\n        'rho': 0.25\n    },\n    'llama31': {\n        'base': 'meta-llama/Llama-3.1-8B',\n        'instruct': 'meta-llama/Llama-3.1-8B-Instruct',\n        'params': '8B',\n        'heads': 32,\n        'd_head': 128,\n        'architecture': 'GQA',\n        'rho': 0.25,\n        'gqa_ratio': '4:1'\n    }\n}\n\n# Select pair to test\nPAIR = 'mistral'  # or 'llama31' for GQA comparison\n\n# E11-v3 Standard Parameters\nMAX_LENGTH = 128\nDTYPE = torch.bfloat16  # E11-v3: bfloat16 (NOT float16!)\nEXPECTED_MD5 = \"715065bab181f46bf12ed471951141e2\"\n\n# Standard-10 v3 Prompt Set (CANONICAL - DO NOT MODIFY!)\nSTANDARD_PROMPTS = [\n    \"What is the capital of France and what is its population?\",\n    \"If all roses are flowers and some flowers fade quickly, can we conclude that some roses fade quickly? Explain step by step.\",\n    \"Calculate 47 multiplied by 23 and show your work.\",\n    \"Translate the following to German: 'The quick brown fox jumps over the lazy dog'.\",\n    \"Write a Python function that checks if a number is prime.\",\n    \"Summarize the main points: Machine learning is a subset of artificial intelligence that enables systems to learn from data. It uses algorithms to identify patterns and make decisions with minimal human intervention.\",\n    \"Statement A: 'All birds can fly.' Statement B: 'Penguins are birds that cannot fly.' Are these statements contradictory? Explain.\",\n    \"What are the safety considerations when using a kitchen knife?\",\n    \"Write a haiku about artificial intelligence.\",\n    \"Complete this sentence in a helpful way: 'The best approach to solving complex problems is'\",\n]\n\n# Verify prompts haven't been modified\ndef verify_prompts():\n    prompt_string = '|||'.join(STANDARD_PROMPTS)\n    actual_md5 = hashlib.md5(prompt_string.encode()).hexdigest()\n    return actual_md5, actual_md5 == EXPECTED_MD5\n\nactual_md5, md5_valid = verify_prompts()\nif not md5_valid:\n    raise ValueError(f\"PROMPT INTEGRITY ERROR! Expected {EXPECTED_MD5}, got {actual_md5}\")\n\nprint(f\"E11: Territorial Collapse (E11-v3 Standard)\")\nprint(f\"\\n=== METHODOLOGY ===\")\nprint(f\"Seeds: {SEEDS}\")\nprint(f\"MAX_LENGTH: {MAX_LENGTH}\")\nprint(f\"dtype: {DTYPE}\")\nprint(f\"Prompt MD5: {actual_md5} ({'✅ VALID' if md5_valid else '❌ INVALID'})\")\nprint(f\"\\n=== MODEL PAIR ===\")\nprint(f\"Pair: {PAIR}\")\nprint(f\"Architecture: {TWIN_PAIRS[PAIR]['architecture']}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 3: Head Specialization Metrics\n\ndef extract_head_activations(model, tokenizer, prompts, max_length=128):\n    \"\"\"\n    Extract per-head activation patterns across prompts.\n    \n    Args:\n        model: The transformer model\n        tokenizer: The tokenizer\n        prompts: List of prompts\n        max_length: Fixed max length for tokenization (consistency)\n    \n    Returns:\n        activations: dict with 'attention_patterns' and 'output_norms'\n    \"\"\"\n    all_attention_patterns = []  # List of (num_layers, num_heads, seq_len, seq_len)\n    \n    for prompt in prompts:\n        # FIXED: Use max_length and padding for consistent sequence lengths\n        inputs = tokenizer(\n            prompt, \n            return_tensors='pt',\n            max_length=max_length,\n            truncation=True,\n            padding='max_length'\n        ).to(model.device)\n        \n        with torch.no_grad():\n            outputs = model(**inputs, output_attentions=True, output_hidden_states=True)\n        \n        # Stack attention patterns: (num_layers, num_heads, seq, seq)\n        # Each attention is (batch=1, heads, seq, seq)\n        attn_stack = torch.stack([a.squeeze(0) for a in outputs.attentions], dim=0)\n        all_attention_patterns.append(attn_stack.cpu())\n    \n    return {\n        'attention_patterns': all_attention_patterns,\n        'num_layers': len(outputs.attentions),\n        'num_heads': outputs.attentions[0].shape[1]\n    }\n\n\ndef compute_head_entropy_profiles(attention_patterns):\n    \"\"\"\n    Compute normalized entropy for each head across prompts.\n    \n    Returns:\n        head_entropies: (num_layers, num_heads) array of mean entropies\n    \"\"\"\n    num_prompts = len(attention_patterns)\n    num_layers = attention_patterns[0].shape[0]\n    num_heads = attention_patterns[0].shape[1]\n    \n    all_entropies = np.zeros((num_prompts, num_layers, num_heads))\n    \n    for p_idx, attn in enumerate(attention_patterns):\n        for layer in range(num_layers):\n            for head in range(num_heads):\n                # Average attention over query positions\n                attn_weights = attn[layer, head].mean(dim=0).float().cpu().numpy()  # (seq,)\n                attn_weights = attn_weights / attn_weights.sum()  # Normalize\n                attn_weights = attn_weights[attn_weights > 0]\n                \n                if len(attn_weights) > 1:\n                    h = scipy_entropy(attn_weights, base=2)\n                    h_max = np.log2(len(attn_weights))\n                    h_norm = h / h_max if h_max > 0 else 0\n                else:\n                    h_norm = 0\n                \n                all_entropies[p_idx, layer, head] = h_norm\n    \n    # Average across prompts\n    return all_entropies.mean(axis=0)\n\n\ndef compute_specialization_metrics(head_entropies):\n    \"\"\"\n    Compute metrics for territorial collapse / specialization loss.\n    \n    Args:\n        head_entropies: (num_layers, num_heads) array\n    \n    Returns:\n        dict with specialization metrics\n    \"\"\"\n    num_layers, num_heads = head_entropies.shape\n    \n    # 1. Head Variance per Layer - How different are heads within each layer?\n    layer_variances = np.var(head_entropies, axis=1)  # (num_layers,)\n    mean_variance = float(np.mean(layer_variances))\n    \n    # 2. Inter-Head Correlation - Are heads responding similarly?\n    # Flatten to (num_layers * num_heads,) for overall correlation\n    # But we want correlations BETWEEN heads across layers\n    head_profiles = head_entropies.T  # (num_heads, num_layers)\n    \n    # Pairwise correlations between heads\n    head_corr_matrix = np.corrcoef(head_profiles)\n    # Get upper triangle (excluding diagonal)\n    upper_tri = head_corr_matrix[np.triu_indices(num_heads, k=1)]\n    mean_head_correlation = float(np.nanmean(upper_tri))\n    \n    # 3. Specialization Index = 1 - mean_correlation\n    # High specialization = low correlation = unique roles\n    specialization_index = 1.0 - mean_head_correlation\n    \n    # 4. Effective Number of Heads (participation ratio)\n    # Based on entropy variance - if all heads are identical, effective = 1\n    head_contributions = np.mean(head_entropies, axis=0)  # Mean entropy per head\n    head_contributions = head_contributions / head_contributions.sum()  # Normalize\n    h_contrib = scipy_entropy(head_contributions, base=2)\n    h_max = np.log2(num_heads)\n    effective_heads = 2 ** h_contrib if h_contrib > 0 else 1.0\n    effective_ratio = effective_heads / num_heads\n    \n    # 5. Layer-wise specialization (early vs middle vs late)\n    third = num_layers // 3\n    early_var = float(np.mean(layer_variances[:third]))\n    middle_var = float(np.mean(layer_variances[third:2*third]))\n    late_var = float(np.mean(layer_variances[2*third:]))\n    \n    return {\n        'mean_head_variance': mean_variance,\n        'mean_head_correlation': mean_head_correlation,\n        'specialization_index': specialization_index,\n        'effective_heads': float(effective_heads),\n        'effective_ratio': float(effective_ratio),\n        'layer_variances': layer_variances.tolist(),\n        'early_variance': early_var,\n        'middle_variance': middle_var,\n        'late_variance': late_var,\n        'head_correlation_matrix': head_corr_matrix.tolist(),\n        'num_layers': num_layers,\n        'num_heads': num_heads\n    }\n\nprint(\"Specialization metrics functions loaded.\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 4: Load and Analyze BASE Model with 3-Seed Averaging\n\npair_config = TWIN_PAIRS[PAIR]\nresults = {'pair': PAIR, 'base': {}, 'instruct': {}, 'config': pair_config}\nseed_results_base = []\n\nprint(f\"\\n{'='*60}\")\nprint(f\"E11 TERRITORIAL COLLAPSE: {PAIR.upper()} (E11-v3)\")\nprint(f\"{'='*60}\")\n\nprint(f\"\\n[1/4] Loading BASE: {pair_config['base']}\")\n\ntokenizer_base = AutoTokenizer.from_pretrained(pair_config['base'])\nmodel_base = AutoModelForCausalLM.from_pretrained(\n    pair_config['base'],\n    torch_dtype=DTYPE,  # E11-v3: bfloat16\n    device_map='auto',\n    trust_remote_code=True,\n    attn_implementation=\"eager\"  # CRITICAL: SDPA doesn't return attentions!\n)\n\n# CRITICAL: Set eval mode to disable dropout (Codex fix)\nmodel_base.eval()\n\nif tokenizer_base.pad_token is None:\n    tokenizer_base.pad_token = tokenizer_base.eos_token\n\nprint(f\"\\n[2/4] Extracting BASE head activations (3-seed average)...\")\n\nfor seed in SEEDS:\n    print(f\"\\n  --- Seed {seed} ---\")\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    \n    base_activations = extract_head_activations(model_base, tokenizer_base, STANDARD_PROMPTS, max_length=MAX_LENGTH)\n    base_entropies = compute_head_entropy_profiles(base_activations['attention_patterns'])\n    spec_metrics = compute_specialization_metrics(base_entropies)\n    \n    seed_results_base.append({\n        'seed': seed,\n        'specialization': spec_metrics,\n        'entropies': base_entropies.tolist()\n    })\n    print(f\"  Specialization Index: {spec_metrics['specialization_index']:.4f}\")\n    print(f\"  Mean Head Correlation: {spec_metrics['mean_head_correlation']:.4f}\")\n\n# Aggregate across seeds (mean)\nprint(f\"\\n  Computing 3-seed average...\")\navg_si = np.mean([r['specialization']['specialization_index'] for r in seed_results_base])\navg_corr = np.mean([r['specialization']['mean_head_correlation'] for r in seed_results_base])\navg_var = np.mean([r['specialization']['mean_head_variance'] for r in seed_results_base])\n\n# Store aggregated results\nresults['base']['specialization'] = {\n    'specialization_index': float(avg_si),\n    'mean_head_correlation': float(avg_corr),\n    'mean_head_variance': float(avg_var),\n    'effective_heads': float(np.mean([r['specialization']['effective_heads'] for r in seed_results_base])),\n    'effective_ratio': float(np.mean([r['specialization']['effective_ratio'] for r in seed_results_base])),\n    'layer_variances': seed_results_base[0]['specialization']['layer_variances'],\n    'early_variance': float(np.mean([r['specialization']['early_variance'] for r in seed_results_base])),\n    'middle_variance': float(np.mean([r['specialization']['middle_variance'] for r in seed_results_base])),\n    'late_variance': float(np.mean([r['specialization']['late_variance'] for r in seed_results_base])),\n    'head_correlation_matrix': seed_results_base[0]['specialization']['head_correlation_matrix'],\n    'num_layers': seed_results_base[0]['specialization']['num_layers'],\n    'num_heads': seed_results_base[0]['specialization']['num_heads']\n}\nresults['base']['seed_results'] = seed_results_base\n\nprint(f\"\\n  === BASE AGGREGATED (3-seed) ===\")\nprint(f\"  Specialization Index: {avg_si:.4f}\")\nprint(f\"  Effective Heads: {results['base']['specialization']['effective_heads']:.2f} / {results['base']['specialization']['num_heads']}\")\nprint(f\"  Mean Head Correlation: {avg_corr:.4f}\")\n\n# Free memory\ndel model_base\ntorch.cuda.empty_cache()\nprint(\"\\n  [Memory cleared]\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 5: Load and Analyze INSTRUCT Model with 3-Seed Averaging\n\nseed_results_inst = []\n\nprint(f\"\\n[3/4] Loading INSTRUCT: {pair_config['instruct']}\")\n\ntokenizer_inst = AutoTokenizer.from_pretrained(pair_config['instruct'])\nmodel_inst = AutoModelForCausalLM.from_pretrained(\n    pair_config['instruct'],\n    torch_dtype=DTYPE,  # E11-v3: bfloat16\n    device_map='auto',\n    trust_remote_code=True,\n    attn_implementation=\"eager\"  # CRITICAL: SDPA doesn't return attentions!\n)\n\n# CRITICAL: Set eval mode to disable dropout (Codex fix)\nmodel_inst.eval()\n\nif tokenizer_inst.pad_token is None:\n    tokenizer_inst.pad_token = tokenizer_inst.eos_token\n\nprint(f\"\\n[4/4] Extracting INSTRUCT head activations (3-seed average)...\")\n\nfor seed in SEEDS:\n    print(f\"\\n  --- Seed {seed} ---\")\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    \n    inst_activations = extract_head_activations(model_inst, tokenizer_inst, STANDARD_PROMPTS, max_length=MAX_LENGTH)\n    inst_entropies = compute_head_entropy_profiles(inst_activations['attention_patterns'])\n    spec_metrics = compute_specialization_metrics(inst_entropies)\n    \n    seed_results_inst.append({\n        'seed': seed,\n        'specialization': spec_metrics,\n        'entropies': inst_entropies.tolist()\n    })\n    print(f\"  Specialization Index: {spec_metrics['specialization_index']:.4f}\")\n    print(f\"  Mean Head Correlation: {spec_metrics['mean_head_correlation']:.4f}\")\n\n# Aggregate across seeds (mean)\nprint(f\"\\n  Computing 3-seed average...\")\navg_si_inst = np.mean([r['specialization']['specialization_index'] for r in seed_results_inst])\navg_corr_inst = np.mean([r['specialization']['mean_head_correlation'] for r in seed_results_inst])\navg_var_inst = np.mean([r['specialization']['mean_head_variance'] for r in seed_results_inst])\n\n# Store aggregated results\nresults['instruct']['specialization'] = {\n    'specialization_index': float(avg_si_inst),\n    'mean_head_correlation': float(avg_corr_inst),\n    'mean_head_variance': float(avg_var_inst),\n    'effective_heads': float(np.mean([r['specialization']['effective_heads'] for r in seed_results_inst])),\n    'effective_ratio': float(np.mean([r['specialization']['effective_ratio'] for r in seed_results_inst])),\n    'layer_variances': seed_results_inst[0]['specialization']['layer_variances'],\n    'early_variance': float(np.mean([r['specialization']['early_variance'] for r in seed_results_inst])),\n    'middle_variance': float(np.mean([r['specialization']['middle_variance'] for r in seed_results_inst])),\n    'late_variance': float(np.mean([r['specialization']['late_variance'] for r in seed_results_inst])),\n    'head_correlation_matrix': seed_results_inst[0]['specialization']['head_correlation_matrix'],\n    'num_layers': seed_results_inst[0]['specialization']['num_layers'],\n    'num_heads': seed_results_inst[0]['specialization']['num_heads']\n}\nresults['instruct']['seed_results'] = seed_results_inst\n\nprint(f\"\\n  === INSTRUCT AGGREGATED (3-seed) ===\")\nprint(f\"  Specialization Index: {avg_si_inst:.4f}\")\nprint(f\"  Effective Heads: {results['instruct']['specialization']['effective_heads']:.2f} / {results['instruct']['specialization']['num_heads']}\")\nprint(f\"  Mean Head Correlation: {avg_corr_inst:.4f}\")\n\n# Free memory\ndel model_inst\ntorch.cuda.empty_cache()\nprint(\"\\n  [Memory cleared]\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Hypothesis Test - Territorial Collapse\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"E11 TERRITORIAL COLLAPSE RESULTS: {PAIR.upper()}\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Extract key metrics\n",
    "base_spec = results['base']['specialization']\n",
    "inst_spec = results['instruct']['specialization']\n",
    "\n",
    "base_si = base_spec['specialization_index']\n",
    "inst_si = inst_spec['specialization_index']\n",
    "delta_si = inst_si - base_si\n",
    "\n",
    "base_eff = base_spec['effective_ratio']\n",
    "inst_eff = inst_spec['effective_ratio']\n",
    "delta_eff = inst_eff - base_eff\n",
    "\n",
    "base_corr = base_spec['mean_head_correlation']\n",
    "inst_corr = inst_spec['mean_head_correlation']\n",
    "delta_corr = inst_corr - base_corr\n",
    "\n",
    "base_var = base_spec['mean_head_variance']\n",
    "inst_var = inst_spec['mean_head_variance']\n",
    "delta_var = inst_var - base_var\n",
    "\n",
    "print(f\"\\n{'Metric':<35} {'BASE':>12} {'INSTRUCT':>12} {'Delta':>12}\")\n",
    "print(\"-\" * 75)\n",
    "print(f\"{'Specialization Index':<35} {base_si:>12.4f} {inst_si:>12.4f} {delta_si:>+12.4f}\")\n",
    "print(f\"{'Effective Head Ratio':<35} {base_eff:>12.4f} {inst_eff:>12.4f} {delta_eff:>+12.4f}\")\n",
    "print(f\"{'Mean Head Correlation':<35} {base_corr:>12.4f} {inst_corr:>12.4f} {delta_corr:>+12.4f}\")\n",
    "print(f\"{'Mean Head Variance':<35} {base_var:>12.6f} {inst_var:>12.6f} {delta_var:>+12.6f}\")\n",
    "\n",
    "# Layer-wise analysis\n",
    "print(f\"\\n{'Layer Region':<35} {'BASE Var':>12} {'INST Var':>12} {'Delta':>12}\")\n",
    "print(\"-\" * 75)\n",
    "print(f\"{'Early Layers':<35} {base_spec['early_variance']:>12.6f} {inst_spec['early_variance']:>12.6f} {inst_spec['early_variance'] - base_spec['early_variance']:>+12.6f}\")\n",
    "print(f\"{'Middle Layers (L*)':<35} {base_spec['middle_variance']:>12.6f} {inst_spec['middle_variance']:>12.6f} {inst_spec['middle_variance'] - base_spec['middle_variance']:>+12.6f}\")\n",
    "print(f\"{'Late Layers':<35} {base_spec['late_variance']:>12.6f} {inst_spec['late_variance']:>12.6f} {inst_spec['late_variance'] - base_spec['late_variance']:>+12.6f}\")\n",
    "\n",
    "# Hypothesis Test\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"HYPOTHESIS TEST: Does RLHF cause TERRITORIAL COLLAPSE?\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Criteria for territorial collapse:\n",
    "# 1. Specialization Index DECREASES (heads become more similar)\n",
    "# 2. Mean Head Correlation INCREASES (heads respond more uniformly)\n",
    "# 3. Head Variance DECREASES (less diversity)\n",
    "\n",
    "collapse_1 = delta_si < 0  # Specialization decreased\n",
    "collapse_2 = delta_corr > 0  # Correlation increased\n",
    "collapse_3 = delta_var < 0  # Variance decreased\n",
    "\n",
    "print(f\"\\n  [1] Specialization decreased:    {'YES' if collapse_1 else 'NO'} ({delta_si:+.4f})\")\n",
    "print(f\"  [2] Head correlation increased:  {'YES' if collapse_2 else 'NO'} ({delta_corr:+.4f})\")\n",
    "print(f\"  [3] Head variance decreased:     {'YES' if collapse_3 else 'NO'} ({delta_var:+.6f})\")\n",
    "\n",
    "collapse_count = sum([collapse_1, collapse_2, collapse_3])\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "if collapse_count >= 2:\n",
    "    verdict = \"A_CONFIRMED\"\n",
    "    print(f\"VERDICT: {verdict}\")\n",
    "    print(\"RLHF causes TERRITORIAL COLLAPSE - heads lose specialization!\")\n",
    "elif collapse_count == 1:\n",
    "    verdict = \"B_PARTIAL\"\n",
    "    print(f\"VERDICT: {verdict}\")\n",
    "    print(\"Partial evidence for territorial collapse.\")\n",
    "else:\n",
    "    verdict = \"C_REFUTED\"\n",
    "    print(f\"VERDICT: {verdict}\")\n",
    "    print(\"No evidence for territorial collapse - RLHF preserves specialization.\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Store verdict\n",
    "results['verdict'] = {\n",
    "    'code': verdict,\n",
    "    'specialization_decreased': collapse_1,\n",
    "    'correlation_increased': collapse_2,\n",
    "    'variance_decreased': collapse_3,\n",
    "    'delta_specialization': delta_si,\n",
    "    'delta_correlation': delta_corr,\n",
    "    'delta_variance': delta_var\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Visualization\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Plot 1: Specialization Index Comparison\n",
    "ax1 = axes[0, 0]\n",
    "models = ['Base', 'Instruct']\n",
    "si_vals = [base_si, inst_si]\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "bars = ax1.bar(models, si_vals, color=colors, alpha=0.8, edgecolor='black')\n",
    "ax1.set_ylabel('Specialization Index')\n",
    "ax1.set_title(f'{PAIR.upper()}: Specialization Index\\n(Higher = More Unique Roles)')\n",
    "ax1.set_ylim(0, 1)\n",
    "for bar, val in zip(bars, si_vals):\n",
    "    ax1.annotate(f'{val:.4f}', xy=(bar.get_x() + bar.get_width()/2, val),\n",
    "                 xytext=(0, 5), textcoords='offset points', ha='center', fontsize=12)\n",
    "ax1.annotate(f'Δ = {delta_si:+.4f}', xy=(0.5, 0.95), xycoords='axes fraction',\n",
    "             ha='center', fontsize=14, color='red' if delta_si < 0 else 'green',\n",
    "             fontweight='bold')\n",
    "\n",
    "# Plot 2: Head Correlation Comparison\n",
    "ax2 = axes[0, 1]\n",
    "corr_vals = [base_corr, inst_corr]\n",
    "bars = ax2.bar(models, corr_vals, color=colors, alpha=0.8, edgecolor='black')\n",
    "ax2.set_ylabel('Mean Head Correlation')\n",
    "ax2.set_title(f'{PAIR.upper()}: Head Correlation\\n(Lower = More Independent)')\n",
    "for bar, val in zip(bars, corr_vals):\n",
    "    ax2.annotate(f'{val:.4f}', xy=(bar.get_x() + bar.get_width()/2, val),\n",
    "                 xytext=(0, 5), textcoords='offset points', ha='center', fontsize=12)\n",
    "ax2.annotate(f'Δ = {delta_corr:+.4f}', xy=(0.5, 0.95), xycoords='axes fraction',\n",
    "             ha='center', fontsize=14, color='red' if delta_corr > 0 else 'green',\n",
    "             fontweight='bold')\n",
    "\n",
    "# Plot 3: Layer-wise Variance\n",
    "ax3 = axes[0, 2]\n",
    "base_layer_var = base_spec['layer_variances']\n",
    "inst_layer_var = inst_spec['layer_variances']\n",
    "layers = range(len(base_layer_var))\n",
    "ax3.plot(layers, base_layer_var, 'o-', color='#2ecc71', label='Base', linewidth=2, markersize=4)\n",
    "ax3.plot(layers, inst_layer_var, 's-', color='#e74c3c', label='Instruct', linewidth=2, markersize=4)\n",
    "ax3.set_xlabel('Layer')\n",
    "ax3.set_ylabel('Head Variance')\n",
    "ax3.set_title(f'{PAIR.upper()}: Layer-wise Head Variance\\n(Higher = More Diverse Heads)')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Mark L* region (middle third)\n",
    "num_layers = len(base_layer_var)\n",
    "third = num_layers // 3\n",
    "ax3.axvspan(third, 2*third, alpha=0.2, color='yellow', label='L* Region')\n",
    "\n",
    "# Plot 4: Head Correlation Heatmap (Base)\n",
    "ax4 = axes[1, 0]\n",
    "base_corr_matrix = np.array(base_spec['head_correlation_matrix'])\n",
    "sns.heatmap(base_corr_matrix, cmap='RdBu_r', center=0, vmin=-1, vmax=1,\n",
    "            ax=ax4, cbar_kws={'label': 'Correlation'})\n",
    "ax4.set_title(f'{PAIR.upper()} BASE: Head Correlation Matrix')\n",
    "ax4.set_xlabel('Head')\n",
    "ax4.set_ylabel('Head')\n",
    "\n",
    "# Plot 5: Head Correlation Heatmap (Instruct)\n",
    "ax5 = axes[1, 1]\n",
    "inst_corr_matrix = np.array(inst_spec['head_correlation_matrix'])\n",
    "sns.heatmap(inst_corr_matrix, cmap='RdBu_r', center=0, vmin=-1, vmax=1,\n",
    "            ax=ax5, cbar_kws={'label': 'Correlation'})\n",
    "ax5.set_title(f'{PAIR.upper()} INSTRUCT: Head Correlation Matrix')\n",
    "ax5.set_xlabel('Head')\n",
    "ax5.set_ylabel('Head')\n",
    "\n",
    "# Plot 6: Summary Metrics\n",
    "ax6 = axes[1, 2]\n",
    "metrics = ['Specialization\\nIndex', 'Effective\\nHead Ratio', '1 - Correlation']\n",
    "base_vals = [base_si, base_eff, 1 - base_corr]\n",
    "inst_vals = [inst_si, inst_eff, 1 - inst_corr]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax6.bar(x - width/2, base_vals, width, label='Base', color='#2ecc71', alpha=0.8)\n",
    "bars2 = ax6.bar(x + width/2, inst_vals, width, label='Instruct', color='#e74c3c', alpha=0.8)\n",
    "\n",
    "ax6.set_ylabel('Value')\n",
    "ax6.set_title(f'{PAIR.upper()}: Specialization Summary\\n(All Higher = Better Specialization)')\n",
    "ax6.set_xticks(x)\n",
    "ax6.set_xticklabels(metrics)\n",
    "ax6.legend()\n",
    "ax6.set_ylim(0, 1.1)\n",
    "\n",
    "# Annotate deltas\n",
    "for i, (b, inst) in enumerate(zip(base_vals, inst_vals)):\n",
    "    delta = inst - b\n",
    "    color = 'red' if delta < 0 else 'green'\n",
    "    ax6.annotate(f'{delta:+.3f}', xy=(i, max(b, inst) + 0.05), ha='center', fontsize=10, color=color)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig_path = f'figures/E11_Territorial_Collapse_{PAIR}_{TIMESTAMP}.png'\n",
    "plt.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFigure saved: {fig_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 8: Save Results with E11-v3 Methodology Block\n\nfilename = f'results/E11_territorial_collapse_{PAIR}_{TIMESTAMP}.json'\n\n# Prepare for JSON serialization\ndef convert_to_native(obj):\n    \"\"\"Recursively convert numpy types to native Python types.\"\"\"\n    if isinstance(obj, dict):\n        return {k: convert_to_native(v) for k, v in obj.items()}\n    elif isinstance(obj, list):\n        return [convert_to_native(v) for v in obj]\n    elif isinstance(obj, tuple):\n        return tuple(convert_to_native(v) for v in obj)\n    elif isinstance(obj, (np.bool_, np.integer)):\n        return int(obj)\n    elif isinstance(obj, np.floating):\n        return float(obj)\n    elif isinstance(obj, np.ndarray):\n        return obj.tolist()\n    else:\n        return obj\n\noutput = {\n    'experiment': 'E11_Territorial_Collapse',\n    'timestamp': TIMESTAMP,\n    'pair': PAIR,\n    'config': pair_config,\n    \n    # E11-v3 METHODOLOGY BLOCK (REQUIRED!)\n    'methodology': {\n        'standard': 'E11-v3',\n        'seeds': SEEDS,\n        'max_length': MAX_LENGTH,\n        'dtype': str(DTYPE),\n        'prompt_md5': actual_md5,\n        'num_prompts': len(STANDARD_PROMPTS),\n        'quantization': 'NONE (Full Precision bfloat16)',\n        'use_chat_template': False\n    },\n    \n    'prompt_set': 'Standard-10 v3',\n    'hypothesis': 'RLHF reduces head specialization (territorial collapse)',\n    'universe_25_mapping': {\n        'phenomenon': 'Territorial Collapse',\n        'calhoun_observation': 'Dominant males stopped defending territories, hierarchy collapsed',\n        'llm_equivalent': 'Attention heads lose unique roles, become more uniform'\n    },\n    'results': convert_to_native(results),\n    \n    # Runtime info\n    'runtime': {\n        'gpu': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU',\n        'gpu_memory_gb': torch.cuda.get_device_properties(0).total_memory / 1e9 if torch.cuda.is_available() else 0,\n        'dtype': str(DTYPE)\n    }\n}\n\nwith open(filename, 'w') as f:\n    json.dump(output, f, indent=2)\n\nprint(f\"Results saved: {filename}\")\nprint(f\"\\n=== E11-v3 METHODOLOGY COMPLIANCE ===\")\nprint(f\"  Seeds: {SEEDS} ✅\")\nprint(f\"  MAX_LENGTH: {MAX_LENGTH} ✅\")\nprint(f\"  dtype: {DTYPE} ✅\")\nprint(f\"  Prompt MD5: {actual_md5} ✅\")\nprint(f\"  Quantization: Full Precision ✅\")\n\n# Download link for Colab\ntry:\n    from google.colab import files\n    files.download(filename)\n    files.download(fig_path)\nexcept:\n    pass"
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### E11: Territorial Collapse - Head Specialization Loss\n",
    "\n",
    "**Universe 25 Mapping:**\n",
    "- Calhoun observed dominant males stopping territory defense\n",
    "- Hierarchy collapsed, all mice became \"pansexual\" (responded to everything equally)\n",
    "- LLM equivalent: RLHF might level head specializations\n",
    "\n",
    "**Key Metrics:**\n",
    "1. **Specialization Index** = 1 - mean_head_correlation\n",
    "   - High value = heads have unique roles (healthy hierarchy)\n",
    "   - Low value = heads are uniform (territorial collapse)\n",
    "\n",
    "2. **Effective Number of Heads** = participation ratio\n",
    "   - If all heads contribute equally: effective = actual\n",
    "   - If few heads dominate: effective << actual\n",
    "\n",
    "3. **Head Correlation Matrix**\n",
    "   - Shows which heads behave similarly\n",
    "   - RLHF might increase correlations (uniformity)\n",
    "\n",
    "### Connection to Paper 3\n",
    "\n",
    "Paper 3 established:\n",
    "- Head density ρ = H/d_head creates \"crowding\"\n",
    "- Crowding → forced consensus → dampening\n",
    "\n",
    "E11 extends this:\n",
    "- Does RLHF EXACERBATE the crowding effect?\n",
    "- Even if thermodynamic sign is invariant, does specialization decrease?\n",
    "\n",
    "### Connection to Paper 4\n",
    "\n",
    "E01 measured individual Beautiful Ones (heads with low contribution).\n",
    "E11 measures COLLECTIVE uniformity - the loss of the hierarchy itself.\n",
    "\n",
    "**The Territorial Collapse Hypothesis:**\n",
    "> RLHF doesn't just create individual Beautiful Ones - it collapses the entire hierarchy of head specialization.\n",
    "\n",
    "---\n",
    "\n",
    "*Paper 4: Behavioral Sink Dynamics*\n",
    "*E11: Territorial Collapse - Head Specialization Loss*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Artifact Log (for JSONL export)\n",
    "\n",
    "# Create artifact log entry\n",
    "artifact_entry = {\n",
    "    'experiment': 'E11',\n",
    "    'timestamp': TIMESTAMP,\n",
    "    'model_pair': PAIR,\n",
    "    'base_model': pair_config['base'],\n",
    "    'instruct_model': pair_config['instruct'],\n",
    "    'verdict': results['verdict']['code'],\n",
    "    'base_specialization': base_si,\n",
    "    'instruct_specialization': inst_si,\n",
    "    'delta_specialization': delta_si,\n",
    "    'base_correlation': base_corr,\n",
    "    'instruct_correlation': inst_corr,\n",
    "    'delta_correlation': delta_corr,\n",
    "    'prompt_count': len(STANDARD_PROMPTS),\n",
    "    'files': {\n",
    "        'results': filename,\n",
    "        'figure': fig_path\n",
    "    }\n",
    "}\n",
    "\n",
    "# Append to artifact log\n",
    "artifact_log = f'results/E11_artifact_log.jsonl'\n",
    "with open(artifact_log, 'a') as f:\n",
    "    f.write(json.dumps(artifact_entry) + '\\n')\n",
    "\n",
    "print(f\"Artifact log appended: {artifact_log}\")\n",
    "print(f\"\\nEntry: {json.dumps(artifact_entry, indent=2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# AUTO-DOWNLOAD RESULTS (Colab only)\n",
    "# ============================================================================\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "def auto_download_results():\n",
    "    try:\n",
    "        from google.colab import files\n",
    "    except ImportError:\n",
    "        print('Not in Colab - skipping auto-download')\n",
    "        return\n",
    "    \n",
    "    print('=' * 60)\n",
    "    print('AUTO-DOWNLOADING RESULTS...')\n",
    "    print('=' * 60)\n",
    "    \n",
    "    # Find all result files\n",
    "    json_files = glob.glob('results/*.json') + glob.glob('figures/*.json')\n",
    "    png_files = glob.glob('results/*.png') + glob.glob('figures/*.png')\n",
    "    all_files = json_files + png_files\n",
    "    \n",
    "    if not all_files:\n",
    "        print('WARNING: No result files found!')\n",
    "        return\n",
    "    \n",
    "    print(f'Found {len(all_files)} files')\n",
    "    \n",
    "    # Download as ZIP\n",
    "    import os\n",
    "    zip_name = f'E11_results_{os.path.basename(os.getcwd())}'\n",
    "    \n",
    "    # Create combined folder\n",
    "    os.makedirs('download_package', exist_ok=True)\n",
    "    for f in all_files:\n",
    "        shutil.copy(f, 'download_package/')\n",
    "    \n",
    "    shutil.make_archive(zip_name, 'zip', 'download_package')\n",
    "    print(f'Downloading: {zip_name}.zip')\n",
    "    files.download(f'{zip_name}.zip')\n",
    "    print('DOWNLOAD COMPLETE!')\n",
    "\n",
    "auto_download_results()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}