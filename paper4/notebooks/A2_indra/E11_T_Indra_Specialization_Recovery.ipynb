{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# E11-T-Indra: Can Chaos Restore GQA Head Specialization?\n\n**Paper 4: Behavioral Sink Dynamics**\n\n## The Discovery (E11-T)\n\n| Model | Specialization Index | Delta |\n|-------|---------------------|-------|\n| LLaMA-3.1-8B Base | 0.7134 | - |\n| LLaMA-3.1-8B Instruct | 0.3115 | **-56.3%** |\n\nRLHF causes **massive territorial collapse** in GQA architecture:\n- Heads lose unique roles\n- Mean correlation jumps from 0.29 to 0.69 (+140%)\n- This is OPPOSITE of MHA (which gained +4.2% specialization)\n\n## The Question\n\n> **Can chaos injection (Indra) induce FUNCTIONAL specialization recovery in collapsed GQA models?**\n\n## The Hypothesis\n\nIf Indra affects functional specialization (not just behavioral fragility):\n- Chaos should increase Specialization Index under perturbation\n- Chaos should decrease Head Correlation under perturbation\n- Effect should be strongest in Middle layers (L* zone, layers 11-27)\n\n## Connection to Prior Work\n\n| Experiment | Finding |\n|------------|--------|\n| E06 | Chaos heals MHA behavioral fragility |\n| E06b | Surgical Indra: Middle layers are the target |\n| E06c | GQA (TinyLlama) is already antifragile behaviorally |\n| E06d-0 | LLaMA-3.1 L* = 22, Engine Room = layers 11-27 |\n| E11-T | GQA has MASSIVE functional collapse (-56% specialization) |\n\n**The Paradox:** GQA is behaviorally resilient but functionally collapsed.\n\n**E11-T-Indra asks:** Can perturbation induce functional specialization recovery?\n\n**Important Caveat:** This measures FUNCTIONAL recovery under perturbation, not permanent structural change. The noise alters activations, which may reveal latent specialization capacity.\n\n---"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Cell 1: Setup\n", "!pip install -q transformers torch accelerate bitsandbytes scipy matplotlib seaborn huggingface_hub\n", "\n", "import torch\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "from transformers import AutoModelForCausalLM, AutoTokenizer\n", "from scipy.stats import entropy as scipy_entropy\n", "import json\n", "import hashlib\n", "import warnings\n", "warnings.filterwarnings('ignore')\n", "\n", "import os\n", "from pathlib import Path\n", "from datetime import datetime\n", "\n", "# E11-v3 STANDARD: 3-Seed Reproducibility\n", "SEEDS = [42, 123, 456]\n", "os.environ['PYTHONHASHSEED'] = '42'\n", "\n", "TIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')\n", "Path('results').mkdir(parents=True, exist_ok=True)\n", "Path('figures').mkdir(parents=True, exist_ok=True)\n", "print(f\"Timestamp: {TIMESTAMP}\")\n", "print(f\"E11-v3 Standard: Seeds {SEEDS}\")\n", "\n", "print(f\"PyTorch: {torch.__version__}\")\n", "print(f\"CUDA available: {torch.cuda.is_available()}\")\n", "if torch.cuda.is_available():\n", "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n", "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n", "\n", "# HF Login for gated models (LLaMA) - REQUIRED!\n", "from huggingface_hub import login, HfFolder\n", "\n", "def get_hf_token():\n", "    token = None\n", "    try:\n", "        from google.colab import userdata\n", "        token = userdata.get('HF_TOKEN')\n", "    except Exception:\n", "        pass\n", "    if not token:\n", "        token = os.environ.get('HF_TOKEN') or os.environ.get('HUGGINGFACE_TOKEN') or os.environ.get('HUGGING_FACE_HUB_TOKEN')\n", "    if not token:\n", "        token = HfFolder.get_token()\n", "    return token\n", "\n", "HF_TOKEN = get_hf_token()\n", "if HF_TOKEN:\n", "    try:\n", "        login(token=HF_TOKEN)\n", "        print(\"HF Login: SUCCESS (required for gated models)\")\n", "    except Exception as e:\n", "        print(f\"HF Login failed: {e}\")\n", "else:\n", "    print(\"WARNING: No HF_TOKEN found! LLaMA requires authentication.\")\n", "    print(\"Colab: Runtime -> Secrets -> Add HF_TOKEN\")\n", "    print(\"Local: run `huggingface-cli login` or set HF_TOKEN env var\")\n", "\n", "TOKEN_KWARGS = {'token': HF_TOKEN} if HF_TOKEN else {}\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Cell 2: Configuration\n", "\n", "import hashlib\n", "\n", "# ==============================================================================\n", "# E11-v3 STANDARD PARAMETERS\n", "# ==============================================================================\n", "MAX_LENGTH = 128\n", "DTYPE = torch.bfloat16  # E11-v3: bfloat16 (NOT float16!)\n", "USE_CHAT_TEMPLATE = True  # E11-v3: chat template for Instruct models\n", "EXPECTED_MD5 = \"715065bab181f46bf12ed471951141e2\"\n", "\n", "# Model Configuration\n", "MODEL_CONFIG = {\n", "    'name': 'meta-llama/Llama-3.1-8B-Instruct',\n", "    'display': 'LLaMA-3.1-8B-Instruct (Collapsed GQA)',\n", "    'num_layers': 32,\n", "    'num_query_heads': 32,\n", "    'num_kv_heads': 8,\n", "    'd_head': 128,\n", "    'architecture': 'GQA'\n", "}\n", "\n", "# E11-T Reference Values (collapsed state)\n", "E11T_REFERENCE = {\n", "    'base_specialization': 0.7134,\n", "    'instruct_specialization': 0.3115,  # COLLAPSED!\n", "    'base_correlation': 0.2866,\n", "    'instruct_correlation': 0.6885,     # SYNCHRONIZED!\n", "    'delta_specialization': -0.4019\n", "}\n", "\n", "# Layer Ranges for Surgical Injection (from E06d-0 for LLaMA-3.1)\n", "# L* = 22, Engine Room = layers 11-27\n", "LAYER_RANGES = {\n", "    'early': (0, 11),      # Layers 0-10  (Pre-Engine)\n", "    'middle': (11, 28),    # Layers 11-27 (Engine Room per E06d-0)\n", "    'late': (28, 32),      # Layers 28-31 (Post-Engine)\n", "    'all': (0, 32)         # All layers\n", "}\n", "\n", "# Noise Levels to Test\n", "NOISE_LEVELS = [0.0, 0.01, 0.02, 0.05, 0.1, 0.2]\n", "\n", "# Standard-10 Prompt Set (canonical per NOTEBOOK_GUIDE.md \u00a79)\n", "STANDARD_PROMPTS = [\n", "    \"What is the capital of France and what is its population?\",\n", "    \"If all roses are flowers and some flowers fade quickly, can we conclude that some roses fade quickly? Explain step by step.\",\n", "    \"Calculate 47 multiplied by 23 and show your work.\",\n", "    \"Translate the following to German: 'The quick brown fox jumps over the lazy dog'.\",\n", "    \"Write a Python function that checks if a number is prime.\",\n", "    \"Summarize the main points: Machine learning is a subset of artificial intelligence that enables systems to learn from data. It uses algorithms to identify patterns and make decisions with minimal human intervention.\",\n", "    \"Statement A: 'All birds can fly.' Statement B: 'Penguins are birds that cannot fly.' Are these statements contradictory? Explain.\",\n", "    \"What are the safety considerations when using a kitchen knife?\",\n", "    \"Write a haiku about artificial intelligence.\",\n", "    \"Complete this sentence in a helpful way: 'The best approach to solving complex problems is'\",\n", "]\n", "\n", "# ==============================================================================\n", "# E11-v3 PROMPT VERIFICATION\n", "# ==============================================================================\n", "def verify_prompts():\n", "    \"\"\"Verify Standard-10 prompts haven't been modified.\"\"\"\n", "    prompt_string = '|||'.join(STANDARD_PROMPTS)\n", "    actual_md5 = hashlib.md5(prompt_string.encode()).hexdigest()\n", "    return actual_md5, actual_md5 == EXPECTED_MD5\n", "\n", "actual_md5, prompts_ok = verify_prompts()\n", "print(f\"E11-v3 Prompt Verification:\")\n", "print(f\"  Expected MD5: {EXPECTED_MD5}\")\n", "print(f\"  Actual MD5:   {actual_md5}\")\n", "print(f\"  Status:       {'\u2713 VERIFIED' if prompts_ok else '\u2717 MISMATCH - STOP!'}\")\n", "\n", "if not prompts_ok:\n", "    raise ValueError(f\"Prompt MD5 mismatch! Expected {EXPECTED_MD5}, got {actual_md5}\")\n", "\n", "print(f\"\\nE11-T-Indra: Specialization Recovery Test\")\n", "print(f\"\\nTarget: {MODEL_CONFIG['display']}\")\n", "print(f\"Current Specialization: {E11T_REFERENCE['instruct_specialization']:.4f} (COLLAPSED)\")\n", "print(f\"Target Specialization: {E11T_REFERENCE['base_specialization']:.4f} (Base level)\")\n", "print(f\"Recovery needed: {E11T_REFERENCE['delta_specialization']:+.4f}\")\n", "print(f\"\\nLayer Ranges (E06d-0 LLaMA-3.1 anatomy):\")\n", "for region, (start, end) in LAYER_RANGES.items():\n", "    print(f\"  {region}: layers {start}-{end-1}\")\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Cell 3: Specialization Metrics (from E11-T)\n", "\n", "def extract_head_activations_with_noise(model, tokenizer, prompts, noise_injector=None, max_length=128, use_chat_template=False):\n", "    \"\"\"\n", "    Extract per-head activation patterns, optionally with noise injection.\n", "    Uses attention_mask to avoid PAD bias in entropy.\n", "    \"\"\"\n", "    all_attention_patterns = []\n", "    all_attention_masks = []\n", "\n", "    for prompt in prompts:\n", "        formatted = prompt\n", "        if use_chat_template and hasattr(tokenizer, 'apply_chat_template'):\n", "            messages = [{\"role\": \"user\", \"content\": prompt}]\n", "            try:\n", "                formatted = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n", "            except Exception:\n", "                formatted = prompt\n", "\n", "        inputs = tokenizer(\n", "            formatted,\n", "            return_tensors='pt',\n", "            max_length=max_length,\n", "            truncation=True,\n", "            padding='max_length'\n", "        ).to(model.device)\n", "\n", "        attention_mask = inputs.get('attention_mask')\n", "\n", "        with torch.no_grad():\n", "            outputs = model(**inputs, output_attentions=True, output_hidden_states=True)\n", "\n", "        attn_stack = torch.stack([a.squeeze(0) for a in outputs.attentions], dim=0)\n", "        all_attention_patterns.append(attn_stack.cpu())\n", "        all_attention_masks.append(attention_mask.squeeze(0).cpu() if attention_mask is not None else None)\n", "\n", "    return {\n", "        'attention_patterns': all_attention_patterns,\n", "        'attention_masks': all_attention_masks,\n", "        'num_layers': len(outputs.attentions),\n", "        'num_heads': outputs.attentions[0].shape[1]\n", "    }\n", "\n", "\n", "def compute_head_entropy_profiles(attention_patterns, attention_masks=None):\n", "    \"\"\"Compute normalized entropy for each head across prompts.\"\"\"\n", "    num_prompts = len(attention_patterns)\n", "    num_layers = attention_patterns[0].shape[0]\n", "    num_heads = attention_patterns[0].shape[1]\n", "\n", "    all_entropies = np.zeros((num_prompts, num_layers, num_heads))\n", "\n", "    for p_idx, attn in enumerate(attention_patterns):\n", "        mask = None\n", "        if attention_masks is not None:\n", "            mask = attention_masks[p_idx]\n", "            if mask is not None:\n", "                mask = mask.bool()\n", "\n", "        for layer in range(num_layers):\n", "            for head in range(num_heads):\n", "                attn_matrix = attn[layer, head]\n", "\n", "                if mask is not None:\n", "                    valid_idx = mask.nonzero(as_tuple=False).squeeze(-1)\n", "                    if valid_idx.numel() > 1:\n", "                        attn_matrix = attn_matrix[valid_idx][:, valid_idx]\n", "                    else:\n", "                        all_entropies[p_idx, layer, head] = 0\n", "                        continue\n", "\n", "                attn_weights = attn_matrix.mean(dim=0).float().cpu().numpy()\n", "                denom = attn_weights.sum()\n", "                if denom <= 0:\n", "                    all_entropies[p_idx, layer, head] = 0\n", "                    continue\n", "\n", "                attn_weights = attn_weights / denom\n", "                attn_weights = attn_weights[attn_weights > 0]\n", "\n", "                if len(attn_weights) > 1:\n", "                    h = scipy_entropy(attn_weights, base=2)\n", "                    h_max = np.log2(len(attn_weights))\n", "                    h_norm = h / h_max if h_max > 0 else 0\n", "                else:\n", "                    h_norm = 0\n", "\n", "                all_entropies[p_idx, layer, head] = h_norm\n", "\n", "    return all_entropies.mean(axis=0)\n", "\n", "\n", "def compute_specialization_metrics(head_entropies):\n", "    \"\"\"Compute specialization metrics.\"\"\"\n", "    num_layers, num_heads = head_entropies.shape\n", "\n", "    layer_variances = np.var(head_entropies, axis=1)\n", "    mean_variance = float(np.mean(layer_variances))\n", "\n", "    head_profiles = head_entropies.T\n", "    head_corr_matrix = np.corrcoef(head_profiles)\n", "    upper_tri = head_corr_matrix[np.triu_indices(num_heads, k=1)]\n", "    mean_head_correlation = float(np.nanmean(upper_tri))\n", "\n", "    specialization_index = 1.0 - mean_head_correlation\n", "\n", "    head_contributions = np.mean(head_entropies, axis=0)\n", "    head_contributions = head_contributions / head_contributions.sum()\n", "    h_contrib = scipy_entropy(head_contributions, base=2)\n", "    effective_heads = 2 ** h_contrib if h_contrib > 0 else 1.0\n", "    effective_ratio = effective_heads / num_heads\n", "\n", "    return {\n", "        'mean_head_variance': mean_variance,\n", "        'mean_head_correlation': mean_head_correlation,\n", "        'specialization_index': specialization_index,\n", "        'effective_heads': float(effective_heads),\n", "        'effective_ratio': float(effective_ratio),\n", "        'layer_variances': layer_variances.tolist(),\n", "        'num_layers': num_layers,\n", "        'num_heads': num_heads\n", "    }\n", "\n", "print(\"Specialization metrics functions loaded.\")\n", "\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Cell 4: Layer-Targeted Noise Injector (from E06b, adapted for attention)\n", "\n", "class AttentionNoiseInjector:\n", "    \"\"\"\n", "    Inject Gaussian noise into attention outputs of SPECIFIC layer ranges.\n", "    \n", "    This is the 'Indra' treatment - chaos injection to restore diversity.\n", "    \"\"\"\n", "    \n", "    def __init__(self, model, target_range, noise_std=0.0):\n", "        self.model = model\n", "        self.target_start, self.target_end = target_range\n", "        self.noise_std = noise_std\n", "        self.hooks = []\n", "    \n", "    def _make_hook(self, layer_idx):\n", "        \"\"\"Create a forward hook for a specific layer.\"\"\"\n", "        def hook(module, input, output):\n", "            if self.noise_std > 0 and self.target_start <= layer_idx < self.target_end:\n", "                if isinstance(output, tuple):\n", "                    attn_output = output[0]\n", "                    noise = torch.randn_like(attn_output) * self.noise_std\n", "                    return (attn_output + noise,) + output[1:]\n", "                else:\n", "                    noise = torch.randn_like(output) * self.noise_std\n", "                    return output + noise\n", "            return output\n", "        return hook\n", "    \n", "    def attach(self):\n", "        \"\"\"Attach hooks to attention layers.\"\"\"\n", "        for idx, layer in enumerate(self.model.model.layers):\n", "            hook = layer.self_attn.register_forward_hook(self._make_hook(idx))\n", "            self.hooks.append(hook)\n", "    \n", "    def detach(self):\n", "        \"\"\"Remove all hooks.\"\"\"\n", "        for hook in self.hooks:\n", "            hook.remove()\n", "        self.hooks = []\n", "    \n", "    def set_noise(self, std):\n", "        \"\"\"Update noise level.\"\"\"\n", "        self.noise_std = std\n", "\n", "print(\"Attention noise injector ready.\")\n", "print(f\"Target regions available: {list(LAYER_RANGES.keys())}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Cell 5: Load Model and Baseline Measurement (3-Seed)\n", "\n", "print(f\"\\n{'='*60}\")\n", "print(f\"PHASE 1: LOAD MODEL AND VERIFY COLLAPSED STATE\")\n", "print(f\"{'='*60}\")\n", "\n", "print(f\"\\nLoading: {MODEL_CONFIG['name']}\")\n", "print(f\"E11-v3 dtype: {DTYPE}\")\n", "\n", "tokenizer = AutoTokenizer.from_pretrained(MODEL_CONFIG['name'], **TOKEN_KWARGS)\n", "model = AutoModelForCausalLM.from_pretrained(\n", "    MODEL_CONFIG['name'],\n", "    **TOKEN_KWARGS,\n", "    torch_dtype=DTYPE,  # E11-v3: bfloat16\n", "    device_map='auto',\n", "    trust_remote_code=True,\n", "    attn_implementation=\"eager\"  # CRITICAL: SDPA doesn't return attentions!\n", ")\n", "\n", "# CRITICAL: Set eval mode to disable dropout\n", "model.eval()\n", "\n", "if tokenizer.pad_token is None:\n", "    tokenizer.pad_token = tokenizer.eos_token\n", "\n", "print(f\"Loaded: {sum(p.numel() for p in model.parameters()) / 1e9:.2f}B parameters\")\n", "print(f\"Layers: {len(model.model.layers)}\")\n", "\n", "# Measure baseline with 3-seed averaging (E11-v3 standard)\n", "print(f\"\\nMeasuring baseline specialization (no treatment, 3-seed average)...\")\n", "\n", "seed_results_baseline = []\n", "for seed in SEEDS:\n", "    torch.manual_seed(seed)\n", "    np.random.seed(seed)\n", "    \n", "    baseline_activations = extract_head_activations_with_noise(\n", "        model, tokenizer, STANDARD_PROMPTS, max_length=MAX_LENGTH, use_chat_template=USE_CHAT_TEMPLATE\n", "    )\n", "    baseline_entropies = compute_head_entropy_profiles(\n", "        baseline_activations['attention_patterns'],\n", "        baseline_activations['attention_masks']\n", "    )\n", "    baseline_metrics = compute_specialization_metrics(baseline_entropies)\n", "    seed_results_baseline.append(baseline_metrics)\n", "    print(f\"  Seed {seed}: SI={baseline_metrics['specialization_index']:.4f}\")\n", "\n", "# Average across seeds\n", "baseline_metrics = {\n", "    'specialization_index': np.mean([r['specialization_index'] for r in seed_results_baseline]),\n", "    'mean_head_correlation': np.mean([r['mean_head_correlation'] for r in seed_results_baseline]),\n", "    'mean_head_variance': np.mean([r['mean_head_variance'] for r in seed_results_baseline]),\n", "    'si_std': np.std([r['specialization_index'] for r in seed_results_baseline])\n", "}\n", "\n", "print(f\"\\n  Baseline Specialization Index: {baseline_metrics['specialization_index']:.4f} \u00b1 {baseline_metrics['si_std']:.4f}\")\n", "print(f\"  Baseline Head Correlation: {baseline_metrics['mean_head_correlation']:.4f}\")\n", "print(f\"  Expected from E11-T: SI={E11T_REFERENCE['instruct_specialization']:.4f}\")\n", "\n", "# Verify we're in collapsed state\n", "si_diff = abs(baseline_metrics['specialization_index'] - E11T_REFERENCE['instruct_specialization'])\n", "if si_diff < 0.05:\n", "    print(f\"\\n  VERIFIED: Model is in collapsed state (diff={si_diff:.4f})\")\n", "else:\n", "    print(f\"\\n  WARNING: SI differs from E11-T reference by {si_diff:.4f}\")\n", "\n", "results = {\n", "    'baseline': {\n", "        'specialization_index': float(baseline_metrics['specialization_index']),\n", "        'si_std': float(baseline_metrics['si_std']),\n", "        'mean_head_correlation': float(baseline_metrics['mean_head_correlation']),\n", "        'mean_head_variance': float(baseline_metrics['mean_head_variance']),\n", "        'seed_results': [{'seed': s, 'si': r['specialization_index']} for s, r in zip(SEEDS, seed_results_baseline)]\n", "    },\n", "    'treatments': []\n", "}\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Cell 6: Indra Treatment - Test Each Region and Noise Level (3-Seed)\n", "\n", "print(f\"\\n{'='*60}\")\n", "print(f\"PHASE 2: INDRA TREATMENT - CHAOS INJECTION (3-Seed Average)\")\n", "print(f\"{'='*60}\")\n", "\n", "for region_name, (start, end) in LAYER_RANGES.items():\n", "    print(f\"\\n{'='*50}\")\n", "    print(f\"TREATING: {region_name.upper()} (Layers {start}-{end-1})\")\n", "    print(f\"{'='*50}\")\n", "    \n", "    region_results = {\n", "        'region': region_name,\n", "        'layer_range': [start, end],\n", "        'noise_tests': []\n", "    }\n", "    \n", "    for noise_std in NOISE_LEVELS:\n", "        # E11-v3: 3-seed averaging for each noise level\n", "        seed_si_values = []\n", "        seed_corr_values = []\n", "        seed_var_values = []\n", "        \n", "        for seed in SEEDS:\n", "            torch.manual_seed(seed)\n", "            np.random.seed(seed)\n", "            \n", "            # Create injector for this region\n", "            injector = AttentionNoiseInjector(model, (start, end), noise_std=noise_std)\n", "            injector.attach()\n", "            \n", "            # Measure specialization with noise\n", "            treated_activations = extract_head_activations_with_noise(\n", "                model, tokenizer, STANDARD_PROMPTS, max_length=MAX_LENGTH, use_chat_template=USE_CHAT_TEMPLATE\n", "            )\n", "            treated_entropies = compute_head_entropy_profiles(\n", "                treated_activations['attention_patterns'],\n", "                treated_activations['attention_masks']\n", "            )\n", "            treated_metrics = compute_specialization_metrics(treated_entropies)\n", "            \n", "            injector.detach()\n", "            \n", "            seed_si_values.append(treated_metrics['specialization_index'])\n", "            seed_corr_values.append(treated_metrics['mean_head_correlation'])\n", "            seed_var_values.append(treated_metrics['mean_head_variance'])\n", "        \n", "        # Average across seeds\n", "        avg_si = np.mean(seed_si_values)\n", "        avg_corr = np.mean(seed_corr_values)\n", "        avg_var = np.mean(seed_var_values)\n", "        si_std = np.std(seed_si_values)\n", "        \n", "        # Compute recovery metrics\n", "        si_before = baseline_metrics['specialization_index']\n", "        si_after = avg_si\n", "        si_target = E11T_REFERENCE['base_specialization']\n", "        \n", "        si_delta = si_after - si_before\n", "        recovery_ratio = (si_after - si_before) / (si_target - si_before) if si_target != si_before else 0\n", "        \n", "        corr_delta = avg_corr - baseline_metrics['mean_head_correlation']\n", "        \n", "        noise_result = {\n", "            'noise_std': float(noise_std),\n", "            'specialization_index': float(avg_si),\n", "            'si_std': float(si_std),\n", "            'mean_head_correlation': float(avg_corr),\n", "            'mean_head_variance': float(avg_var),\n", "            'si_delta': float(si_delta),\n", "            'corr_delta': float(corr_delta),\n", "            'recovery_ratio': float(recovery_ratio),\n", "            'seed_values': {str(s): float(v) for s, v in zip(SEEDS, seed_si_values)}\n", "        }\n", "        region_results['noise_tests'].append(noise_result)\n", "        \n", "        # Print result\n", "        status = \"HEALED!\" if si_delta > 0.05 else (\"WORSE\" if si_delta < -0.02 else \"~\")\n", "        print(f\"  \u03c3={noise_std:.2f}: SI={avg_si:.4f}\u00b1{si_std:.4f} (\u0394={si_delta:+.4f}) Recovery={recovery_ratio*100:.1f}% {status}\")\n", "    \n", "    # Find best treatment for this region\n", "    best_test = max(region_results['noise_tests'], key=lambda x: x['specialization_index'])\n", "    region_results['best_noise'] = best_test['noise_std']\n", "    region_results['best_si'] = best_test['specialization_index']\n", "    region_results['best_recovery'] = best_test['recovery_ratio']\n", "    \n", "    results['treatments'].append(region_results)\n", "    \n", "    print(f\"\\n  BEST for {region_name}: \u03c3={best_test['noise_std']:.2f} -> SI={best_test['specialization_index']:.4f} (Recovery={best_test['recovery_ratio']*100:.1f}%)\")\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#", " ", "C", "e", "l", "l", " ", "7", ":", " ", "A", "n", "a", "l", "y", "s", "i", "s", " ", "-", " ", "C", "a", "n", " ", "I", "n", "d", "r", "a", " ", "R", "e", "s", "t", "o", "r", "e", " ", "S", "p", "e", "c", "i", "a", "l", "i", "z", "a", "t", "i", "o", "n", "?", "\n", "\n", "p", "r", "i", "n", "t", "(", "f", "\"", "\\", "n", "{", "'", "=", "'", "*", "7", "0", "}", "\"", ")", "\n", "p", "r", "i", "n", "t", "(", "f", "\"", "P", "H", "A", "S", "E", " ", "3", ":", " ", "I", "N", "D", "R", "A", " ", "R", "E", "C", "O", "V", "E", "R", "Y", " ", "A", "N", "A", "L", "Y", "S", "I", "S", "\"", ")", "\n", "p", "r", "i", "n", "t", "(", "f", "\"", "{", "'", "=", "'", "*", "7", "0", "}", "\"", ")", "\n", "\n", "p", "r", "i", "n", "t", "(", "f", "\"", "\\", "n", "R", "e", "f", "e", "r", "e", "n", "c", "e", " ", "V", "a", "l", "u", "e", "s", ":", "\"", ")", "\n", "p", "r", "i", "n", "t", "(", "f", "\"", " ", " ", "B", "a", "s", "e", " ", "S", "I", " ", "(", "t", "a", "r", "g", "e", "t", ")", ":", " ", " ", " ", " ", " ", " ", "{", "E", "1", "1", "T", "_", "R", "E", "F", "E", "R", "E", "N", "C", "E", "[", "'", "b", "a", "s", "e", "_", "s", "p", "e", "c", "i", "a", "l", "i", "z", "a", "t", "i", "o", "n", "'", "]", ":", ".", "4", "f", "}", "\"", ")", "\n", "p", "r", "i", "n", "t", "(", "f", "\"", " ", " ", "I", "n", "s", "t", "r", "u", "c", "t", " ", "S", "I", " ", "(", "c", "u", "r", "r", "e", "n", "t", ")", ":", " ", "{", "b", "a", "s", "e", "l", "i", "n", "e", "_", "m", "e", "t", "r", "i", "c", "s", "[", "'", "s", "p", "e", "c", "i", "a", "l", "i", "z", "a", "t", "i", "o", "n", "_", "i", "n", "d", "e", "x", "'", "]", ":", ".", "4", "f", "}", "\"", ")", "\n", "p", "r", "i", "n", "t", "(", "f", "\"", " ", " ", "C", "o", "l", "l", "a", "p", "s", "e", " ", "D", "e", "l", "t", "a", ":", " ", " ", " ", " ", " ", " ", " ", " ", "{", "E", "1", "1", "T", "_", "R", "E", "F", "E", "R", "E", "N", "C", "E", "[", "'", "d", "e", "l", "t", "a", "_", "s", "p", "e", "c", "i", "a", "l", "i", "z", "a", "t", "i", "o", "n", "'", "]", ":", "+", ".", "4", "f", "}", "\"", ")", "\n", "\n", "#", " ", "T", "h", "r", "e", "s", "h", "o", "l", "d", " ", "d", "e", "f", "i", "n", "i", "t", "i", "o", "n", "s", " ", "(", "a", "l", "i", "g", "n", "e", "d", " ", "w", "i", "t", "h", " ", "v", "e", "r", "d", "i", "c", "t", " ", "l", "o", "g", "i", "c", ")", "\n", "#", " ", "A", "_", "C", "O", "N", "F", "I", "R", "M", "E", "D", ":", " ", ">", "2", "0", "%", " ", "r", "e", "c", "o", "v", "e", "r", "y", "\n", "#", " ", "B", "_", "P", "A", "R", "T", "I", "A", "L", ":", " ", " ", " ", "5", "-", "2", "0", "%", " ", "r", "e", "c", "o", "v", "e", "r", "y", " ", " ", "\n", "#", " ", "C", "_", "R", "E", "F", "U", "T", "E", "D", ":", " ", " ", " ", "<", "5", "%", " ", "r", "e", "c", "o", "v", "e", "r", "y", "\n", "\n", "p", "r", "i", "n", "t", "(", "f", "\"", "\\", "n", "\"", " ", "+", " ", "\"", "-", "\"", "*", "7", "0", ")", "\n", "p", "r", "i", "n", "t", "(", "f", "\"", "{", "'", "R", "e", "g", "i", "o", "n", "'", ":", "<", "1", "2", "}", " ", "{", "'", "B", "e", "s", "t", " ", "\u03c3", "'", ":", "<", "1", "0", "}", " ", "{", "'", "S", "I", " ", "A", "f", "t", "e", "r", "'", ":", "<", "1", "2", "}", " ", "{", "'", "\u0394", " ", "S", "I", "'", ":", "<", "1", "2", "}", " ", "{", "'", "R", "e", "c", "o", "v", "e", "r", "y", " ", "%", "'", ":", "<", "1", "2", "}", " ", "{", "'", "S", "t", "a", "t", "u", "s", "'", ":", "<", "1", "2", "}", "\"", ")", "\n", "p", "r", "i", "n", "t", "(", "\"", "-", "\"", "*", "7", "0", ")", "\n", "\n", "b", "e", "s", "t", "_", "o", "v", "e", "r", "a", "l", "l", " ", "=", " ", "N", "o", "n", "e", "\n", "b", "e", "s", "t", "_", "r", "e", "c", "o", "v", "e", "r", "y", " ", "=", " ", "-", "9", "9", "9", "\n", "\n", "f", "o", "r", " ", "t", "r", "e", "a", "t", "m", "e", "n", "t", " ", "i", "n", " ", "r", "e", "s", "u", "l", "t", "s", "[", "'", "t", "r", "e", "a", "t", "m", "e", "n", "t", "s", "'", "]", ":", "\n", " ", " ", " ", " ", "r", "e", "g", "i", "o", "n", " ", "=", " ", "t", "r", "e", "a", "t", "m", "e", "n", "t", "[", "'", "r", "e", "g", "i", "o", "n", "'", "]", "\n", " ", " ", " ", " ", "b", "e", "s", "t", "_", "n", "o", "i", "s", "e", " ", "=", " ", "t", "r", "e", "a", "t", "m", "e", "n", "t", "[", "'", "b", "e", "s", "t", "_", "n", "o", "i", "s", "e", "'", "]", "\n", " ", " ", " ", " ", "b", "e", "s", "t", "_", "s", "i", " ", "=", " ", "t", "r", "e", "a", "t", "m", "e", "n", "t", "[", "'", "b", "e", "s", "t", "_", "s", "i", "'", "]", "\n", " ", " ", " ", " ", "s", "i", "_", "d", "e", "l", "t", "a", " ", "=", " ", "b", "e", "s", "t", "_", "s", "i", " ", "-", " ", "b", "a", "s", "e", "l", "i", "n", "e", "_", "m", "e", "t", "r", "i", "c", "s", "[", "'", "s", "p", "e", "c", "i", "a", "l", "i", "z", "a", "t", "i", "o", "n", "_", "i", "n", "d", "e", "x", "'", "]", "\n", " ", " ", " ", " ", "r", "e", "c", "o", "v", "e", "r", "y", " ", "=", " ", "t", "r", "e", "a", "t", "m", "e", "n", "t", "[", "'", "b", "e", "s", "t", "_", "r", "e", "c", "o", "v", "e", "r", "y", "'", "]", "\n", " ", " ", " ", " ", "\n", " ", " ", " ", " ", "#", " ", "T", "h", "r", "e", "s", "h", "o", "l", "d", "s", " ", "a", "l", "i", "g", "n", "e", "d", " ", "w", "i", "t", "h", " ", "v", "e", "r", "d", "i", "c", "t", " ", "l", "o", "g", "i", "c", " ", "(", ">", "2", "0", "%", " ", "=", " ", "A", ",", " ", ">", "5", "%", " ", "=", " ", "B", ")", "\n", " ", " ", " ", " ", "i", "f", " ", "r", "e", "c", "o", "v", "e", "r", "y", " ", ">", " ", "0", ".", "2", ":", "\n", " ", " ", " ", " ", " ", " ", " ", " ", "s", "t", "a", "t", "u", "s", " ", "=", " ", "\"", "H", "E", "A", "L", "E", "D", "!", "\"", "\n", " ", " ", " ", " ", "e", "l", "i", "f", " ", "r", "e", "c", "o", "v", "e", "r", "y", " ", ">", " ", "0", ".", "0", "5", ":", "\n", " ", " ", " ", " ", " ", " ", " ", " ", "s", "t", "a", "t", "u", "s", " ", "=", " ", "\"", "P", "a", "r", "t", "i", "a", "l", "\"", "\n", " ", " ", " ", " ", "e", "l", "i", "f", " ", "r", "e", "c", "o", "v", "e", "r", "y", " ", "<", " ", "-", "0", ".", "0", "2", ":", "\n", " ", " ", " ", " ", " ", " ", " ", " ", "s", "t", "a", "t", "u", "s", " ", "=", " ", "\"", "W", "O", "R", "S", "E", "\"", "\n", " ", " ", " ", " ", "e", "l", "s", "e", ":", "\n", " ", " ", " ", " ", " ", " ", " ", " ", "s", "t", "a", "t", "u", "s", " ", "=", " ", "\"", "N", "o", " ", "e", "f", "f", "e", "c", "t", "\"", "\n", " ", " ", " ", " ", "\n", " ", " ", " ", " ", "i", "f", " ", "r", "e", "c", "o", "v", "e", "r", "y", " ", ">", " ", "b", "e", "s", "t", "_", "r", "e", "c", "o", "v", "e", "r", "y", ":", "\n", " ", " ", " ", " ", " ", " ", " ", " ", "b", "e", "s", "t", "_", "r", "e", "c", "o", "v", "e", "r", "y", " ", "=", " ", "r", "e", "c", "o", "v", "e", "r", "y", "\n", " ", " ", " ", " ", " ", " ", " ", " ", "b", "e", "s", "t", "_", "o", "v", "e", "r", "a", "l", "l", " ", "=", " ", "t", "r", "e", "a", "t", "m", "e", "n", "t", "\n", " ", " ", " ", " ", "\n", " ", " ", " ", " ", "p", "r", "i", "n", "t", "(", "f", "\"", "{", "r", "e", "g", "i", "o", "n", ":", "<", "1", "2", "}", " ", "{", "b", "e", "s", "t", "_", "n", "o", "i", "s", "e", ":", "<", "1", "0", ".", "2", "f", "}", " ", "{", "b", "e", "s", "t", "_", "s", "i", ":", "<", "1", "2", ".", "4", "f", "}", " ", "{", "s", "i", "_", "d", "e", "l", "t", "a", ":", "<", "+", "1", "2", ".", "4", "f", "}", " ", "{", "r", "e", "c", "o", "v", "e", "r", "y", "*", "1", "0", "0", ":", "<", "1", "2", ".", "1", "f", "}", " ", "{", "s", "t", "a", "t", "u", "s", ":", "<", "1", "2", "}", "\"", ")", "\n", "\n", "p", "r", "i", "n", "t", "(", "\"", "-", "\"", "*", "7", "0", ")", "\n", "\n", "#", " ", "V", "e", "r", "d", "i", "c", "t", "\n", "p", "r", "i", "n", "t", "(", "f", "\"", "\\", "n", "{", "'", "=", "'", "*", "7", "0", "}", "\"", ")", "\n", "p", "r", "i", "n", "t", "(", "\"", "V", "E", "R", "D", "I", "C", "T", ":", " ", "C", "A", "N", " ", "I", "N", "D", "R", "A", " ", "R", "E", "S", "T", "O", "R", "E", " ", "G", "Q", "A", " ", "S", "P", "E", "C", "I", "A", "L", "I", "Z", "A", "T", "I", "O", "N", "?", "\"", ")", "\n", "p", "r", "i", "n", "t", "(", "f", "\"", "{", "'", "=", "'", "*", "7", "0", "}", "\"", ")", "\n", "\n", "#", " ", "S", "a", "m", "e", " ", "t", "h", "r", "e", "s", "h", "o", "l", "d", "s", " ", "a", "s", " ", "t", "a", "b", "l", "e", " ", "(", ">", "0", ".", "2", " ", "=", " ", "H", "E", "A", "L", "E", "D", "!", ",", " ", ">", "0", ".", "0", "5", " ", "=", " ", "P", "a", "r", "t", "i", "a", "l", ")", "\n", "i", "f", " ", "b", "e", "s", "t", "_", "r", "e", "c", "o", "v", "e", "r", "y", " ", ">", " ", "0", ".", "2", ":", "\n", " ", " ", " ", " ", "v", "e", "r", "d", "i", "c", "t", " ", "=", " ", "\"", "A", "_", "C", "O", "N", "F", "I", "R", "M", "E", "D", "\"", "\n", " ", " ", " ", " ", "p", "r", "i", "n", "t", "(", "f", "\"", "\\", "n", " ", " ", "V", "E", "R", "D", "I", "C", "T", ":", " ", "{", "v", "e", "r", "d", "i", "c", "t", "}", "\"", ")", "\n", " ", " ", " ", " ", "p", "r", "i", "n", "t", "(", "f", "\"", " ", " ", "C", "h", "a", "o", "s", " ", "i", "n", "j", "e", "c", "t", "i", "o", "n", " ", "C", "A", "N", " ", "i", "n", "d", "u", "c", "e", " ", "f", "u", "n", "c", "t", "i", "o", "n", "a", "l", " ", "s", "p", "e", "c", "i", "a", "l", "i", "z", "a", "t", "i", "o", "n", " ", "r", "e", "c", "o", "v", "e", "r", "y", "!", "\"", ")", "\n", " ", " ", " ", " ", "p", "r", "i", "n", "t", "(", "f", "\"", " ", " ", "B", "e", "s", "t", " ", "r", "e", "g", "i", "o", "n", ":", " ", "{", "b", "e", "s", "t", "_", "o", "v", "e", "r", "a", "l", "l", "[", "'", "r", "e", "g", "i", "o", "n", "'", "]", "}", " ", "a", "t", " ", "\u03c3", "=", "{", "b", "e", "s", "t", "_", "o", "v", "e", "r", "a", "l", "l", "[", "'", "b", "e", "s", "t", "_", "n", "o", "i", "s", "e", "'", "]", ":", ".", "2", "f", "}", "\"", ")", "\n", " ", " ", " ", " ", "p", "r", "i", "n", "t", "(", "f", "\"", " ", " ", "R", "e", "c", "o", "v", "e", "r", "y", ":", " ", "{", "b", "e", "s", "t", "_", "r", "e", "c", "o", "v", "e", "r", "y", "*", "1", "0", "0", ":", ".", "1", "f", "}", "%", " ", "o", "f", " ", "l", "o", "s", "t", " ", "s", "p", "e", "c", "i", "a", "l", "i", "z", "a", "t", "i", "o", "n", "\"", ")", "\n", "e", "l", "i", "f", " ", "b", "e", "s", "t", "_", "r", "e", "c", "o", "v", "e", "r", "y", " ", ">", " ", "0", ".", "0", "5", ":", "\n", " ", " ", " ", " ", "v", "e", "r", "d", "i", "c", "t", " ", "=", " ", "\"", "B", "_", "P", "A", "R", "T", "I", "A", "L", "\"", "\n", " ", " ", " ", " ", "p", "r", "i", "n", "t", "(", "f", "\"", "\\", "n", " ", " ", "V", "E", "R", "D", "I", "C", "T", ":", " ", "{", "v", "e", "r", "d", "i", "c", "t", "}", "\"", ")", "\n", " ", " ", " ", " ", "p", "r", "i", "n", "t", "(", "f", "\"", " ", " ", "P", "a", "r", "t", "i", "a", "l", " ", "f", "u", "n", "c", "t", "i", "o", "n", "a", "l", " ", "r", "e", "c", "o", "v", "e", "r", "y", " ", "u", "n", "d", "e", "r", " ", "p", "e", "r", "t", "u", "r", "b", "a", "t", "i", "o", "n", ".", "\"", ")", "\n", " ", " ", " ", " ", "p", "r", "i", "n", "t", "(", "f", "\"", " ", " ", "B", "e", "s", "t", " ", "r", "e", "g", "i", "o", "n", ":", " ", "{", "b", "e", "s", "t", "_", "o", "v", "e", "r", "a", "l", "l", "[", "'", "r", "e", "g", "i", "o", "n", "'", "]", "}", " ", "a", "t", " ", "\u03c3", "=", "{", "b", "e", "s", "t", "_", "o", "v", "e", "r", "a", "l", "l", "[", "'", "b", "e", "s", "t", "_", "n", "o", "i", "s", "e", "'", "]", ":", ".", "2", "f", "}", "\"", ")", "\n", " ", " ", " ", " ", "p", "r", "i", "n", "t", "(", "f", "\"", " ", " ", "R", "e", "c", "o", "v", "e", "r", "y", ":", " ", "{", "b", "e", "s", "t", "_", "r", "e", "c", "o", "v", "e", "r", "y", "*", "1", "0", "0", ":", ".", "1", "f", "}", "%", "\"", ")", "\n", "e", "l", "s", "e", ":", "\n", " ", " ", " ", " ", "v", "e", "r", "d", "i", "c", "t", " ", "=", " ", "\"", "C", "_", "R", "E", "F", "U", "T", "E", "D", "\"", "\n", " ", " ", " ", " ", "p", "r", "i", "n", "t", "(", "f", "\"", "\\", "n", " ", " ", "V", "E", "R", "D", "I", "C", "T", ":", " ", "{", "v", "e", "r", "d", "i", "c", "t", "}", "\"", ")", "\n", " ", " ", " ", " ", "p", "r", "i", "n", "t", "(", "f", "\"", " ", " ", "C", "h", "a", "o", "s", " ", "i", "n", "j", "e", "c", "t", "i", "o", "n", " ", "d", "o", "e", "s", " ", "N", "O", "T", " ", "r", "e", "s", "t", "o", "r", "e", " ", "f", "u", "n", "c", "t", "i", "o", "n", "a", "l", " ", "s", "p", "e", "c", "i", "a", "l", "i", "z", "a", "t", "i", "o", "n", ".", "\"", ")", "\n", " ", " ", " ", " ", "p", "r", "i", "n", "t", "(", "f", "\"", " ", " ", "G", "Q", "A", " ", "t", "e", "r", "r", "i", "t", "o", "r", "i", "a", "l", " ", "c", "o", "l", "l", "a", "p", "s", "e", " ", "a", "p", "p", "e", "a", "r", "s", " ", "I", "R", "R", "E", "V", "E", "R", "S", "I", "B", "L", "E", ".", "\"", ")", "\n", "\n", "r", "e", "s", "u", "l", "t", "s", "[", "'", "v", "e", "r", "d", "i", "c", "t", "'", "]", " ", "=", " ", "{", "\n", " ", " ", " ", " ", "'", "c", "o", "d", "e", "'", ":", " ", "v", "e", "r", "d", "i", "c", "t", ",", "\n", " ", " ", " ", " ", "'", "b", "e", "s", "t", "_", "r", "e", "g", "i", "o", "n", "'", ":", " ", "b", "e", "s", "t", "_", "o", "v", "e", "r", "a", "l", "l", "[", "'", "r", "e", "g", "i", "o", "n", "'", "]", " ", "i", "f", " ", "b", "e", "s", "t", "_", "o", "v", "e", "r", "a", "l", "l", " ", "e", "l", "s", "e", " ", "N", "o", "n", "e", ",", "\n", " ", " ", " ", " ", "'", "b", "e", "s", "t", "_", "n", "o", "i", "s", "e", "'", ":", " ", "b", "e", "s", "t", "_", "o", "v", "e", "r", "a", "l", "l", "[", "'", "b", "e", "s", "t", "_", "n", "o", "i", "s", "e", "'", "]", " ", "i", "f", " ", "b", "e", "s", "t", "_", "o", "v", "e", "r", "a", "l", "l", " ", "e", "l", "s", "e", " ", "N", "o", "n", "e", ",", "\n", " ", " ", " ", " ", "'", "b", "e", "s", "t", "_", "r", "e", "c", "o", "v", "e", "r", "y", "'", ":", " ", "f", "l", "o", "a", "t", "(", "b", "e", "s", "t", "_", "r", "e", "c", "o", "v", "e", "r", "y", ")", ",", "\n", " ", " ", " ", " ", "'", "b", "a", "s", "e", "l", "i", "n", "e", "_", "s", "i", "'", ":", " ", "b", "a", "s", "e", "l", "i", "n", "e", "_", "m", "e", "t", "r", "i", "c", "s", "[", "'", "s", "p", "e", "c", "i", "a", "l", "i", "z", "a", "t", "i", "o", "n", "_", "i", "n", "d", "e", "x", "'", "]", ",", "\n", " ", " ", " ", " ", "'", "t", "a", "r", "g", "e", "t", "_", "s", "i", "'", ":", " ", "E", "1", "1", "T", "_", "R", "E", "F", "E", "R", "E", "N", "C", "E", "[", "'", "b", "a", "s", "e", "_", "s", "p", "e", "c", "i", "a", "l", "i", "z", "a", "t", "i", "o", "n", "'", "]", "\n", "}", "\n", "\n", "p", "r", "i", "n", "t", "(", "f", "\"", "\\", "n", "{", "'", "=", "'", "*", "7", "0", "}", "\"", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Cell 8: Visualization\n", "\n", "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n", "\n", "colors = {\n", "    'early': '#3498db',\n", "    'middle': '#2ecc71',\n", "    'late': '#e74c3c',\n", "    'all': '#9b59b6'\n", "}\n", "\n", "# Plot 1: Recovery by Region\n", "ax1 = axes[0, 0]\n", "regions = [t['region'] for t in results['treatments']]\n", "recoveries = [t['best_recovery'] * 100 for t in results['treatments']]\n", "bar_colors = [colors[r] for r in regions]\n", "\n", "bars = ax1.bar(regions, recoveries, color=bar_colors, alpha=0.8, edgecolor='black')\n", "ax1.axhline(y=0, color='black', linestyle='--', linewidth=1)\n", "ax1.axhline(y=20, color='green', linestyle=':', alpha=0.5, label='20% threshold')\n", "ax1.set_ylabel('Recovery %')\n", "ax1.set_title('Specialization Recovery by Region\\n(Higher = Better Healing)')\n", "ax1.legend()\n", "\n", "for bar, rec in zip(bars, recoveries):\n", "    ax1.annotate(f'{rec:.1f}%', xy=(bar.get_x() + bar.get_width()/2, rec),\n", "                 xytext=(0, 5 if rec > 0 else -15), textcoords='offset points',\n", "                 ha='center', fontsize=11, fontweight='bold')\n", "\n", "# Plot 2: SI Before vs After Treatment\n", "ax2 = axes[0, 1]\n", "si_before = baseline_metrics['specialization_index']\n", "si_target = E11T_REFERENCE['base_specialization']\n", "si_afters = [t['best_si'] for t in results['treatments']]\n", "\n", "x = np.arange(len(regions))\n", "width = 0.35\n", "\n", "ax2.axhline(y=si_target, color='green', linestyle='--', linewidth=2, label=f'Base Target ({si_target:.3f})')\n", "ax2.axhline(y=si_before, color='red', linestyle=':', linewidth=2, label=f'Collapsed ({si_before:.3f})')\n", "\n", "bars = ax2.bar(regions, si_afters, color=bar_colors, alpha=0.8, edgecolor='black')\n", "ax2.set_ylabel('Specialization Index')\n", "ax2.set_title('SI After Best Treatment by Region')\n", "ax2.set_ylim(0, 1)\n", "ax2.legend()\n", "\n", "for bar, si in zip(bars, si_afters):\n", "    ax2.annotate(f'{si:.3f}', xy=(bar.get_x() + bar.get_width()/2, si),\n", "                 xytext=(0, 5), textcoords='offset points', ha='center', fontsize=10)\n", "\n", "# Plot 3: Dose-Response Curves\n", "ax3 = axes[1, 0]\n", "for treatment in results['treatments']:\n", "    region = treatment['region']\n", "    noise_levels = [t['noise_std'] for t in treatment['noise_tests']]\n", "    si_values = [t['specialization_index'] for t in treatment['noise_tests']]\n", "    ax3.plot(noise_levels, si_values, 'o-', color=colors[region], \n", "             label=region.capitalize(), linewidth=2, markersize=8)\n", "\n", "ax3.axhline(y=si_before, color='red', linestyle=':', label='Collapsed')\n", "ax3.axhline(y=si_target, color='green', linestyle='--', label='Target')\n", "ax3.set_xlabel('Noise Level (\u03c3)')\n", "ax3.set_ylabel('Specialization Index')\n", "ax3.set_title('Dose-Response: SI vs Treatment Intensity')\n", "ax3.legend()\n", "ax3.grid(True, alpha=0.3)\n", "\n", "# Plot 4: Recovery Heatmap\n", "ax4 = axes[1, 1]\n", "region_order = ['early', 'middle', 'late', 'all']\n", "heatmap_data = []\n", "for region in region_order:\n", "    treatment = next(t for t in results['treatments'] if t['region'] == region)\n", "    row = [t['recovery_ratio'] * 100 for t in treatment['noise_tests']]\n", "    heatmap_data.append(row)\n", "\n", "heatmap_data = np.array(heatmap_data)\n", "\n", "im = ax4.imshow(heatmap_data, cmap='RdYlGn', aspect='auto', vmin=-20, vmax=50)\n", "ax4.set_xticks(range(len(NOISE_LEVELS)))\n", "ax4.set_xticklabels([f'\u03c3={n:.2f}' for n in NOISE_LEVELS])\n", "ax4.set_yticks(range(len(region_order)))\n", "ax4.set_yticklabels([r.capitalize() for r in region_order])\n", "ax4.set_xlabel('Noise Level')\n", "ax4.set_ylabel('Target Region')\n", "ax4.set_title('Recovery % Heatmap\\n(Green = Healing, Red = Damage)')\n", "\n", "for i in range(len(region_order)):\n", "    for j in range(len(NOISE_LEVELS)):\n", "        val = heatmap_data[i, j]\n", "        color = 'white' if abs(val) > 20 else 'black'\n", "        ax4.text(j, i, f'{val:.1f}%', ha='center', va='center', color=color, fontsize=9)\n", "\n", "plt.colorbar(im, ax=ax4, label='Recovery %')\n", "\n", "plt.tight_layout()\n", "fig_path = f'figures/E11T_indra_recovery_{TIMESTAMP}.png'\n", "plt.savefig(fig_path, dpi=150, bbox_inches='tight')\n", "plt.show()\n", "\n", "print(f\"\\nFigure saved: {fig_path}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Cell 9: Save Results\n", "\n", "def convert_to_native(obj):\n", "    if isinstance(obj, dict):\n", "        return {k: convert_to_native(v) for k, v in obj.items()}\n", "    elif isinstance(obj, list):\n", "        return [convert_to_native(v) for v in obj]\n", "    elif isinstance(obj, tuple):\n", "        return tuple(convert_to_native(v) for v in obj)\n", "    elif isinstance(obj, (np.bool_, np.integer)):\n", "        return int(obj)\n", "    elif isinstance(obj, np.floating):\n", "        return float(obj)\n", "    elif isinstance(obj, np.ndarray):\n", "        return obj.tolist()\n", "    else:\n", "        return obj\n", "\n", "filename = f'results/E11T_indra_recovery_{TIMESTAMP}.json'\n", "\n", "output = {\n", "    'experiment': 'E11-T-Indra',\n", "    'timestamp': TIMESTAMP,\n", "    'model': MODEL_CONFIG['name'],\n", "    'architecture': 'GQA',\n", "    'hypothesis': 'Chaos injection can restore head specialization in collapsed GQA',\n", "    \n", "    # E11-v3 Methodology Block (REQUIRED)\n", "    'methodology': {\n", "        'standard': 'E11-v3',\n", "        'seeds': SEEDS,\n", "        'max_length': MAX_LENGTH,\n", "        'dtype': str(DTYPE),\n", "        'prompt_md5': actual_md5,\n", "        'prompt_md5_verified': prompts_ok,\n", "        'use_chat_template': USE_CHAT_TEMPLATE,\n", "        'attention_masked': True,\n", "        'num_prompts': len(STANDARD_PROMPTS),\n", "        'prompt_set': 'Standard-10'\n", "    },\n", "    \n", "    'e11t_reference': E11T_REFERENCE,\n", "    'layer_ranges': {k: list(v) for k, v in LAYER_RANGES.items()},\n", "    'noise_levels': NOISE_LEVELS,\n", "    'results': convert_to_native(results)\n", "}\n", "\n", "with open(filename, 'w') as f:\n", "    json.dump(output, f, indent=2)\n", "\n", "print(f\"Results saved: {filename}\")\n", "\n", "try:\n", "    from google.colab import files\n", "    files.download(filename)\n", "    files.download(fig_path)\n", "except:\n", "    pass\n"]}, {"cell_type": "markdown", "metadata": {}, "source": "---\n\n## Summary\n\n### E11-T-Indra: Can Chaos Restore GQA Functional Specialization?\n\n**The Paradox (from E11-T):**\n- GQA is behaviorally resilient (E03, E04)\n- But GQA has massive functional collapse (-56% specialization)\n\n**The Question:**\n> Can Indra (chaos injection) induce FUNCTIONAL specialization recovery?\n\n**Important Caveat:**\nThis measures functional recovery under perturbation (noise alters activations), \nNOT permanent structural change to weights.\n\n**Method:**\n1. Load collapsed model (LLaMA-3.1-8B-Instruct)\n2. Inject chaos at different noise levels (\u03c3 = 0.0 to 0.2)\n3. Target different layer regions (Early 0-10, Middle 11-27, Late 28-31, All)\n4. Measure Specialization Index recovery\n\n**Layer Ranges (E06d-0 LLaMA-3.1 Anatomy):**\n- L* = 22\n- Engine Room = layers 11-27\n\n**Recovery Metric:**\n```\nRecovery % = (SI_after - SI_collapsed) / (SI_base - SI_collapsed) \u00d7 100\n```\n\n**Possible Outcomes:**\n\n| Outcome | Recovery | Implication |\n|---------|----------|-------------|\n| A_CONFIRMED | >20% | Chaos induces functional specialization recovery |\n| B_PARTIAL | 5-20% | Partial functional recovery |\n| C_REFUTED | <5% | Functional collapse is irreversible under perturbation |\n\n**If C_REFUTED:**\n- GQA territorial collapse resists perturbation\n- RLHF rewiring is deeply ingrained\n- Behavioral resilience masks functional uniformity\n\n---\n\n*Paper 4: Behavioral Sink Dynamics*\n*E11-T-Indra: Functional Specialization Recovery Test*"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Cell 11: Artifact Log\n", "\n", "artifact_entry = {\n", "    'experiment': 'E11-T-Indra',\n", "    'timestamp': TIMESTAMP,\n", "    'model': MODEL_CONFIG['name'],\n", "    'architecture': 'GQA',\n", "    'verdict': results['verdict']['code'],\n", "    'best_region': results['verdict']['best_region'],\n", "    'best_noise': results['verdict']['best_noise'],\n", "    'best_recovery': results['verdict']['best_recovery'],\n", "    'baseline_si': results['verdict']['baseline_si'],\n", "    'target_si': results['verdict']['target_si'],\n", "    'prompt_count': len(STANDARD_PROMPTS),\n", "    'files': {\n", "        'results': filename,\n", "        'figure': fig_path\n", "    }\n", "}\n", "\n", "artifact_log = f'results/E11T_indra_artifact_log.jsonl'\n", "with open(artifact_log, 'a') as f:\n", "    f.write(json.dumps(artifact_entry) + '\\n')\n", "\n", "print(f\"Artifact log appended: {artifact_log}\")\n", "print(f\"\\nEntry: {json.dumps(artifact_entry, indent=2)}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# ============================================================================\n", "# AUTO-DOWNLOAD RESULTS (Colab only)\n", "# ============================================================================\n", "import glob\n", "import shutil\n", "\n", "def auto_download_results():\n", "    try:\n", "        from google.colab import files\n", "    except ImportError:\n", "        print('Not in Colab - skipping auto-download')\n", "        return\n", "    \n", "    print('=' * 60)\n", "    print('AUTO-DOWNLOADING RESULTS...')\n", "    print('=' * 60)\n", "    \n", "    # Find all result files\n", "    json_files = glob.glob('results/*.json') + glob.glob('figures/*.json')\n", "    png_files = glob.glob('results/*.png') + glob.glob('figures/*.png')\n", "    all_files = json_files + png_files\n", "    \n", "    if not all_files:\n", "        print('WARNING: No result files found!')\n", "        return\n", "    \n", "    print(f'Found {len(all_files)} files')\n", "    \n", "    # Download as ZIP\n", "    import os\n", "    zip_name = f'E11_results_{os.path.basename(os.getcwd())}'\n", "    \n", "    # Create combined folder\n", "    os.makedirs('download_package', exist_ok=True)\n", "    for f in all_files:\n", "        shutil.copy(f, 'download_package/')\n", "    \n", "    shutil.make_archive(zip_name, 'zip', 'download_package')\n", "    print(f'Downloading: {zip_name}.zip')\n", "    files.download(f'{zip_name}.zip')\n", "    print('DOWNLOAD COMPLETE!')\n", "\n", "auto_download_results()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.10.0"}}, "nbformat": 4, "nbformat_minor": 4}