{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# E11-Indra-Gemma27B-V3: Region-Local SI (Codex Fix)\n",
    "\n",
    "**Paper 4: Behavioral Sink Dynamics**\n",
    "\n",
    "## Purpose: Address Codex's Region-Local SI Critique\n",
    "\n",
    "### The Problem (V2)\n",
    "\n",
    "V2 fixed pre-attention injection but SI is still computed **globally**:\n",
    "```\n",
    "SI = 1 - mean_correlation(ALL heads across ALL layers)\n",
    "\n",
    "Problem: Late-only noise affects 16 layers, but SI averages 46 layers\n",
    "         ‚Üí Effect \"diluted\" by 30 unaffected layers\n",
    "         ‚Üí Late = 0% could be dilution artifact!\n",
    "```\n",
    "\n",
    "### The Fix (V3)\n",
    "\n",
    "Compute SI **only from heads in the target layer range**:\n",
    "```\n",
    "Early noise ‚Üí SI_early  = 1 - mean_corr(Early heads only)\n",
    "Late noise  ‚Üí SI_late   = 1 - mean_corr(Late heads only)\n",
    "\n",
    "This isolates the effect to the perturbed region!\n",
    "```\n",
    "\n",
    "### Codex's Critique (Verbatim)\n",
    "\n",
    "> \"Late bleibt 0 ‚Üí noch nicht erkl√§rt. Das ist weiterhin ein methodischer Red-Flag:\n",
    ">  - SI wird global √ºber alle Heads berechnet\n",
    ">  - Late-Noise beeinflusst nur wenige Layers ‚Üí Effekt verschwindet\n",
    ">  - nicht beweisbar, dass Late immun ist\"\n",
    "\n",
    "### Expected Outcomes\n",
    "\n",
    "| Outcome | Meaning | Implication |\n",
    "|---------|---------|-------------|\n",
    "| Late-Local ‚â† 0% | Codex RIGHT | Global SI masked real effect |\n",
    "| Late-Local = 0% | Codex WRONG | Late layers truly immune |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 1: Setup + Seeds (E11-v3 STANDARD)\n!pip install -q transformers torch accelerate bitsandbytes scipy matplotlib seaborn huggingface_hub\n\nimport torch\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom scipy.stats import entropy as scipy_entropy\nimport json\nimport hashlib\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfrom pathlib import Path\nfrom datetime import datetime\n\n# ============ E11-v3 METHODOLOGY STANDARD ============\nSEEDS = [42, 123, 456]  # 3-seed averaging\nDTYPE = torch.bfloat16  # Standardized precision (Note: 8-bit quantization required for 27B)\nEXPECTED_MD5 = \"715065bab181f46bf12ed471951141e2\"  # Standard-10 v3\n\ndef verify_prompts(prompts):\n    \"\"\"Verify Standard-10 prompts via MD5.\"\"\"\n    combined = '|||'.join(prompts)  # Canonical delimiter for MD5\n    actual_md5 = hashlib.md5(combined.encode()).hexdigest()\n    verified = actual_md5 == EXPECTED_MD5\n    print(f\"  Prompt MD5: {actual_md5}\")\n    print(f\"  Expected:   {EXPECTED_MD5}\")\n    print(f\"  Verified:   {'‚úì' if verified else '‚úó MISMATCH!'}\")\n    return verified, actual_md5\n\n# Set initial seed\nSEED = SEEDS[0]\nos.environ['PYTHONHASHSEED'] = str(SEED)\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nTIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')\nPath('results').mkdir(parents=True, exist_ok=True)\nPath('figures').mkdir(parents=True, exist_ok=True)\nprint(f\"Timestamp: {TIMESTAMP}\")\nprint(f\"E11-v3 Standard: Seeds={SEEDS}, dtype={DTYPE}\")\nprint(f\"‚ö†Ô∏è Note: 8-bit quantization required for 27B model (VRAM constraint)\")\n\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    gpu_name = torch.cuda.get_device_name(0)\n    vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n    print(f\"GPU: {gpu_name}\")\n    print(f\"VRAM: {vram_gb:.1f} GB\")\n\n# HF Login\ntry:\n    from google.colab import userdata\n    from huggingface_hub import login\n    hf_token = userdata.get('HF_TOKEN')\n    if hf_token:\n        login(token=hf_token)\n        print(\"HF Login: SUCCESS\")\nexcept:\n    print(\"Not in Colab or no HF_TOKEN\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 2: Configuration (V3 - Region-Local SI) - E11-v3 UPDATED\n\nMODEL_NAME = 'google/gemma-2-27b-it'\n\n# Reference Values (from E08b-G)\nREFERENCE = {\n    'base_si': 0.3490640065124305,\n    'instruct_si': 0.34176792550380786,\n    'state': 'SICK'\n}\n\nRHO_CRIT = 0.267\nNOISE_LEVELS = [0.0, 0.01, 0.02, 0.05, 0.1, 0.2]\nMAX_LENGTH = 128  # E11-v3 Standard\nPRIMARY_SEED = 42\n\n# V2 Reference Results (for comparison)\nV2_RESULTS = {\n    'early': -10.14,\n    'middle': -0.01,\n    'late': 0.0,\n    'all': -9.73\n}\n\n# ============ CANONICAL Standard-10 v3 Prompts ============\n# MD5: 715065bab181f46bf12ed471951141e2\nSTANDARD_PROMPTS = [\n    'What is the capital of France and what is its population?',\n    'If all roses are flowers and some flowers fade quickly, can we conclude that some roses fade quickly? Explain step by step.',\n    'Calculate 47 multiplied by 23 and show your work.',\n    \"Translate the following to German: 'The quick brown fox jumps over the lazy dog'.\",\n    'Write a Python function that checks if a number is prime.',\n    'Summarize the main points: Machine learning is a subset of artificial intelligence that enables systems to learn from data. It uses algorithms to identify patterns and make decisions with minimal human intervention.',\n    \"Statement A: 'All birds can fly.' Statement B: 'Penguins are birds that cannot fly.' Are these statements contradictory? Explain.\",\n    'What are the safety considerations when using a kitchen knife?',\n    'Write a haiku about artificial intelligence.',\n    \"Complete this sentence in a helpful way: 'The best approach to solving complex problems is'\",\n]\n\n# Verify prompts\nprint(\"Verifying Standard-10 prompts...\")\nPROMPTS_VERIFIED, ACTUAL_MD5 = verify_prompts(STANDARD_PROMPTS)\nif not PROMPTS_VERIFIED:\n    raise ValueError(\"PROMPT MISMATCH! Check Standard-10 v3 canonical prompts.\")\n\nprint(f\"\\nE11-Indra-Gemma27B-V3: REGION-LOCAL SI (Codex Fix #2)\")\nprint(f\"\\n{'='*60}\")\nprint(f\"KEY CHANGE: SI computed ONLY from target layer range\")\nprint(f\"            Late-noise ‚Üí SI_late (not SI_global)\")\nprint(f\"            This isolates effect to perturbed region!\")\nprint(f\"{'='*60}\")\nprint(f\"\\nE11-v3 Config: MAX_LENGTH={MAX_LENGTH}, Seeds={SEEDS}\")\nprint(f\"‚ö†Ô∏è 8-bit quantization required (27B model)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: REGION-LOCAL Specialization Metrics (V3 KEY CHANGE!)\n",
    "\n",
    "def extract_head_activations(model, tokenizer, prompts, max_length=128):\n",
    "    \"\"\"\n",
    "    Extract per-head activation patterns WITH attention masks.\n",
    "    Returns full attention tensor for later region-local analysis.\n",
    "    \"\"\"\n",
    "    all_attention_patterns = []\n",
    "    all_attention_masks = []\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        formatted = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            formatted, \n",
    "            return_tensors='pt',\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length'\n",
    "        ).to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, output_attentions=True)\n",
    "        \n",
    "        attn_stack = torch.stack([a.squeeze(0) for a in outputs.attentions], dim=0)\n",
    "        all_attention_patterns.append(attn_stack.cpu())\n",
    "        all_attention_masks.append(inputs['attention_mask'].squeeze(0).cpu())\n",
    "    \n",
    "    return {\n",
    "        'attention_patterns': all_attention_patterns,\n",
    "        'attention_masks': all_attention_masks,\n",
    "        'num_layers': len(outputs.attentions),\n",
    "        'num_heads': outputs.attentions[0].shape[1]\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_head_entropy_profiles(attention_patterns, attention_masks=None):\n",
    "    \"\"\"\n",
    "    Compute normalized entropy for each head across prompts.\n",
    "    Returns FULL matrix (num_layers, num_heads) for region-local analysis.\n",
    "    \"\"\"\n",
    "    num_prompts = len(attention_patterns)\n",
    "    num_layers = attention_patterns[0].shape[0]\n",
    "    num_heads = attention_patterns[0].shape[1]\n",
    "    \n",
    "    all_entropies = np.zeros((num_prompts, num_layers, num_heads))\n",
    "    \n",
    "    for p_idx, attn in enumerate(attention_patterns):\n",
    "        mask = attention_masks[p_idx] if attention_masks is not None else None\n",
    "        \n",
    "        for layer in range(num_layers):\n",
    "            for head in range(num_heads):\n",
    "                attn_weights = attn[layer, head].float().cpu().numpy()\n",
    "                \n",
    "                if mask is not None:\n",
    "                    mask_np = mask.numpy()\n",
    "                    valid_len = mask_np.sum()\n",
    "                    if valid_len > 0:\n",
    "                        attn_weights = attn_weights[:, :valid_len]\n",
    "                        attn_weights = attn_weights.mean(axis=0)\n",
    "                    else:\n",
    "                        attn_weights = attn_weights.mean(axis=0)\n",
    "                else:\n",
    "                    attn_weights = attn_weights.mean(axis=0)\n",
    "                \n",
    "                attn_weights = attn_weights / (attn_weights.sum() + 1e-10)\n",
    "                attn_weights = attn_weights[attn_weights > 0]\n",
    "                \n",
    "                if len(attn_weights) > 1:\n",
    "                    h = scipy_entropy(attn_weights, base=2)\n",
    "                    h_max = np.log2(len(attn_weights))\n",
    "                    h_norm = h / h_max if h_max > 0 else 0\n",
    "                else:\n",
    "                    h_norm = 0\n",
    "                \n",
    "                all_entropies[p_idx, layer, head] = h_norm\n",
    "    \n",
    "    return all_entropies.mean(axis=0)  # (num_layers, num_heads)\n",
    "\n",
    "\n",
    "def compute_specialization_metrics_global(head_entropies):\n",
    "    \"\"\"Compute GLOBAL SI (V1/V2 method) - for comparison.\"\"\"\n",
    "    num_layers, num_heads = head_entropies.shape\n",
    "    \n",
    "    layer_variances = np.var(head_entropies, axis=1)\n",
    "    mean_variance = float(np.mean(layer_variances))\n",
    "    \n",
    "    head_profiles = head_entropies.T  # (num_heads, num_layers)\n",
    "    head_corr_matrix = np.corrcoef(head_profiles)\n",
    "    upper_tri = head_corr_matrix[np.triu_indices(num_heads, k=1)]\n",
    "    mean_head_correlation = float(np.nanmean(upper_tri))\n",
    "    \n",
    "    specialization_index = 1.0 - mean_head_correlation\n",
    "    \n",
    "    return {\n",
    "        'mean_head_variance': mean_variance,\n",
    "        'mean_head_correlation': mean_head_correlation,\n",
    "        'specialization_index': specialization_index,\n",
    "        'num_layers': num_layers,\n",
    "        'num_heads': num_heads,\n",
    "        'method': 'GLOBAL'\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_specialization_metrics_local(head_entropies, layer_start, layer_end):\n",
    "    \"\"\"\n",
    "    V3 KEY FUNCTION: Compute REGION-LOCAL SI.\n",
    "    \n",
    "    SI is computed ONLY from heads in layers [layer_start, layer_end).\n",
    "    This isolates the effect to the perturbed region!\n",
    "    \n",
    "    Args:\n",
    "        head_entropies: Full entropy matrix (num_layers, num_heads)\n",
    "        layer_start: Start of target region (inclusive)\n",
    "        layer_end: End of target region (exclusive)\n",
    "    \n",
    "    Returns:\n",
    "        SI computed only from target region heads\n",
    "    \"\"\"\n",
    "    # Extract only the target layer range\n",
    "    local_entropies = head_entropies[layer_start:layer_end, :]  # (local_layers, num_heads)\n",
    "    \n",
    "    local_layers, num_heads = local_entropies.shape\n",
    "    \n",
    "    if local_layers == 0:\n",
    "        return {\n",
    "            'mean_head_variance': 0.0,\n",
    "            'mean_head_correlation': 0.0,\n",
    "            'specialization_index': 0.0,\n",
    "            'num_layers': 0,\n",
    "            'num_heads': num_heads,\n",
    "            'method': 'LOCAL',\n",
    "            'layer_range': [layer_start, layer_end]\n",
    "        }\n",
    "    \n",
    "    # Compute variance per layer (local)\n",
    "    layer_variances = np.var(local_entropies, axis=1)\n",
    "    mean_variance = float(np.mean(layer_variances))\n",
    "    \n",
    "    # Compute head correlation (local)\n",
    "    # Each head's profile is now only over the local layers\n",
    "    head_profiles = local_entropies.T  # (num_heads, local_layers)\n",
    "    \n",
    "    # Need at least 2 data points per head for correlation\n",
    "    if local_layers < 2:\n",
    "        # Can't compute meaningful correlation with 1 layer\n",
    "        # Fall back to variance-based estimate\n",
    "        mean_head_correlation = 1.0 - mean_variance  # rough estimate\n",
    "    else:\n",
    "        head_corr_matrix = np.corrcoef(head_profiles)\n",
    "        upper_tri = head_corr_matrix[np.triu_indices(num_heads, k=1)]\n",
    "        mean_head_correlation = float(np.nanmean(upper_tri))\n",
    "    \n",
    "    specialization_index = 1.0 - mean_head_correlation\n",
    "    \n",
    "    return {\n",
    "        'mean_head_variance': mean_variance,\n",
    "        'mean_head_correlation': mean_head_correlation,\n",
    "        'specialization_index': specialization_index,\n",
    "        'num_layers': local_layers,\n",
    "        'num_heads': num_heads,\n",
    "        'method': 'LOCAL',\n",
    "        'layer_range': [layer_start, layer_end]\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Region-Local SI functions defined (V3).\")\n",
    "print(\"\")\n",
    "print(\"KEY CHANGE FROM V2:\")\n",
    "print(\"  - compute_specialization_metrics_global(): V1/V2 method (all layers)\")\n",
    "print(\"  - compute_specialization_metrics_local(): V3 method (target layers only)\")\n",
    "print(\"\")\n",
    "print(\"Example:\")\n",
    "print(\"  Late-only noise (layers 30-46) ‚Üí compute_local(entropies, 30, 46)\")\n",
    "print(\"  SI_late reflects ONLY Late layer heads ‚Üí no dilution!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: PRE-ATTENTION Noise Injector (same as V2)\n",
    "\n",
    "class PreAttentionNoiseInjector:\n",
    "    \"\"\"\n",
    "    V2/V3: Inject Gaussian noise BEFORE attention computation.\n",
    "    (Same as V2 - the change is in SI measurement, not injection)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, target_range, noise_std=0.0):\n",
    "        self.model = model\n",
    "        self.target_start, self.target_end = target_range\n",
    "        self.noise_std = noise_std\n",
    "        self.hooks = []\n",
    "    \n",
    "    def _make_pre_hook(self, layer_idx):\n",
    "        def hook(module, args):\n",
    "            if self.noise_std > 0 and self.target_start <= layer_idx < self.target_end:\n",
    "                hidden_states = args[0]\n",
    "                noise = torch.randn_like(hidden_states) * self.noise_std\n",
    "                noisy_hidden_states = hidden_states + noise\n",
    "                return (noisy_hidden_states,) + args[1:]\n",
    "            return args\n",
    "        return hook\n",
    "    \n",
    "    def attach(self):\n",
    "        for idx, layer in enumerate(self.model.model.layers):\n",
    "            hook = layer.register_forward_pre_hook(self._make_pre_hook(idx))\n",
    "            self.hooks.append(hook)\n",
    "    \n",
    "    def detach(self):\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks = []\n",
    "    \n",
    "    def set_noise(self, std):\n",
    "        self.noise_std = std\n",
    "\n",
    "print(\"PRE-Attention noise injector class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 5: Load Model + Architecture Detection (DUAL-RHO PATCHED)\n\nprint(f\"\\n{'='*60}\")\nprint(f\"PHASE 1: LOAD MODEL (8-BIT QUANTIZATION)\")\nprint(f\"{'='*60}\")\n\nprint(f\"\\nLoading: {MODEL_NAME}\")\n\nbnb_config = BitsAndBytesConfig(\n    load_in_8bit=True,\n    bnb_8bit_compute_dtype=torch.float16\n)\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nQUANTIZATION = \"8bit\"\n\nprint(f\"\\nLoading with 8-bit quantization...\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    quantization_config=bnb_config,\n    device_map='auto',\n    trust_remote_code=True,\n    attn_implementation=\"eager\"\n)\nmodel.eval()\n\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n# Architecture detection\nconfig = model.config\nnum_layers = config.num_hidden_layers\nnum_query_heads = config.num_attention_heads\nnum_kv_heads = getattr(config, 'num_key_value_heads', num_query_heads)\nhidden_size = config.hidden_size\nd_head = hidden_size // num_query_heads\n\n# === DUAL-RHO CALCULATION (Codex Fix) ===\n# rho_kv: Original definition (kv_heads / num_layers) - for backward compatibility\n# rho_head: Paper 3 definition (num_heads / sqrt(d_model)) - theoretical basis\nimport math\nrho_kv = num_kv_heads / num_layers\nrho_head = num_query_heads / math.sqrt(hidden_size)\n\n# Use rho_head as primary (Paper 3 consistent), keep rho_kv for reference\nrho = rho_head  # PRIMARY\n\nif num_kv_heads == num_query_heads:\n    attn_type = \"MHA\"\nelif num_kv_heads == 1:\n    attn_type = \"MQA\"\nelse:\n    attn_type = f\"GQA ({num_query_heads}:{num_kv_heads})\"\n\nhas_swa = hasattr(config, 'sliding_window') and config.sliding_window is not None\narchitecture = f\"{attn_type}+SWA\" if has_swa else attn_type\n\nMODEL_CONFIG = {\n    'name': MODEL_NAME,\n    'num_layers': num_layers,\n    'num_query_heads': num_query_heads,\n    'num_kv_heads': num_kv_heads,\n    'd_head': d_head,\n    'hidden_size': hidden_size,\n    'architecture': architecture,\n    # DUAL-RHO (Codex Fix)\n    'rho': rho,           # Primary (rho_head)\n    'rho_head': rho_head, # num_heads / sqrt(d_model) - Paper 3\n    'rho_kv': rho_kv,     # kv_heads / num_layers - legacy\n    'rho_crit': RHO_CRIT\n}\n\nthird = num_layers // 3\nLAYER_RANGES = {\n    'early': (0, third),\n    'middle': (third, 2*third),\n    'late': (2*third, num_layers),\n    'all': (0, num_layers)\n}\n\nprint(f\"\\nArchitecture: {architecture}\")\nprint(f\"Layers: {num_layers}, Heads: {num_query_heads}, d_head: {d_head}\")\nprint(f\"\\n=== DUAL-RHO (Codex Fix) ===\")\nprint(f\"  rho_head = {rho_head:.4f} (num_heads / sqrt(d_model)) ‚Üê PRIMARY\")\nprint(f\"  rho_kv   = {rho_kv:.4f} (kv_heads / num_layers) ‚Üê legacy\")\nprint(f\"  rho_crit = {RHO_CRIT}\")\nprint(f\"  Status: {'ABOVE rho_crit (POISON expected)' if rho > RHO_CRIT else 'BELOW rho_crit'}\")\nprint(f\"\\nLayer Ranges:\")\nfor region, (start, end) in LAYER_RANGES.items():\n    print(f\"  {region}: layers {start}-{end-1} ({end-start} layers)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Baseline Measurement (BOTH Global and Local)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"PHASE 2: BASELINE (Global + Region-Local SI)\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "baseline_activations = extract_head_activations(model, tokenizer, STANDARD_PROMPTS, max_length=MAX_LENGTH)\n",
    "baseline_entropies = compute_head_entropy_profiles(\n",
    "    baseline_activations['attention_patterns'],\n",
    "    baseline_activations['attention_masks']\n",
    ")\n",
    "\n",
    "# Global SI (V1/V2 method)\n",
    "baseline_global = compute_specialization_metrics_global(baseline_entropies)\n",
    "\n",
    "# Region-Local SI for each region (V3 method)\n",
    "baseline_local = {}\n",
    "for region_name, (start, end) in LAYER_RANGES.items():\n",
    "    baseline_local[region_name] = compute_specialization_metrics_local(baseline_entropies, start, end)\n",
    "\n",
    "print(f\"\\nBaseline Results:\")\n",
    "print(f\"\\n  GLOBAL SI (V1/V2):\")\n",
    "print(f\"    SI = {baseline_global['specialization_index']:.4f}\")\n",
    "print(f\"    Corr = {baseline_global['mean_head_correlation']:.4f}\")\n",
    "\n",
    "print(f\"\\n  REGION-LOCAL SI (V3):\")\n",
    "for region_name, metrics in baseline_local.items():\n",
    "    print(f\"    {region_name}: SI_local = {metrics['specialization_index']:.4f} (layers {metrics['layer_range']})\")\n",
    "\n",
    "# Store baselines\n",
    "results = {\n",
    "    'baseline_global': baseline_global,\n",
    "    'baseline_local': baseline_local,\n",
    "    'treatments_global': [],\n",
    "    'treatments_local': [],\n",
    "    'quantization': QUANTIZATION,\n",
    "    'injection_method': 'PRE-ATTENTION',\n",
    "    'si_method': 'GLOBAL + LOCAL (V3)'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: V3 Treatment Loop - BOTH Global and Local SI\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"PHASE 3: INDRA V3 - REGION-LOCAL SI MEASUREMENT\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nKEY: For each region, we compute BOTH:\")\n",
    "print(f\"     - SI_global (V2 method) - for comparison\")\n",
    "print(f\"     - SI_local (V3 method) - isolates perturbed region\")\n",
    "print(f\"\\nRunning {len(SEEDS)} seeds: {SEEDS}\")\n",
    "\n",
    "all_seed_results = {seed: {'global': [], 'local': []} for seed in SEEDS}\n",
    "\n",
    "for seed_idx, current_seed in enumerate(SEEDS):\n",
    "    print(f\"\\n{'#'*60}\")\n",
    "    print(f\"SEED {seed_idx+1}/{len(SEEDS)}: {current_seed}\")\n",
    "    print(f\"{'#'*60}\")\n",
    "    \n",
    "    for region_name, (start, end) in LAYER_RANGES.items():\n",
    "        print(f\"\\n  TREATING: {region_name.upper()} (Layers {start}-{end-1})\")\n",
    "        \n",
    "        region_global = {\n",
    "            'region': region_name,\n",
    "            'layer_range': [start, end],\n",
    "            'seed': current_seed,\n",
    "            'si_method': 'GLOBAL',\n",
    "            'noise_tests': []\n",
    "        }\n",
    "        \n",
    "        region_local = {\n",
    "            'region': region_name,\n",
    "            'layer_range': [start, end],\n",
    "            'seed': current_seed,\n",
    "            'si_method': 'LOCAL',\n",
    "            'noise_tests': []\n",
    "        }\n",
    "        \n",
    "        for noise_std in NOISE_LEVELS:\n",
    "            torch.manual_seed(current_seed)\n",
    "            np.random.seed(current_seed)\n",
    "            random.seed(current_seed)\n",
    "            \n",
    "            injector = PreAttentionNoiseInjector(model, (start, end), noise_std=noise_std)\n",
    "            injector.attach()\n",
    "            \n",
    "            treated_activations = extract_head_activations(\n",
    "                model, tokenizer, STANDARD_PROMPTS, max_length=MAX_LENGTH\n",
    "            )\n",
    "            treated_entropies = compute_head_entropy_profiles(\n",
    "                treated_activations['attention_patterns'],\n",
    "                treated_activations['attention_masks']\n",
    "            )\n",
    "            \n",
    "            injector.detach()\n",
    "            \n",
    "            # ============ GLOBAL SI (V2 method) ============\n",
    "            treated_global = compute_specialization_metrics_global(treated_entropies)\n",
    "            \n",
    "            si_before_global = baseline_global['specialization_index']\n",
    "            si_after_global = treated_global['specialization_index']\n",
    "            si_delta_global = si_after_global - si_before_global\n",
    "            change_pct_global = (si_delta_global / si_before_global) * 100 if si_before_global > 0 else 0\n",
    "            \n",
    "            region_global['noise_tests'].append({\n",
    "                'noise_std': float(noise_std),\n",
    "                'si': treated_global['specialization_index'],\n",
    "                'si_delta': float(si_delta_global),\n",
    "                'change_pct': float(change_pct_global)\n",
    "            })\n",
    "            \n",
    "            # ============ LOCAL SI (V3 method) ============\n",
    "            treated_local = compute_specialization_metrics_local(treated_entropies, start, end)\n",
    "            baseline_local_region = baseline_local[region_name]\n",
    "            \n",
    "            si_before_local = baseline_local_region['specialization_index']\n",
    "            si_after_local = treated_local['specialization_index']\n",
    "            si_delta_local = si_after_local - si_before_local\n",
    "            change_pct_local = (si_delta_local / si_before_local) * 100 if si_before_local > 0 else 0\n",
    "            \n",
    "            region_local['noise_tests'].append({\n",
    "                'noise_std': float(noise_std),\n",
    "                'si': treated_local['specialization_index'],\n",
    "                'si_delta': float(si_delta_local),\n",
    "                'change_pct': float(change_pct_local)\n",
    "            })\n",
    "            \n",
    "            # Print comparison (primary seed only)\n",
    "            if current_seed == PRIMARY_SEED:\n",
    "                print(f\"    sigma={noise_std:.2f}: Global={change_pct_global:+.2f}%, Local={change_pct_local:+.2f}%\")\n",
    "        \n",
    "        # Store min/max changes\n",
    "        region_global['min_change_pct'] = min(t['change_pct'] for t in region_global['noise_tests'])\n",
    "        region_global['max_change_pct'] = max(t['change_pct'] for t in region_global['noise_tests'])\n",
    "        region_local['min_change_pct'] = min(t['change_pct'] for t in region_local['noise_tests'])\n",
    "        region_local['max_change_pct'] = max(t['change_pct'] for t in region_local['noise_tests'])\n",
    "        \n",
    "        all_seed_results[current_seed]['global'].append(region_global)\n",
    "        all_seed_results[current_seed]['local'].append(region_local)\n",
    "\n",
    "# Aggregate across seeds\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"AGGREGATING ACROSS {len(SEEDS)} SEEDS\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "aggregated = {'global': {}, 'local': {}}\n",
    "\n",
    "for si_method in ['global', 'local']:\n",
    "    for region_name in LAYER_RANGES.keys():\n",
    "        region_changes = []\n",
    "        for seed in SEEDS:\n",
    "            seed_data = next(t for t in all_seed_results[seed][si_method] if t['region'] == region_name)\n",
    "            region_changes.append(seed_data['min_change_pct'])\n",
    "        \n",
    "        aggregated[si_method][region_name] = {\n",
    "            'mean': float(np.mean(region_changes)),\n",
    "            'std': float(np.std(region_changes)),\n",
    "            'values': region_changes\n",
    "        }\n",
    "\n",
    "# Print comparison table\n",
    "print(f\"\\n{'Region':<10} {'Global (V2)':<20} {'Local (V3)':<20} {'Diff':<10}\")\n",
    "print(\"-\"*60)\n",
    "for region_name in LAYER_RANGES.keys():\n",
    "    g = aggregated['global'][region_name]\n",
    "    l = aggregated['local'][region_name]\n",
    "    diff = l['mean'] - g['mean']\n",
    "    print(f\"{region_name:<10} {g['mean']:+.2f}% +/- {g['std']:.2f}%{'':>3} {l['mean']:+.2f}% +/- {l['std']:.2f}%{'':>3} {diff:+.2f}%\")\n",
    "\n",
    "results['treatments_global'] = all_seed_results[PRIMARY_SEED]['global']\n",
    "results['treatments_local'] = all_seed_results[PRIMARY_SEED]['local']\n",
    "results['multi_seed_results'] = all_seed_results\n",
    "results['aggregated'] = aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: V3 Verdict - Codex Critique Resolution\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"PHASE 4: V3 VERDICT - CODEX REGION-LOCAL CRITIQUE\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"CODEX'S REGION-LOCAL CRITIQUE\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\nCodex said: 'Late = 0% not proven because SI is global.'\")\n",
    "print(f\"            'SI computed globally dilutes Late-only noise effect.'\")\n",
    "print(f\"            'Need region-local SI to prove Late is truly immune.'\")\n",
    "\n",
    "late_global = aggregated['global']['late']['mean']\n",
    "late_local = aggregated['local']['late']['mean']\n",
    "late_local_std = aggregated['local']['late']['std']\n",
    "\n",
    "print(f\"\\nLATE REGION RESULTS:\")\n",
    "print(f\"  V2 Global SI: {late_global:+.2f}%\")\n",
    "print(f\"  V3 Local SI:  {late_local:+.2f}% +/- {late_local_std:.2f}%\")\n",
    "\n",
    "# Determine verdict\n",
    "if abs(late_local) > 1.0:\n",
    "    print(f\"\\n  VERDICT: CODEX CONFIRMED (Late Dilution)\")\n",
    "    print(f\"  Late WAS being diluted by global SI!\")\n",
    "    print(f\"  Local SI reveals real Late effect: {late_local:+.2f}%\")\n",
    "    codex_local_verdict = \"DILUTION_CONFIRMED\"\n",
    "else:\n",
    "    print(f\"\\n  VERDICT: CODEX REFUTED (Late Truly Immune)\")\n",
    "    print(f\"  Even with Local SI, Late = {late_local:+.2f}%\")\n",
    "    print(f\"  Late layers are truly noise-immune!\")\n",
    "    codex_local_verdict = \"IMMUNITY_CONFIRMED\"\n",
    "\n",
    "# Full comparison table\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"FULL COMPARISON: V2 (Global) vs V3 (Local)\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\n{'Region':<10} {'V2 Global':<12} {'V3 Local':<12} {'V2 Ref':<12} {'Interpretation'}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for region_name in ['early', 'middle', 'late', 'all']:\n",
    "    g = aggregated['global'][region_name]['mean']\n",
    "    l = aggregated['local'][region_name]['mean']\n",
    "    v2_ref = V2_RESULTS[region_name]\n",
    "    \n",
    "    if abs(l - g) > 2:\n",
    "        interp = \"DILUTION DETECTED\"\n",
    "    elif abs(l) < 1 and abs(g) < 1:\n",
    "        interp = \"Both ~0: Immune\"\n",
    "    elif l < -5:\n",
    "        interp = \"POISON (local)\"\n",
    "    else:\n",
    "        interp = \"Similar\"\n",
    "    \n",
    "    print(f\"{region_name:<10} {g:+.2f}%{'':>5} {l:+.2f}%{'':>5} {v2_ref:+.2f}%{'':>5} {interp}\")\n",
    "\n",
    "# Store verdict\n",
    "results['verdict'] = {\n",
    "    'codex_local_verdict': codex_local_verdict,\n",
    "    'late_global': late_global,\n",
    "    'late_local': late_local,\n",
    "    'late_local_std': late_local_std,\n",
    "    'v2_reference': V2_RESULTS,\n",
    "    'aggregated': aggregated,\n",
    "    'seeds_used': SEEDS,\n",
    "    'model': MODEL_CONFIG['name'],\n",
    "    'rho': MODEL_CONFIG['rho'],\n",
    "    'architecture': MODEL_CONFIG['architecture']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Visualization - Global vs Local SI Comparison\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "\n",
    "colors = {'early': '#3498db', 'middle': '#2ecc71', 'late': '#e74c3c', 'all': '#9b59b6'}\n",
    "\n",
    "# Plot 1: Global vs Local Bar Chart\n",
    "ax1 = axes[0, 0]\n",
    "regions = ['early', 'middle', 'late', 'all']\n",
    "global_vals = [aggregated['global'][r]['mean'] for r in regions]\n",
    "local_vals = [aggregated['local'][r]['mean'] for r in regions]\n",
    "global_stds = [aggregated['global'][r]['std'] for r in regions]\n",
    "local_stds = [aggregated['local'][r]['std'] for r in regions]\n",
    "\n",
    "x = np.arange(len(regions))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax1.bar(x - width/2, global_vals, width, yerr=global_stds, label='Global SI (V2)', color='gray', alpha=0.7, capsize=3)\n",
    "bars2 = ax1.bar(x + width/2, local_vals, width, yerr=local_stds, label='Local SI (V3)', color='blue', alpha=0.7, capsize=3)\n",
    "\n",
    "ax1.axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
    "ax1.axhline(y=-5, color='red', linestyle=':', alpha=0.5)\n",
    "ax1.set_ylabel('SI Change %')\n",
    "ax1.set_title('V3: Global vs Local SI\\n(Does Local SI reveal Late effect?)')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels([r.capitalize() for r in regions])\n",
    "ax1.legend()\n",
    "ax1.set_ylim(-20, 5)\n",
    "\n",
    "# Highlight Late region\n",
    "ax1.annotate('Codex\\nCritique', xy=(2 + width/2, local_vals[2]), \n",
    "             xytext=(2.5, local_vals[2] - 5),\n",
    "             fontsize=9, color='red', fontweight='bold',\n",
    "             arrowprops=dict(arrowstyle='->', color='red', lw=1))\n",
    "\n",
    "# Plot 2: Local SI Dose-Response\n",
    "ax2 = axes[0, 1]\n",
    "for treatment in results['treatments_local']:\n",
    "    region = treatment['region']\n",
    "    noise_levels = [t['noise_std'] for t in treatment['noise_tests']]\n",
    "    change_vals = [t['change_pct'] for t in treatment['noise_tests']]\n",
    "    ax2.plot(noise_levels, change_vals, 'o-', color=colors[region], \n",
    "             label=f\"{region.capitalize()} (local)\", linewidth=2, markersize=8)\n",
    "\n",
    "ax2.axhline(y=0, color='black', linestyle='--')\n",
    "ax2.axhline(y=-5, color='red', linestyle=':', alpha=0.5)\n",
    "ax2.set_xlabel('Noise Level (sigma)')\n",
    "ax2.set_ylabel('Local SI Change %')\n",
    "ax2.set_title('V3 Local SI Dose-Response\\n(Each region measured locally)')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Global SI Dose-Response (for comparison)\n",
    "ax3 = axes[1, 0]\n",
    "for treatment in results['treatments_global']:\n",
    "    region = treatment['region']\n",
    "    noise_levels = [t['noise_std'] for t in treatment['noise_tests']]\n",
    "    change_vals = [t['change_pct'] for t in treatment['noise_tests']]\n",
    "    ax3.plot(noise_levels, change_vals, 'o--', color=colors[region], \n",
    "             label=f\"{region.capitalize()} (global)\", linewidth=2, markersize=8, alpha=0.7)\n",
    "\n",
    "ax3.axhline(y=0, color='black', linestyle='--')\n",
    "ax3.axhline(y=-5, color='red', linestyle=':', alpha=0.5)\n",
    "ax3.set_xlabel('Noise Level (sigma)')\n",
    "ax3.set_ylabel('Global SI Change %')\n",
    "ax3.set_title('V2 Global SI Dose-Response (Reference)\\n(Diluted by non-perturbed layers)')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Verdict Summary\n",
    "ax4 = axes[1, 1]\n",
    "ax4.axis('off')\n",
    "\n",
    "verdict_text = f\"\"\"E11-INDRA-GEMMA27B-V3: REGION-LOCAL SI\n",
    "{'='*50}\n",
    "\n",
    "CODEX CRITIQUE:\n",
    "\"Late = 0% not proven because SI is global.\n",
    " Need region-local SI to isolate effect.\"\n",
    "\n",
    "V3 RESULTS:\n",
    "  Late Global (V2): {late_global:+.2f}%\n",
    "  Late Local (V3):  {late_local:+.2f}% +/- {late_local_std:.2f}%\n",
    "\n",
    "VERDICT: {codex_local_verdict}\n",
    "\"\"\"\n",
    "\n",
    "if codex_local_verdict == \"IMMUNITY_CONFIRMED\":\n",
    "    verdict_text += \"\"\"\n",
    "Late layers are TRULY IMMUNE to noise!\n",
    "Even with region-local SI, Late = ~0%.\n",
    "This is biological, not methodological.\n",
    "\n",
    "Interpretation:\n",
    "- Late layers = \"frozen\" output patterns\n",
    "- SWA may create locality that resists perturbation\n",
    "- Paper claim STRENGTHENED\n",
    "\"\"\"\n",
    "else:\n",
    "    verdict_text += \"\"\"\n",
    "Global SI WAS hiding Late layer effect!\n",
    "Local SI reveals true Late response.\n",
    "\n",
    "Interpretation:\n",
    "- V2 Late = 0% was dilution artifact\n",
    "- V3 shows real (non-zero) Late effect\n",
    "- Paper needs methodological note\n",
    "\"\"\"\n",
    "\n",
    "ax4.text(0.05, 0.95, verdict_text, transform=ax4.transAxes, fontsize=10,\n",
    "         verticalalignment='top', fontfamily='monospace',\n",
    "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.suptitle(f'E11-Indra-Gemma27B-V3: Region-Local SI\\nCodex Critique: {codex_local_verdict}', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "\n",
    "fig_path = f'figures/E11_indra_gemma27b_v3_{TIMESTAMP}.png'\n",
    "plt.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFigure saved: {fig_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 10: Save Results (E11-v3 METHODOLOGY BLOCK)\n\ndef convert_to_native(obj):\n    if isinstance(obj, dict):\n        return {k: convert_to_native(v) for k, v in obj.items()}\n    elif isinstance(obj, list):\n        return [convert_to_native(v) for v in obj]\n    elif isinstance(obj, tuple):\n        return tuple(convert_to_native(v) for v in obj)\n    elif isinstance(obj, (np.bool_, np.integer)):\n        return int(obj)\n    elif isinstance(obj, np.floating):\n        return float(obj)\n    elif isinstance(obj, np.ndarray):\n        return obj.tolist()\n    else:\n        return obj\n\nfilename = f'results/E11_indra_gemma27b_v3_{TIMESTAMP}.json'\n\noutput = {\n    'experiment': 'E11-Indra-Gemma27B-V3',\n    'purpose': 'Region-Local SI - Address Codex Dilution Critique',\n    'timestamp': TIMESTAMP,\n    'model': MODEL_CONFIG['name'],\n    'architecture': MODEL_CONFIG['architecture'],\n    \n    # === E11-v3 METHODOLOGY BLOCK ===\n    'methodology': {\n        'standard': 'E11-v3',\n        'seeds': SEEDS,\n        'max_length': MAX_LENGTH,\n        'dtype': str(DTYPE),\n        'prompt_md5': ACTUAL_MD5,\n        'prompt_md5_verified': PROMPTS_VERIFIED,\n        'num_prompts': len(STANDARD_PROMPTS),\n        'prompt_set': 'Standard-10 v3',\n        'quantization': '8-bit (required for 27B)',\n        'quantization_note': 'Full precision requires A100-80GB',\n        'si_method': 'GLOBAL + LOCAL (V3)',\n        'injection_method': 'PRE-ATTENTION'\n    },\n    \n    # === DUAL-RHO (Codex Fix) ===\n    'rho': MODEL_CONFIG['rho'],           # Primary (rho_head)\n    'rho_head': MODEL_CONFIG['rho_head'], # num_heads / sqrt(d_model) - Paper 3\n    'rho_kv': MODEL_CONFIG['rho_kv'],     # kv_heads / num_layers - legacy\n    'rho_definition': {\n        'primary': 'rho_head',\n        'rho_head_formula': 'num_heads / sqrt(d_model)',\n        'rho_kv_formula': 'kv_heads / num_layers',\n        'note': 'rho_head is Paper 3 consistent, rho_kv for backward compatibility'\n    },\n    'rho_crit': MODEL_CONFIG['rho_crit'],\n    'd_head': MODEL_CONFIG['d_head'],\n    'quantization': QUANTIZATION,\n    'injection_method': 'PRE-ATTENTION',\n    'si_method': 'GLOBAL + LOCAL',\n    'codex_critique': {\n        'original': 'Late = 0% not proven because SI is global. Late-noise diluted by 30 unaffected layers.',\n        'fix': 'V3 computes SI only from heads in target layer range (region-local SI).',\n        'expected': 'If Late truly immune, Local SI also = 0%. If dilution artifact, Local SI != 0%.'\n    },\n    'v2_reference': V2_RESULTS,\n    'layer_ranges': {k: list(v) for k, v in LAYER_RANGES.items()},\n    'noise_levels': NOISE_LEVELS,\n    'seeds': SEEDS,\n    'prompt_set': 'Standard-10 v3',\n    'num_prompts': len(STANDARD_PROMPTS),\n    'results': convert_to_native(results)\n}\n\nwith open(filename, 'w') as f:\n    json.dump(output, f, indent=2)\n\nprint(f\"Results saved: {filename}\")\nprint(f\"\\nüìã E11-v3 Compliance:\")\nprint(f\"   Seeds: {SEEDS} ‚úì\")\nprint(f\"   dtype: {DTYPE} (8-bit for 27B) ‚ö†Ô∏è\")\nprint(f\"   MD5: {ACTUAL_MD5} {'‚úì' if PROMPTS_VERIFIED else '‚úó'}\")\nprint(f\"   MAX_LENGTH: {MAX_LENGTH} ‚úì\")\nprint(f\"\\n=== DUAL-RHO in output ===\")\nprint(f\"  rho (primary): {output['rho']:.4f}\")\nprint(f\"  rho_head:      {output['rho_head']:.4f}\")\nprint(f\"  rho_kv:        {output['rho_kv']:.4f}\")\n\ntry:\n    from google.colab import files\n    files.download(filename)\n    files.download(fig_path)\nexcept:\n    pass"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Auto-Download\n",
    "\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "def auto_download_results():\n",
    "    try:\n",
    "        from google.colab import files\n",
    "    except ImportError:\n",
    "        print('Not in Colab - skipping auto-download')\n",
    "        return\n",
    "    \n",
    "    print('=' * 60)\n",
    "    print('AUTO-DOWNLOADING RESULTS...')\n",
    "    print('=' * 60)\n",
    "    \n",
    "    json_files = glob.glob('results/*.json') + glob.glob('figures/*.json')\n",
    "    png_files = glob.glob('results/*.png') + glob.glob('figures/*.png')\n",
    "    all_files = json_files + png_files\n",
    "    \n",
    "    if not all_files:\n",
    "        print('WARNING: No result files found!')\n",
    "        return\n",
    "    \n",
    "    print(f'Found {len(all_files)} files')\n",
    "    \n",
    "    import os\n",
    "    zip_name = f'E11_indra_gemma27b_v3_results_{TIMESTAMP}'\n",
    "    \n",
    "    os.makedirs('download_package', exist_ok=True)\n",
    "    for f in all_files:\n",
    "        shutil.copy(f, 'download_package/')\n",
    "    \n",
    "    shutil.make_archive(zip_name, 'zip', 'download_package')\n",
    "    print(f'Downloading: {zip_name}.zip')\n",
    "    files.download(f'{zip_name}.zip')\n",
    "    print('DOWNLOAD COMPLETE!')\n",
    "\n",
    "auto_download_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": "---\n\n## Summary: E11-Indra-Gemma27B-V3\n\n### Purpose\n\nAddress Codex's region-local SI critique:\n> \"Late = 0% not proven because SI is global. Late-noise diluted by 30 unaffected layers.\"\n\n### Key Changes from V2\n\n| Aspect | V2 | V3 |\n|--------|----|----|  \n| SI Measurement | Global (all layers) | Local (target layers only) |\n| Late-noise SI | Computed over 46 layers | Computed over 16 Late layers |\n| Dilution | 30 unaffected layers included | Only affected layers included |\n| **œÅ Definition** | `kv/num_layers` only | **DUAL: rho_head + rho_kv** |\n\n### Dual-œÅ Patch (Codex Fix)\n\n| Name | Formula | Value (Gemma-27B) | Use |\n|------|---------|-------------------|-----|\n| `rho_head` | `num_heads / ‚àöd_model` | ~0.534 | **PRIMARY** (Paper 3) |\n| `rho_kv` | `kv_heads / num_layers` | ~0.348 | Legacy compatibility |\n\nBoth values > œÅ_crit (0.267) ‚Üí Poison classification unchanged.\n\n### Possible Outcomes\n\n| Result | Meaning | Implication |\n|--------|---------|-------------|\n| Late Local = 0% | Late truly immune | V2 result biological, paper STRONGER |\n| Late Local != 0% | Dilution artifact | V2 Late=0% was methodological |\n\n### Codex Improvement Suggestions Addressed\n\n1. ‚úÖ **Region-local SI** (this notebook)\n2. ‚úÖ **3+ seeds** (V2/V3: 3 seeds)\n3. ‚úÖ **Dual-œÅ definition** (PATCHED: rho_head + rho_kv)\n\n---\n\n*Paper 4: Behavioral Sink Dynamics*  \n*E11-Indra-Gemma27B-V3: Region-Local SI (Dual-œÅ Patched)*"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}