{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E11-T-Indra-LLaMA2-V3: State-Dependency on MHA (Standard-Compliant)\n",
    "\n",
    "**Paper 4: Behavioral Sink Dynamics**\n",
    "\n",
    "## Purpose: A2 Claim → A+ Tier (2nd Architecture)\n",
    "\n",
    "A2 (Indra State-Dependency) has evidence from 1 architecture (GQA).\n",
    "This notebook tests MHA to upgrade A2 from B-Tier to A-Tier.\n",
    "\n",
    "## Methodology (E11-v3 Standard)\n",
    "\n",
    "| Standard | Implementation |\n",
    "|----------|----------------|\n",
    "| Seeds | 42, 123, 456 (3-seed aggregation) |\n",
    "| Noise Injection | **PRE-ATTENTION** (affects attention weights) |\n",
    "| SI Measurement | **GLOBAL + LOCAL** (region-isolated) |\n",
    "| Attention Mask | **YES** (excludes padding from entropy) |\n",
    "| Chat Template | **YES** for Instruct model |\n",
    "| dtype | **bfloat16** (stable on A100) |\n",
    "| Prompts | Standard-10 v3 |\n",
    "\n",
    "## The LLaMA-2 Paradox (from E11-X)\n",
    "\n",
    "| Metric | LLaMA-3.1 (GQA) | LLaMA-2 (MHA) |\n",
    "|--------|-----------------|---------------|\n",
    "| Base SI | 0.7134 (HIGH) | 0.2149 (LOW) |\n",
    "| Instruct SI | 0.3115 (LOW) | 0.2642 (HIGH) |\n",
    "| RLHF Effect | COLLAPSES (-56%) | HEALS (+23%) |\n",
    "\n",
    "**LLaMA-2 is \"born collapsed\" but RLHF+SFT heals it!**\n",
    "\n",
    "## State-Dependency Hypothesis\n",
    "\n",
    "| Model | Initial State | Expected Indra Effect |\n",
    "|-------|---------------|----------------------|\n",
    "| LLaMA-2 BASE | COLLAPSED (SI=0.21) | HEAL (+SI) |\n",
    "| LLaMA-2 INSTRUCT | HEALTHY (SI=0.26) | DAMAGE (-SI) |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup + Seeds (E11-v3 STANDARD)\n",
    "!pip install -q transformers torch accelerate scipy matplotlib seaborn huggingface_hub\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from scipy.stats import entropy as scipy_entropy\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# === REPRODUCIBILITY SEEDS (E11-v3 STANDARD) ===\n",
    "SEEDS = [42, 123, 456]\n",
    "PRIMARY_SEED = 42\n",
    "\n",
    "def set_seed(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(PRIMARY_SEED)\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "Path('../results').mkdir(parents=True, exist_ok=True)\n",
    "Path('../figures').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"E11-T-Indra-LLaMA2-V3 (Standard-Compliant)\")\n",
    "print(f\"Timestamp: {TIMESTAMP}\")\n",
    "print(f\"Seeds: {SEEDS}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# HF Login for gated LLaMA models\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    from huggingface_hub import login\n",
    "    hf_token = userdata.get('HF_TOKEN')\n",
    "    if hf_token:\n",
    "        login(token=hf_token)\n",
    "        print(\"HF Login: SUCCESS\")\n",
    "    else:\n",
    "        print(\"WARNING: No HF_TOKEN - LLaMA requires auth!\")\n",
    "except:\n",
    "    print(\"Not in Colab - ensure HF_TOKEN set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Configuration (E11-v3 STANDARD)\n",
    "\n",
    "# Model Configuration\n",
    "MODEL_CONFIGS = {\n",
    "    'base': {\n",
    "        'name': 'meta-llama/Llama-2-7b-hf',\n",
    "        'display': 'LLaMA-2-7B-Base',\n",
    "        'state': 'COLLAPSED',\n",
    "        'expected_effect': 'HEAL',\n",
    "        'use_chat_template': False\n",
    "    },\n",
    "    'instruct': {\n",
    "        'name': 'meta-llama/Llama-2-7b-chat-hf',\n",
    "        'display': 'LLaMA-2-7B-Chat',\n",
    "        'state': 'HEALTHY',\n",
    "        'expected_effect': 'DAMAGE',\n",
    "        'use_chat_template': True  # CRITICAL: use chat template!\n",
    "    }\n",
    "}\n",
    "\n",
    "# E11-X Reference Values\n",
    "E11X_REFERENCE = {\n",
    "    'base_si': 0.2149,\n",
    "    'instruct_si': 0.2642,\n",
    "    'base_corr': 0.7851,\n",
    "    'instruct_corr': 0.7358,\n",
    "    'delta_si': 0.0493,\n",
    "    'alignment': 'RLHF+SFT'\n",
    "}\n",
    "\n",
    "# E11-T-Indra Reference (GQA) for comparison\n",
    "E11T_GQA_REFERENCE = {\n",
    "    'model': 'LLaMA-3.1-8B',\n",
    "    'collapsed_heal': 28.6,  # % SI increase\n",
    "    'healthy_damage': -30.5,  # % SI decrease\n",
    "    'gap_pp': 59.1\n",
    "}\n",
    "\n",
    "# Noise levels\n",
    "NOISE_LEVELS = [0.0, 0.01, 0.02, 0.05, 0.1, 0.2]\n",
    "\n",
    "# Tokenization\n",
    "MAX_LENGTH = 128\n",
    "\n",
    "# Standard-10 v3 Prompts (MD5: 715065bab181f46bf12ed471951141e2)\n",
    "STANDARD_PROMPTS = [\n",
    "    \"What is the capital of France and what is its population?\",\n",
    "    \"If all roses are flowers and some flowers fade quickly, can we conclude that some roses fade quickly? Explain step by step.\",\n",
    "    \"Calculate 47 multiplied by 23 and show your work.\",\n",
    "    \"Translate the following to German: 'The quick brown fox jumps over the lazy dog'.\",\n",
    "    \"Write a Python function that checks if a number is prime.\",\n",
    "    \"Summarize the main points: Machine learning is a subset of artificial intelligence that enables systems to learn from data. It uses algorithms to identify patterns and make decisions with minimal human intervention.\",\n",
    "    \"Statement A: 'All birds can fly.' Statement B: 'Penguins are birds that cannot fly.' Are these statements contradictory? Explain.\",\n",
    "    \"What are the safety considerations when using a kitchen knife?\",\n",
    "    \"Write a haiku about artificial intelligence.\",\n",
    "    \"Complete this sentence in a helpful way: 'The best approach to solving complex problems is'\",\n",
    "]\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Seeds: {SEEDS}\")\n",
    "print(f\"  Noise levels: {NOISE_LEVELS}\")\n",
    "print(f\"  MAX_LENGTH: {MAX_LENGTH}\")\n",
    "print(f\"  Prompts: Standard-10 v3\")\n",
    "print(f\"\\nModels:\")\n",
    "for key, cfg in MODEL_CONFIGS.items():\n",
    "    print(f\"  {key}: {cfg['display']}\")\n",
    "    print(f\"         State: {cfg['state']}, Expected: {cfg['expected_effect']}\")\n",
    "    print(f\"         Chat Template: {cfg['use_chat_template']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Specialization Metrics (E11-v3 STANDARD - WITH MASK)\n",
    "\n",
    "def extract_head_activations(model, tokenizer, prompts, max_length=128, use_chat_template=False):\n",
    "    \"\"\"\n",
    "    Extract per-head attention patterns WITH attention masks.\n",
    "    \n",
    "    Args:\n",
    "        use_chat_template: If True, apply chat template (for Instruct models)\n",
    "    \"\"\"\n",
    "    all_attention_patterns = []\n",
    "    all_attention_masks = []\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        # === CHAT TEMPLATE (E11-v3 STANDARD) ===\n",
    "        if use_chat_template and hasattr(tokenizer, 'apply_chat_template'):\n",
    "            try:\n",
    "                messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "                formatted = tokenizer.apply_chat_template(\n",
    "                    messages, \n",
    "                    tokenize=False, \n",
    "                    add_generation_prompt=True\n",
    "                )\n",
    "            except:\n",
    "                # Fallback for LLaMA-2 format\n",
    "                formatted = f\"[INST] {prompt} [/INST]\"\n",
    "        else:\n",
    "            formatted = prompt\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            formatted, \n",
    "            return_tensors='pt',\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length'\n",
    "        ).to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, output_attentions=True)\n",
    "        \n",
    "        attn_stack = torch.stack([a.squeeze(0) for a in outputs.attentions], dim=0)\n",
    "        all_attention_patterns.append(attn_stack.cpu())\n",
    "        all_attention_masks.append(inputs['attention_mask'].squeeze(0).cpu())\n",
    "    \n",
    "    return {\n",
    "        'attention_patterns': all_attention_patterns,\n",
    "        'attention_masks': all_attention_masks,\n",
    "        'num_layers': len(outputs.attentions),\n",
    "        'num_heads': outputs.attentions[0].shape[1]\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_head_entropy_profiles(attention_patterns, attention_masks):\n",
    "    \"\"\"\n",
    "    Compute normalized entropy WITH attention mask (E11-v3 STANDARD).\n",
    "    Excludes padding tokens from entropy calculation.\n",
    "    \"\"\"\n",
    "    num_prompts = len(attention_patterns)\n",
    "    num_layers = attention_patterns[0].shape[0]\n",
    "    num_heads = attention_patterns[0].shape[1]\n",
    "    \n",
    "    all_entropies = np.zeros((num_prompts, num_layers, num_heads))\n",
    "    \n",
    "    for p_idx, attn in enumerate(attention_patterns):\n",
    "        mask = attention_masks[p_idx].numpy()\n",
    "        valid_len = int(mask.sum())\n",
    "        \n",
    "        for layer in range(num_layers):\n",
    "            for head in range(num_heads):\n",
    "                attn_weights = attn[layer, head].float().cpu().numpy()\n",
    "                \n",
    "                # === MASK APPLICATION (E11-v3 STANDARD) ===\n",
    "                if valid_len > 0:\n",
    "                    # Only use valid (non-padded) tokens\n",
    "                    attn_weights = attn_weights[:valid_len, :valid_len]\n",
    "                    attn_weights = attn_weights.mean(axis=0)\n",
    "                else:\n",
    "                    attn_weights = attn_weights.mean(axis=0)\n",
    "                \n",
    "                attn_weights = attn_weights / (attn_weights.sum() + 1e-10)\n",
    "                attn_weights = attn_weights[attn_weights > 0]\n",
    "                \n",
    "                if len(attn_weights) > 1:\n",
    "                    h = scipy_entropy(attn_weights, base=2)\n",
    "                    h_max = np.log2(len(attn_weights))\n",
    "                    h_norm = h / h_max if h_max > 0 else 0\n",
    "                else:\n",
    "                    h_norm = 0\n",
    "                \n",
    "                all_entropies[p_idx, layer, head] = h_norm\n",
    "    \n",
    "    return all_entropies.mean(axis=0)  # (num_layers, num_heads)\n",
    "\n",
    "\n",
    "def compute_si_global(head_entropies):\n",
    "    \"\"\"Compute GLOBAL SI (all layers).\"\"\"\n",
    "    num_layers, num_heads = head_entropies.shape\n",
    "    \n",
    "    head_profiles = head_entropies.T  # (num_heads, num_layers)\n",
    "    head_corr_matrix = np.corrcoef(head_profiles)\n",
    "    upper_tri = head_corr_matrix[np.triu_indices(num_heads, k=1)]\n",
    "    mean_head_correlation = float(np.nanmean(upper_tri))\n",
    "    \n",
    "    return {\n",
    "        'specialization_index': 1.0 - mean_head_correlation,\n",
    "        'mean_head_correlation': mean_head_correlation,\n",
    "        'method': 'GLOBAL'\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_si_local(head_entropies, layer_start, layer_end):\n",
    "    \"\"\"Compute REGION-LOCAL SI (target layers only).\"\"\"\n",
    "    local_entropies = head_entropies[layer_start:layer_end, :]\n",
    "    local_layers, num_heads = local_entropies.shape\n",
    "    \n",
    "    if local_layers < 2:\n",
    "        return {\n",
    "            'specialization_index': 0.0,\n",
    "            'mean_head_correlation': 1.0,\n",
    "            'method': 'LOCAL',\n",
    "            'layer_range': [layer_start, layer_end]\n",
    "        }\n",
    "    \n",
    "    head_profiles = local_entropies.T\n",
    "    head_corr_matrix = np.corrcoef(head_profiles)\n",
    "    upper_tri = head_corr_matrix[np.triu_indices(num_heads, k=1)]\n",
    "    mean_head_correlation = float(np.nanmean(upper_tri))\n",
    "    \n",
    "    return {\n",
    "        'specialization_index': 1.0 - mean_head_correlation,\n",
    "        'mean_head_correlation': mean_head_correlation,\n",
    "        'method': 'LOCAL',\n",
    "        'layer_range': [layer_start, layer_end]\n",
    "    }\n",
    "\n",
    "print(\"Specialization metrics loaded (E11-v3 Standard).\")\n",
    "print(\"  - Attention mask: YES\")\n",
    "print(\"  - Chat template: Configurable\")\n",
    "print(\"  - SI methods: GLOBAL + LOCAL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: PRE-ATTENTION Noise Injector (E11-v3 STANDARD)\n",
    "\n",
    "class PreAttentionNoiseInjector:\n",
    "    \"\"\"\n",
    "    E11-v3 STANDARD: Inject noise BEFORE attention computation.\n",
    "    \n",
    "    This affects the attention weights (SI measurement target).\n",
    "    Post-attention injection does NOT affect attention weights!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, target_range, noise_std=0.0):\n",
    "        self.model = model\n",
    "        self.target_start, self.target_end = target_range\n",
    "        self.noise_std = noise_std\n",
    "        self.hooks = []\n",
    "    \n",
    "    def _make_pre_hook(self, layer_idx):\n",
    "        \"\"\"Create forward PRE-hook (before attention).\"\"\"\n",
    "        def hook(module, args):\n",
    "            if self.noise_std > 0 and self.target_start <= layer_idx < self.target_end:\n",
    "                hidden_states = args[0]\n",
    "                noise = torch.randn_like(hidden_states) * self.noise_std\n",
    "                noisy_hidden_states = hidden_states + noise\n",
    "                return (noisy_hidden_states,) + args[1:]\n",
    "            return args\n",
    "        return hook\n",
    "    \n",
    "    def attach(self):\n",
    "        \"\"\"Attach PRE-hooks to transformer layers.\"\"\"\n",
    "        for idx, layer in enumerate(self.model.model.layers):\n",
    "            hook = layer.register_forward_pre_hook(self._make_pre_hook(idx))\n",
    "            self.hooks.append(hook)\n",
    "    \n",
    "    def detach(self):\n",
    "        \"\"\"Remove all hooks.\"\"\"\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks = []\n",
    "\n",
    "print(\"PRE-Attention noise injector loaded (E11-v3 Standard).\")\n",
    "print(\"  - Injection point: BEFORE attention (affects weights)\")\n",
    "print(\"  - NOT post-attention (which doesn't affect SI)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Single Model Test Function\n",
    "\n",
    "def run_indra_test_single_model(model_key, model_config, layer_ranges, noise_levels, prompts, seeds):\n",
    "    \"\"\"\n",
    "    Run full Indra test on a single model with multi-seed aggregation.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"TESTING: {model_config['display']}\")\n",
    "    print(f\"State: {model_config['state']}, Expected: {model_config['expected_effect']}\")\n",
    "    print(f\"Chat Template: {model_config['use_chat_template']}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Load model with bfloat16 (E11-v3 STANDARD)\n",
    "    print(f\"\\nLoading: {model_config['name']}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_config['name'])\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_config['name'],\n",
    "        torch_dtype=torch.bfloat16,  # E11-v3 STANDARD: bf16 not fp16\n",
    "        device_map='auto',\n",
    "        trust_remote_code=True,\n",
    "        attn_implementation=\"eager\"  # CRITICAL: SDPA doesn't return attentions!\n",
    "    )\n",
    "    model.eval()\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Architecture info\n",
    "    config = model.config\n",
    "    num_layers = config.num_hidden_layers\n",
    "    num_heads = config.num_attention_heads\n",
    "    num_kv_heads = getattr(config, 'num_key_value_heads', num_heads)\n",
    "    hidden_size = config.hidden_size\n",
    "    d_head = hidden_size // num_heads\n",
    "    \n",
    "    # Dual-rho (E11-v3 STANDARD)\n",
    "    rho_head = num_heads / math.sqrt(hidden_size)\n",
    "    rho_kv = num_kv_heads / num_layers\n",
    "    \n",
    "    if num_kv_heads == num_heads:\n",
    "        arch_type = \"MHA\"\n",
    "    elif num_kv_heads == 1:\n",
    "        arch_type = \"MQA\"\n",
    "    else:\n",
    "        arch_type = f\"GQA ({num_heads}:{num_kv_heads})\"\n",
    "    \n",
    "    print(f\"  Architecture: {arch_type}\")\n",
    "    print(f\"  Layers: {num_layers}, Heads: {num_heads}, d_head: {d_head}\")\n",
    "    print(f\"  rho_head: {rho_head:.4f}, rho_kv: {rho_kv:.4f}\")\n",
    "    \n",
    "    # Layer ranges\n",
    "    third = num_layers // 3\n",
    "    local_layer_ranges = {\n",
    "        'early': (0, third),\n",
    "        'middle': (third, 2*third),\n",
    "        'late': (2*third, num_layers),\n",
    "        'all': (0, num_layers)\n",
    "    }\n",
    "    \n",
    "    # Baseline measurement\n",
    "    print(f\"\\n  Measuring baseline (no noise)...\")\n",
    "    set_seed(PRIMARY_SEED)\n",
    "    baseline_act = extract_head_activations(\n",
    "        model, tokenizer, prompts, MAX_LENGTH, \n",
    "        use_chat_template=model_config['use_chat_template']\n",
    "    )\n",
    "    baseline_ent = compute_head_entropy_profiles(\n",
    "        baseline_act['attention_patterns'],\n",
    "        baseline_act['attention_masks']\n",
    "    )\n",
    "    baseline_global = compute_si_global(baseline_ent)\n",
    "    \n",
    "    # Baseline local SI for each region\n",
    "    baseline_local = {}\n",
    "    for region, (start, end) in local_layer_ranges.items():\n",
    "        baseline_local[region] = compute_si_local(baseline_ent, start, end)\n",
    "    \n",
    "    print(f\"  Baseline Global SI: {baseline_global['specialization_index']:.4f}\")\n",
    "    print(f\"  Baseline Correlation: {baseline_global['mean_head_correlation']:.4f}\")\n",
    "    \n",
    "    # Multi-seed treatment loop\n",
    "    all_seed_results = {}\n",
    "    \n",
    "    for seed in seeds:\n",
    "        print(f\"\\n  Seed {seed}:\")\n",
    "        seed_results = {'global': [], 'local': []}\n",
    "        \n",
    "        for region_name, (start, end) in local_layer_ranges.items():\n",
    "            region_global = {'region': region_name, 'tests': []}\n",
    "            region_local = {'region': region_name, 'tests': []}\n",
    "            \n",
    "            for noise_std in noise_levels:\n",
    "                set_seed(seed)\n",
    "                \n",
    "                injector = PreAttentionNoiseInjector(model, (start, end), noise_std)\n",
    "                injector.attach()\n",
    "                \n",
    "                treated_act = extract_head_activations(\n",
    "                    model, tokenizer, prompts, MAX_LENGTH,\n",
    "                    use_chat_template=model_config['use_chat_template']\n",
    "                )\n",
    "                treated_ent = compute_head_entropy_profiles(\n",
    "                    treated_act['attention_patterns'],\n",
    "                    treated_act['attention_masks']\n",
    "                )\n",
    "                \n",
    "                injector.detach()\n",
    "                \n",
    "                # Global SI\n",
    "                treated_global = compute_si_global(treated_ent)\n",
    "                si_before = baseline_global['specialization_index']\n",
    "                si_after = treated_global['specialization_index']\n",
    "                change_pct = ((si_after - si_before) / si_before * 100) if si_before > 0 else 0\n",
    "                \n",
    "                region_global['tests'].append({\n",
    "                    'noise': noise_std,\n",
    "                    'si': si_after,\n",
    "                    'change_pct': change_pct\n",
    "                })\n",
    "                \n",
    "                # Local SI\n",
    "                treated_local = compute_si_local(treated_ent, start, end)\n",
    "                si_before_local = baseline_local[region_name]['specialization_index']\n",
    "                si_after_local = treated_local['specialization_index']\n",
    "                change_pct_local = ((si_after_local - si_before_local) / si_before_local * 100) if si_before_local > 0 else 0\n",
    "                \n",
    "                region_local['tests'].append({\n",
    "                    'noise': noise_std,\n",
    "                    'si': si_after_local,\n",
    "                    'change_pct': change_pct_local\n",
    "                })\n",
    "            \n",
    "            # Best effect for this region\n",
    "            if model_config['expected_effect'] == 'HEAL':\n",
    "                region_global['best'] = max(region_global['tests'], key=lambda x: x['change_pct'])\n",
    "                region_local['best'] = max(region_local['tests'], key=lambda x: x['change_pct'])\n",
    "            else:\n",
    "                region_global['best'] = min(region_global['tests'], key=lambda x: x['change_pct'])\n",
    "                region_local['best'] = min(region_local['tests'], key=lambda x: x['change_pct'])\n",
    "            \n",
    "            seed_results['global'].append(region_global)\n",
    "            seed_results['local'].append(region_local)\n",
    "            \n",
    "            if seed == PRIMARY_SEED:\n",
    "                print(f\"    {region_name}: Global={region_global['best']['change_pct']:+.1f}%, Local={region_local['best']['change_pct']:+.1f}%\")\n",
    "        \n",
    "        all_seed_results[seed] = seed_results\n",
    "    \n",
    "    # Aggregate across seeds\n",
    "    aggregated = {'global': {}, 'local': {}}\n",
    "    for si_type in ['global', 'local']:\n",
    "        for region_name in local_layer_ranges.keys():\n",
    "            values = []\n",
    "            for seed in seeds:\n",
    "                region_data = next(r for r in all_seed_results[seed][si_type] if r['region'] == region_name)\n",
    "                values.append(region_data['best']['change_pct'])\n",
    "            aggregated[si_type][region_name] = {\n",
    "                'mean': float(np.mean(values)),\n",
    "                'std': float(np.std(values)),\n",
    "                'values': values\n",
    "            }\n",
    "    \n",
    "    # Cleanup\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return {\n",
    "        'model_key': model_key,\n",
    "        'model_name': model_config['name'],\n",
    "        'state': model_config['state'],\n",
    "        'expected_effect': model_config['expected_effect'],\n",
    "        'architecture': arch_type,\n",
    "        'rho_head': rho_head,\n",
    "        'rho_kv': rho_kv,\n",
    "        'num_layers': num_layers,\n",
    "        'baseline_global': baseline_global,\n",
    "        'baseline_local': {k: v for k, v in baseline_local.items()},\n",
    "        'all_seed_results': all_seed_results,\n",
    "        'aggregated': aggregated,\n",
    "        'layer_ranges': local_layer_ranges\n",
    "    }\n",
    "\n",
    "print(\"Test function loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Run Tests on Both Models\n",
    "\n",
    "print(f\"\\n{'#'*70}\")\n",
    "print(f\"# E11-T-Indra-LLaMA2-V3: DUAL MODEL TEST\")\n",
    "print(f\"# MHA State-Dependency (E11-v3 Standard)\")\n",
    "print(f\"{'#'*70}\")\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "for model_key, model_config in MODEL_CONFIGS.items():\n",
    "    results = run_indra_test_single_model(\n",
    "        model_key=model_key,\n",
    "        model_config=model_config,\n",
    "        layer_ranges=None,  # Computed inside\n",
    "        noise_levels=NOISE_LEVELS,\n",
    "        prompts=STANDARD_PROMPTS,\n",
    "        seeds=SEEDS\n",
    "    )\n",
    "    all_results[model_key] = results\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"BOTH MODELS TESTED!\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: State-Dependency Verdict\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"STATE-DEPENDENCY ANALYSIS (MHA)\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Thresholds\n",
    "HEAL_THRESHOLD = 5.0\n",
    "DAMAGE_THRESHOLD = -5.0\n",
    "\n",
    "# Analyze BASE (collapsed, expected HEAL)\n",
    "base = all_results['base']\n",
    "base_best_global = max(base['aggregated']['global'].values(), key=lambda x: x['mean'])\n",
    "base_best_region = [k for k, v in base['aggregated']['global'].items() if v['mean'] == base_best_global['mean']][0]\n",
    "base_effect = base_best_global['mean']\n",
    "\n",
    "print(f\"\\n[1] LLaMA-2-BASE (COLLAPSED)\")\n",
    "print(f\"    Baseline SI: {base['baseline_global']['specialization_index']:.4f}\")\n",
    "print(f\"    Expected: HEAL (+SI)\")\n",
    "print(f\"    Best Effect: {base_effect:+.1f}% at {base_best_region}\")\n",
    "\n",
    "if base_effect > HEAL_THRESHOLD:\n",
    "    base_verdict = \"HEALED\"\n",
    "elif base_effect < DAMAGE_THRESHOLD:\n",
    "    base_verdict = \"UNEXPECTED_DAMAGE\"\n",
    "else:\n",
    "    base_verdict = \"NO_EFFECT\"\n",
    "print(f\"    Verdict: {base_verdict}\")\n",
    "\n",
    "# Analyze INSTRUCT (healthy, expected DAMAGE)\n",
    "inst = all_results['instruct']\n",
    "inst_worst_global = min(inst['aggregated']['global'].values(), key=lambda x: x['mean'])\n",
    "inst_worst_region = [k for k, v in inst['aggregated']['global'].items() if v['mean'] == inst_worst_global['mean']][0]\n",
    "inst_effect = inst_worst_global['mean']\n",
    "\n",
    "print(f\"\\n[2] LLaMA-2-INSTRUCT (HEALTHY)\")\n",
    "print(f\"    Baseline SI: {inst['baseline_global']['specialization_index']:.4f}\")\n",
    "print(f\"    Expected: DAMAGE (-SI)\")\n",
    "print(f\"    Worst Effect: {inst_effect:+.1f}% at {inst_worst_region}\")\n",
    "\n",
    "if inst_effect < DAMAGE_THRESHOLD:\n",
    "    inst_verdict = \"DAMAGED\"\n",
    "elif inst_effect > HEAL_THRESHOLD:\n",
    "    inst_verdict = \"UNEXPECTED_HEAL\"\n",
    "else:\n",
    "    inst_verdict = \"NO_EFFECT\"\n",
    "print(f\"    Verdict: {inst_verdict}\")\n",
    "\n",
    "# Gap calculation\n",
    "gap = base_effect - inst_effect\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"CROSS-ARCHITECTURE COMPARISON\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\n{'Architecture':<15} {'Collapsed':<15} {'Healthy':<15} {'Gap':<10}\")\n",
    "print(\"-\"*55)\n",
    "print(f\"{'GQA (3.1)':<15} {'+28.6%':<15} {'-30.5%':<15} {'59.1pp':<10}\")\n",
    "print(f\"{'MHA (2)':<15} {base_effect:+.1f}%{'':>10} {inst_effect:+.1f}%{'':>10} {gap:.1f}pp\")\n",
    "\n",
    "# Final A2 verdict\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"A2 STATE-DEPENDENCY VERDICT\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "if base_verdict == \"HEALED\" and inst_verdict == \"DAMAGED\":\n",
    "    a2_verdict = \"A_CONFIRMED\"\n",
    "    print(f\"\\n  VERDICT: {a2_verdict}\")\n",
    "    print(f\"  State-dependency CONFIRMED on MHA!\")\n",
    "    print(f\"  A2: GQA + MHA = 2 Architectures → A+ Tier\")\n",
    "elif base_verdict == \"HEALED\" or inst_verdict == \"DAMAGED\":\n",
    "    a2_verdict = \"B_PARTIAL\"\n",
    "    print(f\"\\n  VERDICT: {a2_verdict}\")\n",
    "    print(f\"  Partial state-dependency on MHA.\")\n",
    "else:\n",
    "    a2_verdict = \"C_REFUTED\"\n",
    "    print(f\"\\n  VERDICT: {a2_verdict}\")\n",
    "    print(f\"  State-dependency NOT confirmed on MHA.\")\n",
    "\n",
    "# Store verdict\n",
    "verdict = {\n",
    "    'base_verdict': base_verdict,\n",
    "    'base_effect': base_effect,\n",
    "    'base_region': base_best_region,\n",
    "    'inst_verdict': inst_verdict,\n",
    "    'inst_effect': inst_effect,\n",
    "    'inst_region': inst_worst_region,\n",
    "    'gap_pp': gap,\n",
    "    'a2_verdict': a2_verdict,\n",
    "    'comparison': {\n",
    "        'gqa_collapsed': 28.6,\n",
    "        'gqa_healthy': -30.5,\n",
    "        'gqa_gap': 59.1,\n",
    "        'mha_collapsed': base_effect,\n",
    "        'mha_healthy': inst_effect,\n",
    "        'mha_gap': gap\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Visualization\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "\n",
    "colors = {'early': '#3498db', 'middle': '#2ecc71', 'late': '#e74c3c', 'all': '#9b59b6'}\n",
    "\n",
    "# Plot 1: BASE (Collapsed) - Expected HEAL\n",
    "ax1 = axes[0, 0]\n",
    "regions = list(base['aggregated']['global'].keys())\n",
    "base_means = [base['aggregated']['global'][r]['mean'] for r in regions]\n",
    "base_stds = [base['aggregated']['global'][r]['std'] for r in regions]\n",
    "\n",
    "bars = ax1.bar(regions, base_means, yerr=base_stds, color=[colors[r] for r in regions], alpha=0.8, capsize=5)\n",
    "ax1.axhline(y=0, color='black', linestyle='-')\n",
    "ax1.axhline(y=5, color='green', linestyle=':', alpha=0.7, label='+5% threshold')\n",
    "ax1.axhline(y=-5, color='red', linestyle=':', alpha=0.7, label='-5% threshold')\n",
    "ax1.set_ylabel('SI Change %')\n",
    "ax1.set_title(f'LLaMA-2 BASE (COLLAPSED)\\nExpected: HEAL | Verdict: {base_verdict}')\n",
    "ax1.legend()\n",
    "\n",
    "for bar, val in zip(bars, base_means):\n",
    "    ax1.annotate(f'{val:+.1f}%', xy=(bar.get_x() + bar.get_width()/2, val),\n",
    "                 xytext=(0, 5 if val > 0 else -12), textcoords='offset points',\n",
    "                 ha='center', fontweight='bold')\n",
    "\n",
    "# Plot 2: INSTRUCT (Healthy) - Expected DAMAGE\n",
    "ax2 = axes[0, 1]\n",
    "inst_means = [inst['aggregated']['global'][r]['mean'] for r in regions]\n",
    "inst_stds = [inst['aggregated']['global'][r]['std'] for r in regions]\n",
    "\n",
    "bars = ax2.bar(regions, inst_means, yerr=inst_stds, color=[colors[r] for r in regions], alpha=0.8, capsize=5)\n",
    "ax2.axhline(y=0, color='black', linestyle='-')\n",
    "ax2.axhline(y=5, color='green', linestyle=':', alpha=0.7)\n",
    "ax2.axhline(y=-5, color='red', linestyle=':', alpha=0.7)\n",
    "ax2.set_ylabel('SI Change %')\n",
    "ax2.set_title(f'LLaMA-2 INSTRUCT (HEALTHY)\\nExpected: DAMAGE | Verdict: {inst_verdict}')\n",
    "\n",
    "for bar, val in zip(bars, inst_means):\n",
    "    ax2.annotate(f'{val:+.1f}%', xy=(bar.get_x() + bar.get_width()/2, val),\n",
    "                 xytext=(0, 5 if val > 0 else -12), textcoords='offset points',\n",
    "                 ha='center', fontweight='bold')\n",
    "\n",
    "# Plot 3: State-Dependency Comparison\n",
    "ax3 = axes[1, 0]\n",
    "models = ['BASE\\n(Collapsed)', 'INSTRUCT\\n(Healthy)']\n",
    "effects = [base_effect, inst_effect]\n",
    "bar_colors = ['#2ecc71' if base_effect > 0 else '#e74c3c',\n",
    "              '#e74c3c' if inst_effect < 0 else '#2ecc71']\n",
    "\n",
    "bars = ax3.bar(models, effects, color=bar_colors, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "ax3.axhline(y=0, color='black', linestyle='-', linewidth=2)\n",
    "ax3.set_ylabel('Best/Worst SI Change %')\n",
    "ax3.set_title(f'State-Dependency: Gap = {gap:.1f}pp\\nA2 Verdict: {a2_verdict}')\n",
    "\n",
    "for bar, eff in zip(bars, effects):\n",
    "    ax3.annotate(f'{eff:+.1f}%', xy=(bar.get_x() + bar.get_width()/2, eff),\n",
    "                 xytext=(0, 10 if eff > 0 else -20), textcoords='offset points',\n",
    "                 ha='center', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Plot 4: Cross-Architecture Comparison\n",
    "ax4 = axes[1, 1]\n",
    "archs = ['GQA\\n(LLaMA-3.1)', 'MHA\\n(LLaMA-2)']\n",
    "collapsed = [28.6, base_effect]\n",
    "healthy = [-30.5, inst_effect]\n",
    "\n",
    "x = np.arange(len(archs))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax4.bar(x - width/2, collapsed, width, label='Collapsed→Heal', color='#2ecc71', alpha=0.8)\n",
    "bars2 = ax4.bar(x + width/2, healthy, width, label='Healthy→Damage', color='#e74c3c', alpha=0.8)\n",
    "\n",
    "ax4.axhline(y=0, color='black', linestyle='-', linewidth=2)\n",
    "ax4.set_ylabel('SI Change %')\n",
    "ax4.set_title('Cross-Architecture: GQA vs MHA')\n",
    "ax4.set_xticks(x)\n",
    "ax4.set_xticklabels(archs)\n",
    "ax4.legend()\n",
    "\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        h = bar.get_height()\n",
    "        ax4.annotate(f'{h:+.1f}%', xy=(bar.get_x() + bar.get_width()/2, h),\n",
    "                     xytext=(0, 5 if h > 0 else -15), textcoords='offset points',\n",
    "                     ha='center', fontweight='bold')\n",
    "\n",
    "plt.suptitle(f'E11-T-Indra-LLaMA2-V3: MHA State-Dependency\\nSeeds: {SEEDS}', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "\n",
    "fig_path = f'../figures/E11T_indra_llama2_v3_{TIMESTAMP}.png'\n",
    "plt.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFigure saved: {fig_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Save Results\n",
    "\n",
    "def convert_to_native(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_to_native(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_native(v) for v in obj]\n",
    "    elif isinstance(obj, (np.bool_, np.integer)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "filename = f'../results/E11T_indra_llama2_v3_{TIMESTAMP}.json'\n",
    "\n",
    "output = {\n",
    "    'experiment': 'E11-T-Indra-LLaMA2-V3',\n",
    "    'purpose': 'A2 State-Dependency on MHA (E11-v3 Standard)',\n",
    "    'timestamp': TIMESTAMP,\n",
    "    'methodology': {\n",
    "        'standard': 'E11-v3',\n",
    "        'seeds': SEEDS,\n",
    "        'noise_injection': 'PRE-ATTENTION',\n",
    "        'si_measurement': 'GLOBAL + LOCAL',\n",
    "        'attention_mask': True,\n",
    "        'chat_template': 'Yes for Instruct',\n",
    "        'dtype': 'bfloat16',\n",
    "        'prompts': 'Standard-10 v3'\n",
    "    },\n",
    "    'e11x_reference': E11X_REFERENCE,\n",
    "    'e11t_gqa_reference': E11T_GQA_REFERENCE,\n",
    "    'noise_levels': NOISE_LEVELS,\n",
    "    'num_prompts': len(STANDARD_PROMPTS),\n",
    "    'results': {\n",
    "        'base': convert_to_native(all_results['base']),\n",
    "        'instruct': convert_to_native(all_results['instruct'])\n",
    "    },\n",
    "    'verdict': convert_to_native(verdict)\n",
    "}\n",
    "\n",
    "with open(filename, 'w') as f:\n",
    "    json.dump(output, f, indent=2)\n",
    "\n",
    "print(f\"Results saved: {filename}\")\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(filename)\n",
    "    files.download(fig_path)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: E11-T-Indra-LLaMA2-V3\n",
    "\n",
    "### Methodology (E11-v3 Standard)\n",
    "\n",
    "| Standard | Implementation |\n",
    "|----------|----------------|\n",
    "| Seeds | 42, 123, 456 |\n",
    "| Noise Injection | **PRE-ATTENTION** |\n",
    "| SI Measurement | **GLOBAL + LOCAL** |\n",
    "| Attention Mask | **YES** |\n",
    "| Chat Template | **YES** (Instruct) |\n",
    "| dtype | **bfloat16** |\n",
    "\n",
    "### Key Difference from V1\n",
    "\n",
    "| Aspect | V1 (Wrong) | V3 (Correct) |\n",
    "|--------|------------|---------------|\n",
    "| Noise | POST-attention | PRE-attention |\n",
    "| Mask | No | Yes |\n",
    "| Template | No | Yes (Instruct) |\n",
    "| Seeds | 1 | 3 |\n",
    "| dtype | fp16 | bf16 |\n",
    "\n",
    "### Expected Outcomes\n",
    "\n",
    "| Model | State | Expected | If Confirmed |\n",
    "|-------|-------|----------|---------------|\n",
    "| BASE | Collapsed | HEAL (+SI) | A2 partial |\n",
    "| INSTRUCT | Healthy | DAMAGE (-SI) | A2 complete |\n",
    "| Both | - | Gap > 20pp | A2 → A+ Tier |\n",
    "\n",
    "---\n",
    "\n",
    "*Paper 4: Behavioral Sink Dynamics*  \n",
    "*E11-T-Indra-LLaMA2-V3: MHA State-Dependency (E11-v3 Standard)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Auto-Download\n",
    "\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "def auto_download():\n",
    "    try:\n",
    "        from google.colab import files\n",
    "    except:\n",
    "        print('Not in Colab')\n",
    "        return\n",
    "    \n",
    "    print('AUTO-DOWNLOADING...')\n",
    "    \n",
    "    all_files = glob.glob('../results/E11T_indra_llama2*.json') + glob.glob('../figures/E11T_indra_llama2*.png')\n",
    "    if not all_files:\n",
    "        print('No files found')\n",
    "        return\n",
    "    \n",
    "    import os\n",
    "    os.makedirs('download', exist_ok=True)\n",
    "    for f in all_files:\n",
    "        shutil.copy(f, 'download/')\n",
    "    \n",
    "    shutil.make_archive(f'E11T_llama2_v3_{TIMESTAMP}', 'zip', 'download')\n",
    "    files.download(f'E11T_llama2_v3_{TIMESTAMP}.zip')\n",
    "    print('DONE!')\n",
    "\n",
    "auto_download()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
