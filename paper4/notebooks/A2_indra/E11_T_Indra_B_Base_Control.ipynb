{"cells": [{"cell_type": "markdown", "id": "cell-0", "metadata": {}, "source": "# E11-T-Indra-B: Base Control (Artifact Check)\n\n**Paper 4: Behavioral Sink Dynamics**\n\n## Critical Question\n\n> **Does noise artificially inflate Specialization Index even in HEALTHY (non-collapsed) models?**\n\n## Why This Matters\n\nE11-T-Indra showed 28.6% \"recovery\" in collapsed Instruct model. But:\n- We measured SI **while noise was active**\n- Noise mechanically forces different head responses\n- This could be an artifact, not real specialization recovery\n\n## The Control\n\nRun the **exact same protocol** on LLaMA-3.1-8B-**BASE** (healthy, SI=0.7134):\n\n| Expected if REAL | Expected if ARTIFACT |\n|------------------|---------------------|\n| Base SI stays ~same or decreases | Base SI INCREASES under noise |\n| (already specialized, noise = disruption) | (noise = artificial variance) |\n\n## Verdict Logic (Unified Thresholds)\n\n| Base SI Change | Interpretation | E11-T-Indra Status |\n|----------------|----------------|---------------------|\n| **< 5%** | Noise doesn't inflate healthy SI | **REAL** - Recovery is genuine |\n| **5-15%** | Noise has some effect | **PARTIAL** - Some artifact |\n| **> 15%** | Noise inflates SI universally | **ARTIFACT** - Recovery may be fake |\n| **Negative** | Noise disrupts specialization | **STRONGLY REAL** - Collapsed model has latent capacity |\n\n## Dual-Check Approach\n\n1. **Primary:** Compare Early@\u03c3=0.02 (direct comparison with E11-T-Indra)\n2. **Secondary:** Check if ANY region/\u03c3 exceeds artifact threshold\n\n---"}, {"cell_type": "code", "execution_count": null, "id": "cell-1", "metadata": {}, "outputs": [], "source": ["# Cell 1: Setup\n", "!pip install -q transformers torch accelerate bitsandbytes scipy matplotlib seaborn huggingface_hub\n", "\n", "import torch\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "from transformers import AutoModelForCausalLM, AutoTokenizer\n", "from scipy.stats import entropy as scipy_entropy\n", "import json\n", "import hashlib\n", "import warnings\n", "warnings.filterwarnings('ignore')\n", "\n", "import os\n", "from pathlib import Path\n", "from datetime import datetime\n", "\n", "# E11-v3 STANDARD: 3-Seed Reproducibility\n", "SEEDS = [42, 123, 456]\n", "os.environ['PYTHONHASHSEED'] = '42'\n", "torch.manual_seed(42)\n", "np.random.seed(42)\n", "\n", "TIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')\n", "Path('results').mkdir(parents=True, exist_ok=True)\n", "Path('figures').mkdir(parents=True, exist_ok=True)\n", "print(f\"Timestamp: {TIMESTAMP}\")\n", "print(f\"E11-v3 Standard: Seeds {SEEDS}\")\n", "\n", "print(f\"PyTorch: {torch.__version__}\")\n", "print(f\"CUDA available: {torch.cuda.is_available()}\")\n", "if torch.cuda.is_available():\n", "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n", "\n", "# HF Login for gated models (LLaMA) - REQUIRED!\n", "from huggingface_hub import login, HfFolder\n", "\n", "def get_hf_token():\n", "    token = None\n", "    try:\n", "        from google.colab import userdata\n", "        token = userdata.get('HF_TOKEN')\n", "    except Exception:\n", "        pass\n", "    if not token:\n", "        token = os.environ.get('HF_TOKEN') or os.environ.get('HUGGINGFACE_TOKEN') or os.environ.get('HUGGING_FACE_HUB_TOKEN')\n", "    if not token:\n", "        token = HfFolder.get_token()\n", "    return token\n", "\n", "HF_TOKEN = get_hf_token()\n", "if HF_TOKEN:\n", "    try:\n", "        login(token=HF_TOKEN)\n", "        print(\"HF Login: SUCCESS (required for gated models)\")\n", "    except Exception as e:\n", "        print(f\"HF Login failed: {e}\")\n", "else:\n", "    print(\"WARNING: No HF_TOKEN found! LLaMA requires authentication.\")\n", "    print(\"Colab: Runtime -> Secrets -> Add HF_TOKEN\")\n", "    print(\"Local: run `huggingface-cli login` or set HF_TOKEN env var\")\n", "\n", "TOKEN_KWARGS = {'token': HF_TOKEN} if HF_TOKEN else {}\n", "\n"]}, {"cell_type": "code", "execution_count": null, "id": "cell-2", "metadata": {}, "outputs": [], "source": ["# Cell 2: Configuration - BASE MODEL (not Instruct!)\n", "\n", "# ==============================================================================\n", "# E11-v3 STANDARD PARAMETERS\n", "# ==============================================================================\n", "MAX_LENGTH = 128\n", "DTYPE = torch.bfloat16  # E11-v3: bfloat16 (NOT float16!)\n", "USE_CHAT_TEMPLATE = False  # E11-v3: no chat template for Base model\n", "EXPECTED_MD5 = \"715065bab181f46bf12ed471951141e2\"\n", "\n", "# CRITICAL: This is the BASE model (healthy, not collapsed)\n", "MODEL_CONFIG = {\n", "    'name': 'meta-llama/Llama-3.1-8B',  # BASE, not Instruct!\n", "    'display': 'LLaMA-3.1-8B-Base (HEALTHY - Control)',\n", "    'num_layers': 32,\n", "    'num_query_heads': 32,\n", "    'num_kv_heads': 8,\n", "    'd_head': 128,\n", "    'architecture': 'GQA'\n", "}\n", "\n", "# Reference Values from E11-T\n", "E11T_REFERENCE = {\n", "    'base_specialization': 0.7134,      # HEALTHY - this is what we expect\n", "    'instruct_specialization': 0.3115,  # Collapsed\n", "    'base_correlation': 0.2866,         # Low correlation = specialized\n", "    'instruct_correlation': 0.6885,     # High correlation = uniform\n", "}\n", "\n", "# Layer Ranges (same as E11-T-Indra for fair comparison)\n", "LAYER_RANGES = {\n", "    'early': (0, 11),      # Layers 0-10  (Pre-Engine)\n", "    'middle': (11, 28),    # Layers 11-27 (Engine Room per E06d-0)\n", "    'late': (28, 32),      # Layers 28-31 (Post-Engine)\n", "    'all': (0, 32)         # All layers\n", "}\n", "\n", "# Noise Levels (same as E11-T-Indra)\n", "NOISE_LEVELS = [0.0, 0.01, 0.02, 0.05, 0.1, 0.2]\n", "\n", "# Standard-10 Prompt Set (canonical per NOTEBOOK_GUIDE.md \u00a79)\n", "STANDARD_PROMPTS = [\n", "    \"What is the capital of France and what is its population?\",\n", "    \"If all roses are flowers and some flowers fade quickly, can we conclude that some roses fade quickly? Explain step by step.\",\n", "    \"Calculate 47 multiplied by 23 and show your work.\",\n", "    \"Translate the following to German: 'The quick brown fox jumps over the lazy dog'.\",\n", "    \"Write a Python function that checks if a number is prime.\",\n", "    \"Summarize the main points: Machine learning is a subset of artificial intelligence that enables systems to learn from data. It uses algorithms to identify patterns and make decisions with minimal human intervention.\",\n", "    \"Statement A: 'All birds can fly.' Statement B: 'Penguins are birds that cannot fly.' Are these statements contradictory? Explain.\",\n", "    \"What are the safety considerations when using a kitchen knife?\",\n", "    \"Write a haiku about artificial intelligence.\",\n", "    \"Complete this sentence in a helpful way: 'The best approach to solving complex problems is'\",\n", "]\n", "\n", "# ==============================================================================\n", "# E11-v3 PROMPT VERIFICATION\n", "# ==============================================================================\n", "def verify_prompts():\n", "    \"\"\"Verify Standard-10 prompts haven't been modified.\"\"\"\n", "    prompt_string = '|||'.join(STANDARD_PROMPTS)\n", "    actual_md5 = hashlib.md5(prompt_string.encode()).hexdigest()\n", "    return actual_md5, actual_md5 == EXPECTED_MD5\n", "\n", "actual_md5, prompts_ok = verify_prompts()\n", "print(f\"E11-v3 Prompt Verification:\")\n", "print(f\"  Expected MD5: {EXPECTED_MD5}\")\n", "print(f\"  Actual MD5:   {actual_md5}\")\n", "print(f\"  Status:       {'\u2713 VERIFIED' if prompts_ok else '\u2717 MISMATCH - STOP!'}\")\n", "\n", "if not prompts_ok:\n", "    raise ValueError(f\"Prompt MD5 mismatch! Expected {EXPECTED_MD5}, got {actual_md5}\")\n", "\n", "print(f\"\\nE11-T-Indra-B: BASE CONTROL (Artifact Check)\")\n", "print(f\"\\nTarget: {MODEL_CONFIG['display']}\")\n", "print(f\"Expected SI: {E11T_REFERENCE['base_specialization']:.4f} (HEALTHY)\")\n", "print(f\"\\nThis is a CONTROL experiment:\")\n", "print(f\"  If noise increases SI here too \u2192 E11-T-Indra may be artifact\")\n", "print(f\"  If noise doesn't change SI \u2192 E11-T-Indra recovery is REAL\")\n", "\n"]}, {"cell_type": "code", "execution_count": null, "id": "cell-3", "metadata": {}, "outputs": [], "source": ["# Cell 3: Specialization Metrics (from E11-T)\n", "\n", "def extract_head_activations_with_noise(model, tokenizer, prompts, noise_injector=None, max_length=128, use_chat_template=False):\n", "    \"\"\"\n", "    Extract per-head activation patterns, optionally with noise injection.\n", "    Uses attention_mask to avoid PAD bias in entropy.\n", "    \"\"\"\n", "    all_attention_patterns = []\n", "    all_attention_masks = []\n", "\n", "    for prompt in prompts:\n", "        formatted = prompt\n", "        if use_chat_template and hasattr(tokenizer, 'apply_chat_template'):\n", "            messages = [{\"role\": \"user\", \"content\": prompt}]\n", "            try:\n", "                formatted = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n", "            except Exception:\n", "                formatted = prompt\n", "\n", "        inputs = tokenizer(\n", "            formatted,\n", "            return_tensors='pt',\n", "            max_length=max_length,\n", "            truncation=True,\n", "            padding='max_length'\n", "        ).to(model.device)\n", "\n", "        attention_mask = inputs.get('attention_mask')\n", "\n", "        with torch.no_grad():\n", "            outputs = model(**inputs, output_attentions=True, output_hidden_states=True)\n", "\n", "        attn_stack = torch.stack([a.squeeze(0) for a in outputs.attentions], dim=0)\n", "        all_attention_patterns.append(attn_stack.cpu())\n", "        all_attention_masks.append(attention_mask.squeeze(0).cpu() if attention_mask is not None else None)\n", "\n", "    return {\n", "        'attention_patterns': all_attention_patterns,\n", "        'attention_masks': all_attention_masks,\n", "        'num_layers': len(outputs.attentions),\n", "        'num_heads': outputs.attentions[0].shape[1]\n", "    }\n", "\n", "\n", "def compute_head_entropy_profiles(attention_patterns, attention_masks=None):\n", "    \"\"\"Compute normalized entropy for each head across prompts.\"\"\"\n", "    num_prompts = len(attention_patterns)\n", "    num_layers = attention_patterns[0].shape[0]\n", "    num_heads = attention_patterns[0].shape[1]\n", "\n", "    all_entropies = np.zeros((num_prompts, num_layers, num_heads))\n", "\n", "    for p_idx, attn in enumerate(attention_patterns):\n", "        mask = None\n", "        if attention_masks is not None:\n", "            mask = attention_masks[p_idx]\n", "            if mask is not None:\n", "                mask = mask.bool()\n", "\n", "        for layer in range(num_layers):\n", "            for head in range(num_heads):\n", "                attn_matrix = attn[layer, head]\n", "\n", "                if mask is not None:\n", "                    valid_idx = mask.nonzero(as_tuple=False).squeeze(-1)\n", "                    if valid_idx.numel() > 1:\n", "                        attn_matrix = attn_matrix[valid_idx][:, valid_idx]\n", "                    else:\n", "                        all_entropies[p_idx, layer, head] = 0\n", "                        continue\n", "\n", "                attn_weights = attn_matrix.mean(dim=0).float().cpu().numpy()\n", "                denom = attn_weights.sum()\n", "                if denom <= 0:\n", "                    all_entropies[p_idx, layer, head] = 0\n", "                    continue\n", "\n", "                attn_weights = attn_weights / denom\n", "                attn_weights = attn_weights[attn_weights > 0]\n", "\n", "                if len(attn_weights) > 1:\n", "                    h = scipy_entropy(attn_weights, base=2)\n", "                    h_max = np.log2(len(attn_weights))\n", "                    h_norm = h / h_max if h_max > 0 else 0\n", "                else:\n", "                    h_norm = 0\n", "\n", "                all_entropies[p_idx, layer, head] = h_norm\n", "\n", "    return all_entropies.mean(axis=0)\n", "\n", "\n", "def compute_specialization_metrics(head_entropies):\n", "    \"\"\"Compute specialization metrics.\"\"\"\n", "    num_layers, num_heads = head_entropies.shape\n", "\n", "    layer_variances = np.var(head_entropies, axis=1)\n", "    mean_variance = float(np.mean(layer_variances))\n", "\n", "    head_profiles = head_entropies.T\n", "    head_corr_matrix = np.corrcoef(head_profiles)\n", "    upper_tri = head_corr_matrix[np.triu_indices(num_heads, k=1)]\n", "    mean_head_correlation = float(np.nanmean(upper_tri))\n", "\n", "    specialization_index = 1.0 - mean_head_correlation\n", "\n", "    head_contributions = np.mean(head_entropies, axis=0)\n", "    head_contributions = head_contributions / head_contributions.sum()\n", "    h_contrib = scipy_entropy(head_contributions, base=2)\n", "    effective_heads = 2 ** h_contrib if h_contrib > 0 else 1.0\n", "    effective_ratio = effective_heads / num_heads\n", "\n", "    return {\n", "        'mean_head_variance': mean_variance,\n", "        'mean_head_correlation': mean_head_correlation,\n", "        'specialization_index': specialization_index,\n", "        'effective_heads': float(effective_heads),\n", "        'effective_ratio': float(effective_ratio),\n", "        'layer_variances': layer_variances.tolist(),\n", "        'num_layers': num_layers,\n", "        'num_heads': num_heads\n", "    }\n", "\n", "print(\"Specialization metrics functions loaded.\")\n", "\n", "\n", "\n"]}, {"cell_type": "code", "execution_count": null, "id": "cell-4", "metadata": {}, "outputs": [], "source": ["# Cell 4: Layer-Targeted Noise Injector (identical to E11-T-Indra)\n", "\n", "class AttentionNoiseInjector:\n", "    \"\"\"Inject Gaussian noise into attention outputs of SPECIFIC layer ranges.\"\"\"\n", "    \n", "    def __init__(self, model, target_range, noise_std=0.0):\n", "        self.model = model\n", "        self.target_start, self.target_end = target_range\n", "        self.noise_std = noise_std\n", "        self.hooks = []\n", "    \n", "    def _make_hook(self, layer_idx):\n", "        \"\"\"Create a forward hook for a specific layer.\"\"\"\n", "        def hook(module, input, output):\n", "            if self.noise_std > 0 and self.target_start <= layer_idx < self.target_end:\n", "                if isinstance(output, tuple):\n", "                    attn_output = output[0]\n", "                    noise = torch.randn_like(attn_output) * self.noise_std\n", "                    return (attn_output + noise,) + output[1:]\n", "                else:\n", "                    noise = torch.randn_like(output) * self.noise_std\n", "                    return output + noise\n", "            return output\n", "        return hook\n", "    \n", "    def attach(self):\n", "        \"\"\"Attach hooks to attention layers.\"\"\"\n", "        for idx, layer in enumerate(self.model.model.layers):\n", "            hook = layer.self_attn.register_forward_hook(self._make_hook(idx))\n", "            self.hooks.append(hook)\n", "    \n", "    def detach(self):\n", "        \"\"\"Remove all hooks.\"\"\"\n", "        for hook in self.hooks:\n", "            hook.remove()\n", "        self.hooks = []\n", "    \n", "    def set_noise(self, std):\n", "        \"\"\"Update noise level.\"\"\"\n", "        self.noise_std = std\n", "\n", "print(\"Attention noise injector ready.\")"]}, {"cell_type": "code", "execution_count": null, "id": "cell-5", "metadata": {}, "outputs": [], "source": ["# Cell 5: Load BASE Model and Verify Healthy State (3-Seed)\n", "\n", "print(f\"\\n{'='*60}\")\n", "print(f\"PHASE 1: LOAD BASE MODEL AND VERIFY HEALTHY STATE\")\n", "print(f\"{'='*60}\")\n", "\n", "print(f\"\\nLoading: {MODEL_CONFIG['name']}\")\n", "print(f\"E11-v3 dtype: {DTYPE}\")\n", "\n", "tokenizer = AutoTokenizer.from_pretrained(MODEL_CONFIG['name'], **TOKEN_KWARGS)\n", "model = AutoModelForCausalLM.from_pretrained(\n", "    MODEL_CONFIG['name'],\n", "    **TOKEN_KWARGS,\n", "    torch_dtype=DTYPE,  # E11-v3: bfloat16\n", "    device_map='auto',\n", "    trust_remote_code=True,\n", "    attn_implementation=\"eager\"  # CRITICAL: SDPA doesn't return attentions!\n", ")\n", "\n", "model.eval()\n", "\n", "if tokenizer.pad_token is None:\n", "    tokenizer.pad_token = tokenizer.eos_token\n", "\n", "print(f\"Loaded: {sum(p.numel() for p in model.parameters()) / 1e9:.2f}B parameters\")\n", "print(f\"Layers: {len(model.model.layers)}\")\n", "\n", "# Measure baseline with 3-seed averaging (E11-v3 standard)\n", "print(f\"\\nMeasuring baseline specialization (HEALTHY model, 3-seed average)...\")\n", "\n", "seed_results_baseline = []\n", "for seed in SEEDS:\n", "    torch.manual_seed(seed)\n", "    np.random.seed(seed)\n", "    \n", "    baseline_activations = extract_head_activations_with_noise(\n", "        model, tokenizer, STANDARD_PROMPTS, max_length=MAX_LENGTH, use_chat_template=USE_CHAT_TEMPLATE\n", "    )\n", "    baseline_entropies = compute_head_entropy_profiles(\n", "        baseline_activations['attention_patterns'],\n", "        baseline_activations['attention_masks']\n", "    )\n", "    baseline_metrics_seed = compute_specialization_metrics(baseline_entropies)\n", "    seed_results_baseline.append(baseline_metrics_seed)\n", "    print(f\"  Seed {seed}: SI={baseline_metrics_seed['specialization_index']:.4f}\")\n", "\n", "# Average across seeds\n", "baseline_metrics = {\n", "    'specialization_index': np.mean([r['specialization_index'] for r in seed_results_baseline]),\n", "    'mean_head_correlation': np.mean([r['mean_head_correlation'] for r in seed_results_baseline]),\n", "    'mean_head_variance': np.mean([r['mean_head_variance'] for r in seed_results_baseline]),\n", "    'si_std': np.std([r['specialization_index'] for r in seed_results_baseline])\n", "}\n", "\n", "print(f\"\\n  Baseline Specialization Index: {baseline_metrics['specialization_index']:.4f} \u00b1 {baseline_metrics['si_std']:.4f}\")\n", "print(f\"  Baseline Head Correlation: {baseline_metrics['mean_head_correlation']:.4f}\")\n", "print(f\"  Expected from E11-T: SI={E11T_REFERENCE['base_specialization']:.4f}\")\n", "\n", "# Verify we're in HEALTHY state\n", "si_diff = abs(baseline_metrics['specialization_index'] - E11T_REFERENCE['base_specialization'])\n", "if si_diff < 0.1:\n", "    print(f\"\\n  VERIFIED: Model is in HEALTHY state (diff={si_diff:.4f})\")\n", "else:\n", "    print(f\"\\n  WARNING: SI differs from E11-T reference by {si_diff:.4f}\")\n", "\n", "results = {\n", "    'baseline': {\n", "        'specialization_index': float(baseline_metrics['specialization_index']),\n", "        'si_std': float(baseline_metrics['si_std']),\n", "        'mean_head_correlation': float(baseline_metrics['mean_head_correlation']),\n", "        'mean_head_variance': float(baseline_metrics['mean_head_variance']),\n", "        'seed_results': [{'seed': s, 'si': r['specialization_index']} for s, r in zip(SEEDS, seed_results_baseline)]\n", "    },\n", "    'treatments': []\n", "}\n", "\n"]}, {"cell_type": "code", "execution_count": null, "id": "cell-6", "metadata": {}, "outputs": [], "source": ["# Cell 6: Noise Injection on HEALTHY Model (3-Seed)\n", "\n", "print(f\"\\n{'='*60}\")\n", "print(f\"PHASE 2: NOISE INJECTION ON HEALTHY BASE MODEL (3-Seed Average)\")\n", "print(f\"{'='*60}\")\n", "\n", "# Unified threshold (15% = artifact, aligned with verdict logic)\n", "ARTIFACT_THRESHOLD = 0.15  # 15%\n", "DISRUPTION_THRESHOLD = -0.05  # -5%\n", "\n", "for region_name, (start, end) in LAYER_RANGES.items():\n", "    print(f\"\\n{'='*50}\")\n", "    print(f\"TESTING: {region_name.upper()} (Layers {start}-{end-1})\")\n", "    print(f\"{'='*50}\")\n", "    \n", "    region_results = {\n", "        'region': region_name,\n", "        'layer_range': [start, end],\n", "        'noise_tests': []\n", "    }\n", "    \n", "    for noise_std in NOISE_LEVELS:\n", "        # E11-v3: 3-seed averaging for each noise level\n", "        seed_si_values = []\n", "        seed_corr_values = []\n", "        seed_var_values = []\n", "        \n", "        for seed in SEEDS:\n", "            torch.manual_seed(seed)\n", "            np.random.seed(seed)\n", "            \n", "            injector = AttentionNoiseInjector(model, (start, end), noise_std=noise_std)\n", "            injector.attach()\n", "            \n", "            treated_activations = extract_head_activations_with_noise(\n", "                model, tokenizer, STANDARD_PROMPTS, max_length=MAX_LENGTH, use_chat_template=USE_CHAT_TEMPLATE\n", "            )\n", "            treated_entropies = compute_head_entropy_profiles(\n", "                treated_activations['attention_patterns'],\n", "                treated_activations['attention_masks']\n", "            )\n", "            treated_metrics = compute_specialization_metrics(treated_entropies)\n", "            \n", "            injector.detach()\n", "            \n", "            seed_si_values.append(treated_metrics['specialization_index'])\n", "            seed_corr_values.append(treated_metrics['mean_head_correlation'])\n", "            seed_var_values.append(treated_metrics['mean_head_variance'])\n", "        \n", "        # Average across seeds\n", "        avg_si = np.mean(seed_si_values)\n", "        avg_corr = np.mean(seed_corr_values)\n", "        avg_var = np.mean(seed_var_values)\n", "        si_std = np.std(seed_si_values)\n", "        \n", "        si_before = baseline_metrics['specialization_index']\n", "        si_after = avg_si\n", "        si_delta = si_after - si_before\n", "        si_delta_pct = (si_delta / si_before) * 100 if si_before != 0 else 0\n", "        \n", "        corr_delta = avg_corr - baseline_metrics['mean_head_correlation']\n", "        \n", "        noise_result = {\n", "            'noise_std': float(noise_std),\n", "            'specialization_index': float(avg_si),\n", "            'si_std': float(si_std),\n", "            'mean_head_correlation': float(avg_corr),\n", "            'mean_head_variance': float(avg_var),\n", "            'si_delta': float(si_delta),\n", "            'si_delta_pct': float(si_delta_pct),\n", "            'corr_delta': float(corr_delta),\n", "            'seed_values': {str(s): float(v) for s, v in zip(SEEDS, seed_si_values)}\n", "        }\n", "        region_results['noise_tests'].append(noise_result)\n", "        \n", "        # Status based on UNIFIED thresholds (15% artifact, -5% disruption)\n", "        if si_delta_pct / 100 > ARTIFACT_THRESHOLD:\n", "            status = \"ARTIFACT?\"\n", "        elif si_delta_pct / 100 < DISRUPTION_THRESHOLD:\n", "            status = \"DISRUPTED\"\n", "        else:\n", "            status = \"STABLE\"\n", "        \n", "        print(f\"  \u03c3={noise_std:.2f}: SI={avg_si:.4f}\u00b1{si_std:.4f} (\u0394={si_delta:+.4f}, {si_delta_pct:+.1f}%) {status}\")\n", "    \n", "    # Find max SI change\n", "    max_change = max(region_results['noise_tests'], key=lambda x: abs(x['si_delta_pct']))\n", "    region_results['max_change_noise'] = max_change['noise_std']\n", "    region_results['max_change_pct'] = max_change['si_delta_pct']\n", "    \n", "    results['treatments'].append(region_results)\n", "    \n", "    print(f\"\\n  MAX CHANGE for {region_name}: \u03c3={max_change['noise_std']:.2f} -> {max_change['si_delta_pct']:+.1f}%\")\n"]}, {"cell_type": "code", "execution_count": null, "id": "cell-7", "metadata": {}, "outputs": [], "source": ["#", " ", "C", "e", "l", "l", " ", "7", ":", " ", "A", "r", "t", "i", "f", "a", "c", "t", " ", "A", "n", "a", "l", "y", "s", "i", "s", "\n", "\n", "p", "r", "i", "n", "t", "(", "f", "\"", "\\", "n", "{", "'", "=", "'", "*", "7", "0", "}", "\"", ")", "\n", "p", "r", "i", "n", "t", "(", "f", "\"", "P", "H", "A", "S", "E", " ", "3", ":", " ", "A", "R", "T", "I", "F", "A", "C", "T", " ", "A", "N", "A", "L", "Y", "S", "I", "S", "\"", ")", "\n", "p", "r", "i", "n", "t", "(", "f", "\"", "{", "'", "=", "'", "*", "7", "0", "}", "\"", ")", "\n", "\n", "#", " ", "U", "n", "i", "f", "i", "e", "d", " ", "t", "h", "r", "e", "s", "h", "o", "l", "d", "s", "\n", "A", "R", "T", "I", "F", "A", "C", "T", "_", "T", "H", "R", "E", "S", "H", "O", "L", "D", "_", "P", "C", "T", " ", "=", " ", "1", "5", ".", "0", " ", " ", " ", "#", " ", ">", "1", "5", "%", " ", "=", " ", "a", "r", "t", "i", "f", "a", "c", "t", "\n", "P", "A", "R", "T", "I", "A", "L", "_", "T", "H", "R", "E", "S", "H", "O", "L", "D", "_", "P", "C", "T", " ", "=", " ", "5", ".", "0", " ", " ", " ", " ", "#", " ", "5", "-", "1", "5", "%", " ", "=", " ", "p", "a", "r", "t", "i", "a", "l", " ", "a", "r", "t", "i", "f", "a", "c", "t", "\n", "D", "I", "S", "R", "U", "P", "T", "I", "O", "N", "_", "T", "H", "R", "E", "S", "H", "O", "L", "D", "_", "P", "C", "T", " ", "=", " ", "-", "5", ".", "0", " ", " ", "#", " ", "<", "-", "5", "%", " ", "=", " ", "d", "i", "s", "r", "u", "p", "t", "i", "o", "n", " ", "(", "g", "o", "o", "d", " ", "f", "o", "r", " ", "u", "s", ")", "\n", "\n", "p", "r", "i", "n", "t", "(", "f", "\"", "\\", "n", "T", "h", "r", "e", "s", "h", "o", "l", "d", "s", " ", "(", "u", "n", "i", "f", "i", "e", "d", ")", ":", "\"", ")", "\n", "p", "r", "i", "n", "t", "(", "f", "\"", " ", " ", "A", "R", "T", "I", "F", "A", "C", "T", ":", " ", " ", " ", " ", ">", " ", "{", "A", "R", "T", "I", "F", "A", "C", "T", "_", "T", "H", "R", "E", "S", "H", "O", "L", "D", "_", "P", "C", "T", "}", "%", "\"", ")", "\n", "p", "r", "i", "n", "t", "(", "f", "\"", " ", " ", "P", "A", "R", "T", "I", "A", "L", ":", " ", " ", " ", " ", " ", "{", "P", "A", "R", "T", "I", "A", "L", "_", "T", "H", "R", "E", "S", "H", "O", "L", "D", "_", "P", "C", "T", "}", "%", " ", "-", " ", "{", "A", "R", "T", "I", "F", "A", "C", "T", "_", "T", "H", "R", "E", "S", "H", "O", "L", "D", "_", "P", "C", "T", "}", "%", "\"", ")", "\n", "p", "r", "i", "n", "t", "(", "f", "\"", " ", " ", "S", "T", "A", "B", "L", "E", ":", " ", " ", " ", " ", " ", " ", "{", "D", "I", "S", "R", "U", "P", "T", "I", "O", "N", "_", "T", "H", "R", "E", "S", "H", "O", "L", "D", "_", "P", "C", "T", "}", "%", " ", "-", " ", "{", "P", "A", "R", "T", "I", "A", "L", "_", "T", "H", "R", "E", "S", "H", "O", "L", "D", "_", "P", "C", "T", "}", "%", "\"", ")", "\n", "p", "r", "i", "n", "t", "(", "f", "\"", " ", " ", "D", "I", "S", "R", "U", "P", "T", "I", "O", "N", ":", " ", " ", "<", " ", "{", "D", "I", "S", "R", "U", "P", "T", "I", "O", "N", "_", "T", "H", "R", "E", "S", "H", "O", "L", "D", "_", "P", "C", "T", "}", "%", "\"", ")", "\n", "\n", "p", "r", "i", "n", "t", "(", "f", "\"", "\\", "n", "B", "a", "s", "e", "l", "i", "n", "e", " ", "(", "H", "E", "A", "L", "T", "H", "Y", " ", "B", "a", "s", "e", " ", "M", "o", "d", "e", "l", ")", ":", "\"", ")", "\n", "p", "r", "i", "n", "t", "(", "f", "\"", " ", " ", "S", "I", ":", " ", "{", "b", "a", "s", "e", "l", "i", "n", "e", "_", "m", "e", "t", "r", "i", "c", "s", "[", "'", "s", "p", "e", "c", "i", "a", "l", "i", "z", "a", "t", "i", "o", "n", "_", "i", "n", "d", "e", "x", "'", "]", ":", ".", "4", "f", "}", "\"", ")", "\n", "p", "r", "i", "n", "t", "(", "f", "\"", " ", " ", "C", "o", "r", "r", "e", "l", "a", "t", "i", "o", "n", ":", " ", "{", "b", "a", "s", "e", "l", "i", "n", "e", "_", "m", "e", "t", "r", "i", "c", "s", "[", "'", "m", "e", "a", "n", "_", "h", "e", "a", "d", "_", "c", "o", "r", "r", "e", "l", "a", "t", "i", "o", "n", "'", "]", ":", ".", "4", "f", "}", "\"", ")", "\n", "\n", "p", "r", "i", "n", "t", "(", "f", "\"", "\\", "n", "\"", " ", "+", " ", "\"", "-", "\"", "*", "7", "0", ")", "\n", "p", "r", "i", "n", "t", "(", "f", "\"", "{", "'", "R", "e", "g", "i", "o", "n", "'", ":", "<", "1", "2", "}", " ", "{", "'", "M", "a", "x", " ", "\u0394", " ", "\u03c3", "'", ":", "<", "1", "0", "}", " ", "{", "'", "S", "I", " ", "A", "f", "t", "e", "r", "'", ":", "<", "1", "2", "}", " ", "{", "'", "\u0394", " ", "S", "I", "'", ":", "<", "1", "2", "}", " ", "{", "'", "\u0394", " ", "%", "'", ":", "<", "1", "2", "}", " ", "{", "'", "S", "t", "a", "t", "u", "s", "'", ":", "<", "1", "2", "}", "\"", ")", "\n", "p", "r", "i", "n", "t", "(", "\"", "-", "\"", "*", "7", "0", ")", "\n", "\n", "a", "r", "t", "i", "f", "a", "c", "t", "_", "e", "v", "i", "d", "e", "n", "c", "e", " ", "=", " ", "[", "]", "\n", "a", "n", "y", "_", "a", "r", "t", "i", "f", "a", "c", "t", " ", "=", " ", "F", "a", "l", "s", "e", "\n", "m", "a", "x", "_", "a", "r", "t", "i", "f", "a", "c", "t", "_", "p", "c", "t", " ", "=", " ", "0", "\n", "\n", "f", "o", "r", " ", "t", "r", "e", "a", "t", "m", "e", "n", "t", " ", "i", "n", " ", "r", "e", "s", "u", "l", "t", "s", "[", "'", "t", "r", "e", "a", "t", "m", "e", "n", "t", "s", "'", "]", ":", "\n", " ", " ", " ", " ", "r", "e", "g", "i", "o", "n", " ", "=", " ", "t", "r", "e", "a", "t", "m", "e", "n", "t", "[", "'", "r", "e", "g", "i", "o", "n", "'", "]", "\n", " ", " ", " ", " ", "m", "a", "x", "_", "n", "o", "i", "s", "e", " ", "=", " ", "t", "r", "e", "a", "t", "m", "e", "n", "t", "[", "'", "m", "a", "x", "_", "c", "h", "a", "n", "g", "e", "_", "n", "o", "i", "s", "e", "'", "]", "\n", " ", " ", " ", " ", "m", "a", "x", "_", "p", "c", "t", " ", "=", " ", "t", "r", "e", "a", "t", "m", "e", "n", "t", "[", "'", "m", "a", "x", "_", "c", "h", "a", "n", "g", "e", "_", "p", "c", "t", "'", "]", "\n", " ", " ", " ", " ", "\n", " ", " ", " ", " ", "m", "a", "x", "_", "t", "e", "s", "t", " ", "=", " ", "n", "e", "x", "t", "(", "t", " ", "f", "o", "r", " ", "t", " ", "i", "n", " ", "t", "r", "e", "a", "t", "m", "e", "n", "t", "[", "'", "n", "o", "i", "s", "e", "_", "t", "e", "s", "t", "s", "'", "]", " ", "i", "f", " ", "t", "[", "'", "n", "o", "i", "s", "e", "_", "s", "t", "d", "'", "]", " ", "=", "=", " ", "m", "a", "x", "_", "n", "o", "i", "s", "e", ")", "\n", " ", " ", " ", " ", "s", "i", "_", "a", "f", "t", "e", "r", " ", "=", " ", "m", "a", "x", "_", "t", "e", "s", "t", "[", "'", "s", "p", "e", "c", "i", "a", "l", "i", "z", "a", "t", "i", "o", "n", "_", "i", "n", "d", "e", "x", "'", "]", "\n", " ", " ", " ", " ", "s", "i", "_", "d", "e", "l", "t", "a", " ", "=", " ", "m", "a", "x", "_", "t", "e", "s", "t", "[", "'", "s", "i", "_", "d", "e", "l", "t", "a", "'", "]", "\n", " ", " ", " ", " ", "\n", " ", " ", " ", " ", "#", " ", "U", "s", "e", " ", "u", "n", "i", "f", "i", "e", "d", " ", "t", "h", "r", "e", "s", "h", "o", "l", "d", "s", "\n", " ", " ", " ", " ", "i", "f", " ", "m", "a", "x", "_", "p", "c", "t", " ", ">", " ", "A", "R", "T", "I", "F", "A", "C", "T", "_", "T", "H", "R", "E", "S", "H", "O", "L", "D", "_", "P", "C", "T", ":", "\n", " ", " ", " ", " ", " ", " ", " ", " ", "s", "t", "a", "t", "u", "s", " ", "=", " ", "\"", "A", "R", "T", "I", "F", "A", "C", "T", "!", "\"", "\n", " ", " ", " ", " ", " ", " ", " ", " ", "a", "r", "t", "i", "f", "a", "c", "t", "_", "e", "v", "i", "d", "e", "n", "c", "e", ".", "a", "p", "p", "e", "n", "d", "(", "(", "r", "e", "g", "i", "o", "n", ",", " ", "m", "a", "x", "_", "p", "c", "t", ",", " ", "m", "a", "x", "_", "n", "o", "i", "s", "e", ")", ")", "\n", " ", " ", " ", " ", " ", " ", " ", " ", "a", "n", "y", "_", "a", "r", "t", "i", "f", "a", "c", "t", " ", "=", " ", "T", "r", "u", "e", "\n", " ", " ", " ", " ", "e", "l", "i", "f", " ", "m", "a", "x", "_", "p", "c", "t", " ", ">", " ", "P", "A", "R", "T", "I", "A", "L", "_", "T", "H", "R", "E", "S", "H", "O", "L", "D", "_", "P", "C", "T", ":", "\n", " ", " ", " ", " ", " ", " ", " ", " ", "s", "t", "a", "t", "u", "s", " ", "=", " ", "\"", "P", "A", "R", "T", "I", "A", "L", "?", "\"", "\n", " ", " ", " ", " ", "e", "l", "i", "f", " ", "m", "a", "x", "_", "p", "c", "t", " ", "<", " ", "D", "I", "S", "R", "U", "P", "T", "I", "O", "N", "_", "T", "H", "R", "E", "S", "H", "O", "L", "D", "_", "P", "C", "T", ":", "\n", " ", " ", " ", " ", " ", " ", " ", " ", "s", "t", "a", "t", "u", "s", " ", "=", " ", "\"", "D", "I", "S", "R", "U", "P", "T", "E", "D", "\"", "\n", " ", " ", " ", " ", "e", "l", "s", "e", ":", "\n", " ", " ", " ", " ", " ", " ", " ", " ", "s", "t", "a", "t", "u", "s", " ", "=", " ", "\"", "S", "T", "A", "B", "L", "E", "\"", "\n", " ", " ", " ", " ", "\n", " ", " ", " ", " ", "i", "f", " ", "m", "a", "x", "_", "p", "c", "t", " ", ">", " ", "m", "a", "x", "_", "a", "r", "t", "i", "f", "a", "c", "t", "_", "p", "c", "t", ":", "\n", " ", " ", " ", " ", " ", " ", " ", " ", "m", "a", "x", "_", "a", "r", "t", "i", "f", "a", "c", "t", "_", "p", "c", "t", " ", "=", " ", "m", "a", "x", "_", "p", "c", "t", "\n", " ", " ", " ", " ", "\n", " ", " ", " ", " ", "p", "r", "i", "n", "t", "(", "f", "\"", "{", "r", "e", "g", "i", "o", "n", ":", "<", "1", "2", "}", " ", "{", "m", "a", "x", "_", "n", "o", "i", "s", "e", ":", "<", "1", "0", ".", "2", "f", "}", " ", "{", "s", "i", "_", "a", "f", "t", "e", "r", ":", "<", "1", "2", ".", "4", "f", "}", " ", "{", "s", "i", "_", "d", "e", "l", "t", "a", ":", "<", "+", "1", "2", ".", "4", "f", "}", " ", "{", "m", "a", "x", "_", "p", "c", "t", ":", "<", "+", "1", "2", ".", "1", "f", "}", " ", "{", "s", "t", "a", "t", "u", "s", ":", "<", "1", "2", "}", "\"", ")", "\n", "\n", "p", "r", "i", "n", "t", "(", "\"", "-", "\"", "*", "7", "0", ")", "\n", "\n", "#", " ", "E", "a", "r", "l", "y", " ", "@", " ", "0", ".", "0", "2", " ", "c", "o", "m", "p", "a", "r", "i", "s", "o", "n", " ", "(", "p", "r", "i", "m", "a", "r", "y", " ", "c", "o", "m", "p", "a", "r", "i", "s", "o", "n", " ", "w", "i", "t", "h", " ", "E", "1", "1", "-", "T", "-", "I", "n", "d", "r", "a", ")", "\n", "e", "a", "r", "l", "y", "_", "t", "r", "e", "a", "t", "m", "e", "n", "t", " ", "=", " ", "n", "e", "x", "t", "(", "t", " ", "f", "o", "r", " ", "t", " ", "i", "n", " ", "r", "e", "s", "u", "l", "t", "s", "[", "'", "t", "r", "e", "a", "t", "m", "e", "n", "t", "s", "'", "]", " ", "i", "f", " ", "t", "[", "'", "r", "e", "g", "i", "o", "n", "'", "]", " ", "=", "=", " ", "'", "e", "a", "r", "l", "y", "'", ")", "\n", "e", "a", "r", "l", "y", "_", "a", "t", "_", "0", "0", "2", " ", "=", " ", "n", "e", "x", "t", "(", "t", " ", "f", "o", "r", " ", "t", " ", "i", "n", " ", "e", "a", "r", "l", "y", "_", "t", "r", "e", "a", "t", "m", "e", "n", "t", "[", "'", "n", "o", "i", "s", "e", "_", "t", "e", "s", "t", "s", "'", "]", " ", "i", "f", " ", "t", "[", "'", "n", "o", "i", "s", "e", "_", "s", "t", "d", "'", "]", " ", "=", "=", " ", "0", ".", "0", "2", ")", "\n", "e", "a", "r", "l", "y", "_", "p", "c", "t", "_", "c", "h", "a", "n", "g", "e", " ", "=", " ", "e", "a", "r", "l", "y", "_", "a", "t", "_", "0", "0", "2", "[", "'", "s", "i", "_", "d", "e", "l", "t", "a", "_", "p", "c", "t", "'", "]", "\n", "\n", "p", "r", "i", "n", "t", "(", "f", "\"", "\\", "n", "{", "'", "=", "'", "*", "7", "0", "}", "\"", ")", "\n", "p", "r", "i", "n", "t", "(", "\"", "V", "E", "R", "D", "I", "C", "T", ":", " ", "I", "S", " ", "E", "1", "1", "-", "T", "-", "I", "N", "D", "R", "A", " ", "A", "N", " ", "A", "R", "T", "I", "F", "A", "C", "T", "?", "\"", ")", "\n", "p", "r", "i", "n", "t", "(", "f", "\"", "{", "'", "=", "'", "*", "7", "0", "}", "\"", ")", "\n", "\n", "p", "r", "i", "n", "t", "(", "f", "\"", "\\", "n", "[", "P", "r", "i", "m", "a", "r", "y", " ", "C", "o", "m", "p", "a", "r", "i", "s", "o", "n", ":", " ", "E", "a", "r", "l", "y", " ", "@", " ", "\u03c3", "=", "0", ".", "0", "2", "]", "\"", ")", "\n", "p", "r", "i", "n", "t", "(", "f", "\"", " ", " ", "E", "1", "1", "-", "T", "-", "I", "n", "d", "r", "a", " ", "(", "C", "o", "l", "l", "a", "p", "s", "e", "d", " ", "I", "n", "s", "t", "r", "u", "c", "t", ")", ":", " ", "+", "2", "8", ".", "6", "%", " ", "S", "I", " ", "i", "n", "c", "r", "e", "a", "s", "e", "\"", ")", "\n", "p", "r", "i", "n", "t", "(", "f", "\"", " ", " ", "T", "h", "i", "s", " ", "C", "o", "n", "t", "r", "o", "l", " ", "(", "H", "e", "a", "l", "t", "h", "y", " ", "B", "a", "s", "e", ")", ":", " ", " ", " ", " ", " ", " ", "{", "e", "a", "r", "l", "y", "_", "p", "c", "t", "_", "c", "h", "a", "n", "g", "e", ":", "+", ".", "1", "f", "}", "%", " ", "S", "I", " ", "c", "h", "a", "n", "g", "e", "\"", ")", "\n", "p", "r", "i", "n", "t", "(", "f", "\"", " ", " ", "G", "a", "p", ":", " ", "{", "2", "8", ".", "6", " ", "-", " ", "e", "a", "r", "l", "y", "_", "p", "c", "t", "_", "c", "h", "a", "n", "g", "e", ":", ".", "1", "f", "}", "p", "p", "\"", ")", "\n", "\n", "p", "r", "i", "n", "t", "(", "f", "\"", "\\", "n", "[", "S", "e", "c", "o", "n", "d", "a", "r", "y", " ", "C", "h", "e", "c", "k", ":", " ", "A", "N", "Y", " ", "r", "e", "g", "i", "o", "n", "/", "\u03c3", " ", ">", " ", "{", "A", "R", "T", "I", "F", "A", "C", "T", "_", "T", "H", "R", "E", "S", "H", "O", "L", "D", "_", "P", "C", "T", "}", "%", "]", "\"", ")", "\n", "i", "f", " ", "a", "r", "t", "i", "f", "a", "c", "t", "_", "e", "v", "i", "d", "e", "n", "c", "e", ":", "\n", " ", " ", " ", " ", "p", "r", "i", "n", "t", "(", "f", "\"", " ", " ", "F", "O", "U", "N", "D", ":", " ", "{", "l", "e", "n", "(", "a", "r", "t", "i", "f", "a", "c", "t", "_", "e", "v", "i", "d", "e", "n", "c", "e", ")", "}", " ", "a", "r", "t", "i", "f", "a", "c", "t", " ", "c", "a", "s", "e", "s", "\"", ")", "\n", " ", " ", " ", " ", "f", "o", "r", " ", "r", "e", "g", "i", "o", "n", ",", " ", "p", "c", "t", ",", " ", "s", "i", "g", "m", "a", " ", "i", "n", " ", "a", "r", "t", "i", "f", "a", "c", "t", "_", "e", "v", "i", "d", "e", "n", "c", "e", ":", "\n", " ", " ", " ", " ", " ", " ", " ", " ", "p", "r", "i", "n", "t", "(", "f", "\"", " ", " ", " ", " ", "-", " ", "{", "r", "e", "g", "i", "o", "n", "}", " ", "@", " ", "\u03c3", "=", "{", "s", "i", "g", "m", "a", "}", ":", " ", "{", "p", "c", "t", ":", "+", ".", "1", "f", "}", "%", "\"", ")", "\n", "e", "l", "s", "e", ":", "\n", " ", " ", " ", " ", "p", "r", "i", "n", "t", "(", "f", "\"", " ", " ", "N", "o", "n", "e", " ", "f", "o", "u", "n", "d", " ", "(", "a", "l", "l", " ", "r", "e", "g", "i", "o", "n", "s", " ", "<", " ", "{", "A", "R", "T", "I", "F", "A", "C", "T", "_", "T", "H", "R", "E", "S", "H", "O", "L", "D", "_", "P", "C", "T", "}", "%", ")", "\"", ")", "\n", "\n", "#", " ", "V", "e", "r", "d", "i", "c", "t", " ", "l", "o", "g", "i", "c", " ", "u", "s", "i", "n", "g", " ", "B", "O", "T", "H", " ", "c", "h", "e", "c", "k", "s", "\n", "p", "r", "i", "n", "t", "(", "f", "\"", "\\", "n", "{", "'", "=", "'", "*", "7", "0", "}", "\"", ")", "\n", "\n", "#", " ", "P", "r", "i", "m", "a", "r", "y", " ", "v", "e", "r", "d", "i", "c", "t", " ", "b", "a", "s", "e", "d", " ", "o", "n", " ", "E", "a", "r", "l", "y", "@", "0", ".", "0", "2", " ", "(", "d", "i", "r", "e", "c", "t", " ", "c", "o", "m", "p", "a", "r", "i", "s", "o", "n", ")", "\n", "i", "f", " ", "e", "a", "r", "l", "y", "_", "p", "c", "t", "_", "c", "h", "a", "n", "g", "e", " ", ">", " ", "A", "R", "T", "I", "F", "A", "C", "T", "_", "T", "H", "R", "E", "S", "H", "O", "L", "D", "_", "P", "C", "T", ":", "\n", " ", " ", " ", " ", "p", "r", "i", "m", "a", "r", "y", "_", "v", "e", "r", "d", "i", "c", "t", " ", "=", " ", "\"", "A", "R", "T", "I", "F", "A", "C", "T", "_", "L", "I", "K", "E", "L", "Y", "\"", "\n", "e", "l", "i", "f", " ", "e", "a", "r", "l", "y", "_", "p", "c", "t", "_", "c", "h", "a", "n", "g", "e", " ", ">", " ", "P", "A", "R", "T", "I", "A", "L", "_", "T", "H", "R", "E", "S", "H", "O", "L", "D", "_", "P", "C", "T", ":", "\n", " ", " ", " ", " ", "p", "r", "i", "m", "a", "r", "y", "_", "v", "e", "r", "d", "i", "c", "t", " ", "=", " ", "\"", "P", "A", "R", "T", "I", "A", "L", "_", "A", "R", "T", "I", "F", "A", "C", "T", "\"", "\n", "e", "l", "i", "f", " ", "e", "a", "r", "l", "y", "_", "p", "c", "t", "_", "c", "h", "a", "n", "g", "e", " ", "<", " ", "D", "I", "S", "R", "U", "P", "T", "I", "O", "N", "_", "T", "H", "R", "E", "S", "H", "O", "L", "D", "_", "P", "C", "T", ":", "\n", " ", " ", " ", " ", "p", "r", "i", "m", "a", "r", "y", "_", "v", "e", "r", "d", "i", "c", "t", " ", "=", " ", "\"", "R", "E", "A", "L", "_", "S", "T", "R", "O", "N", "G", "L", "Y", "_", "C", "O", "N", "F", "I", "R", "M", "E", "D", "\"", "\n", "e", "l", "s", "e", ":", "\n", " ", " ", " ", " ", "p", "r", "i", "m", "a", "r", "y", "_", "v", "e", "r", "d", "i", "c", "t", " ", "=", " ", "\"", "R", "E", "A", "L", "_", "C", "O", "N", "F", "I", "R", "M", "E", "D", "\"", "\n", "\n", "#", " ", "S", "e", "c", "o", "n", "d", "a", "r", "y", " ", "v", "e", "r", "d", "i", "c", "t", " ", "b", "a", "s", "e", "d", " ", "o", "n", " ", "A", "N", "Y", " ", "a", "r", "t", "i", "f", "a", "c", "t", "\n", "i", "f", " ", "a", "n", "y", "_", "a", "r", "t", "i", "f", "a", "c", "t", ":", "\n", " ", " ", " ", " ", "s", "e", "c", "o", "n", "d", "a", "r", "y", "_", "c", "o", "n", "c", "e", "r", "n", " ", "=", " ", "T", "r", "u", "e", "\n", "e", "l", "s", "e", ":", "\n", " ", " ", " ", " ", "s", "e", "c", "o", "n", "d", "a", "r", "y", "_", "c", "o", "n", "c", "e", "r", "n", " ", "=", " ", "F", "a", "l", "s", "e", "\n", "\n", "#", " ", "C", "o", "m", "b", "i", "n", "e", "d", " ", "v", "e", "r", "d", "i", "c", "t", "\n", "i", "f", " ", "p", "r", "i", "m", "a", "r", "y", "_", "v", "e", "r", "d", "i", "c", "t", " ", "=", "=", " ", "\"", "A", "R", "T", "I", "F", "A", "C", "T", "_", "L", "I", "K", "E", "L", "Y", "\"", " ", "o", "r", " ", "(", "s", "e", "c", "o", "n", "d", "a", "r", "y", "_", "c", "o", "n", "c", "e", "r", "n", " ", "a", "n", "d", " ", "m", "a", "x", "_", "a", "r", "t", "i", "f", "a", "c", "t", "_", "p", "c", "t", " ", ">", " ", "2", "0", ")", ":", "\n", " ", " ", " ", " ", "v", "e", "r", "d", "i", "c", "t", " ", "=", " ", "\"", "A", "R", "T", "I", "F", "A", "C", "T", "_", "L", "I", "K", "E", "L", "Y", "\"", "\n", " ", " ", " ", " ", "p", "r", "i", "n", "t", "(", "f", "\"", "\\", "n", " ", " ", "V", "E", "R", "D", "I", "C", "T", ":", " ", "{", "v", "e", "r", "d", "i", "c", "t", "}", "\"", ")", "\n", " ", " ", " ", " ", "p", "r", "i", "n", "t", "(", "f", "\"", " ", " ", "N", "o", "i", "s", "e", " ", "i", "n", "f", "l", "a", "t", "e", "s", " ", "S", "I", " ", "e", "v", "e", "n", " ", "i", "n", " ", "h", "e", "a", "l", "t", "h", "y", " ", "m", "o", "d", "e", "l", "s", ".", "\"", ")", "\n", " ", " ", " ", " ", "p", "r", "i", "n", "t", "(", "f", "\"", " ", " ", "E", "1", "1", "-", "T", "-", "I", "n", "d", "r", "a", " ", "'", "r", "e", "c", "o", "v", "e", "r", "y", "'", " ", "m", "a", "y", " ", "b", "e", " ", "m", "e", "a", "s", "u", "r", "e", "m", "e", "n", "t", " ", "a", "r", "t", "i", "f", "a", "c", "t", "!", "\"", ")", "\n", "e", "l", "i", "f", " ", "p", "r", "i", "m", "a", "r", "y", "_", "v", "e", "r", "d", "i", "c", "t", " ", "=", "=", " ", "\"", "P", "A", "R", "T", "I", "A", "L", "_", "A", "R", "T", "I", "F", "A", "C", "T", "\"", " ", "o", "r", " ", "s", "e", "c", "o", "n", "d", "a", "r", "y", "_", "c", "o", "n", "c", "e", "r", "n", ":", "\n", " ", " ", " ", " ", "v", "e", "r", "d", "i", "c", "t", " ", "=", " ", "\"", "P", "A", "R", "T", "I", "A", "L", "_", "A", "R", "T", "I", "F", "A", "C", "T", "\"", "\n", " ", " ", " ", " ", "p", "r", "i", "n", "t", "(", "f", "\"", "\\", "n", " ", " ", "V", "E", "R", "D", "I", "C", "T", ":", " ", "{", "v", "e", "r", "d", "i", "c", "t", "}", "\"", ")", "\n", " ", " ", " ", " ", "p", "r", "i", "n", "t", "(", "f", "\"", " ", " ", "N", "o", "i", "s", "e", " ", "h", "a", "s", " ", "s", "o", "m", "e", " ", "e", "f", "f", "e", "c", "t", " ", "o", "n", " ", "h", "e", "a", "l", "t", "h", "y", " ", "S", "I", ".", "\"", ")", "\n", " ", " ", " ", " ", "p", "r", "i", "n", "t", "(", "f", "\"", " ", " ", "E", "1", "1", "-", "T", "-", "I", "n", "d", "r", "a", " ", "r", "e", "c", "o", "v", "e", "r", "y", " ", "i", "s", " ", "P", "A", "R", "T", "I", "A", "L", "L", "Y", " ", "r", "e", "a", "l", ",", " ", "p", "a", "r", "t", "i", "a", "l", "l", "y", " ", "a", "r", "t", "i", "f", "a", "c", "t", ".", "\"", ")", "\n", "e", "l", "i", "f", " ", "p", "r", "i", "m", "a", "r", "y", "_", "v", "e", "r", "d", "i", "c", "t", " ", "=", "=", " ", "\"", "R", "E", "A", "L", "_", "S", "T", "R", "O", "N", "G", "L", "Y", "_", "C", "O", "N", "F", "I", "R", "M", "E", "D", "\"", ":", "\n", " ", " ", " ", " ", "v", "e", "r", "d", "i", "c", "t", " ", "=", " ", "\"", "R", "E", "A", "L", "_", "S", "T", "R", "O", "N", "G", "L", "Y", "_", "C", "O", "N", "F", "I", "R", "M", "E", "D", "\"", "\n", " ", " ", " ", " ", "p", "r", "i", "n", "t", "(", "f", "\"", "\\", "n", " ", " ", "V", "E", "R", "D", "I", "C", "T", ":", " ", "{", "v", "e", "r", "d", "i", "c", "t", "}", "\"", ")", "\n", " ", " ", " ", " ", "p", "r", "i", "n", "t", "(", "f", "\"", " ", " ", "N", "o", "i", "s", "e", " ", "D", "E", "C", "R", "E", "A", "S", "E", "S", " ", "h", "e", "a", "l", "t", "h", "y", " ", "S", "I", " ", "(", "d", "i", "s", "r", "u", "p", "t", "s", " ", "e", "x", "i", "s", "t", "i", "n", "g", " ", "s", "p", "e", "c", "i", "a", "l", "i", "z", "a", "t", "i", "o", "n", ")", "!", "\"", ")", "\n", " ", " ", " ", " ", "p", "r", "i", "n", "t", "(", "f", "\"", " ", " ", "E", "1", "1", "-", "T", "-", "I", "n", "d", "r", "a", " ", "r", "e", "c", "o", "v", "e", "r", "y", " ", "o", "n", " ", "c", "o", "l", "l", "a", "p", "s", "e", "d", " ", "m", "o", "d", "e", "l", " ", "i", "s", " ", "S", "T", "R", "O", "N", "G", "L", "Y", " ", "C", "O", "N", "F", "I", "R", "M", "E", "D", " ", "R", "E", "A", "L", "!", "\"", ")", "\n", " ", " ", " ", " ", "p", "r", "i", "n", "t", "(", "f", "\"", " ", " ", "T", "h", "e", " ", "c", "o", "l", "l", "a", "p", "s", "e", "d", " ", "m", "o", "d", "e", "l", " ", "h", "a", "s", " ", "l", "a", "t", "e", "n", "t", " ", "s", "p", "e", "c", "i", "a", "l", "i", "z", "a", "t", "i", "o", "n", " ", "c", "a", "p", "a", "c", "i", "t", "y", " ", "t", "h", "a", "t", " ", "n", "o", "i", "s", "e", " ", "u", "n", "l", "o", "c", "k", "s", ".", "\"", ")", "\n", "e", "l", "s", "e", ":", "\n", " ", " ", " ", " ", "v", "e", "r", "d", "i", "c", "t", " ", "=", " ", "\"", "R", "E", "A", "L", "_", "C", "O", "N", "F", "I", "R", "M", "E", "D", "\"", "\n", " ", " ", " ", " ", "p", "r", "i", "n", "t", "(", "f", "\"", "\\", "n", " ", " ", "V", "E", "R", "D", "I", "C", "T", ":", " ", "{", "v", "e", "r", "d", "i", "c", "t", "}", "\"", ")", "\n", " ", " ", " ", " ", "p", "r", "i", "n", "t", "(", "f", "\"", " ", " ", "N", "o", "i", "s", "e", " ", "d", "o", "e", "s", " ", "N", "O", "T", " ", "s", "i", "g", "n", "i", "f", "i", "c", "a", "n", "t", "l", "y", " ", "i", "n", "f", "l", "a", "t", "e", " ", "h", "e", "a", "l", "t", "h", "y", " ", "S", "I", ".", "\"", ")", "\n", " ", " ", " ", " ", "p", "r", "i", "n", "t", "(", "f", "\"", " ", " ", "E", "1", "1", "-", "T", "-", "I", "n", "d", "r", "a", " ", "r", "e", "c", "o", "v", "e", "r", "y", " ", "o", "n", " ", "c", "o", "l", "l", "a", "p", "s", "e", "d", " ", "m", "o", "d", "e", "l", " ", "i", "s", " ", "R", "E", "A", "L", "!", "\"", ")", "\n", "\n", "r", "e", "s", "u", "l", "t", "s", "[", "'", "v", "e", "r", "d", "i", "c", "t", "'", "]", " ", "=", " ", "{", "\n", " ", " ", " ", " ", "'", "c", "o", "d", "e", "'", ":", " ", "v", "e", "r", "d", "i", "c", "t", ",", "\n", " ", " ", " ", " ", "'", "b", "a", "s", "e", "l", "i", "n", "e", "_", "s", "i", "'", ":", " ", "b", "a", "s", "e", "l", "i", "n", "e", "_", "m", "e", "t", "r", "i", "c", "s", "[", "'", "s", "p", "e", "c", "i", "a", "l", "i", "z", "a", "t", "i", "o", "n", "_", "i", "n", "d", "e", "x", "'", "]", ",", "\n", " ", " ", " ", " ", "'", "e", "a", "r", "l", "y", "_", "0", "0", "2", "_", "c", "h", "a", "n", "g", "e", "_", "p", "c", "t", "'", ":", " ", "e", "a", "r", "l", "y", "_", "p", "c", "t", "_", "c", "h", "a", "n", "g", "e", ",", "\n", " ", " ", " ", " ", "'", "m", "a", "x", "_", "a", "r", "t", "i", "f", "a", "c", "t", "_", "p", "c", "t", "'", ":", " ", "m", "a", "x", "_", "a", "r", "t", "i", "f", "a", "c", "t", "_", "p", "c", "t", ",", "\n", " ", " ", " ", " ", "'", "a", "r", "t", "i", "f", "a", "c", "t", "_", "e", "v", "i", "d", "e", "n", "c", "e", "'", ":", " ", "a", "r", "t", "i", "f", "a", "c", "t", "_", "e", "v", "i", "d", "e", "n", "c", "e", ",", "\n", " ", " ", " ", " ", "'", "a", "n", "y", "_", "a", "r", "t", "i", "f", "a", "c", "t", "_", "a", "b", "o", "v", "e", "_", "t", "h", "r", "e", "s", "h", "o", "l", "d", "'", ":", " ", "a", "n", "y", "_", "a", "r", "t", "i", "f", "a", "c", "t", ",", "\n", " ", " ", " ", " ", "'", "t", "h", "r", "e", "s", "h", "o", "l", "d", "s", "'", ":", " ", "{", "\n", " ", " ", " ", " ", " ", " ", " ", " ", "'", "a", "r", "t", "i", "f", "a", "c", "t", "'", ":", " ", "A", "R", "T", "I", "F", "A", "C", "T", "_", "T", "H", "R", "E", "S", "H", "O", "L", "D", "_", "P", "C", "T", ",", "\n", " ", " ", " ", " ", " ", " ", " ", " ", "'", "p", "a", "r", "t", "i", "a", "l", "'", ":", " ", "P", "A", "R", "T", "I", "A", "L", "_", "T", "H", "R", "E", "S", "H", "O", "L", "D", "_", "P", "C", "T", ",", "\n", " ", " ", " ", " ", " ", " ", " ", " ", "'", "d", "i", "s", "r", "u", "p", "t", "i", "o", "n", "'", ":", " ", "D", "I", "S", "R", "U", "P", "T", "I", "O", "N", "_", "T", "H", "R", "E", "S", "H", "O", "L", "D", "_", "P", "C", "T", "\n", " ", " ", " ", " ", "}", ",", "\n", " ", " ", " ", " ", "'", "c", "o", "m", "p", "a", "r", "i", "s", "o", "n", "'", ":", " ", "{", "\n", " ", " ", " ", " ", " ", " ", " ", " ", "'", "e", "1", "1", "t", "_", "i", "n", "d", "r", "a", "_", "e", "a", "r", "l", "y", "_", "0", "0", "2", "'", ":", " ", "2", "8", ".", "6", ",", "\n", " ", " ", " ", " ", " ", " ", " ", " ", "'", "c", "o", "n", "t", "r", "o", "l", "_", "e", "a", "r", "l", "y", "_", "0", "0", "2", "'", ":", " ", "e", "a", "r", "l", "y", "_", "p", "c", "t", "_", "c", "h", "a", "n", "g", "e", ",", "\n", " ", " ", " ", " ", " ", " ", " ", " ", "'", "g", "a", "p", "_", "p", "p", "'", ":", " ", "2", "8", ".", "6", " ", "-", " ", "e", "a", "r", "l", "y", "_", "p", "c", "t", "_", "c", "h", "a", "n", "g", "e", "\n", " ", " ", " ", " ", "}", "\n", "}", "\n", "\n", "p", "r", "i", "n", "t", "(", "f", "\"", "\\", "n", "{", "'", "=", "'", "*", "7", "0", "}", "\"", ")"]}, {"cell_type": "code", "execution_count": null, "id": "cell-8", "metadata": {}, "outputs": [], "source": ["# Cell 8: Visualization\n", "\n", "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n", "\n", "colors = {\n", "    'early': '#3498db',\n", "    'middle': '#2ecc71',\n", "    'late': '#e74c3c',\n", "    'all': '#9b59b6'\n", "}\n", "\n", "# Plot 1: SI Change % by Region\n", "ax1 = axes[0, 0]\n", "regions = [t['region'] for t in results['treatments']]\n", "max_changes = [t['max_change_pct'] for t in results['treatments']]\n", "bar_colors = [colors[r] for r in regions]\n", "\n", "bars = ax1.bar(regions, max_changes, color=bar_colors, alpha=0.8, edgecolor='black')\n", "ax1.axhline(y=0, color='black', linestyle='-', linewidth=1)\n", "ax1.axhline(y=10, color='red', linestyle=':', alpha=0.7, label='Artifact threshold (10%)')\n", "ax1.axhline(y=-5, color='green', linestyle=':', alpha=0.7, label='Disruption threshold (-5%)')\n", "ax1.set_ylabel('Max SI Change %')\n", "ax1.set_title('Healthy Base: Max SI Change Under Noise\\n(Should be near 0 if E11-T-Indra is real)')\n", "ax1.legend()\n", "\n", "for bar, change in zip(bars, max_changes):\n", "    ax1.annotate(f'{change:+.1f}%', xy=(bar.get_x() + bar.get_width()/2, change),\n", "                 xytext=(0, 5 if change > 0 else -15), textcoords='offset points',\n", "                 ha='center', fontsize=11, fontweight='bold')\n", "\n", "# Plot 2: Comparison with E11-T-Indra\n", "ax2 = axes[0, 1]\n", "comparison_regions = ['Early (\u03c3=0.02)']\n", "e11t_indra_values = [28.6]  # From E11-T-Indra results\n", "control_values = [results['verdict']['comparison']['control_early_002']]\n", "\n", "x = np.arange(len(comparison_regions))\n", "width = 0.35\n", "\n", "bars1 = ax2.bar(x - width/2, e11t_indra_values, width, label='E11-T-Indra (Collapsed)', color='#e74c3c', alpha=0.8)\n", "bars2 = ax2.bar(x + width/2, control_values, width, label='Control (Healthy)', color='#3498db', alpha=0.8)\n", "\n", "ax2.set_ylabel('SI Change %')\n", "ax2.set_title('Critical Comparison: Collapsed vs Healthy\\n(Large gap = E11-T-Indra is REAL)')\n", "ax2.set_xticks(x)\n", "ax2.set_xticklabels(comparison_regions)\n", "ax2.legend()\n", "ax2.axhline(y=0, color='black', linestyle='-', linewidth=1)\n", "\n", "for bar in bars1:\n", "    ax2.annotate(f'{bar.get_height():.1f}%', xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n", "                 xytext=(0, 5), textcoords='offset points', ha='center', fontsize=11, fontweight='bold')\n", "for bar in bars2:\n", "    ax2.annotate(f'{bar.get_height():.1f}%', xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n", "                 xytext=(0, 5), textcoords='offset points', ha='center', fontsize=11, fontweight='bold')\n", "\n", "# Plot 3: Dose-Response on Healthy Base\n", "ax3 = axes[1, 0]\n", "for treatment in results['treatments']:\n", "    region = treatment['region']\n", "    noise_levels = [t['noise_std'] for t in treatment['noise_tests']]\n", "    si_values = [t['specialization_index'] for t in treatment['noise_tests']]\n", "    ax3.plot(noise_levels, si_values, 'o-', color=colors[region], \n", "             label=region.capitalize(), linewidth=2, markersize=8)\n", "\n", "ax3.axhline(y=baseline_metrics['specialization_index'], color='green', linestyle='--', \n", "            label=f'Baseline ({baseline_metrics[\"specialization_index\"]:.3f})')\n", "ax3.set_xlabel('Noise Level (\u03c3)')\n", "ax3.set_ylabel('Specialization Index')\n", "ax3.set_title('Dose-Response: Healthy Base Under Noise\\n(Flat = No artifact, Increasing = Artifact)')\n", "ax3.legend()\n", "ax3.grid(True, alpha=0.3)\n", "\n", "# Plot 4: SI Change Heatmap\n", "ax4 = axes[1, 1]\n", "region_order = ['early', 'middle', 'late', 'all']\n", "heatmap_data = []\n", "for region in region_order:\n", "    treatment = next(t for t in results['treatments'] if t['region'] == region)\n", "    row = [t['si_delta_pct'] for t in treatment['noise_tests']]\n", "    heatmap_data.append(row)\n", "\n", "heatmap_data = np.array(heatmap_data)\n", "\n", "im = ax4.imshow(heatmap_data, cmap='RdYlGn_r', aspect='auto', vmin=-20, vmax=20)\n", "ax4.set_xticks(range(len(NOISE_LEVELS)))\n", "ax4.set_xticklabels([f'\u03c3={n:.2f}' for n in NOISE_LEVELS])\n", "ax4.set_yticks(range(len(region_order)))\n", "ax4.set_yticklabels([r.capitalize() for r in region_order])\n", "ax4.set_xlabel('Noise Level')\n", "ax4.set_ylabel('Target Region')\n", "ax4.set_title('SI Change % Heatmap (Healthy Base)\\n(Green = Stable/Decrease, Red = Increase = Artifact)')\n", "\n", "for i in range(len(region_order)):\n", "    for j in range(len(NOISE_LEVELS)):\n", "        val = heatmap_data[i, j]\n", "        color = 'white' if abs(val) > 10 else 'black'\n", "        ax4.text(j, i, f'{val:+.1f}%', ha='center', va='center', color=color, fontsize=9)\n", "\n", "plt.colorbar(im, ax=ax4, label='SI Change %')\n", "\n", "plt.tight_layout()\n", "fig_path = f'figures/E11T_indra_B_base_control_{TIMESTAMP}.png'\n", "plt.savefig(fig_path, dpi=150, bbox_inches='tight')\n", "plt.show()\n", "\n", "print(f\"\\nFigure saved: {fig_path}\")"]}, {"cell_type": "code", "execution_count": null, "id": "cell-9", "metadata": {}, "outputs": [], "source": ["# Cell 9: Save Results\n", "\n", "def convert_to_native(obj):\n", "    if isinstance(obj, dict):\n", "        return {k: convert_to_native(v) for k, v in obj.items()}\n", "    elif isinstance(obj, list):\n", "        return [convert_to_native(v) for v in obj]\n", "    elif isinstance(obj, tuple):\n", "        return tuple(convert_to_native(v) for v in obj)\n", "    elif isinstance(obj, (np.bool_, np.integer)):\n", "        return int(obj)\n", "    elif isinstance(obj, np.floating):\n", "        return float(obj)\n", "    elif isinstance(obj, np.ndarray):\n", "        return obj.tolist()\n", "    else:\n", "        return obj\n", "\n", "filename = f'results/E11T_indra_B_base_control_{TIMESTAMP}.json'\n", "\n", "output = {\n", "    'experiment': 'E11-T-Indra-B',\n", "    'timestamp': TIMESTAMP,\n", "    'model': MODEL_CONFIG['name'],\n", "    'model_type': 'BASE (healthy, not collapsed)',\n", "    'architecture': 'GQA',\n", "    'purpose': 'Artifact check - does noise inflate SI in healthy models?',\n", "    \n", "    # E11-v3 Methodology Block (REQUIRED)\n", "    'methodology': {\n", "        'standard': 'E11-v3',\n", "        'seeds': SEEDS,\n", "        'max_length': MAX_LENGTH,\n", "        'dtype': str(DTYPE),\n", "        'prompt_md5': actual_md5,\n", "        'prompt_md5_verified': prompts_ok,\n", "        'use_chat_template': USE_CHAT_TEMPLATE,\n", "        'attention_masked': True,\n", "        'num_prompts': len(STANDARD_PROMPTS),\n", "        'prompt_set': 'Standard-10'\n", "    },\n", "    \n", "    'e11t_reference': E11T_REFERENCE,\n", "    'layer_ranges': {k: list(v) for k, v in LAYER_RANGES.items()},\n", "    'noise_levels': NOISE_LEVELS,\n", "    'results': convert_to_native(results)\n", "}\n", "\n", "with open(filename, 'w') as f:\n", "    json.dump(output, f, indent=2)\n", "\n", "print(f\"Results saved: {filename}\")\n", "\n", "try:\n", "    from google.colab import files\n", "    files.download(filename)\n", "    files.download(fig_path)\n", "except:\n", "    pass\n"]}, {"cell_type": "markdown", "id": "cell-10", "metadata": {}, "source": "---\n\n## Summary\n\n### E11-T-Indra-B: Base Control (Artifact Check)\n\n**Purpose:**\nDetermine if E11-T-Indra's \"recovery\" is real or a measurement artifact.\n\n**Method:**\nRun identical noise injection protocol on HEALTHY Base model (SI=0.7134).\n\n**Unified Thresholds:**\n\n| Base SI Change | Status | Interpretation |\n|----------------|--------|----------------|\n| **< 5%** | STABLE | E11-T-Indra is **REAL** |\n| **5-15%** | PARTIAL | Some artifact, partially real |\n| **> 15%** | ARTIFACT | E11-T-Indra may be fake |\n| **< -5%** | DISRUPTED | **STRONGLY REAL** (noise hurts healthy specialization) |\n\n**Dual-Check Approach:**\n1. Primary: Early @ \u03c3=0.02 (direct comparison)\n2. Secondary: ANY region/\u03c3 > 15% (catches hidden artifacts)\n\n**The Key Comparison:**\n- E11-T-Indra (Collapsed): +28.6% SI at Early/\u03c3=0.02\n- This Control (Healthy): ??? % SI at Early/\u03c3=0.02\n\nIf the gap is large (Collapsed >> Healthy), E11-T-Indra recovery is REAL.\n\n---\n\n*Paper 4: Behavioral Sink Dynamics*\n*E11-T-Indra-B: Base Control Experiment*"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# ============================================================================\n", "# AUTO-DOWNLOAD RESULTS (Colab only)\n", "# ============================================================================\n", "import glob\n", "import shutil\n", "\n", "def auto_download_results():\n", "    try:\n", "        from google.colab import files\n", "    except ImportError:\n", "        print('Not in Colab - skipping auto-download')\n", "        return\n", "    \n", "    print('=' * 60)\n", "    print('AUTO-DOWNLOADING RESULTS...')\n", "    print('=' * 60)\n", "    \n", "    # Find all result files\n", "    json_files = glob.glob('results/*.json') + glob.glob('figures/*.json')\n", "    png_files = glob.glob('results/*.png') + glob.glob('figures/*.png')\n", "    all_files = json_files + png_files\n", "    \n", "    if not all_files:\n", "        print('WARNING: No result files found!')\n", "        return\n", "    \n", "    print(f'Found {len(all_files)} files')\n", "    \n", "    # Download as ZIP\n", "    import os\n", "    zip_name = f'E11_results_{os.path.basename(os.getcwd())}'\n", "    \n", "    # Create combined folder\n", "    os.makedirs('download_package', exist_ok=True)\n", "    for f in all_files:\n", "        shutil.copy(f, 'download_package/')\n", "    \n", "    shutil.make_archive(zip_name, 'zip', 'download_package')\n", "    print(f'Downloading: {zip_name}.zip')\n", "    files.download(f'{zip_name}.zip')\n", "    print('DOWNLOAD COMPLETE!')\n", "\n", "auto_download_results()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.10.0"}}, "nbformat": 4, "nbformat_minor": 5}