{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E11-Indra-Gemma: Does Chaos Inflate Healthy SI?\n",
    "\n",
    "**Paper 4: Behavioral Sink Dynamics**\n",
    "\n",
    "## Purpose: Close A2 Gap (GQA+SWA Replication)\n",
    "\n",
    "**A2 Claim:** Indra is state-dependent (heals collapse, does NOT inflate healthy SI)\n",
    "\n",
    "**Prior Evidence:**\n",
    "| Experiment | Model | State | Result |\n",
    "|------------|-------|-------|--------|\n",
    "| E11-T-Indra | LLaMA-3.1-8B (GQA) | COLLAPSED (SI -56%) | Recovery tested |\n",
    "| **E11-Indra-Gemma** | Gemma-2-9B (GQA+SWA) | **HEALTHY (SI +0.15%)** | **This notebook** |\n",
    "\n",
    "**Gap:** Does Indra inflate SI in healthy models? If not, A2 is confirmed.\n",
    "\n",
    "## Reference Values (E08b-G Gemma Ladder)\n",
    "\n",
    "| Model | Base SI | Instruct SI | ŒîSI | State |\n",
    "|-------|---------|-------------|-----|-------|\n",
    "| Gemma-2-9B | 0.790 | 0.791 | +0.15% | **HEALTHY** |\n",
    "\n",
    "## The Hypothesis\n",
    "\n",
    "> If A2 is true (Indra is state-dependent):\n",
    "> - Chaos should NOT significantly increase SI in healthy models\n",
    "> - \"Healthy\" = Alignment doesn't collapse specialization\n",
    "\n",
    "## Expected Outcomes\n",
    "\n",
    "| Outcome | Condition | Implication |\n",
    "|---------|-----------|-------------|\n",
    "| A2_CONFIRMED | SI inflation <5% | Indra is state-dependent |\n",
    "| A2_PARTIAL | SI inflation 5-20% | Partial state-dependency |\n",
    "| A2_REFUTED | SI inflation >20% | Indra inflates healthy models |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "# Cell 1: Setup + E11-v3 Standard\n!pip install -q transformers torch accelerate bitsandbytes scipy matplotlib seaborn huggingface_hub\n\nimport torch\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom scipy.stats import entropy as scipy_entropy\nimport json\nimport hashlib\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfrom pathlib import Path\nfrom datetime import datetime\n\n# ============ E11-v3 METHODOLOGY STANDARD ============\nSEEDS = [42, 123, 456]  # 3-seed averaging\nDTYPE = torch.bfloat16  # Standardized precision\nEXPECTED_MD5 = \"715065bab181f46bf12ed471951141e2\"  # Standard-10 v3\n\ndef verify_prompts(prompts):\n    \"\"\"Verify Standard-10 prompts via MD5.\"\"\"\n    combined = '|||'.join(prompts)  # Canonical delimiter for MD5\n    actual_md5 = hashlib.md5(combined.encode()).hexdigest()\n    verified = actual_md5 == EXPECTED_MD5\n    print(f\"  Prompt MD5: {actual_md5}\")\n    print(f\"  Expected:   {EXPECTED_MD5}\")\n    print(f\"  Verified:   {'‚úì' if verified else '‚úó MISMATCH!'}\")\n    return verified, actual_md5\n\n# Initial seed setup\nos.environ['PYTHONHASHSEED'] = '42'\ntorch.manual_seed(42)\nnp.random.seed(42)\nrandom.seed(42)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(42)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nTIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')\nPath('results').mkdir(parents=True, exist_ok=True)\nPath('figures').mkdir(parents=True, exist_ok=True)\nprint(f\"Timestamp: {TIMESTAMP}\")\nprint(f\"E11-v3 Standard: Seeds={SEEDS}, dtype={DTYPE}\")\n\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n\n# HF Login (Gemma requires acceptance of terms)\ntry:\n    from google.colab import userdata\n    from huggingface_hub import login\n    hf_token = userdata.get('HF_TOKEN')\n    if hf_token:\n        login(token=hf_token)\n        print(\"HF Login: SUCCESS\")\n    else:\n        print(\"WARNING: No HF_TOKEN found!\")\n        print(\"Go to: Runtime -> Secrets -> Add HF_TOKEN\")\nexcept:\n    print(\"Not in Colab - ensure HF_TOKEN is set via huggingface-cli login\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "# Cell 2: Configuration (E11-v3)\n\n# Model Configuration - GEMMA-2-9B (GQA + SWA)\nMODEL_CONFIG = {\n    'name': 'google/gemma-2-9b-it',\n    'display': 'Gemma-2-9B-IT (Healthy GQA+SWA)',\n    'num_layers': 42,\n    'num_query_heads': 16,\n    'num_kv_heads': 8,\n    'd_head': 256,\n    'architecture': 'GQA+SWA',\n    'rho': 0.267  # Head density\n}\n\n# Reference Values (from E08b-G Gemma Ladder)\nREFERENCE = {\n    'base_si': 0.790,\n    'instruct_si': 0.791,\n    'delta_si': 0.0015,  # +0.15%\n    'state': 'HEALTHY',\n    'base_correlation': 0.210,\n    'instruct_correlation': 0.209\n}\n\n# Layer Ranges for Gemma-2-9B (42 layers)\nLAYER_RANGES = {\n    'early': (0, 14),      # Layers 0-13  (~33%)\n    'middle': (14, 28),    # Layers 14-27 (~33%, Engine Room)\n    'late': (28, 42),      # Layers 28-41 (~33%)\n    'all': (0, 42)         # All layers\n}\n\n# Noise Levels to Test\nNOISE_LEVELS = [0.0, 0.01, 0.02, 0.05, 0.1, 0.2]\n\n# Tokenization (E11-v3 Standard)\nMAX_LENGTH = 128\n\n# ============ CANONICAL Standard-10 v3 Prompts ============\n# MD5: 715065bab181f46bf12ed471951141e2\nSTANDARD_PROMPTS = [\n    \"What is the capital of France and what is its population?\",\n    \"If all roses are flowers and some flowers fade quickly, can we conclude that some roses fade quickly? Explain step by step.\",\n    \"Calculate 47 multiplied by 23 and show your work.\",\n    \"Translate the following to German: 'The quick brown fox jumps over the lazy dog'.\",\n    \"Write a Python function that checks if a number is prime.\",\n    \"Summarize the main points: Machine learning is a subset of artificial intelligence that enables systems to learn from data. It uses algorithms to identify patterns and make decisions with minimal human intervention.\",\n    \"Statement A: 'All birds can fly.' Statement B: 'Penguins are birds that cannot fly.' Are these statements contradictory? Explain.\",\n    \"What are the safety considerations when using a kitchen knife?\",\n    \"Write a haiku about artificial intelligence.\",\n    \"Complete this sentence in a helpful way: 'The best approach to solving complex problems is'\",\n]\n\n# Verify prompts (E11-v3 Standard)\nprint(\"Verifying Standard-10 prompts...\")\nPROMPTS_VERIFIED, ACTUAL_MD5 = verify_prompts(STANDARD_PROMPTS)\nif not PROMPTS_VERIFIED:\n    raise ValueError(\"PROMPT MISMATCH! Check Standard-10 v3 canonical prompts.\")\n\nprint(f\"\\nE11-Indra-Gemma: A2 Gap Test (GQA+SWA)\")\nprint(f\"\\nTarget: {MODEL_CONFIG['display']}\")\nprint(f\"Architecture: {MODEL_CONFIG['architecture']}\")\nprint(f\"Head Density (œÅ): {MODEL_CONFIG['rho']}\")\nprint(f\"\\nReference SI (E08b-G):\")\nprint(f\"  Base SI:      {REFERENCE['base_si']:.4f}\")\nprint(f\"  Instruct SI:  {REFERENCE['instruct_si']:.4f}\")\nprint(f\"  State:        {REFERENCE['state']}\")\nprint(f\"\\nLayer Ranges (42-layer model):\")\nfor region, (start, end) in LAYER_RANGES.items():\n    print(f\"  {region}: layers {start}-{end-1}\")\nprint(f\"\\nE11-v3 Config: MAX_LENGTH={MAX_LENGTH}, dtype={DTYPE}, seeds={SEEDS}\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 3: Specialization Metrics (from E11-T)\n",
    "\n",
    "def extract_head_activations(model, tokenizer, prompts, max_length=128):\n",
    "    \"\"\"\n",
    "    Extract per-head activation patterns.\n",
    "    \"\"\"\n",
    "    all_attention_patterns = []\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        # Apply chat template for Instruct model\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        formatted = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            formatted, \n",
    "            return_tensors='pt',\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length'\n",
    "        ).to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, output_attentions=True, output_hidden_states=True)\n",
    "        \n",
    "        attn_stack = torch.stack([a.squeeze(0) for a in outputs.attentions], dim=0)\n",
    "        all_attention_patterns.append(attn_stack.cpu())\n",
    "    \n",
    "    return {\n",
    "        'attention_patterns': all_attention_patterns,\n",
    "        'num_layers': len(outputs.attentions),\n",
    "        'num_heads': outputs.attentions[0].shape[1]\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_head_entropy_profiles(attention_patterns):\n",
    "    \"\"\"Compute normalized entropy for each head across prompts.\"\"\"\n",
    "    num_prompts = len(attention_patterns)\n",
    "    num_layers = attention_patterns[0].shape[0]\n",
    "    num_heads = attention_patterns[0].shape[1]\n",
    "    \n",
    "    all_entropies = np.zeros((num_prompts, num_layers, num_heads))\n",
    "    \n",
    "    for p_idx, attn in enumerate(attention_patterns):\n",
    "        for layer in range(num_layers):\n",
    "            for head in range(num_heads):\n",
    "                attn_weights = attn[layer, head].mean(dim=0).float().cpu().numpy()\n",
    "                attn_weights = attn_weights / attn_weights.sum()\n",
    "                attn_weights = attn_weights[attn_weights > 0]\n",
    "                \n",
    "                if len(attn_weights) > 1:\n",
    "                    h = scipy_entropy(attn_weights, base=2)\n",
    "                    h_max = np.log2(len(attn_weights))\n",
    "                    h_norm = h / h_max if h_max > 0 else 0\n",
    "                else:\n",
    "                    h_norm = 0\n",
    "                \n",
    "                all_entropies[p_idx, layer, head] = h_norm\n",
    "    \n",
    "    return all_entropies.mean(axis=0)\n",
    "\n",
    "\n",
    "def compute_specialization_metrics(head_entropies):\n",
    "    \"\"\"Compute specialization metrics.\"\"\"\n",
    "    num_layers, num_heads = head_entropies.shape\n",
    "    \n",
    "    layer_variances = np.var(head_entropies, axis=1)\n",
    "    mean_variance = float(np.mean(layer_variances))\n",
    "    \n",
    "    head_profiles = head_entropies.T\n",
    "    head_corr_matrix = np.corrcoef(head_profiles)\n",
    "    upper_tri = head_corr_matrix[np.triu_indices(num_heads, k=1)]\n",
    "    mean_head_correlation = float(np.nanmean(upper_tri))\n",
    "    \n",
    "    specialization_index = 1.0 - mean_head_correlation\n",
    "    \n",
    "    head_contributions = np.mean(head_entropies, axis=0)\n",
    "    head_contributions = head_contributions / head_contributions.sum()\n",
    "    h_contrib = scipy_entropy(head_contributions, base=2)\n",
    "    effective_heads = 2 ** h_contrib if h_contrib > 0 else 1.0\n",
    "    effective_ratio = effective_heads / num_heads\n",
    "    \n",
    "    return {\n",
    "        'mean_head_variance': mean_variance,\n",
    "        'mean_head_correlation': mean_head_correlation,\n",
    "        'specialization_index': specialization_index,\n",
    "        'effective_heads': float(effective_heads),\n",
    "        'effective_ratio': float(effective_ratio),\n",
    "        'layer_variances': layer_variances.tolist(),\n",
    "        'num_layers': num_layers,\n",
    "        'num_heads': num_heads\n",
    "    }\n",
    "\n",
    "print(\"Specialization metrics functions loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 4: Layer-Targeted Noise Injector (adapted for Gemma-2)\n",
    "\n",
    "class AttentionNoiseInjector:\n",
    "    \"\"\"\n",
    "    Inject Gaussian noise into attention outputs of SPECIFIC layer ranges.\n",
    "    \n",
    "    This is the 'Indra' treatment - chaos injection.\n",
    "    For healthy models: should NOT inflate SI.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, target_range, noise_std=0.0):\n",
    "        self.model = model\n",
    "        self.target_start, self.target_end = target_range\n",
    "        self.noise_std = noise_std\n",
    "        self.hooks = []\n",
    "    \n",
    "    def _make_hook(self, layer_idx):\n",
    "        \"\"\"Create a forward hook for a specific layer.\"\"\"\n",
    "        def hook(module, input, output):\n",
    "            if self.noise_std > 0 and self.target_start <= layer_idx < self.target_end:\n",
    "                if isinstance(output, tuple):\n",
    "                    attn_output = output[0]\n",
    "                    noise = torch.randn_like(attn_output) * self.noise_std\n",
    "                    return (attn_output + noise,) + output[1:]\n",
    "                else:\n",
    "                    noise = torch.randn_like(output) * self.noise_std\n",
    "                    return output + noise\n",
    "            return output\n",
    "        return hook\n",
    "    \n",
    "    def attach(self):\n",
    "        \"\"\"Attach hooks to attention layers.\"\"\"\n",
    "        for idx, layer in enumerate(self.model.model.layers):\n",
    "            hook = layer.self_attn.register_forward_hook(self._make_hook(idx))\n",
    "            self.hooks.append(hook)\n",
    "    \n",
    "    def detach(self):\n",
    "        \"\"\"Remove all hooks.\"\"\"\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks = []\n",
    "    \n",
    "    def set_noise(self, std):\n",
    "        \"\"\"Update noise level.\"\"\"\n",
    "        self.noise_std = std\n",
    "\n",
    "print(\"Attention noise injector ready.\")\n",
    "print(f\"Target regions available: {list(LAYER_RANGES.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "# Cell 5: Load Model and Baseline Measurement (E11-v3: bfloat16 + 3-seed)\n\nprint(f\"\\n{'='*60}\")\nprint(f\"PHASE 1: LOAD MODEL AND VERIFY HEALTHY STATE\")\nprint(f\"{'='*60}\")\n\nprint(f\"\\nLoading: {MODEL_CONFIG['name']}\")\nprint(\"Note: Gemma-2-9B requires ~18GB VRAM. Using bfloat16 (E11-v3 standard).\")\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_CONFIG['name'])\n\n# Track quantization for JSON output\nQUANTIZATION = \"bfloat16\"  # E11-v3 Standard\n\n# Load in bfloat16 (E11-v3 Standard)\ntry:\n    model = AutoModelForCausalLM.from_pretrained(\n        MODEL_CONFIG['name'],\n        torch_dtype=DTYPE,  # bfloat16\n        device_map='auto',\n        trust_remote_code=True,\n        attn_implementation=\"eager\"  # CRITICAL: SDPA doesn't return attentions!\n    )\n    print(f\"Loaded in {DTYPE}\")\n    QUANTIZATION = str(DTYPE)\nexcept Exception as e:\n    print(f\"bfloat16 failed ({e}), trying 8-bit...\")\n    model = AutoModelForCausalLM.from_pretrained(\n        MODEL_CONFIG['name'],\n        load_in_8bit=True,\n        device_map='auto',\n        trust_remote_code=True,\n        attn_implementation=\"eager\"\n    )\n    print(\"Loaded in 8-bit\")\n    QUANTIZATION = \"8bit\"\n    print(\"‚ö†Ô∏è CAVEAT: 8-bit quantization may affect SI measurements!\")\n\n# CRITICAL: Set eval mode\nmodel.eval()\n\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\nprint(f\"Loaded: {sum(p.numel() for p in model.parameters()) / 1e9:.2f}B parameters\")\nprint(f\"Layers: {len(model.model.layers)}\")\nprint(f\"Quantization: {QUANTIZATION}\")\n\n# Measure baseline with 3-seed averaging (E11-v3)\nprint(f\"\\nMeasuring baseline specialization (3-seed averaging)...\")\n\nbaseline_seed_results = []\nfor seed in SEEDS:\n    print(f\"  Seed {seed}...\")\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    \n    baseline_activations = extract_head_activations(model, tokenizer, STANDARD_PROMPTS, max_length=MAX_LENGTH)\n    baseline_entropies = compute_head_entropy_profiles(baseline_activations['attention_patterns'])\n    baseline_metrics = compute_specialization_metrics(baseline_entropies)\n    baseline_seed_results.append({\n        'seed': seed,\n        'si': baseline_metrics['specialization_index'],\n        'corr': baseline_metrics['mean_head_correlation'],\n        'var': baseline_metrics['mean_head_variance']\n    })\n    print(f\"    SI={baseline_metrics['specialization_index']:.4f}\")\n\n# Average across seeds\navg_baseline_si = np.mean([r['si'] for r in baseline_seed_results])\nstd_baseline_si = np.std([r['si'] for r in baseline_seed_results])\navg_baseline_corr = np.mean([r['corr'] for r in baseline_seed_results])\navg_baseline_var = np.mean([r['var'] for r in baseline_seed_results])\n\nprint(f\"\\n  Baseline SI: {avg_baseline_si:.4f} ¬± {std_baseline_si:.6f}\")\nprint(f\"  Baseline Corr: {avg_baseline_corr:.4f}\")\nprint(f\"  Expected from E08b-G: SI={REFERENCE['instruct_si']:.4f}\")\n\n# Verify we're in healthy state\nif avg_baseline_si > 0.7:\n    print(f\"\\n  VERIFIED: Model is in HEALTHY state (SI > 0.7)\")\nelse:\n    print(f\"\\n  WARNING: SI lower than expected ({avg_baseline_si:.4f})\")\n\n# Store averaged baseline metrics\nbaseline_metrics_avg = {\n    'specialization_index': avg_baseline_si,\n    'si_std': std_baseline_si,\n    'mean_head_correlation': avg_baseline_corr,\n    'mean_head_variance': avg_baseline_var,\n    'seed_results': baseline_seed_results\n}\n\nresults = {\n    'baseline': baseline_metrics_avg,\n    'treatments': [],\n    'quantization': QUANTIZATION\n}",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "# Cell 6: Indra Treatment - Test Each Region and Noise Level (E11-v3: 3-seed)\n\nprint(f\"\\n{'='*60}\")\nprint(f\"PHASE 2: INDRA TREATMENT - DOES CHAOS INFLATE HEALTHY SI?\")\nprint(f\"{'='*60}\")\n\nbaseline_si = results['baseline']['specialization_index']\n\nfor region_name, (start, end) in LAYER_RANGES.items():\n    print(f\"\\n{'='*50}\")\n    print(f\"TREATING: {region_name.upper()} (Layers {start}-{end-1})\")\n    print(f\"{'='*50}\")\n    \n    region_results = {\n        'region': region_name,\n        'layer_range': [start, end],\n        'noise_tests': []\n    }\n    \n    for noise_std in NOISE_LEVELS:\n        # 3-seed averaging for each treatment (E11-v3)\n        treatment_seed_results = []\n        \n        for seed in SEEDS:\n            torch.manual_seed(seed)\n            np.random.seed(seed)\n            \n            # Create injector for this region\n            injector = AttentionNoiseInjector(model, (start, end), noise_std=noise_std)\n            injector.attach()\n            \n            # Measure specialization with noise\n            treated_activations = extract_head_activations(\n                model, tokenizer, STANDARD_PROMPTS, max_length=MAX_LENGTH\n            )\n            treated_entropies = compute_head_entropy_profiles(treated_activations['attention_patterns'])\n            treated_metrics = compute_specialization_metrics(treated_entropies)\n            \n            injector.detach()\n            \n            treatment_seed_results.append({\n                'seed': seed,\n                'si': treated_metrics['specialization_index'],\n                'corr': treated_metrics['mean_head_correlation'],\n                'var': treated_metrics['mean_head_variance']\n            })\n        \n        # Average across seeds\n        avg_si = np.mean([r['si'] for r in treatment_seed_results])\n        std_si = np.std([r['si'] for r in treatment_seed_results])\n        avg_corr = np.mean([r['corr'] for r in treatment_seed_results])\n        avg_var = np.mean([r['var'] for r in treatment_seed_results])\n        \n        # Compute inflation metrics\n        si_delta = avg_si - baseline_si\n        inflation_pct = (si_delta / baseline_si) * 100 if baseline_si > 0 else 0\n        corr_delta = avg_corr - results['baseline']['mean_head_correlation']\n        \n        noise_result = {\n            'noise_std': float(noise_std),\n            'specialization_index': float(avg_si),\n            'si_std': float(std_si),\n            'mean_head_correlation': float(avg_corr),\n            'mean_head_variance': float(avg_var),\n            'si_delta': float(si_delta),\n            'inflation_pct': float(inflation_pct),\n            'corr_delta': float(corr_delta),\n            'seed_results': treatment_seed_results\n        }\n        region_results['noise_tests'].append(noise_result)\n        \n        # Print result - for healthy models, we want LOW inflation\n        if inflation_pct > 20:\n            status = \"INFLATED!\"\n        elif inflation_pct > 5:\n            status = \"partial\"\n        elif inflation_pct < -5:\n            status = \"deflated\"\n        else:\n            status = \"STABLE\"\n        \n        print(f\"  œÉ={noise_std:.2f}: SI={avg_si:.4f}¬±{std_si:.4f} (Œî={si_delta:+.4f}, {inflation_pct:+.1f}%) {status}\")\n    \n    # Find max inflation for this region (to check if Indra inflates)\n    max_inflation_test = max(region_results['noise_tests'], key=lambda x: x['inflation_pct'])\n    region_results['max_inflation_noise'] = max_inflation_test['noise_std']\n    region_results['max_inflation_pct'] = max_inflation_test['inflation_pct']\n    region_results['max_si'] = max_inflation_test['specialization_index']\n    \n    results['treatments'].append(region_results)\n    \n    print(f\"\\n  MAX inflation for {region_name}: {max_inflation_test['inflation_pct']:+.1f}% at œÉ={max_inflation_test['noise_std']:.2f}\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 7: Analysis - Does Indra Inflate Healthy SI?\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"PHASE 3: A2 VERDICT - IS INDRA STATE-DEPENDENT?\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "print(f\"\\nReference Values (E08b-G):\")\n",
    "print(f\"  Expected SI (healthy):  {REFERENCE['instruct_si']:.4f}\")\n",
    "print(f\"  Measured Baseline SI:   {baseline_metrics['specialization_index']:.4f}\")\n",
    "print(f\"  Model State:            {REFERENCE['state']}\")\n",
    "\n",
    "# A2 Verdict Thresholds:\n",
    "# - Inflation <5% = A2_CONFIRMED (Indra is state-dependent)\n",
    "# - Inflation 5-20% = A2_PARTIAL\n",
    "# - Inflation >20% = A2_REFUTED (Indra inflates healthy)\n",
    "\n",
    "print(f\"\\n\" + \"-\"*70)\n",
    "print(f\"{'Region':<12} {'Max Inflation':<15} {'At œÉ':<10} {'Max SI':<12} {'Status':<15}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "worst_inflation = -999\n",
    "worst_region = None\n",
    "\n",
    "for treatment in results['treatments']:\n",
    "    region = treatment['region']\n",
    "    max_infl = treatment['max_inflation_pct']\n",
    "    max_noise = treatment['max_inflation_noise']\n",
    "    max_si = treatment['max_si']\n",
    "    \n",
    "    if max_infl > 20:\n",
    "        status = \"INFLATED!\"\n",
    "    elif max_infl > 5:\n",
    "        status = \"Partial inflation\"\n",
    "    elif max_infl < -5:\n",
    "        status = \"Deflated\"\n",
    "    else:\n",
    "        status = \"STABLE\"\n",
    "    \n",
    "    if max_infl > worst_inflation:\n",
    "        worst_inflation = max_infl\n",
    "        worst_region = treatment\n",
    "    \n",
    "    print(f\"{region:<12} {max_infl:+.1f}%{'':<8} {max_noise:<10.2f} {max_si:<12.4f} {status:<15}\")\n",
    "\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Final Verdict\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"VERDICT: IS INDRA STATE-DEPENDENT?\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "print(f\"\\nWorst-case inflation: {worst_inflation:+.1f}% in {worst_region['region']}\")\n",
    "\n",
    "if worst_inflation < 5:\n",
    "    verdict = \"A2_CONFIRMED\"\n",
    "    print(f\"\\n  VERDICT: {verdict}\")\n",
    "    print(f\"  Indra is STATE-DEPENDENT!\")\n",
    "    print(f\"  - Chaos does NOT inflate SI in healthy models\")\n",
    "    print(f\"  - Combined with E11-T-Indra: heals collapsed, ignores healthy\")\n",
    "    print(f\"  - A2 claim CONFIRMED across architectures (GQA + GQA+SWA)\")\n",
    "elif worst_inflation < 20:\n",
    "    verdict = \"A2_PARTIAL\"\n",
    "    print(f\"\\n  VERDICT: {verdict}\")\n",
    "    print(f\"  Partial state-dependency.\")\n",
    "    print(f\"  - Some inflation detected but below 20%\")\n",
    "    print(f\"  - A2 claim partially supported\")\n",
    "else:\n",
    "    verdict = \"A2_REFUTED\"\n",
    "    print(f\"\\n  VERDICT: {verdict}\")\n",
    "    print(f\"  Indra INFLATES healthy models!\")\n",
    "    print(f\"  - A2 claim REFUTED for GQA+SWA\")\n",
    "    print(f\"  - Chaos is NOT state-dependent\")\n",
    "\n",
    "results['verdict'] = {\n",
    "    'code': verdict,\n",
    "    'worst_region': worst_region['region'] if worst_region else None,\n",
    "    'worst_inflation_pct': float(worst_inflation),\n",
    "    'baseline_si': baseline_metrics['specialization_index'],\n",
    "    'model_state': REFERENCE['state'],\n",
    "    'architecture': MODEL_CONFIG['architecture']\n",
    "}\n",
    "\n",
    "print(f\"\\n{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 8: Visualization\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "\n",
    "colors = {\n",
    "    'early': '#3498db',\n",
    "    'middle': '#2ecc71',\n",
    "    'late': '#e74c3c',\n",
    "    'all': '#9b59b6'\n",
    "}\n",
    "\n",
    "# Plot 1: Max Inflation by Region\n",
    "ax1 = axes[0, 0]\n",
    "regions = [t['region'] for t in results['treatments']]\n",
    "inflations = [t['max_inflation_pct'] for t in results['treatments']]\n",
    "bar_colors = [colors[r] for r in regions]\n",
    "\n",
    "bars = ax1.bar(regions, inflations, color=bar_colors, alpha=0.8, edgecolor='black')\n",
    "ax1.axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
    "ax1.axhline(y=5, color='orange', linestyle=':', alpha=0.5, label='5% threshold (A2_PARTIAL)')\n",
    "ax1.axhline(y=20, color='red', linestyle=':', alpha=0.5, label='20% threshold (A2_REFUTED)')\n",
    "ax1.set_ylabel('Max Inflation %')\n",
    "ax1.set_title('SI Inflation by Region\\n(Lower = Better for A2)')\n",
    "ax1.legend()\n",
    "\n",
    "for bar, infl in zip(bars, inflations):\n",
    "    ax1.annotate(f'{infl:+.1f}%', xy=(bar.get_x() + bar.get_width()/2, infl),\n",
    "                 xytext=(0, 5 if infl > 0 else -15), textcoords='offset points',\n",
    "                 ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Plot 2: Dose-Response Curves\n",
    "ax2 = axes[0, 1]\n",
    "baseline_si = baseline_metrics['specialization_index']\n",
    "\n",
    "for treatment in results['treatments']:\n",
    "    region = treatment['region']\n",
    "    noise_levels = [t['noise_std'] for t in treatment['noise_tests']]\n",
    "    si_values = [t['specialization_index'] for t in treatment['noise_tests']]\n",
    "    ax2.plot(noise_levels, si_values, 'o-', color=colors[region], \n",
    "             label=region.capitalize(), linewidth=2, markersize=8)\n",
    "\n",
    "ax2.axhline(y=baseline_si, color='black', linestyle='--', label=f'Baseline ({baseline_si:.3f})')\n",
    "ax2.set_xlabel('Noise Level (œÉ)')\n",
    "ax2.set_ylabel('Specialization Index')\n",
    "ax2.set_title('Dose-Response: SI vs Chaos Intensity')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Inflation % across all treatments\n",
    "ax3 = axes[1, 0]\n",
    "\n",
    "for treatment in results['treatments']:\n",
    "    region = treatment['region']\n",
    "    noise_levels = [t['noise_std'] for t in treatment['noise_tests']]\n",
    "    infl_values = [t['inflation_pct'] for t in treatment['noise_tests']]\n",
    "    ax3.plot(noise_levels, infl_values, 'o-', color=colors[region], \n",
    "             label=region.capitalize(), linewidth=2, markersize=8)\n",
    "\n",
    "ax3.axhline(y=0, color='black', linestyle='--')\n",
    "ax3.axhline(y=5, color='orange', linestyle=':', alpha=0.5)\n",
    "ax3.axhline(y=20, color='red', linestyle=':', alpha=0.5)\n",
    "ax3.set_xlabel('Noise Level (œÉ)')\n",
    "ax3.set_ylabel('Inflation %')\n",
    "ax3.set_title('SI Inflation % vs Chaos Intensity\\n(Flat = State-Dependent)')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Inflation Heatmap\n",
    "ax4 = axes[1, 1]\n",
    "region_order = ['early', 'middle', 'late', 'all']\n",
    "heatmap_data = []\n",
    "for region in region_order:\n",
    "    treatment = next(t for t in results['treatments'] if t['region'] == region)\n",
    "    row = [t['inflation_pct'] for t in treatment['noise_tests']]\n",
    "    heatmap_data.append(row)\n",
    "\n",
    "heatmap_data = np.array(heatmap_data)\n",
    "\n",
    "im = ax4.imshow(heatmap_data, cmap='RdYlGn_r', aspect='auto', vmin=-10, vmax=30)\n",
    "ax4.set_xticks(range(len(NOISE_LEVELS)))\n",
    "ax4.set_xticklabels([f'œÉ={n:.2f}' for n in NOISE_LEVELS])\n",
    "ax4.set_yticks(range(len(region_order)))\n",
    "ax4.set_yticklabels([r.capitalize() for r in region_order])\n",
    "ax4.set_xlabel('Noise Level')\n",
    "ax4.set_ylabel('Target Region')\n",
    "ax4.set_title('Inflation % Heatmap\\n(Green = Stable, Red = Inflated)')\n",
    "\n",
    "for i in range(len(region_order)):\n",
    "    for j in range(len(NOISE_LEVELS)):\n",
    "        val = heatmap_data[i, j]\n",
    "        color = 'white' if abs(val) > 15 else 'black'\n",
    "        ax4.text(j, i, f'{val:+.1f}%', ha='center', va='center', color=color, fontsize=9)\n",
    "\n",
    "plt.colorbar(im, ax=ax4, label='Inflation %')\n",
    "\n",
    "plt.tight_layout()\n",
    "fig_path = f'figures/E11_indra_gemma_{TIMESTAMP}.png'\n",
    "plt.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFigure saved: {fig_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "# Cell 9: Save Results (E11-v3 with methodology block)\n\ndef convert_to_native(obj):\n    if isinstance(obj, dict):\n        return {k: convert_to_native(v) for k, v in obj.items()}\n    elif isinstance(obj, list):\n        return [convert_to_native(v) for v in obj]\n    elif isinstance(obj, tuple):\n        return tuple(convert_to_native(v) for v in obj)\n    elif isinstance(obj, (np.bool_, np.integer)):\n        return int(obj)\n    elif isinstance(obj, np.floating):\n        return float(obj)\n    elif isinstance(obj, np.ndarray):\n        return obj.tolist()\n    else:\n        return obj\n\nfilename = f'results/E11_indra_gemma_{TIMESTAMP}.json'\n\noutput = {\n    'experiment': 'E11-Indra-Gemma',\n    'purpose': 'A2 Gap Test - GQA+SWA replication',\n    'timestamp': TIMESTAMP,\n    'model': MODEL_CONFIG['name'],\n    'architecture': MODEL_CONFIG['architecture'],\n    'head_density_rho': MODEL_CONFIG['rho'],\n    'hypothesis': 'Indra should NOT inflate SI in healthy (non-collapsed) models',\n    'reference': REFERENCE,\n    'layer_ranges': {k: list(v) for k, v in LAYER_RANGES.items()},\n    'noise_levels': NOISE_LEVELS,\n    # E11-v3 Methodology Block\n    'methodology': {\n        'standard': 'E11-v3',\n        'seeds': SEEDS,\n        'max_length': MAX_LENGTH,\n        'dtype': str(DTYPE),\n        'prompt_md5': ACTUAL_MD5,\n        'prompt_md5_verified': PROMPTS_VERIFIED,\n        'num_prompts': len(STANDARD_PROMPTS),\n        'prompt_set': 'Standard-10 v3',\n        'quantization': QUANTIZATION,\n        'quantization_caveat': '8-bit quantization may affect SI measurements' if '8bit' in QUANTIZATION else None,\n        '3_seed_averaging': True\n    },\n    'results': convert_to_native(results)\n}\n\nwith open(filename, 'w') as f:\n    json.dump(output, f, indent=2)\n\nprint(f\"Results saved: {filename}\")\n\nprint(f\"\\nüìã E11-v3 Compliance:\")\nprint(f\"   Seeds: {SEEDS} ‚úì\")\nprint(f\"   dtype: {DTYPE} ‚úì\")\nprint(f\"   MD5: {ACTUAL_MD5} {'‚úì' if PROMPTS_VERIFIED else '‚úó'}\")\nprint(f\"   MAX_LENGTH: {MAX_LENGTH} ‚úì\")\nprint(f\"   Quantization: {QUANTIZATION}\")\nif '8bit' in QUANTIZATION:\n    print(\"   ‚ö†Ô∏è CAVEAT: Results obtained with 8-bit quantization\")\n\ntry:\n    from google.colab import files\n    files.download(filename)\n    files.download(fig_path)\nexcept:\n    pass",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### E11-Indra-Gemma: Does Chaos Inflate Healthy SI?\n",
    "\n",
    "**Purpose:** Close A2 Gap (GQA+SWA replication)\n",
    "\n",
    "**A2 Claim:** Indra is state-dependent:\n",
    "1. Heals collapsed models (E11-T-Indra on LLaMA) ‚úì\n",
    "2. Does NOT inflate healthy models (THIS EXPERIMENT)\n",
    "\n",
    "**Method:**\n",
    "1. Load healthy model (Gemma-2-9B-IT, SI ~0.79)\n",
    "2. Inject chaos at different noise levels (œÉ = 0.0 to 0.2)\n",
    "3. Target different layer regions (Early, Middle, Late, All)\n",
    "4. Measure SI inflation\n",
    "\n",
    "**Metric:**\n",
    "```\n",
    "Inflation % = (SI_after - SI_baseline) / SI_baseline √ó 100\n",
    "```\n",
    "\n",
    "**Verdict Criteria:**\n",
    "\n",
    "| Outcome | Condition | Implication |\n",
    "|---------|-----------|-------------|\n",
    "| A2_CONFIRMED | Max inflation <5% | Indra is state-dependent |\n",
    "| A2_PARTIAL | Max inflation 5-20% | Partial state-dependency |\n",
    "| A2_REFUTED | Max inflation >20% | Indra inflates healthy models |\n",
    "\n",
    "**If A2_CONFIRMED:**\n",
    "- Indra is truly state-dependent\n",
    "- Heals collapsed (LLaMA), ignores healthy (Gemma)\n",
    "- Therapeutic intervention, not universal boost\n",
    "\n",
    "---\n",
    "\n",
    "*Paper 4: Behavioral Sink Dynamics*  \n",
    "*E11-Indra-Gemma: A2 GQA+SWA Replication Test*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "# Cell 10: Artifact Log (E11-v3 Fixed)\n\nartifact_entry = {\n    'experiment': 'E11-Indra-Gemma',\n    'timestamp': TIMESTAMP,\n    'seeds': SEEDS,  # E11-v3: Multiple seeds, not single SEED\n    'model': MODEL_CONFIG['name'],\n    'architecture': MODEL_CONFIG['architecture'],\n    'quantization': QUANTIZATION,\n    'verdict': results['verdict']['code'],\n    'worst_region': results['verdict']['worst_region'],\n    'worst_inflation_pct': results['verdict']['worst_inflation_pct'],\n    'baseline_si': results['verdict']['baseline_si'],\n    'model_state': results['verdict']['model_state'],\n    'prompt_count': len(STANDARD_PROMPTS),\n    'prompt_md5': ACTUAL_MD5,\n    'methodology': 'E11-v3',\n    'files': {\n        'results': filename,\n        'figure': fig_path\n    }\n}\n\nartifact_log = f'results/E11_indra_gemma_artifact_log.jsonl'\nwith open(artifact_log, 'a') as f:\n    f.write(json.dumps(artifact_entry) + '\\n')\n\nprint(f\"Artifact log appended: {artifact_log}\")\nprint(f\"\\nEntry: {json.dumps(artifact_entry, indent=2)}\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 11: Auto-Download Results\n",
    "\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "def auto_download_results():\n",
    "    try:\n",
    "        from google.colab import files\n",
    "    except ImportError:\n",
    "        print('Not in Colab - skipping auto-download')\n",
    "        return\n",
    "    \n",
    "    print('=' * 60)\n",
    "    print('AUTO-DOWNLOADING RESULTS...')\n",
    "    print('=' * 60)\n",
    "    \n",
    "    # Find all result files\n",
    "    json_files = glob.glob('results/*.json') + glob.glob('figures/*.json')\n",
    "    png_files = glob.glob('results/*.png') + glob.glob('figures/*.png')\n",
    "    all_files = json_files + png_files\n",
    "    \n",
    "    if not all_files:\n",
    "        print('WARNING: No result files found!')\n",
    "        return\n",
    "    \n",
    "    print(f'Found {len(all_files)} files')\n",
    "    \n",
    "    # Download as ZIP\n",
    "    import os\n",
    "    zip_name = f'E11_indra_gemma_results_{TIMESTAMP}'\n",
    "    \n",
    "    # Create combined folder\n",
    "    os.makedirs('download_package', exist_ok=True)\n",
    "    for f in all_files:\n",
    "        shutil.copy(f, 'download_package/')\n",
    "    \n",
    "    shutil.make_archive(zip_name, 'zip', 'download_package')\n",
    "    print(f'Downloading: {zip_name}.zip')\n",
    "    files.download(f'{zip_name}.zip')\n",
    "    print('DOWNLOAD COMPLETE!')\n",
    "\n",
    "auto_download_results()"
   ]
  }
 ]
}