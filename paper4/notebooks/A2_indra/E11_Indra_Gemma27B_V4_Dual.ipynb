{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# E11-Indra-Gemma27B-V4: DUAL MODE (8-bit / Full Precision)\n", "\n", "**Paper 4: Behavioral Sink Dynamics**\n", "\n", "## Purpose: Validate Split-Brain Finding\n", "\n", "E08c showed 8-bit MASSIVELY biases SI (~100% underestimation):\n", "- 8-bit Base SI: 0.349\n", "- FP Base SI: 0.693 (+98.7%!)\n", "\n", "This notebook validates if Split-Brain pattern is:\n", "- **REAL** (architecture-determined) \u2192 FP shows same pattern\n", "- **ARTIFACT** (quantization-induced) \u2192 FP shows different pattern\n", "\n", "---\n", "\n", "## MODE SELECTION\n", "\n", "```python\n", "# In Cell 1, set:\n", "PRECISION_MODE = \"fp\"    # Full Precision (A100-80GB required)\n", "PRECISION_MODE = \"8bit\"  # 8-bit Quantization (any GPU with 24GB+)\n", "```\n", "\n", "---"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "source": ["# Cell 1: MODE SELECTION + Setup\n", "# ============================================================\n", "# CRITICAL: SET YOUR MODE HERE!\n", "# ============================================================\n", "\n", "PRECISION_MODE = \"fp\"  # OPTIONS: \"fp\" (Full Precision) or \"8bit\"\n", "\n", "# ============================================================\n", "\n", "!pip install -q transformers torch accelerate bitsandbytes scipy matplotlib seaborn huggingface_hub\n", "\n", "import torch\n", "import numpy as np\n", "import random\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n", "from scipy.stats import entropy as scipy_entropy\n", "import json\n", "import hashlib\n", "import warnings\n", "import gc\n", "import shutil\n", "import psutil\n", "import os\n", "import math\n", "warnings.filterwarnings('ignore')\n", "\n", "from pathlib import Path\n", "from datetime import datetime\n", "\n", "# ============ E11-v3 METHODOLOGY STANDARD ============\n", "SEEDS = [42, 123, 456]\n", "DTYPE = torch.bfloat16\n", "EXPECTED_MD5 = \"715065bab181f46bf12ed471951141e2\"\n", "\n", "def verify_prompts(prompts):\n", "    combined = '|||'.join(prompts)\n", "    actual_md5 = hashlib.md5(combined.encode()).hexdigest()\n", "    verified = actual_md5 == EXPECTED_MD5\n", "    print(f\"  Prompt MD5: {actual_md5}\")\n", "    print(f\"  Expected:   {EXPECTED_MD5}\")\n", "    print(f\"  Verified:   {'YES' if verified else 'NO - MISMATCH!'}\")\n", "    return verified, actual_md5\n", "\n", "# Set initial seed\n", "SEED = SEEDS[0]\n", "os.environ['PYTHONHASHSEED'] = str(SEED)\n", "random.seed(SEED)\n", "np.random.seed(SEED)\n", "torch.manual_seed(SEED)\n", "if torch.cuda.is_available():\n", "    torch.cuda.manual_seed_all(SEED)\n", "    torch.backends.cudnn.deterministic = True\n", "    torch.backends.cudnn.benchmark = False\n", "\n", "TIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')\n", "Path('results').mkdir(parents=True, exist_ok=True)\n", "Path('figures').mkdir(parents=True, exist_ok=True)\n", "\n", "# ============ DISK CLEANUP FUNCTIONS (for FP mode) ============\n", "def get_disk_free_gb():\n", "    disk_path = '/content' if os.path.exists('/content') else '/'\n", "    return shutil.disk_usage(disk_path).free / 1e9\n", "\n", "def clear_hf_cache(model_name=None):\n", "    hf_cache = os.path.expanduser(\"~/.cache/huggingface/hub\")\n", "    if not os.path.exists(hf_cache):\n", "        return\n", "    if model_name:\n", "        cache_name = model_name.replace('/', '--')\n", "        cache_path = os.path.join(hf_cache, f\"models--{cache_name}\")\n", "        if os.path.exists(cache_path):\n", "            size_gb = sum(os.path.getsize(os.path.join(dp, f)) for dp, dn, fn in os.walk(cache_path) for f in fn) / 1e9\n", "            shutil.rmtree(cache_path, ignore_errors=True)\n", "            print(f\"  Cleared {model_name} cache: {size_gb:.1f} GB\")\n", "    else:\n", "        size_gb = sum(os.path.getsize(os.path.join(dp, f)) for dp, dn, fn in os.walk(hf_cache) for f in fn) / 1e9\n", "        shutil.rmtree(hf_cache, ignore_errors=True)\n", "        print(f\"  Cleared ALL HF cache: {size_gb:.1f} GB\")\n", "\n", "def clear_gpu_memory():\n", "    gc.collect()\n", "    if torch.cuda.is_available():\n", "        torch.cuda.empty_cache()\n", "        torch.cuda.synchronize()\n", "\n", "def nuclear_cleanup():\n", "    print(\"\\nNUCLEAR CLEANUP...\")\n", "    clear_gpu_memory()\n", "    clear_hf_cache()\n", "    torch_cache = os.path.expanduser(\"~/.cache/torch\")\n", "    if os.path.exists(torch_cache):\n", "        shutil.rmtree(torch_cache, ignore_errors=True)\n", "    print(f\"  Disk Free: {get_disk_free_gb():.1f} GB\")\n", "\n", "# ============ RESOURCE VALIDATION ============\n", "print(\"=\"*70)\n", "print(f\"E11-INDRA-GEMMA27B-V4: DUAL MODE\")\n", "print(f\"MODE: {PRECISION_MODE.upper()}\")\n", "print(\"=\"*70)\n", "\n", "if not torch.cuda.is_available():\n", "    raise RuntimeError(\"GPU required!\")\n", "\n", "gpu_name = torch.cuda.get_device_name(0)\n", "gpu_mem_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n", "print(f\"\\nGPU: {gpu_name}\")\n", "print(f\"VRAM: {gpu_mem_gb:.1f} GB\")\n", "\n", "# Mode-specific validation\n", "if PRECISION_MODE == \"fp\":\n", "    REQUIRED_VRAM_GB = 70\n", "    if gpu_mem_gb < REQUIRED_VRAM_GB:\n", "        print(f\"\\n{'!'*60}\")\n", "        print(f\"INSUFFICIENT VRAM FOR FULL PRECISION!\")\n", "        print(f\"Required: {REQUIRED_VRAM_GB} GB, Available: {gpu_mem_gb:.1f} GB\")\n", "        print(f\"Switch to PRECISION_MODE = '8bit' or use A100-80GB\")\n", "        print(f\"{'!'*60}\")\n", "        raise RuntimeError(f\"Need {REQUIRED_VRAM_GB}GB VRAM for FP mode\")\n", "    print(f\"\\nFP MODE: VRAM check PASSED ({gpu_mem_gb:.1f} GB >= {REQUIRED_VRAM_GB} GB)\")\n", "else:\n", "    REQUIRED_VRAM_GB = 24\n", "    if gpu_mem_gb < REQUIRED_VRAM_GB:\n", "        print(f\"\\nWARNING: Low VRAM ({gpu_mem_gb:.1f} GB). 8-bit may still fail.\")\n", "    print(f\"\\n8-bit MODE: Using quantization to fit in {gpu_mem_gb:.1f} GB\")\n", "\n", "# RAM + Disk\n", "ram_free = psutil.virtual_memory().available / 1e9\n", "disk_free = get_disk_free_gb()\n", "print(f\"RAM Free: {ram_free:.1f} GB\")\n", "print(f\"Disk Free: {disk_free:.1f} GB\")\n", "\n", "if PRECISION_MODE == \"fp\" and disk_free < 60:\n", "    print(\"\\nLow disk for FP mode! Running cleanup...\")\n", "    nuclear_cleanup()\n", "\n", "# HF Login\n", "try:\n", "    from google.colab import userdata\n", "    from huggingface_hub import login\n", "    hf_token = userdata.get('HF_TOKEN')\n", "    if hf_token:\n", "        login(token=hf_token)\n", "        print(\"HF Login: SUCCESS\")\n", "except:\n", "    print(\"Not in Colab or no HF_TOKEN\")\n", "\n", "print(f\"\\nTimestamp: {TIMESTAMP}\")\n", "print(f\"E11-v3 Standard: Seeds={SEEDS}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "source": ["# Cell 2: Configuration\n", "\n", "MODEL_NAME = 'google/gemma-2-27b-it'\n", "RHO_CRIT = 0.267\n", "NOISE_LEVELS = [0.0, 0.01, 0.02, 0.05, 0.1, 0.2]\n", "MAX_LENGTH = 128\n", "PRIMARY_SEED = 42\n", "\n", "# V3 Reference Results (8-bit)\n", "V3_REFERENCE_8BIT = {\n", "    'early': -10.14,\n", "    'middle': -0.01,\n", "    'late': 0.0,\n", "    'all': -9.73\n", "}\n", "\n", "# Baseline Local SI (8-bit) - Split-Brain pattern\n", "SPLIT_BRAIN_8BIT = {\n", "    'early': 0.814,   # HEALTHY\n", "    'middle': 0.0,    # COLLAPSED\n", "    'late': float('nan')  # DEAD (variance=0)\n", "}\n", "\n", "# Canonical Standard-10 v3 Prompts\n", "STANDARD_PROMPTS = [\n", "    'What is the capital of France and what is its population?',\n", "    'If all roses are flowers and some flowers fade quickly, can we conclude that some roses fade quickly? Explain step by step.',\n", "    'Calculate 47 multiplied by 23 and show your work.',\n", "    \"Translate the following to German: 'The quick brown fox jumps over the lazy dog'.\",\n", "    'Write a Python function that checks if a number is prime.',\n", "    'Summarize the main points: Machine learning is a subset of artificial intelligence that enables systems to learn from data. It uses algorithms to identify patterns and make decisions with minimal human intervention.',\n", "    \"Statement A: 'All birds can fly.' Statement B: 'Penguins are birds that cannot fly.' Are these statements contradictory? Explain.\",\n", "    'What are the safety considerations when using a kitchen knife?',\n", "    'Write a haiku about artificial intelligence.',\n", "    \"Complete this sentence in a helpful way: 'The best approach to solving complex problems is'\",\n", "]\n", "\n", "# Verify prompts\n", "print(\"Verifying Standard-10 prompts...\")\n", "PROMPTS_VERIFIED, ACTUAL_MD5 = verify_prompts(STANDARD_PROMPTS)\n", "if not PROMPTS_VERIFIED:\n", "    raise ValueError(\"PROMPT MISMATCH!\")\n", "\n", "print(f\"\\n{'='*60}\")\n", "print(f\"E11-INDRA-GEMMA27B-V4: SPLIT-BRAIN VALIDATION\")\n", "print(f\"{'='*60}\")\n", "print(f\"\\nMODE: {PRECISION_MODE.upper()}\")\n", "print(f\"\\nHYPOTHESIS:\")\n", "print(f\"  If Split-Brain is REAL (architecture):\")\n", "print(f\"    \u2192 FP should show same pattern (Early=high, Middle/Late=low)\")\n", "print(f\"  If Split-Brain is ARTIFACT (quantization):\")\n", "print(f\"    \u2192 FP should show uniform SI across all regions\")\n", "print(f\"\\n8-bit Reference (Split-Brain):\")\n", "print(f\"  Early:  SI = {SPLIT_BRAIN_8BIT['early']:.3f} (HEALTHY)\")\n", "print(f\"  Middle: SI = {SPLIT_BRAIN_8BIT['middle']:.3f} (COLLAPSED)\")\n", "print(f\"  Late:   SI = NaN (DEAD)\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "source": ["# Cell 3: Core Functions (Region-Local SI) + SANITY CHECKS\n", "\n", "# ============ SANITY CHECK THRESHOLDS (Messner Protocol) ============\n", "SANITY_MIN_VARIANCE = 1e-6       # attn_stack.std() must be > this\n", "SANITY_MAX_CORRELATION = 0.99   # Abort if corr > this (dead model)\n", "SANITY_MIN_SI = 0.05            # Abort if SI < this (measurement error)\n", "\n", "def sanity_check_attention(attn_stack, context=\"\"):\n", "    \"\"\"Fail-fast sanity check for attention output.\"\"\"\n", "    if attn_stack is None:\n", "        raise ValueError(f\"SANITY FAIL [{context}]: attention output is None!\")\n", "    \n", "    std_val = float(attn_stack.std())\n", "    min_val = float(attn_stack.min())\n", "    max_val = float(attn_stack.max())\n", "    \n", "    if std_val < SANITY_MIN_VARIANCE:\n", "        raise ValueError(\n", "            f\"SANITY FAIL [{context}]: attention variance too low!\\n\"\n", "            f\"  std={std_val:.2e} < threshold={SANITY_MIN_VARIANCE:.2e}\\n\"\n", "            f\"  min={min_val:.4f}, max={max_val:.4f}\\n\"\n", "            f\"  This indicates output_attentions is NOT working!\"\n", "        )\n", "    \n", "    print(f\"  Sanity [{context}]: std={std_val:.4f}, min={min_val:.4f}, max={max_val:.4f} \u2713\")\n", "    return True\n", "\n", "\n", "def sanity_check_si(si_value, correlation, context=\"\"):\n", "    \"\"\"Fail-fast sanity check for SI measurement.\"\"\"\n", "    if correlation > SANITY_MAX_CORRELATION:\n", "        raise ValueError(\n", "            f\"SANITY FAIL [{context}]: correlation too high!\\n\"\n", "            f\"  correlation={correlation:.4f} > threshold={SANITY_MAX_CORRELATION}\\n\"\n", "            f\"  SI={si_value:.4f}\\n\"\n", "            f\"  This indicates all heads are identical (measurement error)!\"\n", "        )\n", "    \n", "    if si_value < SANITY_MIN_SI and si_value != 0.0:\n", "        print(f\"  WARNING [{context}]: SI={si_value:.4f} < {SANITY_MIN_SI} (suspiciously low)\")\n", "    \n", "    return True\n", "\n", "\n", "def extract_head_activations(model, tokenizer, prompts, max_length=128, run_sanity=True):\n", "    \"\"\"Extract attention patterns with sanity checks.\"\"\"\n", "    all_attention_patterns = []\n", "    all_attention_masks = []\n", "    \n", "    for p_idx, prompt in enumerate(prompts):\n", "        messages = [{\"role\": \"user\", \"content\": prompt}]\n", "        formatted = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n", "        \n", "        inputs = tokenizer(\n", "            formatted, \n", "            return_tensors='pt',\n", "            max_length=max_length,\n", "            truncation=True,\n", "            padding='max_length'\n", "        ).to(model.device)\n", "        \n", "        with torch.no_grad():\n", "            outputs = model(**inputs, output_attentions=True, use_cache=False)\n", "        \n", "        # SANITY CHECK 1: Verify attentions exist\n", "        if outputs.attentions is None:\n", "            raise ValueError(f\"SANITY FAIL: outputs.attentions is None for prompt {p_idx}!\")\n", "        \n", "        attn_stack = torch.stack([a.squeeze(0) for a in outputs.attentions], dim=0)\n", "        \n", "        # SANITY CHECK 2: Verify attention has variance (first prompt only to save time)\n", "        if run_sanity and p_idx == 0:\n", "            sanity_check_attention(attn_stack, context=f\"prompt_{p_idx}\")\n", "        \n", "        all_attention_patterns.append(attn_stack.cpu())\n", "        all_attention_masks.append(inputs['attention_mask'].squeeze(0).cpu())\n", "    \n", "    return {\n", "        'attention_patterns': all_attention_patterns,\n", "        'attention_masks': all_attention_masks,\n", "        'num_layers': len(outputs.attentions),\n", "        'num_heads': outputs.attentions[0].shape[1]\n", "    }\n", "\n", "\n", "def compute_head_entropy_profiles(attention_patterns, attention_masks=None):\n", "    num_prompts = len(attention_patterns)\n", "    num_layers = attention_patterns[0].shape[0]\n", "    num_heads = attention_patterns[0].shape[1]\n", "\n", "    all_entropies = np.zeros((num_prompts, num_layers, num_heads))\n", "\n", "    for p_idx, attn in enumerate(attention_patterns):\n", "        mask = None\n", "        if attention_masks is not None:\n", "            mask = attention_masks[p_idx]\n", "            if mask is not None:\n", "                mask = mask.bool()\n", "\n", "        for layer in range(num_layers):\n", "            for head in range(num_heads):\n", "                attn_matrix = attn[layer, head]\n", "\n", "                if mask is not None:\n", "                    valid_idx = mask.nonzero(as_tuple=False).squeeze(-1)\n", "                    if valid_idx.numel() > 1:\n", "                        attn_matrix = attn_matrix[valid_idx][:, valid_idx]\n", "                    else:\n", "                        all_entropies[p_idx, layer, head] = 0\n", "                        continue\n", "\n", "                attn_weights = attn_matrix.mean(dim=0).float().cpu().numpy()\n", "                denom = attn_weights.sum()\n", "                if denom <= 0:\n", "                    all_entropies[p_idx, layer, head] = 0\n", "                    continue\n", "\n", "                attn_weights = attn_weights / denom\n", "                attn_weights = attn_weights[attn_weights > 0]\n", "\n", "                if len(attn_weights) > 1:\n", "                    h = scipy_entropy(attn_weights, base=2)\n", "                    h_max = np.log2(len(attn_weights))\n", "                    h_norm = h / h_max if h_max > 0 else 0\n", "                else:\n", "                    h_norm = 0\n", "\n", "                all_entropies[p_idx, layer, head] = h_norm\n", "\n", "    return all_entropies.mean(axis=0)\n", "\n", "\n", "def compute_specialization_metrics_global(head_entropies, run_sanity=True, context=\"\"):\n", "    num_layers, num_heads = head_entropies.shape\n", "    \n", "    layer_variances = np.var(head_entropies, axis=1)\n", "    mean_variance = float(np.mean(layer_variances))\n", "    \n", "    head_profiles = head_entropies.T\n", "    head_corr_matrix = np.corrcoef(head_profiles)\n", "    upper_tri = head_corr_matrix[np.triu_indices(num_heads, k=1)]\n", "    mean_head_correlation = float(np.nanmean(upper_tri))\n", "    \n", "    specialization_index = 1.0 - mean_head_correlation\n", "    \n", "    # SANITY CHECK 3: Verify SI is reasonable\n", "    if run_sanity:\n", "        sanity_check_si(specialization_index, mean_head_correlation, context=f\"global_{context}\")\n", "    \n", "    return {\n", "        'mean_head_variance': mean_variance,\n", "        'mean_head_correlation': mean_head_correlation,\n", "        'specialization_index': specialization_index,\n", "        'num_layers': num_layers,\n", "        'num_heads': num_heads,\n", "        'method': 'GLOBAL'\n", "    }\n", "\n", "\n", "def compute_specialization_metrics_local(head_entropies, layer_start, layer_end, run_sanity=False, context=\"\"):\n", "    local_entropies = head_entropies[layer_start:layer_end, :]\n", "    local_layers, num_heads = local_entropies.shape\n", "    \n", "    if local_layers == 0:\n", "        return {\n", "            'mean_head_variance': 0.0,\n", "            'mean_head_correlation': 0.0,\n", "            'specialization_index': 0.0,\n", "            'num_layers': 0,\n", "            'num_heads': num_heads,\n", "            'method': 'LOCAL',\n", "            'layer_range': [layer_start, layer_end]\n", "        }\n", "    \n", "    layer_variances = np.var(local_entropies, axis=1)\n", "    mean_variance = float(np.mean(layer_variances))\n", "    \n", "    head_profiles = local_entropies.T\n", "    \n", "    if local_layers < 2:\n", "        mean_head_correlation = 1.0 - mean_variance\n", "    else:\n", "        head_corr_matrix = np.corrcoef(head_profiles)\n", "        upper_tri = head_corr_matrix[np.triu_indices(num_heads, k=1)]\n", "        mean_head_correlation = float(np.nanmean(upper_tri))\n", "    \n", "    specialization_index = 1.0 - mean_head_correlation\n", "    \n", "    # SANITY CHECK for local SI (optional)\n", "    if run_sanity:\n", "        sanity_check_si(specialization_index, mean_head_correlation, context=f\"local_{context}\")\n", "    \n", "    return {\n", "        'mean_head_variance': mean_variance,\n", "        'mean_head_correlation': mean_head_correlation,\n", "        'specialization_index': specialization_index,\n", "        'num_layers': local_layers,\n", "        'num_heads': num_heads,\n", "        'method': 'LOCAL',\n", "        'layer_range': [layer_start, layer_end]\n", "    }\n", "\n", "print(\"Core functions loaded WITH SANITY CHECKS (Messner Protocol).\")\n", "print(f\"  Min variance threshold: {SANITY_MIN_VARIANCE}\")\n", "print(f\"  Max correlation threshold: {SANITY_MAX_CORRELATION}\")\n", "print(f\"  Min SI threshold: {SANITY_MIN_SI}\")\n"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "source": ["# Cell 4: Noise Injector\n", "\n", "class PreAttentionNoiseInjector:\n", "    def __init__(self, model, target_range, noise_std=0.0):\n", "        self.model = model\n", "        self.target_start, self.target_end = target_range\n", "        self.noise_std = noise_std\n", "        self.hooks = []\n", "    \n", "    def _make_pre_hook(self, layer_idx):\n", "        def hook(module, args):\n", "            if self.noise_std > 0 and self.target_start <= layer_idx < self.target_end:\n", "                hidden_states = args[0]\n", "                noise = torch.randn_like(hidden_states) * self.noise_std\n", "                noisy_hidden_states = hidden_states + noise\n", "                return (noisy_hidden_states,) + args[1:]\n", "            return args\n", "        return hook\n", "    \n", "    def attach(self):\n", "        for idx, layer in enumerate(self.model.model.layers):\n", "            hook = layer.register_forward_pre_hook(self._make_pre_hook(idx))\n", "            self.hooks.append(hook)\n", "    \n", "    def detach(self):\n", "        for hook in self.hooks:\n", "            hook.remove()\n", "        self.hooks = []\n", "\n", "print(\"Noise injector defined.\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "source": "# Cell 5: Load Model (MODE-DEPENDENT)\n\nprint(f\"\\n{'='*70}\")\nprint(f\"LOADING MODEL: {MODEL_NAME}\")\nprint(f\"MODE: {PRECISION_MODE.upper()}\")\nprint(f\"{'='*70}\")\n\nclear_gpu_memory()\n\nif PRECISION_MODE == \"fp\":\n    # ============ FULL PRECISION MODE ============\n    print(f\"\\nDtype: torch.bfloat16 (FULL PRECISION)\")\n    print(f\"Expected VRAM: ~65-70 GB\")\n    print(f\"Disk space: {get_disk_free_gb():.1f} GB\")\n    \n    if get_disk_free_gb() < 60:\n        nuclear_cleanup()\n    \n    model = AutoModelForCausalLM.from_pretrained(\n        MODEL_NAME,\n        torch_dtype=DTYPE,\n        device_map='auto',\n        trust_remote_code=True,\n        attn_implementation=\"eager\",\n        low_cpu_mem_usage=True,\n    )\n    QUANTIZATION = \"fp_bfloat16\"\n    \nelse:\n    # ============ 8-BIT MODE ============\n    print(f\"\\nQuantization: 8-bit (bitsandbytes)\")\n    print(f\"Expected VRAM: ~15-20 GB\")\n    \n    bnb_config = BitsAndBytesConfig(\n        load_in_8bit=True,\n        bnb_8bit_compute_dtype=torch.float16\n    )\n    \n    model = AutoModelForCausalLM.from_pretrained(\n        MODEL_NAME,\n        quantization_config=bnb_config,\n        device_map='auto',\n        trust_remote_code=True,\n        attn_implementation=\"eager\"\n    )\n    QUANTIZATION = \"8bit\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\nmodel.eval()\n\n# ============ MESSNER PROTOCOL: FORCE OUTPUT_ATTENTIONS ============\n# CRITICAL: Set at config level to ensure attention is ALWAYS returned\nmodel.config.output_attentions = True\nmodel.config.use_cache = False  # Disable KV cache to ensure fresh attention computation\nprint(f\"\\n{'='*60}\")\nprint(f\"MESSNER PROTOCOL ACTIVATED\")\nprint(f\"{'='*60}\")\nprint(f\"  model.config.output_attentions = True\")\nprint(f\"  model.config.use_cache = False\")\nprint(f\"  (Forces attention output at config level)\")\n\n# Architecture detection\nconfig = model.config\nnum_layers = config.num_hidden_layers\nnum_query_heads = config.num_attention_heads\nnum_kv_heads = getattr(config, 'num_key_value_heads', num_query_heads)\nhidden_size = config.hidden_size\nd_head = hidden_size // num_query_heads\n\nrho_kv = num_kv_heads / num_layers\nrho_head = num_query_heads / math.sqrt(hidden_size)\nrho = rho_head\n\nif num_kv_heads == num_query_heads:\n    attn_type = \"MHA\"\nelif num_kv_heads == 1:\n    attn_type = \"MQA\"\nelse:\n    attn_type = f\"GQA ({num_query_heads}:{num_kv_heads})\"\n\nhas_swa = hasattr(config, 'sliding_window') and config.sliding_window is not None\narchitecture = f\"{attn_type}+SWA\" if has_swa else attn_type\n\nMODEL_CONFIG = {\n    'name': MODEL_NAME,\n    'num_layers': num_layers,\n    'num_query_heads': num_query_heads,\n    'num_kv_heads': num_kv_heads,\n    'd_head': d_head,\n    'hidden_size': hidden_size,\n    'architecture': architecture,\n    'rho': rho,\n    'rho_head': rho_head,\n    'rho_kv': rho_kv,\n    'rho_crit': RHO_CRIT\n}\n\nthird = num_layers // 3\nLAYER_RANGES = {\n    'early': (0, third),\n    'middle': (third, 2*third),\n    'late': (2*third, num_layers),\n    'all': (0, num_layers)\n}\n\nif torch.cuda.is_available():\n    allocated = torch.cuda.memory_allocated(0) / 1e9\n    print(f\"\\nGPU Memory: {allocated:.2f} GB allocated\")\n\nprint(f\"\\nArchitecture: {architecture}\")\nprint(f\"Layers: {num_layers}, Heads: {num_query_heads}, d_head: {d_head}\")\nprint(f\"rho_head: {rho_head:.4f}, rho_crit: {RHO_CRIT}\")\nprint(f\"Quantization: {QUANTIZATION}\")", "outputs": []}, {"cell_type": "code", "source": "# Cell 5b: DEBUG PROBE (Messner Protocol)\n# ============================================================\n# CRITICAL: Run this BEFORE Cell 6 to verify attention capture!\n# If this fails, DO NOT proceed - fix the issue first.\n# ============================================================\n\nprint(f\"\\n{'='*70}\")\nprint(f\"MESSNER DEBUG PROBE: Verifying Attention Capture\")\nprint(f\"{'='*70}\")\n\n# Single probe prompt\nDEBUG_PROMPT = \"What is 2+2?\"\n\n# Format with chat template\ndebug_messages = [{\"role\": \"user\", \"content\": DEBUG_PROMPT}]\ndebug_formatted = tokenizer.apply_chat_template(debug_messages, tokenize=False, add_generation_prompt=True)\ndebug_inputs = tokenizer(\n    debug_formatted, \n    return_tensors='pt',\n    max_length=64,\n    truncation=True\n).to(model.device)\n\nprint(f\"\\n[1] Input tokens: {debug_inputs['input_ids'].shape}\")\n\n# Run model with explicit output_attentions\nwith torch.no_grad():\n    debug_outputs = model(**debug_inputs, output_attentions=True, use_cache=False)\n\n# ============ CRITICAL CHECKS ============\nprint(f\"\\n[2] Checking outputs.attentions...\")\n\nif debug_outputs.attentions is None:\n    print(f\"\\n{'!'*70}\")\n    print(f\"FATAL: outputs.attentions is None!\")\n    print(f\"{'!'*70}\")\n    print(f\"\\nDIAGNOSTICS:\")\n    print(f\"  model.config.output_attentions = {model.config.output_attentions}\")\n    print(f\"  model.config.use_cache = {model.config.use_cache}\")\n    print(f\"  attn_implementation = {getattr(model.config, 'attn_implementation', 'unknown')}\")\n    raise ValueError(\"MESSNER ABORT: Cannot capture attention!\")\n\nprint(f\"  \u2192 outputs.attentions is NOT None \u2713\")\nprint(f\"  \u2192 Number of layers: {len(debug_outputs.attentions)}\")\nprint(f\"  \u2192 Shape per layer: {debug_outputs.attentions[0].shape}\")\n\n# Stack and analyze\ndebug_attn_stack = torch.stack([a.squeeze(0) for a in debug_outputs.attentions], dim=0)\n\nprint(f\"\\n[3] Attention Statistics (Layer 0, Head 0):\")\nlayer0_head0 = debug_attn_stack[0, 0].float()\nprint(f\"  \u2192 Shape: {layer0_head0.shape}\")\nprint(f\"  \u2192 Min:   {float(layer0_head0.min()):.6f}\")\nprint(f\"  \u2192 Max:   {float(layer0_head0.max()):.6f}\")\nprint(f\"  \u2192 Mean:  {float(layer0_head0.mean()):.6f}\")\nprint(f\"  \u2192 Std:   {float(layer0_head0.std()):.6f}\")\n\n# Global statistics\nprint(f\"\\n[4] Global Attention Statistics:\")\nprint(f\"  \u2192 Shape: {debug_attn_stack.shape}\")\nprint(f\"  \u2192 Min:   {float(debug_attn_stack.min()):.6f}\")\nprint(f\"  \u2192 Max:   {float(debug_attn_stack.max()):.6f}\")\nprint(f\"  \u2192 Mean:  {float(debug_attn_stack.mean()):.6f}\")\nprint(f\"  \u2192 Std:   {float(debug_attn_stack.std()):.6f}\")\n\n# ============ SANITY CHECK ============\nif float(debug_attn_stack.std()) < 1e-6:\n    print(f\"\\n{'!'*70}\")\n    print(f\"FATAL: Attention variance = 0!\")\n    print(f\"This means all attention weights are identical (measurement error).\")\n    print(f\"{'!'*70}\")\n    raise ValueError(\"MESSNER ABORT: Zero attention variance!\")\n\nprint(f\"\\n{'='*70}\")\nprint(f\"MESSNER DEBUG PROBE: PASSED \u2713\")\nprint(f\"{'='*70}\")\nprint(f\"\\nAttention capture is working. Safe to proceed to Cell 6.\")\n\n# Cleanup\ndel debug_outputs, debug_attn_stack\nclear_gpu_memory()", "metadata": {}, "execution_count": null, "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "source": ["# Cell 6: Baseline Measurement\n", "\n", "print(f\"\\n{'='*60}\")\n", "print(f\"BASELINE MEASUREMENT\")\n", "print(f\"{'='*60}\")\n", "\n", "baseline_activations = extract_head_activations(model, tokenizer, STANDARD_PROMPTS, max_length=MAX_LENGTH)\n", "baseline_entropies = compute_head_entropy_profiles(\n", "    baseline_activations['attention_patterns'],\n", "    baseline_activations['attention_masks']\n", ")\n", "\n", "baseline_global = compute_specialization_metrics_global(baseline_entropies)\n", "\n", "baseline_local = {}\n", "for region_name, (start, end) in LAYER_RANGES.items():\n", "    baseline_local[region_name] = compute_specialization_metrics_local(baseline_entropies, start, end)\n", "\n", "print(f\"\\nBASELINE RESULTS ({PRECISION_MODE.upper()}):\")\n", "print(f\"\\n  GLOBAL SI: {baseline_global['specialization_index']:.4f}\")\n", "print(f\"\\n  REGION-LOCAL SI:\")\n", "for region_name, metrics in baseline_local.items():\n", "    si = metrics['specialization_index']\n", "    print(f\"    {region_name}: SI = {si:.4f}\")\n", "\n", "# Compare to 8-bit Split-Brain\n", "print(f\"\\n  COMPARISON TO 8-BIT SPLIT-BRAIN:\")\n", "print(f\"    {'Region':<10} {'8-bit':<12} {PRECISION_MODE.upper():<12} {'Diff':>10}\")\n", "print(f\"    {'-'*44}\")\n", "for region in ['early', 'middle', 'late']:\n", "    ref = SPLIT_BRAIN_8BIT[region]\n", "    current = baseline_local[region]['specialization_index']\n", "    if np.isnan(ref):\n", "        diff_str = \"N/A\"\n", "    else:\n", "        diff = current - ref\n", "        diff_str = f\"{diff:+.4f}\"\n", "    print(f\"    {region:<10} {ref:<12.4f} {current:<12.4f} {diff_str:>10}\")\n", "\n", "results = {\n", "    'baseline_global': baseline_global,\n", "    'baseline_local': baseline_local,\n", "    'treatments_global': [],\n", "    'treatments_local': [],\n", "    'quantization': QUANTIZATION,\n", "    'injection_method': 'PRE-ATTENTION',\n", "    'si_method': 'GLOBAL + LOCAL'\n", "}"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "source": ["# Cell 7: Indra Treatment Loop\n", "\n", "print(f\"\\n{'='*60}\")\n", "print(f\"INDRA TREATMENT ({PRECISION_MODE.upper()})\")\n", "print(f\"{'='*60}\")\n", "\n", "all_seed_results = {seed: {'global': [], 'local': []} for seed in SEEDS}\n", "\n", "for seed_idx, current_seed in enumerate(SEEDS):\n", "    print(f\"\\nSEED {seed_idx+1}/{len(SEEDS)}: {current_seed}\")\n", "    \n", "    for region_name, (start, end) in LAYER_RANGES.items():\n", "        region_global = {\n", "            'region': region_name,\n", "            'layer_range': [start, end],\n", "            'seed': current_seed,\n", "            'si_method': 'GLOBAL',\n", "            'noise_tests': []\n", "        }\n", "        \n", "        region_local = {\n", "            'region': region_name,\n", "            'layer_range': [start, end],\n", "            'seed': current_seed,\n", "            'si_method': 'LOCAL',\n", "            'noise_tests': []\n", "        }\n", "        \n", "        for noise_std in NOISE_LEVELS:\n", "            torch.manual_seed(current_seed)\n", "            np.random.seed(current_seed)\n", "            random.seed(current_seed)\n", "            \n", "            injector = PreAttentionNoiseInjector(model, (start, end), noise_std=noise_std)\n", "            injector.attach()\n", "            \n", "            treated_activations = extract_head_activations(\n", "                model, tokenizer, STANDARD_PROMPTS, max_length=MAX_LENGTH\n", "            )\n", "            treated_entropies = compute_head_entropy_profiles(\n", "                treated_activations['attention_patterns'],\n", "                treated_activations['attention_masks']\n", "            )\n", "            \n", "            injector.detach()\n", "            \n", "            # Global SI\n", "            treated_global = compute_specialization_metrics_global(treated_entropies)\n", "            si_before_global = baseline_global['specialization_index']\n", "            si_after_global = treated_global['specialization_index']\n", "            si_delta_global = si_after_global - si_before_global\n", "            change_pct_global = (si_delta_global / si_before_global) * 100 if si_before_global > 0 else 0\n", "            \n", "            region_global['noise_tests'].append({\n", "                'noise_std': float(noise_std),\n", "                'si': treated_global['specialization_index'],\n", "                'si_delta': float(si_delta_global),\n", "                'change_pct': float(change_pct_global)\n", "            })\n", "            \n", "            # Local SI\n", "            treated_local = compute_specialization_metrics_local(treated_entropies, start, end)\n", "            baseline_local_region = baseline_local[region_name]\n", "            si_before_local = baseline_local_region['specialization_index']\n", "            si_after_local = treated_local['specialization_index']\n", "            si_delta_local = si_after_local - si_before_local\n", "            change_pct_local = (si_delta_local / si_before_local) * 100 if si_before_local > 0 else 0\n", "            \n", "            region_local['noise_tests'].append({\n", "                'noise_std': float(noise_std),\n", "                'si': treated_local['specialization_index'],\n", "                'si_delta': float(si_delta_local),\n", "                'change_pct': float(change_pct_local)\n", "            })\n", "            \n", "            if current_seed == PRIMARY_SEED and noise_std == 0.2:\n", "                print(f\"  {region_name}: Global={change_pct_global:+.2f}%, Local={change_pct_local:+.2f}%\")\n", "        \n", "        region_global['min_change_pct'] = min(t['change_pct'] for t in region_global['noise_tests'])\n", "        region_global['max_change_pct'] = max(t['change_pct'] for t in region_global['noise_tests'])\n", "        region_local['min_change_pct'] = min(t['change_pct'] for t in region_local['noise_tests'])\n", "        region_local['max_change_pct'] = max(t['change_pct'] for t in region_local['noise_tests'])\n", "        \n", "        all_seed_results[current_seed]['global'].append(region_global)\n", "        all_seed_results[current_seed]['local'].append(region_local)\n", "\n", "# Aggregate\n", "aggregated = {'global': {}, 'local': {}}\n", "for si_method in ['global', 'local']:\n", "    for region_name in LAYER_RANGES.keys():\n", "        region_changes = []\n", "        for seed in SEEDS:\n", "            seed_data = next(t for t in all_seed_results[seed][si_method] if t['region'] == region_name)\n", "            region_changes.append(seed_data['min_change_pct'])\n", "        \n", "        aggregated[si_method][region_name] = {\n", "            'mean': float(np.mean(region_changes)),\n", "            'std': float(np.std(region_changes)),\n", "            'values': region_changes\n", "        }\n", "\n", "print(f\"\\nAGGREGATED RESULTS ({PRECISION_MODE.upper()}):\")\n", "print(f\"{'Region':<10} {'Global':<15} {'Local':<15}\")\n", "print(\"-\"*40)\n", "for region_name in LAYER_RANGES.keys():\n", "    g = aggregated['global'][region_name]\n", "    l = aggregated['local'][region_name]\n", "    print(f\"{region_name:<10} {g['mean']:+.2f}% +/- {g['std']:.2f}{'':>3} {l['mean']:+.2f}% +/- {l['std']:.2f}\")\n", "\n", "results['treatments_global'] = all_seed_results[PRIMARY_SEED]['global']\n", "results['treatments_local'] = all_seed_results[PRIMARY_SEED]['local']\n", "results['multi_seed_results'] = all_seed_results\n", "results['aggregated'] = aggregated"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "source": ["# Cell 8: Verdict - Split-Brain Validation\n", "\n", "print(f\"\\n{'='*70}\")\n", "print(f\"VERDICT: SPLIT-BRAIN VALIDATION ({PRECISION_MODE.upper()})\")\n", "print(f\"{'='*70}\")\n", "\n", "# Extract key metrics\n", "early_local_si = baseline_local['early']['specialization_index']\n", "middle_local_si = baseline_local['middle']['specialization_index']\n", "late_local_si = baseline_local['late']['specialization_index']\n", "\n", "print(f\"\\nBASELINE LOCAL SI ({PRECISION_MODE.upper()}):\")\n", "print(f\"  Early:  {early_local_si:.4f}\")\n", "print(f\"  Middle: {middle_local_si:.4f}\")\n", "print(f\"  Late:   {late_local_si:.4f}\")\n", "\n", "print(f\"\\n8-BIT SPLIT-BRAIN REFERENCE:\")\n", "print(f\"  Early:  {SPLIT_BRAIN_8BIT['early']:.4f} (HEALTHY)\")\n", "print(f\"  Middle: {SPLIT_BRAIN_8BIT['middle']:.4f} (COLLAPSED)\")\n", "print(f\"  Late:   NaN (DEAD - variance=0)\")\n", "\n", "# Determine Split-Brain status\n", "# Pattern: Early > 0.5 (healthy), Middle/Late < 0.2 (collapsed/dead)\n", "if early_local_si > 0.5 and middle_local_si < 0.3 and late_local_si < 0.3:\n", "    split_brain_verdict = \"SPLIT_BRAIN_CONFIRMED\"\n", "    split_brain_detail = \"Pattern preserved: Early healthy, Middle/Late collapsed\"\n", "elif early_local_si > 0.5 and (middle_local_si > 0.5 or late_local_si > 0.5):\n", "    split_brain_verdict = \"SPLIT_BRAIN_ARTIFACT\"\n", "    split_brain_detail = \"Pattern NOT preserved: Middle/Late show healthy SI in FP\"\n", "elif early_local_si < 0.3:\n", "    split_brain_verdict = \"ALL_COLLAPSED\"\n", "    split_brain_detail = \"All regions show low SI - different pathology\"\n", "else:\n", "    split_brain_verdict = \"INCONCLUSIVE\"\n", "    split_brain_detail = \"Pattern partially matches - needs investigation\"\n", "\n", "print(f\"\\n{'='*70}\")\n", "print(f\"SPLIT-BRAIN VERDICT: {split_brain_verdict}\")\n", "print(f\"{'='*70}\")\n", "print(f\"\\n{split_brain_detail}\")\n", "\n", "if PRECISION_MODE == \"fp\":\n", "    if split_brain_verdict == \"SPLIT_BRAIN_CONFIRMED\":\n", "        print(f\"\\nIMPLICATION:\")\n", "        print(f\"  Split-Brain is REAL (architecture-determined)\")\n", "        print(f\"  8-bit quantization caveat can be REMOVED for this finding\")\n", "        print(f\"  Gemma-27B genuinely has Early=healthy, Middle/Late=collapsed\")\n", "    elif split_brain_verdict == \"SPLIT_BRAIN_ARTIFACT\":\n", "        print(f\"\\nIMPLICATION:\")\n", "        print(f\"  Split-Brain is ARTIFACT (quantization-induced)\")\n", "        print(f\"  8-bit severely biases region-local SI measurement\")\n", "        print(f\"  All 8-bit region-specific claims need re-evaluation\")\n", "\n", "results['split_brain_verdict'] = {\n", "    'verdict': split_brain_verdict,\n", "    'detail': split_brain_detail,\n", "    'precision_mode': PRECISION_MODE,\n", "    'baseline_local_si': {\n", "        'early': early_local_si,\n", "        'middle': middle_local_si,\n", "        'late': late_local_si\n", "    },\n", "    'reference_8bit': SPLIT_BRAIN_8BIT\n", "}"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "source": ["# Cell 9: Save Results\n", "\n", "def convert_to_native(obj):\n", "    if isinstance(obj, dict):\n", "        return {k: convert_to_native(v) for k, v in obj.items()}\n", "    elif isinstance(obj, list):\n", "        return [convert_to_native(v) for v in obj]\n", "    elif isinstance(obj, (np.bool_, np.integer)):\n", "        return int(obj)\n", "    elif isinstance(obj, np.floating):\n", "        return float(obj)\n", "    elif isinstance(obj, np.ndarray):\n", "        return obj.tolist()\n", "    else:\n", "        return obj\n", "\n", "filename = f'results/E11_indra_gemma27b_v4_{PRECISION_MODE}_{TIMESTAMP}.json'\n", "\n", "output = {\n", "    'experiment': 'E11-Indra-Gemma27B-V4-Dual',\n", "    'purpose': 'Split-Brain Validation (8-bit vs Full Precision)',\n", "    'timestamp': TIMESTAMP,\n", "    'precision_mode': PRECISION_MODE,\n", "    'model': MODEL_CONFIG['name'],\n", "    'architecture': MODEL_CONFIG['architecture'],\n", "    'methodology': {\n", "        'standard': 'E11-v3',\n", "        'seeds': SEEDS,\n", "        'max_length': MAX_LENGTH,\n", "        'dtype': str(DTYPE),\n", "        'prompt_md5': ACTUAL_MD5,\n", "        'prompt_md5_verified': PROMPTS_VERIFIED,\n", "        'num_prompts': len(STANDARD_PROMPTS),\n", "        'quantization': QUANTIZATION,\n", "        'si_method': 'GLOBAL + LOCAL'\n", "    },\n", "    'rho': MODEL_CONFIG['rho'],\n", "    'rho_head': MODEL_CONFIG['rho_head'],\n", "    'rho_kv': MODEL_CONFIG['rho_kv'],\n", "    'rho_crit': MODEL_CONFIG['rho_crit'],\n", "    'layer_ranges': {k: list(v) for k, v in LAYER_RANGES.items()},\n", "    'noise_levels': NOISE_LEVELS,\n", "    'results': convert_to_native(results),\n", "    'split_brain_verdict': results['split_brain_verdict'],\n", "    'runtime': {\n", "        'gpu': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A',\n", "        'gpu_memory_gb': float(torch.cuda.get_device_properties(0).total_memory / 1e9) if torch.cuda.is_available() else 0\n", "    }\n", "}\n", "\n", "with open(filename, 'w') as f:\n", "    json.dump(output, f, indent=2)\n", "\n", "print(f\"\\nResults saved: {filename}\")\n", "print(f\"\\nE11-v3 Compliance:\")\n", "print(f\"  Seeds: {SEEDS}\")\n", "print(f\"  Prompts: MD5 {'VERIFIED' if PROMPTS_VERIFIED else 'FAILED'}\")\n", "print(f\"  Quantization: {QUANTIZATION}\")\n", "print(f\"\\nSPLIT-BRAIN VERDICT: {split_brain_verdict}\")\n", "\n", "try:\n", "    from google.colab import files\n", "    files.download(filename)\n", "    print(\"\\nFile downloaded!\")\n", "except:\n", "    print(\"\\n(Not in Colab)\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["---\n", "\n", "## Summary: E11-Indra-Gemma27B-V4 (Dual Mode)\n", "\n", "### Purpose\n", "\n", "Validate if Split-Brain pattern is **real** or **artifact**:\n", "\n", "| Mode | GPU Requirement | What it tests |\n", "|------|-----------------|---------------|\n", "| `8bit` | 24GB+ | Reference run (matches V3) |\n", "| `fp` | 80GB (A100) | Full precision validation |\n", "\n", "### Expected Outcomes\n", "\n", "| Verdict | If FP shows... | Meaning |\n", "|---------|----------------|--------|\n", "| `SPLIT_BRAIN_CONFIRMED` | Early=high, Middle/Late=low | Pattern is architecture-determined |\n", "| `SPLIT_BRAIN_ARTIFACT` | All regions=high SI | 8-bit created false pattern |\n", "\n", "### Usage\n", "\n", "```python\n", "# Cell 1:\n", "PRECISION_MODE = \"fp\"    # For A100-80GB\n", "PRECISION_MODE = \"8bit\"  # For smaller GPUs\n", "```\n", "\n", "---\n", "\n", "*Paper 4: Behavioral Sink Dynamics*  \n", "*E11-Indra-Gemma27B-V4: Split-Brain Validation*  \n", "*Gene Kranz Protocol: Mission Control*"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}}, "nbformat": 4, "nbformat_minor": 4}