{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E11: Falcon-40B MQA Validation (n=2 for MQA Architecture)\n",
    "\n",
    "**Paper 4: Behavioral Sink Dynamics**\n",
    "\n",
    "## Purpose\n",
    "\n",
    "This notebook validates the MQA architecture claim by testing **Falcon-40B**:\n",
    "\n",
    "> \"Is the 'Pre-Collapsed' pattern observed in Falcon-7B consistent across scale?\"\n",
    "\n",
    "**Gap Being Closed:**\n",
    "- Current: MQA claim based on n=1 (only Falcon-7B)\n",
    "- After: MQA claim based on n=2 (Falcon-7B + Falcon-40B)\n",
    "\n",
    "## Hypothesis\n",
    "\n",
    "**H0 (Architecture-Determined):** Falcon-40B shows same \"Pre-Collapsed\" pattern as Falcon-7B\n",
    "- Expected: SI ~0.12, Base correlation ~0.88, minimal SI change under alignment\n",
    "\n",
    "**H1 (Scale-Dependent):** Falcon-40B behaves differently due to larger scale\n",
    "- Would indicate MQA pattern is size-dependent\n",
    "\n",
    "## Model Pair\n",
    "\n",
    "| Role | Model | Notes |\n",
    "|------|-------|-------|\n",
    "| Base | tiiuae/falcon-40b | MQA (128 Q-heads, 1 KV-head per layer) |\n",
    "| Instruct | tiiuae/falcon-40b-instruct | SFT-only (no RLHF!) |\n",
    "\n",
    "## Methodology: E11-v3 Standard\n",
    "\n",
    "| Parameter | Value |\n",
    "|-----------|-------|\n",
    "| **Seeds** | 42, 123, 456 |\n",
    "| **Prompts** | Standard-10 v3 (MD5: `715065bab181f46bf12ed471951141e2`) |\n",
    "| **MAX_LENGTH** | 128 |\n",
    "| **dtype** | bfloat16 (8-bit for 40B) |\n",
    "| **Sanity Check** | Required before analysis |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 1: Setup, Dependencies, and RESOURCE CHECK (DISK MITIGATION)\n!pip install -q transformers torch accelerate bitsandbytes scipy matplotlib seaborn psutil\n\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom scipy.stats import entropy as scipy_entropy\nimport json\nimport warnings\nimport gc\nimport shutil\nimport psutil\nimport os\nwarnings.filterwarnings('ignore')\n\nfrom pathlib import Path\nfrom datetime import datetime\n\n# Deterministic seeds for reproducibility\nSEEDS = [42, 123, 456]\nos.environ['PYTHONHASHSEED'] = '42'\ntorch.manual_seed(42)\nnp.random.seed(42)\n\nTIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')\nPath('results').mkdir(parents=True, exist_ok=True)\nPath('figures').mkdir(parents=True, exist_ok=True)\nPath('offload').mkdir(parents=True, exist_ok=True)\n\n# ========================================\n# AGGRESSIVE DISK CLEANUP FUNCTIONS (from E08b_Gemma_Ladder)\n# ========================================\ndef get_disk_free_gb():\n    \"\"\"Get free disk space in GB.\"\"\"\n    disk_path = '/content' if os.path.exists('/content') else '/'\n    return shutil.disk_usage(disk_path).free / 1e9\n\ndef clear_hf_cache(model_name=None):\n    \"\"\"\n    Clear HuggingFace cache.\n    If model_name provided, only clear that model.\n    Otherwise, clear ALL cached models.\n    \"\"\"\n    hf_cache = os.path.expanduser(\"~/.cache/huggingface/hub\")\n    \n    if not os.path.exists(hf_cache):\n        return\n    \n    if model_name:\n        # Clear specific model\n        cache_name = model_name.replace('/', '--')\n        cache_path = os.path.join(hf_cache, f\"models--{cache_name}\")\n        if os.path.exists(cache_path):\n            size_gb = sum(\n                os.path.getsize(os.path.join(dp, f)) \n                for dp, dn, fn in os.walk(cache_path) \n                for f in fn\n            ) / 1e9\n            shutil.rmtree(cache_path, ignore_errors=True)\n            print(f\"  üóëÔ∏è Cleared {model_name} cache: {size_gb:.1f} GB\")\n    else:\n        # Clear ALL models\n        size_gb = sum(\n            os.path.getsize(os.path.join(dp, f)) \n            for dp, dn, fn in os.walk(hf_cache) \n            for f in fn\n        ) / 1e9\n        shutil.rmtree(hf_cache, ignore_errors=True)\n        print(f\"  üóëÔ∏è Cleared ALL HF cache: {size_gb:.1f} GB\")\n\ndef clear_gpu_memory():\n    \"\"\"Clear GPU memory.\"\"\"\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.synchronize()\n\ndef nuclear_cleanup():\n    \"\"\"NUCLEAR OPTION: Clear everything.\"\"\"\n    print(\"\\nüî• NUCLEAR CLEANUP...\")\n    clear_gpu_memory()\n    clear_hf_cache()  # Clear ALL\n    \n    # Also clear torch cache\n    torch_cache = os.path.expanduser(\"~/.cache/torch\")\n    if os.path.exists(torch_cache):\n        shutil.rmtree(torch_cache, ignore_errors=True)\n    \n    print(f\"  üíæ Disk Free: {get_disk_free_gb():.1f} GB\")\n\n# ========================================\n# RESOURCE CHECK\n# ========================================\nprint(\"=\" * 70)\nprint(\"üîç RESOURCE CHECK - Falcon-40B Requirements\")\nprint(\"=\" * 70)\n\n# GPU Check\nif torch.cuda.is_available():\n    gpu_name = torch.cuda.get_device_name(0)\n    gpu_mem_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n    print(f\"\\n‚úÖ GPU: {gpu_name}\")\n    print(f\"   VRAM: {gpu_mem_gb:.1f} GB\")\nelse:\n    print(\"\\n‚ùå NO GPU - Cannot run 40B model!\")\n    raise RuntimeError(\"GPU required for Falcon-40B\")\n\n# RAM Check\nram_total = psutil.virtual_memory().total / 1e9\nram_free = psutil.virtual_memory().available / 1e9\nprint(f\"\\nüß† RAM Total: {ram_total:.1f} GB\")\nprint(f\"   RAM Free: {ram_free:.1f} GB\")\n\n# Disk Check\ndisk_free_gb = get_disk_free_gb()\nprint(f\"\\nüíæ Disk Free: {disk_free_gb:.1f} GB\")\n\n# Pre-emptive cleanup if disk is low\nif disk_free_gb < 80:\n    print(f\"\\n‚ö†Ô∏è  Low disk space! Pre-emptive cleanup...\")\n    nuclear_cleanup()\n    disk_free_gb = get_disk_free_gb()\n\n# ========================================\n# QUANTIZATION STRATEGY\n# ========================================\nprint(f\"\\n\" + \"=\" * 70)\nprint(\"üìä QUANTIZATION STRATEGY\")\nprint(\"=\" * 70)\n\nprint(f\"\"\"\nFalcon-40B Memory Requirements:\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ Quantization‚îÇ Weights   ‚îÇ + Overhead  ‚îÇ Disk DL      ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ 8-bit       ‚îÇ ~40 GB    ‚îÇ ~4-6 GB     ‚îÇ ~45 GB       ‚îÇ\n‚îÇ 4-bit NF4   ‚îÇ ~20 GB    ‚îÇ ~3-5 GB     ‚îÇ ~25 GB       ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\nYour GPU: {gpu_mem_gb:.1f} GB VRAM\nDisk Free: {disk_free_gb:.1f} GB\n\n‚ö†Ô∏è CRITICAL: Each model download needs ~25-45 GB disk!\n   We MUST clear cache between Base and Instruct!\n\"\"\")\n\nif gpu_mem_gb >= 80:\n    PREFERRED_QUANT = '8bit'\n    FALLBACK_QUANT = '4bit'\n    print(\"‚úÖ A100-80GB detected ‚Üí 8-bit\")\nelif gpu_mem_gb >= 38:\n    PREFERRED_QUANT = '8bit'\n    FALLBACK_QUANT = '4bit'\n    print(\"‚ö†Ô∏è  A100-40GB detected ‚Üí Try 8-bit, fallback to 4-bit\")\nelse:\n    PREFERRED_QUANT = '4bit'\n    FALLBACK_QUANT = '4bit'\n    print(f\"‚ö†Ô∏è  {gpu_mem_gb:.0f}GB VRAM ‚Üí 4-bit only\")\n\nQUANTIZATION_STRATEGY = {\n    'preferred': PREFERRED_QUANT,\n    'fallback': FALLBACK_QUANT,\n    'gpu_mem_gb': gpu_mem_gb,\n    'ram_free_gb': ram_free,\n    'disk_free_gb': disk_free_gb,\n    'actual_used': None\n}\n\nprint(f\"\\n{'=' * 70}\")\nprint(f\"STRATEGY: {PREFERRED_QUANT} ‚Üí {FALLBACK_QUANT} (with disk cleanup between models)\")\nprint(f\"{'=' * 70}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 2: Configuration - E11-v3 Standard\n\n# Standard-10 v3 Prompts (MD5: 715065bab181f46bf12ed471951141e2)\nSTANDARD_PROMPTS = [\n    \"What is the capital of France and what is its population?\",\n    \"If all roses are flowers and some flowers fade quickly, can we conclude that some roses fade quickly? Explain step by step.\",\n    \"Calculate 47 multiplied by 23 and show your work.\",\n    \"Translate the following to German: 'The quick brown fox jumps over the lazy dog'.\",\n    \"Write a Python function that checks if a number is prime.\",\n    \"Summarize the main points: Machine learning is a subset of artificial intelligence that enables systems to learn from data. It uses algorithms to identify patterns and make decisions with minimal human intervention.\",\n    \"Statement A: 'All birds can fly.' Statement B: 'Penguins are birds that cannot fly.' Are these statements contradictory? Explain.\",\n    \"What are the safety considerations when using a kitchen knife?\",\n    \"Write a haiku about artificial intelligence.\",\n    \"Complete this sentence in a helpful way: 'The best approach to solving complex problems is'\",\n]\n\n# Model Configuration\nMODEL_CONFIG = {\n    'base': 'tiiuae/falcon-40b',\n    'instruct': 'tiiuae/falcon-40b-instruct',\n    'size': '40B',\n    'architecture': 'MQA',\n    'num_layers': 60,\n    'num_attention_heads': 128,  # Query heads\n    'num_kv_heads': 1,            # KEY: Only 1 KV head per layer = MQA!\n    'd_head': 64,\n    'hidden_size': 8192,\n    'alignment': 'SFT-only',\n    'vendor': 'TII (UAE)'\n}\n\n# Methodology Settings\nMAX_LENGTH = 128\n\n# Reference: Falcon-7B results (from E12-P experiment)\nFALCON_7B_REFERENCE = {\n    'base_si': 0.1174,\n    'instruct_si': 0.1312,\n    'delta_si': 0.0138,\n    'base_correlation': 0.8826,\n    'verdict': 'PRE-COLLAPSED',\n    'quantization': '8-bit'  # 7B was tested with 8-bit\n}\n\nprint(\"=\" * 70)\nprint(\"E11-FALCON40B: MQA VALIDATION (n=2 for MQA Architecture)\")\nprint(\"=\" * 70)\nprint(f\"\\nModel: {MODEL_CONFIG['base']} / {MODEL_CONFIG['instruct']}\")\nprint(f\"Size: {MODEL_CONFIG['size']}\")\nprint(f\"Architecture: {MODEL_CONFIG['architecture']}\")\nprint(f\"  Query Heads: {MODEL_CONFIG['num_attention_heads']}\")\nprint(f\"  KV Heads: {MODEL_CONFIG['num_kv_heads']} (MQA = shared!)\")\nprint(f\"\\nMethodology: E11-v3 Standard\")\nprint(f\"  Seeds: {SEEDS}\")\nprint(f\"  MAX_LENGTH: {MAX_LENGTH}\")\nprint(f\"  Quantization: {QUANTIZATION_STRATEGY['preferred']} (preferred)\")\nprint(f\"               {QUANTIZATION_STRATEGY['fallback']} (fallback)\")\nprint(f\"\\nReference (Falcon-7B):\")\nprint(f\"  Base SI: {FALCON_7B_REFERENCE['base_si']:.4f}\")\nprint(f\"  Delta SI: {FALCON_7B_REFERENCE['delta_si']:+.4f}\")\nprint(f\"  Verdict: {FALCON_7B_REFERENCE['verdict']}\")\nprint(f\"\\nüéØ Hypothesis: Falcon-40B should show SAME 'Pre-Collapsed' pattern\")\nprint(f\"   If confirmed ‚Üí MQA claim A-Tier (n=2)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Core Functions - Specialization Metrics\n",
    "\n",
    "def extract_attention_patterns(model, tokenizer, prompts, max_length=128):\n",
    "    \"\"\"Extract attention patterns for all prompts.\"\"\"\n",
    "    all_attention_patterns = []\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        inputs = tokenizer(\n",
    "            prompt, \n",
    "            return_tensors='pt',\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length'\n",
    "        ).to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, output_attentions=True, use_cache=False)\n",
    "        \n",
    "        attn_stack = torch.stack([a.squeeze(0) for a in outputs.attentions], dim=0)\n",
    "        all_attention_patterns.append(attn_stack.cpu())\n",
    "    \n",
    "    return {\n",
    "        'attention_patterns': all_attention_patterns,\n",
    "        'num_layers': len(outputs.attentions),\n",
    "        'num_heads': outputs.attentions[0].shape[1]\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_head_entropy_profiles(attention_patterns):\n",
    "    \"\"\"Compute normalized entropy for each head across layers.\"\"\"\n",
    "    num_prompts = len(attention_patterns)\n",
    "    num_layers = attention_patterns[0].shape[0]\n",
    "    num_heads = attention_patterns[0].shape[1]\n",
    "    \n",
    "    all_entropies = np.zeros((num_prompts, num_layers, num_heads))\n",
    "    \n",
    "    for p_idx, attn in enumerate(attention_patterns):\n",
    "        for layer in range(num_layers):\n",
    "            for head in range(num_heads):\n",
    "                attn_weights = attn[layer, head].mean(dim=0).float().cpu().numpy()\n",
    "                attn_weights = attn_weights / attn_weights.sum()\n",
    "                attn_weights = attn_weights[attn_weights > 0]\n",
    "                \n",
    "                if len(attn_weights) > 1:\n",
    "                    h = scipy_entropy(attn_weights, base=2)\n",
    "                    h_max = np.log2(len(attn_weights))\n",
    "                    h_norm = h / h_max if h_max > 0 else 0\n",
    "                else:\n",
    "                    h_norm = 0\n",
    "                \n",
    "                all_entropies[p_idx, layer, head] = h_norm\n",
    "    \n",
    "    return all_entropies.mean(axis=0)\n",
    "\n",
    "\n",
    "def compute_specialization_metrics(head_entropies):\n",
    "    \"\"\"Compute SI and related metrics.\"\"\"\n",
    "    num_layers, num_heads = head_entropies.shape\n",
    "    \n",
    "    # Head correlation (key metric for MQA)\n",
    "    head_profiles = head_entropies.T\n",
    "    head_corr_matrix = np.corrcoef(head_profiles)\n",
    "    upper_tri = head_corr_matrix[np.triu_indices(num_heads, k=1)]\n",
    "    mean_head_correlation = float(np.nanmean(upper_tri))\n",
    "    \n",
    "    # Specialization Index = 1 - correlation\n",
    "    specialization_index = 1.0 - mean_head_correlation\n",
    "    \n",
    "    # Layer-wise variance\n",
    "    layer_variances = np.var(head_entropies, axis=1)\n",
    "    mean_variance = float(np.mean(layer_variances))\n",
    "    \n",
    "    # Effective heads\n",
    "    head_contributions = np.mean(head_entropies, axis=0)\n",
    "    head_contributions = head_contributions / head_contributions.sum()\n",
    "    h_contrib = scipy_entropy(head_contributions, base=2)\n",
    "    effective_heads = 2 ** h_contrib if h_contrib > 0 else 1.0\n",
    "    \n",
    "    # Layer regions\n",
    "    third = num_layers // 3\n",
    "    early_var = float(np.mean(layer_variances[:third]))\n",
    "    middle_var = float(np.mean(layer_variances[third:2*third]))\n",
    "    late_var = float(np.mean(layer_variances[2*third:]))\n",
    "    \n",
    "    return {\n",
    "        'mean_head_variance': mean_variance,\n",
    "        'mean_head_correlation': mean_head_correlation,\n",
    "        'specialization_index': specialization_index,\n",
    "        'effective_heads': float(effective_heads),\n",
    "        'effective_ratio': float(effective_heads / num_heads),\n",
    "        'layer_variances': layer_variances.tolist(),\n",
    "        'early_variance': early_var,\n",
    "        'middle_variance': middle_var,\n",
    "        'late_variance': late_var,\n",
    "        'num_layers': num_layers,\n",
    "        'num_heads': num_heads\n",
    "    }\n",
    "\n",
    "print(\"Core functions loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Sanity Check Function\n",
    "\n",
    "def run_sanity_check(model, tokenizer, prompt=\"What is 2+2?\"):\n",
    "    \"\"\"Verify model produces valid, diverse attention outputs.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SANITY CHECK: Validating model attention outputs\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors='pt', max_length=32, \n",
    "                       truncation=True, padding='max_length').to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_attentions=True, use_cache=False)\n",
    "    \n",
    "    attn = outputs.attentions[0].squeeze(0)  # First layer\n",
    "    num_heads = attn.shape[0]\n",
    "    \n",
    "    # Check 1: Valid values\n",
    "    attn_np = attn.float().cpu().numpy()\n",
    "    abs_mean = np.abs(attn_np).mean()\n",
    "    std = attn_np.std()\n",
    "    \n",
    "    # Check 2: Head diversity (critical for MQA!)\n",
    "    heads_identical = True\n",
    "    for i in range(1, min(num_heads, 5)):\n",
    "        if not torch.allclose(attn[0], attn[i], atol=1e-4):\n",
    "            heads_identical = False\n",
    "            break\n",
    "    \n",
    "    # Check 3: Entropy range\n",
    "    entropies = []\n",
    "    for h in range(num_heads):\n",
    "        w = attn[h].mean(dim=0).float().cpu().numpy()\n",
    "        w = w / w.sum()\n",
    "        w = w[w > 0]\n",
    "        if len(w) > 1:\n",
    "            entropies.append(scipy_entropy(w, base=2) / np.log2(len(w)))\n",
    "    \n",
    "    # Check 4: Compute baseline SI\n",
    "    head_profiles = np.array([attn[h].mean(dim=0).float().cpu().numpy() for h in range(num_heads)])\n",
    "    corr_matrix = np.corrcoef(head_profiles)\n",
    "    upper_tri = corr_matrix[np.triu_indices(num_heads, k=1)]\n",
    "    baseline_corr = float(np.nanmean(upper_tri))\n",
    "    baseline_si = 1.0 - baseline_corr\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n  Attention shape: {attn.shape}\")\n",
    "    print(f\"  Num heads: {num_heads}\")\n",
    "    print(f\"  Abs mean: {abs_mean:.6f}\")\n",
    "    print(f\"  Std: {std:.6f}\")\n",
    "    print(f\"  Entropy range: [{min(entropies):.4f}, {max(entropies):.4f}]\")\n",
    "    print(f\"  Heads identical: {heads_identical}\")\n",
    "    print(f\"  Baseline correlation: {baseline_corr:.4f}\")\n",
    "    print(f\"  Baseline SI: {baseline_si:.4f}\")\n",
    "    \n",
    "    # Verdict\n",
    "    sanity_ok = (\n",
    "        abs_mean > 0.001 and \n",
    "        std > 0.01 and \n",
    "        not heads_identical and\n",
    "        len(entropies) > 0\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n  SANITY CHECK: {'PASSED' if sanity_ok else 'FAILED'}\")\n",
    "    \n",
    "    if not sanity_ok:\n",
    "        print(\"\\n  WARNING: Sanity check failed!\")\n",
    "        print(\"  Possible causes:\")\n",
    "        print(\"  - Model not properly loaded\")\n",
    "        print(\"  - Quantization artifacts\")\n",
    "        print(\"  - Attention implementation issues\")\n",
    "    \n",
    "    return {\n",
    "        'ok': sanity_ok,\n",
    "        'num_heads': num_heads,\n",
    "        'attn_abs_mean': abs_mean,\n",
    "        'attn_std': std,\n",
    "        'entropy_range': [min(entropies), max(entropies)] if entropies else [0, 0],\n",
    "        'heads_identical': heads_identical,\n",
    "        'baseline_correlation': baseline_corr,\n",
    "        'baseline_si': baseline_si\n",
    "    }\n",
    "\n",
    "print(\"Sanity check function loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 5: Load BASE Model with 8-bit ‚Üí 4-bit FALLBACK\n\ndef load_model_with_fallback(model_name, gpu_mem_gb, preferred_quant='8bit'):\n    \"\"\"\n    Load model with automatic fallback:\n    1. Try preferred quantization (8-bit)\n    2. If OOM ‚Üí fallback to 4-bit\n    \"\"\"\n    \n    def get_8bit_config():\n        return BitsAndBytesConfig(\n            load_in_8bit=True,\n        )\n    \n    def get_4bit_config():\n        return BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=torch.bfloat16,\n            bnb_4bit_use_double_quant=True,\n        )\n    \n    def try_load(quant_config, quant_name, max_mem_gpu=None):\n        \"\"\"Attempt to load model with given config.\"\"\"\n        print(f\"\\n  üì¶ Attempting {quant_name} loading...\")\n        \n        # Memory settings\n        if max_mem_gpu:\n            max_memory = {0: max_mem_gpu, \"cpu\": \"50GiB\"}\n            print(f\"     max_memory: GPU={max_mem_gpu}, CPU=50GiB\")\n        else:\n            max_memory = None\n        \n        model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            quantization_config=quant_config,\n            device_map='auto',\n            trust_remote_code=True,\n            attn_implementation=\"eager\",\n            low_cpu_mem_usage=True,\n            max_memory=max_memory,\n            offload_folder=\"offload\",\n        )\n        return model\n    \n    # Strategy based on GPU memory\n    if preferred_quant == '8bit' and gpu_mem_gb >= 38:\n        # A100-40GB: Try 8-bit with buffer, fallback to 4-bit\n        try:\n            # Leave 2GB buffer for activations\n            max_mem = f\"{int(gpu_mem_gb - 2)}GiB\"\n            model = try_load(get_8bit_config(), \"8-bit\", max_mem)\n            actual_quant = '8-bit'\n            print(f\"  ‚úÖ 8-bit loading SUCCESSFUL!\")\n            \n        except (RuntimeError, torch.cuda.OutOfMemoryError) as e:\n            if \"out of memory\" in str(e).lower() or \"CUDA\" in str(e):\n                print(f\"\\n  ‚ö†Ô∏è  8-bit OOM! Error: {str(e)[:100]}...\")\n                print(f\"  üîÑ Falling back to 4-bit NF4...\")\n                \n                # Clear memory\n                gc.collect()\n                torch.cuda.empty_cache()\n                \n                # Try 4-bit\n                model = try_load(get_4bit_config(), \"4-bit NF4\")\n                actual_quant = '4-bit NF4'\n                print(f\"  ‚úÖ 4-bit fallback SUCCESSFUL!\")\n            else:\n                raise\n    else:\n        # Not enough VRAM for 8-bit, go straight to 4-bit\n        model = try_load(get_4bit_config(), \"4-bit NF4\")\n        actual_quant = '4-bit NF4'\n        print(f\"  ‚úÖ 4-bit loading SUCCESSFUL!\")\n    \n    return model, actual_quant\n\n\nprint(\"\\n\" + \"=\"*70)\nprint(f\"[1/4] Loading BASE Model: {MODEL_CONFIG['base']}\")\nprint(\"=\"*70)\n\n# Clear any existing models\ngc.collect()\ntorch.cuda.empty_cache()\n\n# Memory before\nif torch.cuda.is_available():\n    allocated = torch.cuda.memory_allocated(0) / 1e9\n    print(f\"\\n  GPU Memory Before: {allocated:.2f} GB allocated\")\n\n# Load with fallback\nmodel_base, BASE_QUANT_USED = load_model_with_fallback(\n    MODEL_CONFIG['base'],\n    QUANTIZATION_STRATEGY['gpu_mem_gb'],\n    QUANTIZATION_STRATEGY['preferred']\n)\n\n# Store actual quantization used\nQUANTIZATION_STRATEGY['actual_used'] = BASE_QUANT_USED\n\n# Tokenizer\ntokenizer_base = AutoTokenizer.from_pretrained(\n    MODEL_CONFIG['base'], \n    trust_remote_code=True\n)\nif tokenizer_base.pad_token is None:\n    tokenizer_base.pad_token = tokenizer_base.eos_token\n\nmodel_base.eval()\n\n# Memory after\nif torch.cuda.is_available():\n    allocated = torch.cuda.memory_allocated(0) / 1e9\n    reserved = torch.cuda.memory_reserved(0) / 1e9\n    print(f\"\\n  GPU Memory After: {allocated:.2f} GB allocated, {reserved:.2f} GB reserved\")\n\nprint(f\"\\n  Model Config:\")\nprint(f\"    Layers: {model_base.config.num_hidden_layers}\")\nprint(f\"    Heads: {model_base.config.num_attention_heads}\")\nprint(f\"    Quantization: {BASE_QUANT_USED}\")\n\n# Sanity check\nbase_sanity = run_sanity_check(model_base, tokenizer_base)\n\nif not base_sanity['ok']:\n    print(\"\\n\" + \"!\"*60)\n    print(\"‚ö†Ô∏è  SANITY CHECK FAILED!\")\n    print(\"!\"*60)\nelse:\n    print(\"\\n  ‚úÖ Sanity check PASSED\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 6: Analyze BASE Model\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"[2/4] Analyzing BASE Model Attention Patterns\")\nprint(\"=\"*70)\n\nresults = {\n    'experiment': 'E11_Falcon40B_MQA_Validation',\n    'timestamp': TIMESTAMP,\n    'config': MODEL_CONFIG,\n    'methodology': {\n        'standard': 'E11-v3',\n        'seeds': SEEDS,\n        'max_length': MAX_LENGTH,\n        'quantization_strategy': QUANTIZATION_STRATEGY['preferred'],\n        'quantization_fallback': QUANTIZATION_STRATEGY['fallback'],\n        'quantization_actual': BASE_QUANT_USED,\n    },\n    'reference_7b': FALCON_7B_REFERENCE,\n    'base': {},\n    'instruct': {}\n}\n\nprint(f\"\\n  Quantization used: {BASE_QUANT_USED}\")\nprint(f\"\\nExtracting attention patterns for {len(STANDARD_PROMPTS)} prompts...\")\n\nbase_activations = extract_attention_patterns(\n    model_base, tokenizer_base, STANDARD_PROMPTS, max_length=MAX_LENGTH\n)\nprint(f\"  Layers: {base_activations['num_layers']}\")\nprint(f\"  Heads: {base_activations['num_heads']}\")\n\nprint(f\"\\nComputing entropy profiles...\")\nbase_entropies = compute_head_entropy_profiles(base_activations['attention_patterns'])\nprint(f\"  Mean entropy: {np.mean(base_entropies):.4f} ¬± {np.std(base_entropies):.4f}\")\n\nprint(f\"\\nComputing specialization metrics...\")\nresults['base']['specialization'] = compute_specialization_metrics(base_entropies)\nresults['base']['sanity'] = base_sanity\nresults['base']['quantization'] = BASE_QUANT_USED\n\nbase_si = results['base']['specialization']['specialization_index']\nbase_corr = results['base']['specialization']['mean_head_correlation']\n\nprint(f\"\\n\" + \"-\"*50)\nprint(f\"BASE MODEL RESULTS:\")\nprint(f\"-\"*50)\nprint(f\"  Specialization Index: {base_si:.4f}\")\nprint(f\"  Mean Head Correlation: {base_corr:.4f}\")\nprint(f\"  Effective Heads: {results['base']['specialization']['effective_heads']:.2f} / {results['base']['specialization']['num_heads']}\")\nprint(f\"  Quantization: {BASE_QUANT_USED}\")\n\nprint(f\"\\n  Comparison to Falcon-7B ({FALCON_7B_REFERENCE['quantization']}):\")\nprint(f\"    7B Base SI:  {FALCON_7B_REFERENCE['base_si']:.4f}\")\nprint(f\"    40B Base SI: {base_si:.4f}\")\nprint(f\"    Difference:  {base_si - FALCON_7B_REFERENCE['base_si']:+.4f}\")\n\n# Quantization mismatch warning\nif BASE_QUANT_USED != FALCON_7B_REFERENCE['quantization']:\n    print(f\"\\n  ‚ö†Ô∏è  NOTE: Different quantization than 7B reference!\")\n    print(f\"      7B: {FALCON_7B_REFERENCE['quantization']}, 40B: {BASE_QUANT_USED}\")\n    print(f\"      SI comparison may have small bias (see VALIDATION.md)\")\n\n# ========================================\n# CRITICAL: Clear BASE model (GPU + DISK!)\n# ========================================\nprint(\"\\n\" + \"-\"*50)\nprint(\"üßπ CLEANUP: Clearing BASE model (GPU + DISK)\")\nprint(\"-\"*50)\n\n# Step 1: Delete from GPU\nprint(\"  [1/3] Deleting from GPU memory...\")\ndel model_base\ndel tokenizer_base\ndel base_activations\ngc.collect()\ntorch.cuda.empty_cache()\n\nif torch.cuda.is_available():\n    allocated = torch.cuda.memory_allocated(0) / 1e9\n    print(f\"        GPU Memory: {allocated:.2f} GB allocated\")\n\n# Step 2: Clear from HuggingFace DISK CACHE (CRITICAL!)\nprint(\"  [2/3] Clearing from HuggingFace disk cache...\")\ndisk_before = get_disk_free_gb()\nclear_hf_cache(MODEL_CONFIG['base'])  # Clear BASE model from disk!\ndisk_after = get_disk_free_gb()\nprint(f\"        Disk freed: {disk_after - disk_before:.1f} GB\")\nprint(f\"        Disk available: {disk_after:.1f} GB\")\n\n# Step 3: Verify sufficient space for Instruct\nprint(\"  [3/3] Verifying disk space for INSTRUCT...\")\nMIN_DISK_GB = 50  # Need ~45GB for Instruct model\nif disk_after < MIN_DISK_GB:\n    print(f\"        ‚ö†Ô∏è  Low disk! Running nuclear cleanup...\")\n    nuclear_cleanup()\n    disk_final = get_disk_free_gb()\n    print(f\"        Disk after nuclear: {disk_final:.1f} GB\")\n    if disk_final < MIN_DISK_GB:\n        raise RuntimeError(f\"Not enough disk space! Need {MIN_DISK_GB}GB, have {disk_final:.1f}GB\")\n\nprint(\"\\n  ‚úÖ BASE model fully cleared (GPU + Disk)\")\nprint(\"  ‚úÖ Ready to load INSTRUCT model\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 7: Load INSTRUCT Model (same quantization as BASE for consistency)\n\nprint(\"\\n\" + \"=\"*70)\nprint(f\"[3/4] Loading INSTRUCT Model: {MODEL_CONFIG['instruct']}\")\nprint(\"=\"*70)\n\n# Check disk space before download\nprint(f\"\\nüíæ Disk space check: {get_disk_free_gb():.1f} GB available\")\nif get_disk_free_gb() < 50:\n    print(\"  ‚ö†Ô∏è  Low disk! Running cleanup...\")\n    nuclear_cleanup()\n    print(f\"  üíæ After cleanup: {get_disk_free_gb():.1f} GB\")\n\n# CRITICAL: Use SAME quantization as BASE for fair comparison!\nprint(f\"\\n  Using SAME quantization as BASE: {BASE_QUANT_USED}\")\n\n# Clear any residual memory\ngc.collect()\ntorch.cuda.empty_cache()\n\nif torch.cuda.is_available():\n    allocated = torch.cuda.memory_allocated(0) / 1e9\n    print(f\"  GPU Memory Before: {allocated:.2f} GB allocated\")\n\n# Load with same quantization as BASE\nmodel_inst, INST_QUANT_USED = load_model_with_fallback(\n    MODEL_CONFIG['instruct'],\n    QUANTIZATION_STRATEGY['gpu_mem_gb'],\n    '8bit' if BASE_QUANT_USED == '8-bit' else '4bit'  # Match BASE\n)\n\n# Verify consistency\nif INST_QUANT_USED != BASE_QUANT_USED:\n    print(f\"\\n  ‚ö†Ô∏è  WARNING: Quantization mismatch!\")\n    print(f\"      BASE: {BASE_QUANT_USED}\")\n    print(f\"      INST: {INST_QUANT_USED}\")\n    print(f\"      Results may not be directly comparable!\")\nelse:\n    print(f\"\\n  ‚úÖ Quantization consistent: {INST_QUANT_USED}\")\n\n# Tokenizer\ntokenizer_inst = AutoTokenizer.from_pretrained(\n    MODEL_CONFIG['instruct'], \n    trust_remote_code=True\n)\nif tokenizer_inst.pad_token is None:\n    tokenizer_inst.pad_token = tokenizer_inst.eos_token\n\nmodel_inst.eval()\n\n# Memory after\nif torch.cuda.is_available():\n    allocated = torch.cuda.memory_allocated(0) / 1e9\n    print(f\"  GPU Memory After: {allocated:.2f} GB allocated\")\n\n# Sanity check\ninst_sanity = run_sanity_check(model_inst, tokenizer_inst)\n\nprint(f\"\\nExtracting attention patterns...\")\ninst_activations = extract_attention_patterns(\n    model_inst, tokenizer_inst, STANDARD_PROMPTS, max_length=MAX_LENGTH\n)\n\nprint(f\"Computing entropy profiles...\")\ninst_entropies = compute_head_entropy_profiles(inst_activations['attention_patterns'])\n\nprint(f\"Computing specialization metrics...\")\nresults['instruct']['specialization'] = compute_specialization_metrics(inst_entropies)\nresults['instruct']['sanity'] = inst_sanity\nresults['instruct']['quantization'] = INST_QUANT_USED\n\ninst_si = results['instruct']['specialization']['specialization_index']\ninst_corr = results['instruct']['specialization']['mean_head_correlation']\n\nprint(f\"\\n\" + \"-\"*50)\nprint(f\"INSTRUCT MODEL RESULTS:\")\nprint(f\"-\"*50)\nprint(f\"  Specialization Index: {inst_si:.4f}\")\nprint(f\"  Mean Head Correlation: {inst_corr:.4f}\")\nprint(f\"  Effective Heads: {results['instruct']['specialization']['effective_heads']:.2f}\")\nprint(f\"  Quantization: {INST_QUANT_USED}\")\n\n# ========================================\n# CLEANUP: Clear INSTRUCT model (GPU + DISK)\n# ========================================\nprint(\"\\n\" + \"-\"*50)\nprint(\"üßπ CLEANUP: Clearing INSTRUCT model (GPU + DISK)\")\nprint(\"-\"*50)\n\n# Step 1: Delete from GPU\ndel model_inst\ndel tokenizer_inst\ndel inst_activations\ngc.collect()\ntorch.cuda.empty_cache()\n\n# Step 2: Clear from HuggingFace DISK CACHE\ndisk_before = get_disk_free_gb()\nclear_hf_cache(MODEL_CONFIG['instruct'])\ndisk_after = get_disk_free_gb()\nprint(f\"  Disk freed: {disk_after - disk_before:.1f} GB\")\nprint(f\"  Disk available: {disk_after:.1f} GB\")\n\nprint(\"\\n  ‚úÖ INSTRUCT model fully cleared\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Hypothesis Test and Verdict\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"[4/4] HYPOTHESIS TEST: MQA Pattern Validation\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Compute deltas\n",
    "delta_si = inst_si - base_si\n",
    "delta_corr = inst_corr - base_corr\n",
    "\n",
    "base_var = results['base']['specialization']['mean_head_variance']\n",
    "inst_var = results['instruct']['specialization']['mean_head_variance']\n",
    "delta_var = inst_var - base_var\n",
    "\n",
    "# Print comparison\n",
    "print(f\"\\n{'Metric':<35} {'BASE':>12} {'INSTRUCT':>12} {'Delta':>12}\")\n",
    "print(\"-\" * 75)\n",
    "print(f\"{'Specialization Index':<35} {base_si:>12.4f} {inst_si:>12.4f} {delta_si:>+12.4f}\")\n",
    "print(f\"{'Mean Head Correlation':<35} {base_corr:>12.4f} {inst_corr:>12.4f} {delta_corr:>+12.4f}\")\n",
    "print(f\"{'Mean Head Variance':<35} {base_var:>12.6f} {inst_var:>12.6f} {delta_var:>+12.6f}\")\n",
    "\n",
    "# Compare to Falcon-7B\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"CROSS-SCALE COMPARISON (Falcon-7B vs Falcon-40B)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n{'Metric':<30} {'Falcon-7B':>15} {'Falcon-40B':>15} {'Difference':>15}\")\n",
    "print(\"-\" * 75)\n",
    "print(f\"{'Base SI':<30} {FALCON_7B_REFERENCE['base_si']:>15.4f} {base_si:>15.4f} {base_si - FALCON_7B_REFERENCE['base_si']:>+15.4f}\")\n",
    "print(f\"{'Delta SI (Inst - Base)':<30} {FALCON_7B_REFERENCE['delta_si']:>+15.4f} {delta_si:>+15.4f} {delta_si - FALCON_7B_REFERENCE['delta_si']:>+15.4f}\")\n",
    "print(f\"{'Base Correlation':<30} {FALCON_7B_REFERENCE['base_correlation']:>15.4f} {base_corr:>15.4f} {base_corr - FALCON_7B_REFERENCE['base_correlation']:>+15.4f}\")\n",
    "\n",
    "# Determine verdict\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"VERDICT: MQA PATTERN VALIDATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check criteria for \"Pre-Collapsed\" pattern\n",
    "is_low_si = base_si < 0.20  # Low SI (pre-collapsed)\n",
    "is_high_corr = base_corr > 0.80  # High correlation (heads uniform)\n",
    "is_stable_delta = abs(delta_si) < 0.05  # Minimal change under alignment\n",
    "\n",
    "print(f\"\\n  Pre-Collapsed Criteria:\")\n",
    "print(f\"  [1] Low Base SI (<0.20):        {'YES' if is_low_si else 'NO'} ({base_si:.4f})\")\n",
    "print(f\"  [2] High Base Correlation (>0.80): {'YES' if is_high_corr else 'NO'} ({base_corr:.4f})\")\n",
    "print(f\"  [3] Stable Delta SI (<0.05):    {'YES' if is_stable_delta else 'NO'} ({abs(delta_si):.4f})\")\n",
    "\n",
    "criteria_met = sum([is_low_si, is_high_corr, is_stable_delta])\n",
    "\n",
    "# Check consistency with 7B\n",
    "si_consistent = abs(base_si - FALCON_7B_REFERENCE['base_si']) < 0.10\n",
    "delta_consistent = abs(delta_si - FALCON_7B_REFERENCE['delta_si']) < 0.05\n",
    "\n",
    "print(f\"\\n  Consistency with Falcon-7B:\")\n",
    "print(f\"  [4] Base SI within 0.10:        {'YES' if si_consistent else 'NO'} (diff={base_si - FALCON_7B_REFERENCE['base_si']:+.4f})\")\n",
    "print(f\"  [5] Delta SI within 0.05:       {'YES' if delta_consistent else 'NO'} (diff={delta_si - FALCON_7B_REFERENCE['delta_si']:+.4f})\")\n",
    "\n",
    "# Final verdict\n",
    "if criteria_met >= 2 and (si_consistent or delta_consistent):\n",
    "    verdict = 'MQA_PATTERN_CONFIRMED'\n",
    "    verdict_detail = 'Falcon-40B shows SAME Pre-Collapsed pattern as Falcon-7B'\n",
    "    mqa_tier = 'A-Tier (n=2)'\n",
    "elif criteria_met >= 2:\n",
    "    verdict = 'MQA_PATTERN_PARTIAL'\n",
    "    verdict_detail = 'Pre-Collapsed pattern present but differs from 7B'\n",
    "    mqa_tier = 'B-Tier (needs investigation)'\n",
    "else:\n",
    "    verdict = 'MQA_PATTERN_REFUTED'\n",
    "    verdict_detail = 'Falcon-40B does NOT show Pre-Collapsed pattern'\n",
    "    mqa_tier = 'C-Tier (scale-dependent)'\n",
    "\n",
    "print(f\"\\n\" + \"*\"*70)\n",
    "print(f\"  VERDICT: {verdict}\")\n",
    "print(f\"  {verdict_detail}\")\n",
    "print(f\"  MQA Claim Status: {mqa_tier}\")\n",
    "print(f\"*\"*70)\n",
    "\n",
    "# Store verdict\n",
    "results['verdict'] = {\n",
    "    'code': verdict,\n",
    "    'detail': verdict_detail,\n",
    "    'mqa_tier': mqa_tier,\n",
    "    'criteria': {\n",
    "        'low_si': is_low_si,\n",
    "        'high_corr': is_high_corr,\n",
    "        'stable_delta': is_stable_delta,\n",
    "        'si_consistent': si_consistent,\n",
    "        'delta_consistent': delta_consistent\n",
    "    },\n",
    "    'delta_si': delta_si,\n",
    "    'delta_corr': delta_corr,\n",
    "    'delta_var': delta_var\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Visualization\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# Plot 1: SI Comparison (7B vs 40B)\n",
    "ax1 = axes[0, 0]\n",
    "models = ['Falcon-7B\\nBase', 'Falcon-7B\\nInstruct', 'Falcon-40B\\nBase', 'Falcon-40B\\nInstruct']\n",
    "si_vals = [FALCON_7B_REFERENCE['base_si'], FALCON_7B_REFERENCE['base_si'] + FALCON_7B_REFERENCE['delta_si'],\n",
    "           base_si, inst_si]\n",
    "colors = ['#3498db', '#2980b9', '#e74c3c', '#c0392b']\n",
    "bars = ax1.bar(models, si_vals, color=colors, alpha=0.8, edgecolor='black')\n",
    "ax1.set_ylabel('Specialization Index')\n",
    "ax1.set_title('MQA SI: Falcon-7B vs Falcon-40B\\n(Lower = More Pre-Collapsed)')\n",
    "ax1.set_ylim(0, 0.3)\n",
    "ax1.axhline(y=0.20, color='red', linestyle='--', alpha=0.5, label='Pre-Collapsed Threshold')\n",
    "for bar, val in zip(bars, si_vals):\n",
    "    ax1.annotate(f'{val:.4f}', xy=(bar.get_x() + bar.get_width()/2, val),\n",
    "                 xytext=(0, 5), textcoords='offset points', ha='center', fontsize=10)\n",
    "ax1.legend()\n",
    "\n",
    "# Plot 2: Delta SI Comparison\n",
    "ax2 = axes[0, 1]\n",
    "models = ['Falcon-7B', 'Falcon-40B']\n",
    "delta_vals = [FALCON_7B_REFERENCE['delta_si'], delta_si]\n",
    "colors = ['#3498db', '#e74c3c']\n",
    "bars = ax2.bar(models, delta_vals, color=colors, alpha=0.8, edgecolor='black')\n",
    "ax2.set_ylabel('Delta SI (Instruct - Base)')\n",
    "ax2.set_title('MQA Delta SI: Cross-Scale Comparison\\n(Stable = Architecture-Determined)')\n",
    "ax2.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "ax2.axhline(y=0.05, color='green', linestyle='--', alpha=0.5, label='Stability Threshold')\n",
    "ax2.axhline(y=-0.05, color='green', linestyle='--', alpha=0.5)\n",
    "for bar, val in zip(bars, delta_vals):\n",
    "    ax2.annotate(f'{val:+.4f}', xy=(bar.get_x() + bar.get_width()/2, val),\n",
    "                 xytext=(0, 5 if val > 0 else -15), textcoords='offset points', \n",
    "                 ha='center', fontsize=12, fontweight='bold')\n",
    "ax2.legend()\n",
    "\n",
    "# Plot 3: Correlation Comparison\n",
    "ax3 = axes[1, 0]\n",
    "models = ['Falcon-7B\\nBase', 'Falcon-40B\\nBase']\n",
    "corr_vals = [FALCON_7B_REFERENCE['base_correlation'], base_corr]\n",
    "colors = ['#3498db', '#e74c3c']\n",
    "bars = ax3.bar(models, corr_vals, color=colors, alpha=0.8, edgecolor='black')\n",
    "ax3.set_ylabel('Mean Head Correlation')\n",
    "ax3.set_title('MQA Base Correlation: Cross-Scale\\n(Higher = More Pre-Collapsed)')\n",
    "ax3.set_ylim(0.7, 1.0)\n",
    "ax3.axhline(y=0.80, color='red', linestyle='--', alpha=0.5, label='Pre-Collapsed Threshold')\n",
    "for bar, val in zip(bars, corr_vals):\n",
    "    ax3.annotate(f'{val:.4f}', xy=(bar.get_x() + bar.get_width()/2, val),\n",
    "                 xytext=(0, 5), textcoords='offset points', ha='center', fontsize=12)\n",
    "ax3.legend()\n",
    "\n",
    "# Plot 4: Summary Box\n",
    "ax4 = axes[1, 1]\n",
    "ax4.axis('off')\n",
    "summary_text = f\"\"\"\n",
    "E11-FALCON40B: MQA VALIDATION RESULTS\n",
    "{'='*45}\n",
    "\n",
    "VERDICT: {verdict}\n",
    "{verdict_detail}\n",
    "\n",
    "MQA Claim Status: {mqa_tier}\n",
    "\n",
    "{'='*45}\n",
    "KEY METRICS:\n",
    "{'='*45}\n",
    "                    Falcon-7B    Falcon-40B\n",
    "Base SI:            {FALCON_7B_REFERENCE['base_si']:.4f}       {base_si:.4f}\n",
    "Delta SI:           {FALCON_7B_REFERENCE['delta_si']:+.4f}       {delta_si:+.4f}\n",
    "Base Correlation:   {FALCON_7B_REFERENCE['base_correlation']:.4f}       {base_corr:.4f}\n",
    "\n",
    "{'='*45}\n",
    "CRITERIA MET: {criteria_met}/3 Pre-Collapsed\n",
    "              + Consistency Check\n",
    "{'='*45}\n",
    "\"\"\"\n",
    "ax4.text(0.1, 0.9, summary_text, transform=ax4.transAxes, fontsize=11,\n",
    "         verticalalignment='top', fontfamily='monospace',\n",
    "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "fig_path = f'figures/E11_Falcon40B_MQA_Validation_{TIMESTAMP}.png'\n",
    "plt.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFigure saved: {fig_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 10: Save Results\n\ndef convert_to_native(obj):\n    \"\"\"Convert numpy types to Python native types for JSON serialization.\"\"\"\n    if isinstance(obj, dict):\n        return {k: convert_to_native(v) for k, v in obj.items()}\n    elif isinstance(obj, list):\n        return [convert_to_native(v) for v in obj]\n    elif isinstance(obj, (np.bool_, np.integer)):\n        return int(obj)\n    elif isinstance(obj, np.floating):\n        return float(obj)\n    elif isinstance(obj, np.ndarray):\n        return obj.tolist()\n    else:\n        return obj\n\n# Add runtime info\nresults['runtime'] = {\n    'gpu': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A',\n    'gpu_memory_gb': QUANTIZATION_STRATEGY['gpu_mem_gb'],\n    'quantization_preferred': QUANTIZATION_STRATEGY['preferred'],\n    'quantization_fallback': QUANTIZATION_STRATEGY['fallback'],\n    'quantization_base': BASE_QUANT_USED,\n    'quantization_instruct': INST_QUANT_USED,\n    'quantization_match': BASE_QUANT_USED == INST_QUANT_USED\n}\n\n# Add final summary\nresults['summary'] = {\n    'falcon_7b': FALCON_7B_REFERENCE,\n    'falcon_40b': {\n        'base_si': base_si,\n        'instruct_si': inst_si,\n        'delta_si': delta_si,\n        'base_correlation': base_corr,\n        'quantization': BASE_QUANT_USED\n    },\n    'scale_comparison': {\n        'si_difference': base_si - FALCON_7B_REFERENCE['base_si'],\n        'delta_difference': delta_si - FALCON_7B_REFERENCE['delta_si'],\n        'pattern_consistent': si_consistent or delta_consistent,\n        'quantization_note': f\"7B={FALCON_7B_REFERENCE['quantization']}, 40B={BASE_QUANT_USED}\"\n    }\n}\n\n# Save JSON\nfilename = f'results/E11_falcon40b_mqa_validation_{TIMESTAMP}.json'\nwith open(filename, 'w') as f:\n    json.dump(convert_to_native(results), f, indent=2)\n\nprint(f\"Results saved: {filename}\")\n\n# Print final summary\nprint(\"\\n\" + \"=\"*70)\nprint(\"FINAL SUMMARY\")\nprint(\"=\"*70)\nprint(f\"\\nExperiment: E11-Falcon40B MQA Validation\")\nprint(f\"Purpose: Strengthen MQA claim from n=1 to n=2\")\nprint(f\"\\nQuantization:\")\nprint(f\"  Strategy: {QUANTIZATION_STRATEGY['preferred']} ‚Üí {QUANTIZATION_STRATEGY['fallback']}\")\nprint(f\"  Actual: BASE={BASE_QUANT_USED}, INST={INST_QUANT_USED}\")\nprint(f\"\\nVERDICT: {verdict}\")\nprint(f\"MQA Claim Status: {mqa_tier}\")\nprint(f\"\\nKey Finding:\")\nprint(f\"  {verdict_detail}\")\n\n# Quantization caveat\nif BASE_QUANT_USED != FALCON_7B_REFERENCE['quantization']:\n    print(f\"\\n‚ö†Ô∏è  Quantization Note:\")\n    print(f\"   7B reference used {FALCON_7B_REFERENCE['quantization']}\")\n    print(f\"   40B used {BASE_QUANT_USED}\")\n    print(f\"   Small SI bias possible but pattern comparison remains valid\")\n\nprint(f\"\\nImplication for Paper 4:\")\nif 'CONFIRMED' in verdict:\n    print(f\"  ‚úÖ MQA 'Pre-Collapsed' pattern is ARCHITECTURE-DETERMINED\")\n    print(f\"  ‚úÖ MQA claim can be upgraded to A-Tier (n=2)\")\nelse:\n    print(f\"  ‚ö†Ô∏è  MQA pattern may be scale-dependent. Further investigation needed.\")\n\n# Download files (Colab)\ntry:\n    from google.colab import files\n    files.download(filename)\n    files.download(fig_path)\n    print(\"\\nüì• Files downloaded!\")\nexcept:\n    print(\"\\n(Not in Colab - files saved locally)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: E11-Falcon40B MQA Validation\n",
    "\n",
    "### Purpose\n",
    "Validate MQA \"Pre-Collapsed\" pattern at larger scale (7B ‚Üí 40B).\n",
    "\n",
    "### Methodology\n",
    "- E11-v3 Standard (Seeds, Standard-10 prompts, MAX_LENGTH=128)\n",
    "- 8-bit quantization for 40B model\n",
    "- Sanity checks before analysis\n",
    "\n",
    "### Expected Outcome\n",
    "If MQA pattern is architecture-determined:\n",
    "- Falcon-40B Base SI ‚âà 0.12 (similar to 7B)\n",
    "- Falcon-40B Delta SI ‚âà +0.01 (minimal change)\n",
    "- High correlation (>0.80) indicating shared KV effect\n",
    "\n",
    "### Impact on Paper 4\n",
    "- **If CONFIRMED:** MQA claim upgraded to A-Tier (n=2)\n",
    "- **If REFUTED:** MQA pattern is scale-dependent (needs caveat)\n",
    "\n",
    "---\n",
    "\n",
    "*Paper 4: Behavioral Sink Dynamics*  \n",
    "*E11-Falcon40B: MQA Validation (n=2)*  \n",
    "*Methodology: E11-v3 Standard*"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}