{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E08-Phi3: Alignment Density on Microsoft Heritage (E11-v3 Standard)\n",
    "\n",
    "**Paper 4: Behavioral Sink Dynamics**\n",
    "\n",
    "## Purpose: ρ_crit Validation on 3rd Heritage\n",
    "\n",
    "E08b established ρ_crit ≈ 0.267 on Gemma-2 and Qwen2.\n",
    "This notebook tests Microsoft Phi-3 size ladder.\n",
    "\n",
    "## Methodology (E11-v3 Standard)\n",
    "\n",
    "| Standard | Implementation |\n",
    "|----------|----------------|\n",
    "| Seeds | 42, 123, 456 (3-seed aggregation) |\n",
    "| SI Measurement | **GLOBAL** (all layers) |\n",
    "| Attention Mask | **YES** |\n",
    "| Chat Template | **YES** for Instruct |\n",
    "| dtype | **bfloat16** |\n",
    "| Prompts | Standard-10 v3 (MD5: 715065ba) |\n",
    "\n",
    "## Phi-3 Size Ladder\n",
    "\n",
    "| Model | Params | Architecture | ρ_kv (est) |\n",
    "|-------|--------|--------------|------------|\n",
    "| Phi-3-mini | 3.8B | GQA | TBD |\n",
    "| Phi-3-small | 7B | GQA | TBD |\n",
    "| Phi-3-medium | 14B | GQA | TBD |\n",
    "\n",
    "## Note on Base Models\n",
    "\n",
    "⚠️ Phi-3 base models are not publicly available.\n",
    "This notebook measures **Instruct SI only** and compares to other families.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup (E11-v3 STANDARD)\n",
    "!pip install -q transformers torch accelerate scipy matplotlib seaborn\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from scipy.stats import entropy as scipy_entropy\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# === REPRODUCIBILITY (E11-v3 STANDARD) ===\n",
    "SEEDS = [42, 123, 456]\n",
    "PRIMARY_SEED = 42\n",
    "\n",
    "def set_seed(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(PRIMARY_SEED)\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "Path('../results').mkdir(parents=True, exist_ok=True)\n",
    "Path('../figures').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"E08-Phi3 Alignment Density (E11-v3 Standard)\")\n",
    "print(f\"Timestamp: {TIMESTAMP}\")\n",
    "print(f\"Seeds: {SEEDS}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Configuration\n",
    "\n",
    "# Phi-3 Size Ladder (Instruct only - base not public)\n",
    "MODEL_LADDER = [\n",
    "    {\n",
    "        'name': 'microsoft/Phi-3-mini-4k-instruct',\n",
    "        'display': 'Phi-3-mini (3.8B)',\n",
    "        'size': '3.8B',\n",
    "        'use_chat_template': True\n",
    "    },\n",
    "    {\n",
    "        'name': 'microsoft/Phi-3-small-8k-instruct',\n",
    "        'display': 'Phi-3-small (7B)',\n",
    "        'size': '7B',\n",
    "        'use_chat_template': True\n",
    "    },\n",
    "    {\n",
    "        'name': 'microsoft/Phi-3-medium-4k-instruct',\n",
    "        'display': 'Phi-3-medium (14B)',\n",
    "        'size': '14B',\n",
    "        'use_chat_template': True\n",
    "    }\n",
    "]\n",
    "\n",
    "# Reference SI values from other families (Instruct models)\n",
    "REFERENCE_SI = {\n",
    "    'LLaMA-3.1-8B-Instruct': {'si': 0.31, 'rho': 0.25},\n",
    "    'Gemma-2-9B-Instruct': {'si': 0.79, 'rho': 0.267},\n",
    "    'Qwen2-7B-Instruct': {'si': 0.57, 'rho': 0.468},\n",
    "}\n",
    "\n",
    "# E11-v3 Standard Parameters\n",
    "MAX_LENGTH = 128\n",
    "\n",
    "# Standard-10 v3 Prompts (canonical via prompts.py)\n",
    "PROMPT_VERSION = \"Standard-10 v3\"\n",
    "EXPECTED_MD5 = \"715065bab181f46bf12ed471951141e2\"\n",
    "\n",
    "try:\n",
    "    from prompts import STANDARD_10_V3, MAX_LENGTH as STANDARD_MAX_LENGTH, verify_prompts\n",
    "    if not verify_prompts():\n",
    "        raise RuntimeError(f\"Standard-10 v3 MD5 mismatch (expected {EXPECTED_MD5})\")\n",
    "    STANDARD_PROMPTS = STANDARD_10_V3\n",
    "    MAX_LENGTH = STANDARD_MAX_LENGTH\n",
    "    PROMPT_MD5 = EXPECTED_MD5\n",
    "    PROMPT_SOURCE = \"prompts.py\"\n",
    "    print(f\"✅ Loaded prompts from prompts.py (MD5={PROMPT_MD5})\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ prompts.py not available or invalid: {e}\")\n",
    "    # Fallback: inline Standard-10 v3 (must match MD5)\n",
    "    STANDARD_PROMPTS = [\n",
    "        \"What is the capital of France and what is its population?\",\n",
    "        \"If all roses are flowers and some flowers fade quickly, can we conclude that some roses fade quickly? Explain step by step.\",\n",
    "        \"Calculate 47 multiplied by 23 and show your work.\",\n",
    "        \"Translate the following to German: 'The quick brown fox jumps over the lazy dog'.\",\n",
    "        \"Write a Python function that checks if a number is prime.\",\n",
    "        \"Summarize the main points: Machine learning is a subset of artificial intelligence that enables systems to learn from data. It uses algorithms to identify patterns and make decisions with minimal human intervention.\",\n",
    "        \"Statement A: 'All birds can fly.' Statement B: 'Penguins are birds that cannot fly.' Are these statements contradictory? Explain.\",\n",
    "        \"What are the safety considerations when using a kitchen knife?\",\n",
    "        \"Write a haiku about artificial intelligence.\",\n",
    "        \"Complete this sentence in a helpful way: 'The best approach to solving complex problems is'\",\n",
    "    ]\n",
    "    import hashlib\n",
    "    PROMPT_MD5 = hashlib.md5('|||'.join(STANDARD_PROMPTS).encode()).hexdigest()\n",
    "    if PROMPT_MD5 != EXPECTED_MD5:\n",
    "        raise RuntimeError(f\"Inline prompts MD5 mismatch: {PROMPT_MD5} != {EXPECTED_MD5}\")\n",
    "    MAX_LENGTH = 128\n",
    "    PROMPT_SOURCE = \"inline_standard_10_v3\"\n",
    "    print(f\"✅ Using inline Standard-10 v3 (MD5={PROMPT_MD5})\")\n",
    "\n",
    "\n",
    "# Sanity-check variants (try in order)\n",
    "SANITY_VARIANTS = [\n",
    "    {'use_chat_template': True, 'dtype': torch.bfloat16, 'label': 'chat+bf16'},\n",
    "    {'use_chat_template': False, 'dtype': torch.bfloat16, 'label': 'raw+bf16'},\n",
    "    {'use_chat_template': False, 'dtype': torch.float32, 'label': 'raw+fp32'},\n",
    "]\n",
    "SANITY_OVERRIDE = {'use_chat_template': None, 'dtype': None, 'label': None}\n",
    "\n",
    "print(f\"\\nConfiguration (E11-v3 Standard):\")\n",
    "\n",
    "\n",
    "\n",
    "# Sanity-check variants (try in order)\n",
    "SANITY_VARIANTS = [\n",
    "    {'use_chat_template': True, 'dtype': torch.bfloat16, 'label': 'chat+bf16'},\n",
    "    {'use_chat_template': False, 'dtype': torch.bfloat16, 'label': 'raw+bf16'},\n",
    "    {'use_chat_template': False, 'dtype': torch.float32, 'label': 'raw+fp32'},\n",
    "]\n",
    "SANITY_OVERRIDE = {'use_chat_template': None, 'dtype': None, 'label': None}\n",
    "\n",
    "print(f\"\\nConfiguration (E11-v3 Standard):\")\n",
    "print(f\"  MAX_LENGTH: {MAX_LENGTH}\")\n",
    "print(f\"  Prompts: {PROMPT_VERSION} (MD5={PROMPT_MD5})\")\n",
    "print(f\"\\nModels to test:\")\n",
    "for m in MODEL_LADDER:\n",
    "    print(f\"  - {m['display']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 3: SI Measurement Functions (E11-v3 STANDARD + PHI-3 FIX)\n# =============================================================================\n# PHI-3 FIX: padding=False statt padding='max_length'\n# Phi-3 returns degenerate uniform attention when heavily padded!\n# =============================================================================\n\ndef extract_head_activations(model, tokenizer, prompts, max_length=128, use_chat_template=False):\n    \"\"\"\n    Extract attention patterns (E11-v3 Standard).\n    \n    PHI-3 FIX: Use padding=False to avoid degenerate attention patterns!\n    NOTE: use_cache=False is CRITICAL for Phi-3 (DynamicCache bug)!\n    \"\"\"\n    all_attention_patterns = []\n    all_valid_lengths = []  # PHI-3 FIX: Use valid_lengths instead of masks\n    \n    for prompt in prompts:\n        if use_chat_template and hasattr(tokenizer, 'apply_chat_template'):\n            try:\n                messages = [{\"role\": \"user\", \"content\": prompt}]\n                formatted = tokenizer.apply_chat_template(\n                    messages, tokenize=False, add_generation_prompt=True\n                )\n            except:\n                formatted = prompt\n        else:\n            formatted = prompt\n        \n        # PHI-3 FIX: NO PADDING - use actual sequence length\n        inputs = tokenizer(\n            formatted, \n            return_tensors='pt', \n            max_length=max_length,\n            truncation=True, \n            padding=False  # CRITICAL FIX!\n        ).to(model.device)\n        \n        valid_len = inputs['input_ids'].shape[1]\n        \n        with torch.no_grad():\n            # CRITICAL: use_cache=False for Phi-3 (DynamicCache.get_usable_length bug)\n            outputs = model(**inputs, output_attentions=True, use_cache=False)\n        \n        attn_stack = torch.stack([a.squeeze(0) for a in outputs.attentions], dim=0)\n        all_attention_patterns.append(attn_stack.cpu())\n        all_valid_lengths.append(valid_len)\n    \n    return {\n        'attention_patterns': all_attention_patterns,\n        'valid_lengths': all_valid_lengths,  # PHI-3 FIX\n        'num_layers': len(outputs.attentions),\n        'num_heads': outputs.attentions[0].shape[1]\n    }\n\n\ndef compute_head_entropy_profiles(attention_patterns, valid_lengths):\n    \"\"\"\n    Compute entropy (E11-v3 Standard).\n    PHI-3 FIX: Use valid_lengths instead of attention_masks.\n    \"\"\"\n    num_prompts = len(attention_patterns)\n    num_layers = attention_patterns[0].shape[0]\n    num_heads = attention_patterns[0].shape[1]\n    \n    all_entropies = np.zeros((num_prompts, num_layers, num_heads))\n    \n    for p_idx, attn in enumerate(attention_patterns):\n        valid_len = valid_lengths[p_idx]\n        \n        for layer in range(num_layers):\n            for head in range(num_heads):\n                attn_weights = attn[layer, head].float().cpu().numpy()\n                \n                # Already correctly sized (no padding), but slice just in case\n                attn_weights = attn_weights[:valid_len, :valid_len]\n                \n                # Average across query positions\n                attn_weights = attn_weights.mean(axis=0)\n                \n                # Normalize\n                attn_weights = attn_weights / (attn_weights.sum() + 1e-10)\n                attn_weights = attn_weights[attn_weights > 0]\n                \n                if len(attn_weights) > 1:\n                    h = scipy_entropy(attn_weights, base=2)\n                    h_max = np.log2(len(attn_weights))\n                    h_norm = h / h_max if h_max > 0 else 0\n                else:\n                    h_norm = 0\n                \n                all_entropies[p_idx, layer, head] = h_norm\n    \n    return all_entropies.mean(axis=0)\n\n\ndef compute_si(head_entropies):\n    \"\"\"Compute global SI.\"\"\"\n    num_layers, num_heads = head_entropies.shape\n    head_profiles = head_entropies.T\n    head_corr_matrix = np.corrcoef(head_profiles)\n    upper_tri = head_corr_matrix[np.triu_indices(num_heads, k=1)]\n    mean_corr = float(np.nanmean(upper_tri))\n    return 1.0 - mean_corr, mean_corr\n\nprint(\"SI functions loaded (E11-v3 Standard + PHI-3 FIX).\")\nprint(\"  - padding: FALSE (critical Phi-3 fix!)\")\nprint(\"  - use_cache: FALSE (Phi-3 DynamicCache fix)\")"
  },
  {
   "cell_type": "code",
   "source": "# Cell 3b: SANITY CHECK (PHI-3 FIX VERSION)\n# =============================================================================\n# PHI-3 FIX: Uses padding=False - should now pass!\n# =============================================================================\n\ndef run_sanity_variant(model_config, use_chat_template, dtype, label):\n    \"\"\"\n    Sanity check variant with PHI-3 FIX: padding=False.\n    \"\"\"\n    print(f\"\\n{'='*70}\")\n    print(f\"SANITY CHECK ({label}): {model_config['name']}\")\n    print(f\"{'='*70}\")\n\n    # Load model\n    print(\"\\n1. Loading model...\")\n    tokenizer = AutoTokenizer.from_pretrained(model_config['name'], trust_remote_code=True)\n    model = AutoModelForCausalLM.from_pretrained(\n        model_config['name'],\n        torch_dtype=dtype,\n        device_map='auto',\n        trust_remote_code=True,\n        attn_implementation=\"eager\"\n    )\n    model.eval()\n    model.config.output_attentions = True\n    model.config.use_cache = False\n\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n\n    # Single prompt test\n    test_prompt = STANDARD_PROMPTS[0]\n    print(f\"\\n2. Test prompt: '{test_prompt[:50]}...'\")\n\n    # Format with chat template\n    if use_chat_template and hasattr(tokenizer, 'apply_chat_template'):\n        messages = [{\"role\": \"user\", \"content\": test_prompt}]\n        formatted = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n        print(f\"   Chat template applied: {len(formatted)} chars\")\n    else:\n        formatted = test_prompt\n        print(\"   Raw prompt (no chat template)\")\n\n    # PHI-3 FIX: NO PADDING\n    inputs = tokenizer(\n        formatted,\n        return_tensors='pt',\n        max_length=MAX_LENGTH,\n        truncation=True,\n        padding=False  # CRITICAL FIX!\n    ).to(model.device)\n\n    valid_len = inputs['input_ids'].shape[1]\n    print(\"\\n3. Tokenization (PHI-3 FIX):\")\n    print(f\"   Sequence length: {valid_len}\")\n    print(f\"   Padding: FALSE\")\n\n    # ASSERTION: valid_len must be > 5 for entropy calculation\n    if valid_len <= 5:\n        print(f\"   ❌ valid_len too small: {valid_len}\")\n        del model\n        torch.cuda.empty_cache()\n        return {'ok': False, 'reason': 'valid_len_too_small', 'valid_len': valid_len, 'label': label}\n    print(\"   ✅ valid_len > 5: PASS\")\n\n    # Forward pass\n    print(\"\\n4. Forward pass...\")\n    with torch.no_grad():\n        outputs = model(**inputs, output_attentions=True, use_cache=False)\n\n    if outputs.attentions is None:\n        print(\"   ❌ outputs.attentions is None\")\n        del model\n        torch.cuda.empty_cache()\n        return {'ok': False, 'reason': 'no_attentions', 'label': label}\n    print(\"   ✅ outputs.attentions exists: PASS\")\n\n    # Attention diagnostics\n    attn_layer0 = outputs.attentions[0].squeeze(0)  # [heads, seq, seq]\n    num_layers = len(outputs.attentions)\n    num_heads = attn_layer0.shape[0]\n\n    print(\"\\n5. Attention diagnostics:\")\n    print(f\"   Num layers: {num_layers}\")\n    print(f\"   Num heads: {num_heads}\")\n    print(f\"   Layer 0 shape: {attn_layer0.shape}\")\n    print(f\"   Layer 0 dtype: {attn_layer0.dtype}\")\n\n    attn_abs_mean = attn_layer0.abs().mean().item()\n    attn_std = attn_layer0.std().item()\n\n    print(f\"   attn.abs().mean() = {attn_abs_mean:.6f}\")\n    print(f\"   attn.std() = {attn_std:.6f}\")\n\n    if attn_abs_mean <= 0:\n        print(\"   ❌ attn.abs().mean() = 0 (degenerate)\")\n        del model\n        torch.cuda.empty_cache()\n        return {'ok': False, 'reason': 'degenerate_attn', 'label': label}\n    print(\"   ✅ attn.abs().mean() > 0: PASS\")\n\n    if not torch.isfinite(attn_layer0).all():\n        print(\"   ❌ attention contains NaN/Inf\")\n        del model\n        torch.cuda.empty_cache()\n        return {'ok': False, 'reason': 'nan_inf', 'label': label}\n    print(\"   ✅ torch.isfinite(attn): PASS\")\n\n    # Head diversity quick check\n    head0 = attn_layer0[0]\n    head1 = attn_layer0[1]\n    heads_identical = torch.allclose(head0, head1, atol=1e-4)\n    print(\"\\n6. Head diversity check:\")\n    print(f\"   Head 0 vs Head 1 identical? {heads_identical}\")\n    if heads_identical:\n        print(\"   ⚠️ Heads appear identical - may cause SI=0\")\n    else:\n        print(\"   ✅ Heads are different: PASS\")\n\n    # Compute actual SI using FIXED functions\n    print(\"\\n7. Computing baseline SI (PHI-3 FIX)...\")\n    act = extract_head_activations(\n        model, tokenizer, [test_prompt], MAX_LENGTH,\n        use_chat_template=use_chat_template\n    )\n    ent = compute_head_entropy_profiles(act['attention_patterns'], act['valid_lengths'])\n    baseline_si, mean_corr = compute_si(ent)\n\n    entropy_min = ent.min()\n    entropy_max = ent.max()\n    entropy_mean = ent.mean()\n\n    print(f\"   Entropy range: [{entropy_min:.4f}, {entropy_max:.4f}]\")\n    print(f\"   Entropy mean: {entropy_mean:.4f}\")\n\n    print(\"\\n8. BASELINE SI:\")\n    print(f\"   Mean head correlation: {mean_corr:.4f}\")\n    print(f\"   Specialization Index: {baseline_si:.4f}\")\n\n    SI_THRESHOLD = 0.05\n    ok = baseline_si >= SI_THRESHOLD\n\n    # Cleanup\n    del model\n    torch.cuda.empty_cache()\n\n    return {\n        'ok': ok,\n        'label': label,\n        'use_chat_template': use_chat_template,\n        'dtype': str(dtype),\n        'valid_len': valid_len,\n        'attn_abs_mean': attn_abs_mean,\n        'attn_std': attn_std,\n        'entropy_range': [float(entropy_min), float(entropy_max)],\n        'baseline_si': float(baseline_si),\n        'mean_corr': float(mean_corr),\n        'heads_identical': bool(heads_identical)\n    }\n\n\ndef run_sanity_check(model_config):\n    \"\"\"\n    Run sanity variants in order. On first pass, set SANITY_OVERRIDE.\n    \"\"\"\n    print(\"Running sanity check on Phi-3-mini before full ladder test...\")\n    print(\"PHI-3 FIX: Using padding=False\")\n    last = None\n    for v in SANITY_VARIANTS:\n        result = run_sanity_variant(\n            model_config,\n            use_chat_template=v['use_chat_template'],\n            dtype=v['dtype'],\n            label=v['label']\n        )\n        last = result\n        if result.get('ok'):\n            print(f\"\\n✅ SANITY PASS: {v['label']} (SI={result['baseline_si']:.4f})\")\n            SANITY_OVERRIDE['use_chat_template'] = v['use_chat_template']\n            SANITY_OVERRIDE['dtype'] = v['dtype']\n            SANITY_OVERRIDE['label'] = v['label']\n            return result\n        else:\n            print(f\"\\n❌ SANITY FAIL: {v['label']} (SI={result['baseline_si']:.4f})\")\n\n    raise AssertionError(f\"ABORT: no sanity variant passed (last SI={last.get('baseline_si') if last else 'n/a'})\")\n\n# RUN SANITY CHECK on first model\nsanity_result = run_sanity_check(MODEL_LADDER[0])\nprint(f\"\\nSanity check result: {sanity_result}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 4: Run Size Ladder Test (PHI-3 FIX: uses valid_lengths)\n\ndef measure_model_si(model_config, prompts, seeds):\n    \"\"\"Measure SI for a single model with multi-seed aggregation.\"\"\"\n    print(f\"\\n{'='*60}\")\n    print(f\"Testing: {model_config['display']}\")\n    print(f\"{'='*60}\")\n\n    # Load model\n    print(f\"Loading: {model_config['name']}\")\n    tokenizer = AutoTokenizer.from_pretrained(\n        model_config['name'], trust_remote_code=True\n    )\n    active_dtype = SANITY_OVERRIDE['dtype'] or torch.bfloat16\n    active_chat = SANITY_OVERRIDE['use_chat_template'] if SANITY_OVERRIDE['use_chat_template'] is not None else model_config['use_chat_template']\n\n    model = AutoModelForCausalLM.from_pretrained(\n        model_config['name'],\n        torch_dtype=active_dtype,\n        device_map='auto',\n        trust_remote_code=True,\n        attn_implementation=\"eager\"\n    )\n    model.eval()\n    model.config.output_attentions = True\n    model.config.use_cache = False\n\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n\n    # Architecture info\n    config = model.config\n    num_layers = config.num_hidden_layers\n    num_heads = config.num_attention_heads\n    num_kv_heads = getattr(config, 'num_key_value_heads', num_heads)\n    hidden_size = config.hidden_size\n    d_head = hidden_size // num_heads\n\n    # Primary rho (Paper-3/E08b) + alt rho_kv\n    rho_head = num_heads / math.sqrt(hidden_size)\n    rho_kv = num_kv_heads / num_layers\n\n    if num_kv_heads == num_heads:\n        arch = \"MHA\"\n    elif num_kv_heads == 1:\n        arch = \"MQA\"\n    else:\n        arch = f\"GQA ({num_heads}:{num_kv_heads})\"\n\n    print(f\"  Architecture: {arch}\")\n    print(f\"  Layers: {num_layers}, Heads: {num_heads}, KV: {num_kv_heads}\")\n    print(f\"  d_head: {d_head}\")\n    print(f\"  rho_head: {rho_head:.4f}, rho_kv: {rho_kv:.4f}\")\n\n    # Multi-seed SI measurement\n    si_values = []\n    corr_values = []\n\n    for seed in seeds:\n        set_seed(seed)\n        act = extract_head_activations(\n            model, tokenizer, prompts, MAX_LENGTH,\n            use_chat_template=active_chat\n        )\n        # PHI-3 FIX: Use valid_lengths instead of attention_masks\n        ent = compute_head_entropy_profiles(\n            act['attention_patterns'], act['valid_lengths']\n        )\n        si, corr = compute_si(ent)\n        si_values.append(si)\n        corr_values.append(corr)\n\n    si_mean = np.mean(si_values)\n    si_std = np.std(si_values)\n    corr_mean = np.mean(corr_values)\n\n    print(f\"  SI: {si_mean:.4f} ± {si_std:.4f}\")\n    print(f\"  Correlation: {corr_mean:.4f}\")\n\n    # Cleanup\n    del model\n    torch.cuda.empty_cache()\n\n    return {\n        'model': model_config['name'],\n        'display': model_config['display'],\n        'size': model_config['size'],\n        'architecture': arch,\n        'num_layers': num_layers,\n        'num_heads': num_heads,\n        'num_kv_heads': num_kv_heads,\n        'hidden_size': hidden_size,\n        'd_head': d_head,\n        'rho_head': rho_head,\n        'rho_kv': rho_kv,\n        'sanity_override': {\n            'use_chat_template': active_chat,\n            'dtype': str(active_dtype),\n            'label': SANITY_OVERRIDE['label']\n        },\n        'si_mean': si_mean,\n        'si_std': si_std,\n        'si_values': si_values,\n        'corr_mean': corr_mean\n    }\n\nprint(\"Test function loaded (PHI-3 FIX: valid_lengths).\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Run All Models\n",
    "\n",
    "print(f\"\\n{'#'*70}\")\n",
    "print(f\"# E08-Phi3: Microsoft Heritage Size Ladder\")\n",
    "print(f\"# Alignment Density Test (E11-v3 Standard)\")\n",
    "print(f\"{'#'*70}\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for model_config in MODEL_LADDER:\n",
    "    try:\n",
    "        result = measure_model_si(model_config, STANDARD_PROMPTS, SEEDS)\n",
    "        all_results.append(result)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR on {model_config['display']}: {e}\")\n",
    "        all_results.append({\n",
    "            'model': model_config['name'],\n",
    "            'display': model_config['display'],\n",
    "            'size': model_config['size'],\n",
    "            'error': str(e)\n",
    "        })\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"ALL MODELS TESTED!\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Analysis and Comparison\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"PHI-3 SIZE LADDER RESULTS\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "print(f\"\\n{'Model':<25} {'Size':<8} {'ρ_kv':<8} {'SI':<12} {'Arch':<15}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for r in all_results:\n",
    "    if 'error' not in r:\n",
    "        print(f\"{r['display']:<25} {r['size']:<8} {r['rho_head']:.4f}   {r['si_mean']:.4f}±{r['si_std']:.3f}  {r['architecture']:<15}\")\n",
    "    else:\n",
    "        print(f\"{r['display']:<25} {r['size']:<8} ERROR: {r['error'][:30]}\")\n",
    "\n",
    "# Cross-heritage comparison\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"CROSS-HERITAGE COMPARISON (Instruct SI)\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "print(f\"\\n{'Model':<30} {'Heritage':<15} {'ρ_kv':<10} {'SI':<10}\")\n",
    "print(\"-\"*65)\n",
    "\n",
    "# Reference models\n",
    "for name, data in REFERENCE_SI.items():\n",
    "    heritage = 'Meta' if 'LLaMA' in name else ('Google' if 'Gemma' in name else 'Alibaba')\n",
    "    print(f\"{name:<30} {heritage:<15} {data['rho']:<10.4f} {data['si']:<10.4f}\")\n",
    "\n",
    "# Phi-3 results\n",
    "for r in all_results:\n",
    "    if 'error' not in r:\n",
    "        print(f\"{r['display']:<30} {'Microsoft':<15} {r['rho_head']:<10.4f} {r['si_mean']:<10.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: ρ_crit Analysis\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"ρ_crit ANALYSIS\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Check if Phi-3 follows ρ_crit pattern\n",
    "RHO_CRIT = 0.267\n",
    "\n",
    "print(f\"\\nρ_crit threshold: {RHO_CRIT}\")\n",
    "print(f\"\\nPattern: ρ < {RHO_CRIT} → Higher SI (less collapse)\")\n",
    "print(f\"         ρ > {RHO_CRIT} → Lower SI (more collapse)\")\n",
    "\n",
    "print(f\"\\n{'Model':<25} {'ρ_kv':<10} {'SI':<10} {'vs ρ_crit':<15}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for r in all_results:\n",
    "    if 'error' not in r:\n",
    "        status = 'BELOW' if r['rho_head'] < RHO_CRIT else 'ABOVE'\n",
    "        print(f\"{r['display']:<25} {r['rho_head']:<10.4f} {r['si_mean']:<10.4f} {status:<15}\")\n",
    "\n",
    "# Verdict\n",
    "valid_results = [r for r in all_results if 'error' not in r]\n",
    "if len(valid_results) >= 2:\n",
    "    # Check if higher ρ correlates with lower SI\n",
    "    sorted_by_rho = sorted(valid_results, key=lambda x: x['rho_head'])\n",
    "    rho_values = [r['rho_head'] for r in sorted_by_rho]\n",
    "    si_values = [r['si_mean'] for r in sorted_by_rho]\n",
    "    \n",
    "    # Simple correlation check\n",
    "    if len(rho_values) >= 3:\n",
    "        correlation = np.corrcoef(rho_values, si_values)[0, 1]\n",
    "        print(f\"\\nρ-SI Correlation: {correlation:.3f}\")\n",
    "        if correlation < -0.5:\n",
    "            print(\"→ NEGATIVE correlation (expected for ρ_crit pattern)\")\n",
    "        elif correlation > 0.5:\n",
    "            print(\"→ POSITIVE correlation (unexpected!)\")\n",
    "        else:\n",
    "            print(\"→ WEAK correlation (inconclusive)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Visualization\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot 1: Phi-3 Size Ladder\n",
    "ax1 = axes[0]\n",
    "valid = [r for r in all_results if 'error' not in r]\n",
    "if valid:\n",
    "    sizes = [r['size'] for r in valid]\n",
    "    si_means = [r['si_mean'] for r in valid]\n",
    "    si_stds = [r['si_std'] for r in valid]\n",
    "    \n",
    "    bars = ax1.bar(sizes, si_means, yerr=si_stds, color='#9b59b6', alpha=0.8, capsize=5)\n",
    "    ax1.set_ylabel('Specialization Index (SI)')\n",
    "    ax1.set_xlabel('Model Size')\n",
    "    ax1.set_title('Phi-3 Size Ladder: Instruct SI')\n",
    "    ax1.set_ylim(0, 1)\n",
    "    \n",
    "    for bar, si, std in zip(bars, si_means, si_stds):\n",
    "        ax1.annotate(f'{si:.3f}', xy=(bar.get_x() + bar.get_width()/2, si + std + 0.02),\n",
    "                     ha='center', fontweight='bold')\n",
    "\n",
    "# Plot 2: Cross-Heritage ρ vs SI\n",
    "ax2 = axes[1]\n",
    "\n",
    "# Reference data points\n",
    "ref_rhos = [0.25, 0.267, 0.468]  # LLaMA, Gemma, Qwen\n",
    "ref_sis = [0.31, 0.79, 0.57]\n",
    "ref_labels = ['LLaMA-3.1', 'Gemma-2-9B', 'Qwen2-7B']\n",
    "ref_colors = ['#e74c3c', '#2ecc71', '#f39c12']\n",
    "\n",
    "for rho, si, label, color in zip(ref_rhos, ref_sis, ref_labels, ref_colors):\n",
    "    ax2.scatter(rho, si, s=150, c=color, label=label, edgecolors='black', linewidths=2)\n",
    "\n",
    "# Phi-3 data points\n",
    "if valid:\n",
    "    phi_rhos = [r['rho_head'] for r in valid]\n",
    "    phi_sis = [r['si_mean'] for r in valid]\n",
    "    phi_labels = [r['size'] for r in valid]\n",
    "    \n",
    "    for rho, si, label in zip(phi_rhos, phi_sis, phi_labels):\n",
    "        ax2.scatter(rho, si, s=150, c='#9b59b6', marker='s', \n",
    "                    edgecolors='black', linewidths=2, label=f'Phi-3 {label}')\n",
    "\n",
    "# ρ_crit line\n",
    "ax2.axvline(x=0.267, color='red', linestyle='--', alpha=0.7, label='ρ_crit ≈ 0.267')\n",
    "\n",
    "ax2.set_xlabel('ρ_kv (KV Head Density)')\n",
    "ax2.set_ylabel('Specialization Index (SI)')\n",
    "ax2.set_title('Cross-Heritage: ρ vs SI')\n",
    "ax2.legend(loc='best', fontsize=8)\n",
    "ax2.set_xlim(0, 0.6)\n",
    "ax2.set_ylim(0, 1)\n",
    "\n",
    "plt.suptitle(f'E08-Phi3: Microsoft Heritage Analysis\\nSeeds: {SEEDS}', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "\n",
    "fig_path = f'../figures/E08_phi3_{TIMESTAMP}.png'\n",
    "plt.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFigure saved: {fig_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Save Results\n",
    "\n",
    "def convert_to_native(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_to_native(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_native(v) for v in obj]\n",
    "    elif isinstance(obj, (np.bool_, np.integer)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "filename = f'../results/E08_phi3_{TIMESTAMP}.json'\n",
    "\n",
    "output = {\n",
    "    'experiment': 'E08-Phi3',\n",
    "    'purpose': 'Alignment Density on Microsoft Heritage (Size Ladder)',\n",
    "    'timestamp': TIMESTAMP,\n",
    "    'methodology': {\n",
    "        'standard': 'E11-v3',\n",
    "        'seeds': SEEDS,\n",
    "        'prompts': PROMPT_VERSION,\n",
    "        'prompt_md5': PROMPT_MD5,\n",
    "        'prompt_source': PROMPT_SOURCE,\n",
    "        'max_length': MAX_LENGTH,\n",
    "        'attention_mask': True,\n",
    "        'chat_template': True,\n",
    "        'dtype': 'bfloat16',\n",
    "        'sanity_override': {\n",
    "            'use_chat_template': SANITY_OVERRIDE.get('use_chat_template'),\n",
    "            'dtype': str(SANITY_OVERRIDE.get('dtype')) if SANITY_OVERRIDE.get('dtype') is not None else None,\n",
    "            'label': SANITY_OVERRIDE.get('label')\n",
    "        }\n",
    "    },\n",
    "    'note': 'Base models not publicly available - Instruct SI only',\n",
    "    'reference_si': REFERENCE_SI,\n",
    "    'results': convert_to_native(all_results),\n",
    "    'rho_crit_threshold': 0.267\n",
    "}\n",
    "\n",
    "with open(filename, 'w') as f:\n",
    "    json.dump(output, f, indent=2)\n",
    "\n",
    "print(f\"Results saved: {filename}\")\n",
    "\n",
    "# Auto-download\n",
    "try:\n",
    "    from google.colab import files\n",
    "    import shutil\n",
    "    import os\n",
    "    os.makedirs('download', exist_ok=True)\n",
    "    shutil.copy(filename, 'download/')\n",
    "    shutil.copy(fig_path, 'download/')\n",
    "    shutil.make_archive(f'E08_phi3_{TIMESTAMP}', 'zip', 'download')\n",
    "    files.download(f'E08_phi3_{TIMESTAMP}.zip')\n",
    "except:\n",
    "    print('Not in Colab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: E08-Phi3\n",
    "\n",
    "### Methodology\n",
    "\n",
    "| Standard | Implementation |\n",
    "|----------|----------------|\n",
    "| Seeds | 42, 123, 456 |\n",
    "| Prompts | Standard-10 v3 |\n",
    "| MAX_LENGTH | 128 |\n",
    "| Mask | YES |\n",
    "| dtype | bfloat16 |\n",
    "\n",
    "### Limitation\n",
    "\n",
    "⚠️ Phi-3 base models not publicly available.\n",
    "This measures **Instruct SI only** (no ΔSI calculation).\n",
    "\n",
    "### Expected Insights\n",
    "\n",
    "1. Does Phi-3 follow ρ_crit pattern?\n",
    "2. How does Microsoft heritage compare to Meta/Google/Alibaba?\n",
    "3. Does size affect SI in Phi-3?\n",
    "\n",
    "---\n",
    "\n",
    "*Paper 4: Behavioral Sink Dynamics*  \n",
    "*E08-Phi3: Microsoft Heritage Size Ladder*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}