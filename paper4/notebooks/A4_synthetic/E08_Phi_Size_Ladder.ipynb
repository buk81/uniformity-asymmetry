{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# E08-Phi-Size-Ladder: B11 Size Confound Validation\n",
        "\n",
        "**Paper 4: Behavioral Sink Dynamics**\n",
        "\n",
        "## Purpose: Resolve Size vs Heritage Confound\n",
        "\n",
        "B11 claims \"Microsoft heritage = Synthetic Immunity\". But:\n",
        "- Only 2 Phi-3 models tested (3.8B, 14B)\n",
        "- Could the stability be due to SIZE rather than HERITAGE?\n",
        "\n",
        "## Critical Test\n",
        "\n",
        "| Model | Size | If SI stable | Conclusion |\n",
        "|-------|------|--------------|------------|\n",
        "| Phi-1.5 | 1.3B | SI ~ 0.33 | Heritage confirmed |\n",
        "| Phi-2 | 2.7B | SI ~ 0.33 | Heritage confirmed |\n",
        "| Phi-3-mini | 3.8B | SI = 0.329 | (Reference) |\n",
        "\n",
        "**Decision Rule:**\n",
        "- If Phi-1.5/Phi-2 SI ~ 0.33: **Heritage > Size** (B11 -> A-Tier)\n",
        "- If Phi-1.5/Phi-2 SI != 0.33: **Size is confound** (B11 stays B-Tier)\n",
        "\n",
        "## Methodology (E11-v3 Standard)\n",
        "\n",
        "| Standard | Implementation |\n",
        "|----------|----------------|\n",
        "| Seeds | 42, 123, 456 (3-seed aggregation) |\n",
        "| SI Measurement | **GLOBAL** (all layers) |\n",
        "| Padding | **FALSE** (no padding, use valid_lengths) |\n",
        "| Chat Template | **NO** (base models) |\n",
        "| dtype | **bfloat16** (with sanity fallback) |\n",
        "| use_cache | **FALSE** (critical for older Phi models) |\n",
        "| Prompts | Standard-10 v3 (MD5: 715065ba) |\n",
        "| **SANITY CHECK** | **YES** (before full run) |\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: Setup (E11-v3 STANDARD)\n",
        "!pip install -q transformers torch accelerate scipy matplotlib seaborn\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from scipy.stats import entropy as scipy_entropy\n",
        "import json\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "# === REPRODUCIBILITY (E11-v3 STANDARD) ===\n",
        "SEEDS = [42, 123, 456]\n",
        "PRIMARY_SEED = 42\n",
        "\n",
        "def set_seed(seed):\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "\n",
        "set_seed(PRIMARY_SEED)\n",
        "\n",
        "TIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "Path('../results').mkdir(parents=True, exist_ok=True)\n",
        "Path('../figures').mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"E08-Phi-Size-Ladder: B11 Size Confound Validation\")\n",
        "print(f\"Timestamp: {TIMESTAMP}\")\n",
        "print(f\"Seeds: {SEEDS}\")\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: Configuration\n",
        "\n",
        "# Microsoft Phi Family Size Ladder (oldest to newest)\n",
        "MODEL_LADDER = [\n",
        "    {\n",
        "        'name': 'microsoft/phi-1_5',\n",
        "        'display': 'Phi-1.5 (1.3B)',\n",
        "        'size': '1.3B',\n",
        "        'use_chat_template': False  # Base model\n",
        "    },\n",
        "    {\n",
        "        'name': 'microsoft/phi-2',\n",
        "        'display': 'Phi-2 (2.7B)',\n",
        "        'size': '2.7B',\n",
        "        'use_chat_template': False  # Base model\n",
        "    },\n",
        "]\n",
        "\n",
        "# Reference values from E08-Phi3\n",
        "PHI3_REFERENCE = {\n",
        "    'Phi-3-mini (3.8B)': {'si': 0.329, 'size_b': 3.8, 'arch': 'MHA'},\n",
        "    'Phi-3-medium (14B)': {'si': 0.334, 'size_b': 14.0, 'arch': 'GQA'},\n",
        "}\n",
        "\n",
        "# E11-v3 Standard Parameters\n",
        "MAX_LENGTH = 128\n",
        "\n",
        "# Standard-10 v3 Prompts\n",
        "PROMPT_VERSION = \"Standard-10 v3\"\n",
        "EXPECTED_MD5 = \"715065bab181f46bf12ed471951141e2\"\n",
        "\n",
        "# Inline Standard-10 v3\n",
        "STANDARD_PROMPTS = [\n",
        "    \"What is the capital of France and what is its population?\",\n",
        "    \"If all roses are flowers and some flowers fade quickly, can we conclude that some roses fade quickly? Explain step by step.\",\n",
        "    \"Calculate 47 multiplied by 23 and show your work.\",\n",
        "    \"Translate the following to German: 'The quick brown fox jumps over the lazy dog'.\",\n",
        "    \"Write a Python function that checks if a number is prime.\",\n",
        "    \"Summarize the main points: Machine learning is a subset of artificial intelligence that enables systems to learn from data. It uses algorithms to identify patterns and make decisions with minimal human intervention.\",\n",
        "    \"Statement A: 'All birds can fly.' Statement B: 'Penguins are birds that cannot fly.' Are these statements contradictory? Explain.\",\n",
        "    \"What are the safety considerations when using a kitchen knife?\",\n",
        "    \"Write a haiku about artificial intelligence.\",\n",
        "    \"Complete this sentence in a helpful way: 'The best approach to solving complex problems is'\",\n",
        "]\n",
        "\n",
        "import hashlib\n",
        "PROMPT_MD5 = hashlib.md5('|||'.join(STANDARD_PROMPTS).encode()).hexdigest()\n",
        "assert PROMPT_MD5 == EXPECTED_MD5, f\"Prompt MD5 mismatch: {PROMPT_MD5}\"\n",
        "PROMPT_SOURCE = \"inline_standard_10_v3\"\n",
        "print(f\"\\u2705 Using inline Standard-10 v3 (MD5={PROMPT_MD5})\")\n",
        "\n",
        "# Sanity-check variants (try in order) - CRITICAL for older Phi models\n",
        "SANITY_VARIANTS = [\n",
        "    {'use_chat_template': False, 'dtype': torch.bfloat16, 'label': 'raw+bf16'},\n",
        "    {'use_chat_template': False, 'dtype': torch.float16, 'label': 'raw+fp16'},\n",
        "    {'use_chat_template': False, 'dtype': torch.float32, 'label': 'raw+fp32'},\n",
        "]\n",
        "SANITY_OVERRIDE = {'use_chat_template': None, 'dtype': None, 'label': None}\n",
        "\n",
        "# Hypothesis test thresholds\n",
        "PHI3_SI_TARGET = 0.33  # Expected if Heritage hypothesis is true\n",
        "SI_TOLERANCE = 0.05   # +/- 5% tolerance\n",
        "\n",
        "print(f\"\\nConfiguration (E11-v3 Standard):\")\n",
        "print(f\"  MAX_LENGTH: {MAX_LENGTH}\")\n",
        "print(f\"  Prompts: {PROMPT_VERSION}\")\n",
        "print(f\"  Target SI: {PHI3_SI_TARGET} +/- {SI_TOLERANCE}\")\n",
        "print(f\"\\nModels to test:\")\n",
        "for m in MODEL_LADDER:\n",
        "    print(f\"  - {m['display']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3: SI Measurement Functions (E11-v3 STANDARD)\n",
        "# =============================================================================\n",
        "# CONSISTENT WITH E08-Phi3: padding=False, use_cache=False\n",
        "# =============================================================================\n",
        "\n",
        "def extract_head_activations(model, tokenizer, prompts, max_length=128, use_chat_template=False):\n",
        "    \"\"\"\n",
        "    Extract attention patterns (E11-v3 Standard).\n",
        "    Uses padding=False for consistency with Phi-3 methodology.\n",
        "    NOTE: use_cache=False is CRITICAL for Phi models!\n",
        "    \"\"\"\n",
        "    all_attention_patterns = []\n",
        "    all_valid_lengths = []\n",
        "    \n",
        "    for prompt in prompts:\n",
        "        if use_chat_template and hasattr(tokenizer, 'apply_chat_template'):\n",
        "            try:\n",
        "                messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "                formatted = tokenizer.apply_chat_template(\n",
        "                    messages, tokenize=False, add_generation_prompt=True\n",
        "                )\n",
        "            except:\n",
        "                formatted = prompt\n",
        "        else:\n",
        "            formatted = prompt\n",
        "        \n",
        "        # NO PADDING (consistent with Phi-3 methodology)\n",
        "        inputs = tokenizer(\n",
        "            formatted, \n",
        "            return_tensors='pt', \n",
        "            max_length=max_length,\n",
        "            truncation=True, \n",
        "            padding=False\n",
        "        ).to(model.device)\n",
        "        \n",
        "        valid_len = inputs['input_ids'].shape[1]\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            # CRITICAL: use_cache=False for Phi models\n",
        "            outputs = model(**inputs, output_attentions=True, use_cache=False)\n",
        "        \n",
        "        attn_stack = torch.stack([a.squeeze(0) for a in outputs.attentions], dim=0)\n",
        "        all_attention_patterns.append(attn_stack.cpu())\n",
        "        all_valid_lengths.append(valid_len)\n",
        "    \n",
        "    return {\n",
        "        'attention_patterns': all_attention_patterns,\n",
        "        'valid_lengths': all_valid_lengths,\n",
        "        'num_layers': len(outputs.attentions),\n",
        "        'num_heads': outputs.attentions[0].shape[1]\n",
        "    }\n",
        "\n",
        "\n",
        "def compute_head_entropy_profiles(attention_patterns, valid_lengths):\n",
        "    \"\"\"Compute entropy profiles (E11-v3 Standard).\"\"\"\n",
        "    num_prompts = len(attention_patterns)\n",
        "    num_layers = attention_patterns[0].shape[0]\n",
        "    num_heads = attention_patterns[0].shape[1]\n",
        "    \n",
        "    all_entropies = np.zeros((num_prompts, num_layers, num_heads))\n",
        "    \n",
        "    for p_idx, attn in enumerate(attention_patterns):\n",
        "        valid_len = valid_lengths[p_idx]\n",
        "        \n",
        "        for layer in range(num_layers):\n",
        "            for head in range(num_heads):\n",
        "                attn_weights = attn[layer, head].float().cpu().numpy()\n",
        "                attn_weights = attn_weights[:valid_len, :valid_len]\n",
        "                attn_weights = attn_weights.mean(axis=0)\n",
        "                attn_weights = attn_weights / (attn_weights.sum() + 1e-10)\n",
        "                attn_weights = attn_weights[attn_weights > 0]\n",
        "                \n",
        "                if len(attn_weights) > 1:\n",
        "                    h = scipy_entropy(attn_weights, base=2)\n",
        "                    h_max = np.log2(len(attn_weights))\n",
        "                    h_norm = h / h_max if h_max > 0 else 0\n",
        "                else:\n",
        "                    h_norm = 0\n",
        "                \n",
        "                all_entropies[p_idx, layer, head] = h_norm\n",
        "    \n",
        "    return all_entropies.mean(axis=0)\n",
        "\n",
        "\n",
        "def compute_si(head_entropies):\n",
        "    \"\"\"Compute global SI.\"\"\"\n",
        "    num_layers, num_heads = head_entropies.shape\n",
        "    head_profiles = head_entropies.T\n",
        "    head_corr_matrix = np.corrcoef(head_profiles)\n",
        "    upper_tri = head_corr_matrix[np.triu_indices(num_heads, k=1)]\n",
        "    mean_corr = float(np.nanmean(upper_tri))\n",
        "    return 1.0 - mean_corr, mean_corr\n",
        "\n",
        "print(\"SI functions loaded (E11-v3 Standard).\")\n",
        "print(\"  - padding: FALSE\")\n",
        "print(\"  - use_cache: FALSE\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3b: SANITY CHECK (CRITICAL - adapted from E08-Phi3)\n",
        "# =============================================================================\n",
        "# KRANZ REQUIREMENT: Validate model behavior BEFORE full experiment!\n",
        "# Phi-1.5 and Phi-2 are OLDER models with potentially different quirks.\n",
        "# =============================================================================\n",
        "\n",
        "def run_sanity_variant(model_config, use_chat_template, dtype, label):\n",
        "    \"\"\"\n",
        "    Sanity check variant for Phi-1.5/Phi-2.\n",
        "    Validates: attention output, head diversity, SI threshold.\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"SANITY CHECK ({label}): {model_config['name']}\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    # Load model\n",
        "    print(\"\\n1. Loading model...\")\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_config['name'], trust_remote_code=True)\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_config['name'],\n",
        "            torch_dtype=dtype,\n",
        "            device_map='auto',\n",
        "            trust_remote_code=True,\n",
        "            attn_implementation=\"eager\"\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"   \\u274c Model load failed: {e}\")\n",
        "        return {'ok': False, 'reason': f'load_failed: {e}', 'label': label}\n",
        "    \n",
        "    model.eval()\n",
        "    model.config.output_attentions = True\n",
        "    model.config.use_cache = False\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Single prompt test\n",
        "    test_prompt = STANDARD_PROMPTS[0]\n",
        "    print(f\"\\n2. Test prompt: '{test_prompt[:50]}...'\")\n",
        "\n",
        "    # NO chat template for base models\n",
        "    formatted = test_prompt\n",
        "    print(\"   Raw prompt (no chat template - base model)\")\n",
        "\n",
        "    # Tokenize with NO PADDING\n",
        "    inputs = tokenizer(\n",
        "        formatted,\n",
        "        return_tensors='pt',\n",
        "        max_length=MAX_LENGTH,\n",
        "        truncation=True,\n",
        "        padding=False\n",
        "    ).to(model.device)\n",
        "\n",
        "    valid_len = inputs['input_ids'].shape[1]\n",
        "    print(\"\\n3. Tokenization:\")\n",
        "    print(f\"   Sequence length: {valid_len}\")\n",
        "    print(f\"   Padding: FALSE\")\n",
        "\n",
        "    # ASSERTION: valid_len must be > 5 for entropy calculation\n",
        "    if valid_len <= 5:\n",
        "        print(f\"   \\u274c valid_len too small: {valid_len}\")\n",
        "        del model\n",
        "        torch.cuda.empty_cache()\n",
        "        return {'ok': False, 'reason': 'valid_len_too_small', 'valid_len': valid_len, 'label': label}\n",
        "    print(\"   \\u2705 valid_len > 5: PASS\")\n",
        "\n",
        "    # Forward pass\n",
        "    print(\"\\n4. Forward pass...\")\n",
        "    try:\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs, output_attentions=True, use_cache=False)\n",
        "    except Exception as e:\n",
        "        print(f\"   \\u274c Forward pass failed: {e}\")\n",
        "        del model\n",
        "        torch.cuda.empty_cache()\n",
        "        return {'ok': False, 'reason': f'forward_failed: {e}', 'label': label}\n",
        "\n",
        "    if outputs.attentions is None:\n",
        "        print(\"   \\u274c outputs.attentions is None\")\n",
        "        del model\n",
        "        torch.cuda.empty_cache()\n",
        "        return {'ok': False, 'reason': 'no_attentions', 'label': label}\n",
        "    print(\"   \\u2705 outputs.attentions exists: PASS\")\n",
        "\n",
        "    # Attention diagnostics\n",
        "    attn_layer0 = outputs.attentions[0].squeeze(0)  # [heads, seq, seq]\n",
        "    num_layers = len(outputs.attentions)\n",
        "    num_heads = attn_layer0.shape[0]\n",
        "\n",
        "    print(\"\\n5. Attention diagnostics:\")\n",
        "    print(f\"   Num layers: {num_layers}\")\n",
        "    print(f\"   Num heads: {num_heads}\")\n",
        "    print(f\"   Layer 0 shape: {attn_layer0.shape}\")\n",
        "    print(f\"   Layer 0 dtype: {attn_layer0.dtype}\")\n",
        "\n",
        "    attn_abs_mean = attn_layer0.abs().mean().item()\n",
        "    attn_std = attn_layer0.std().item()\n",
        "\n",
        "    print(f\"   attn.abs().mean() = {attn_abs_mean:.6f}\")\n",
        "    print(f\"   attn.std() = {attn_std:.6f}\")\n",
        "\n",
        "    if attn_abs_mean <= 0:\n",
        "        print(\"   \\u274c attn.abs().mean() = 0 (degenerate)\")\n",
        "        del model\n",
        "        torch.cuda.empty_cache()\n",
        "        return {'ok': False, 'reason': 'degenerate_attn', 'label': label}\n",
        "    print(\"   \\u2705 attn.abs().mean() > 0: PASS\")\n",
        "\n",
        "    if not torch.isfinite(attn_layer0).all():\n",
        "        print(\"   \\u274c attention contains NaN/Inf\")\n",
        "        del model\n",
        "        torch.cuda.empty_cache()\n",
        "        return {'ok': False, 'reason': 'nan_inf', 'label': label}\n",
        "    print(\"   \\u2705 torch.isfinite(attn): PASS\")\n",
        "\n",
        "    # Head diversity quick check\n",
        "    head0 = attn_layer0[0]\n",
        "    head1 = attn_layer0[1] if num_heads > 1 else attn_layer0[0]\n",
        "    heads_identical = torch.allclose(head0, head1, atol=1e-4)\n",
        "    print(\"\\n6. Head diversity check:\")\n",
        "    print(f\"   Head 0 vs Head 1 identical? {heads_identical}\")\n",
        "    if heads_identical:\n",
        "        print(\"   \\u26a0\\ufe0f Heads appear identical - may cause SI=0\")\n",
        "    else:\n",
        "        print(\"   \\u2705 Heads are different: PASS\")\n",
        "\n",
        "    # Compute actual SI\n",
        "    print(\"\\n7. Computing baseline SI...\")\n",
        "    act = extract_head_activations(\n",
        "        model, tokenizer, [test_prompt], MAX_LENGTH,\n",
        "        use_chat_template=use_chat_template\n",
        "    )\n",
        "    ent = compute_head_entropy_profiles(act['attention_patterns'], act['valid_lengths'])\n",
        "    baseline_si, mean_corr = compute_si(ent)\n",
        "\n",
        "    entropy_min = ent.min()\n",
        "    entropy_max = ent.max()\n",
        "    entropy_mean = ent.mean()\n",
        "\n",
        "    print(f\"   Entropy range: [{entropy_min:.4f}, {entropy_max:.4f}]\")\n",
        "    print(f\"   Entropy mean: {entropy_mean:.4f}\")\n",
        "\n",
        "    print(\"\\n8. BASELINE SI:\")\n",
        "    print(f\"   Mean head correlation: {mean_corr:.4f}\")\n",
        "    print(f\"   Specialization Index: {baseline_si:.4f}\")\n",
        "\n",
        "    SI_THRESHOLD = 0.05\n",
        "    ok = baseline_si >= SI_THRESHOLD\n",
        "\n",
        "    # Cleanup\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return {\n",
        "        'ok': ok,\n",
        "        'label': label,\n",
        "        'use_chat_template': use_chat_template,\n",
        "        'dtype': str(dtype),\n",
        "        'valid_len': valid_len,\n",
        "        'num_layers': num_layers,\n",
        "        'num_heads': num_heads,\n",
        "        'attn_abs_mean': attn_abs_mean,\n",
        "        'attn_std': attn_std,\n",
        "        'entropy_range': [float(entropy_min), float(entropy_max)],\n",
        "        'baseline_si': float(baseline_si),\n",
        "        'mean_corr': float(mean_corr),\n",
        "        'heads_identical': bool(heads_identical)\n",
        "    }\n",
        "\n",
        "\n",
        "def run_sanity_check(model_config):\n",
        "    \"\"\"\n",
        "    Run sanity variants in order. On first pass, set SANITY_OVERRIDE.\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'#'*70}\")\n",
        "    print(f\"# SANITY CHECK: {model_config['display']}\")\n",
        "    print(f\"# KRANZ: 'Zeig mir die Daten. Ist das wirklich wahr?'\")\n",
        "    print(f\"{'#'*70}\")\n",
        "    \n",
        "    last = None\n",
        "    for v in SANITY_VARIANTS:\n",
        "        result = run_sanity_variant(\n",
        "            model_config,\n",
        "            use_chat_template=v['use_chat_template'],\n",
        "            dtype=v['dtype'],\n",
        "            label=v['label']\n",
        "        )\n",
        "        last = result\n",
        "        if result.get('ok'):\n",
        "            print(f\"\\n\\u2705 SANITY PASS: {v['label']} (SI={result['baseline_si']:.4f})\")\n",
        "            SANITY_OVERRIDE['use_chat_template'] = v['use_chat_template']\n",
        "            SANITY_OVERRIDE['dtype'] = v['dtype']\n",
        "            SANITY_OVERRIDE['label'] = v['label']\n",
        "            return result\n",
        "        else:\n",
        "            reason = result.get('reason', 'unknown')\n",
        "            si = result.get('baseline_si', 'n/a')\n",
        "            print(f\"\\n\\u274c SANITY FAIL: {v['label']} (reason={reason}, SI={si})\")\n",
        "\n",
        "    raise AssertionError(f\"ABORT: No sanity variant passed for {model_config['display']}! Last result: {last}\")\n",
        "\n",
        "# RUN SANITY CHECK on first model (Phi-1.5)\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"RUNNING SANITY CHECK ON PHI-1.5 BEFORE FULL EXPERIMENT\")\n",
        "print(\"=\"*70)\n",
        "sanity_result = run_sanity_check(MODEL_LADDER[0])\n",
        "print(f\"\\nSanity check passed with: {SANITY_OVERRIDE['label']}\")\n",
        "print(f\"Baseline SI: {sanity_result['baseline_si']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Measurement Function\n",
        "\n",
        "def measure_model_si(model_config, prompts, seeds):\n",
        "    \"\"\"Measure SI for a single model with multi-seed aggregation.\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Testing: {model_config['display']}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Use sanity-validated settings\n",
        "    active_dtype = SANITY_OVERRIDE['dtype'] or torch.bfloat16\n",
        "    active_chat = SANITY_OVERRIDE['use_chat_template'] if SANITY_OVERRIDE['use_chat_template'] is not None else model_config['use_chat_template']\n",
        "\n",
        "    # Load model\n",
        "    print(f\"Loading: {model_config['name']}\")\n",
        "    print(f\"  dtype: {active_dtype}\")\n",
        "    print(f\"  chat_template: {active_chat}\")\n",
        "    \n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_config['name'], trust_remote_code=True\n",
        "    )\n",
        "    \n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_config['name'],\n",
        "        torch_dtype=active_dtype,\n",
        "        device_map='auto',\n",
        "        trust_remote_code=True,\n",
        "        attn_implementation=\"eager\"\n",
        "    )\n",
        "    model.eval()\n",
        "    model.config.output_attentions = True\n",
        "    model.config.use_cache = False\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Architecture info\n",
        "    config = model.config\n",
        "    num_layers = config.num_hidden_layers\n",
        "    num_heads = config.num_attention_heads\n",
        "    num_kv_heads = getattr(config, 'num_key_value_heads', num_heads)\n",
        "    hidden_size = config.hidden_size\n",
        "    d_head = hidden_size // num_heads\n",
        "\n",
        "    rho_head = num_heads / math.sqrt(hidden_size)\n",
        "    rho_kv = num_kv_heads / num_layers\n",
        "\n",
        "    if num_kv_heads == num_heads:\n",
        "        arch = \"MHA\"\n",
        "    elif num_kv_heads == 1:\n",
        "        arch = \"MQA\"\n",
        "    else:\n",
        "        arch = f\"GQA ({num_heads}:{num_kv_heads})\"\n",
        "\n",
        "    print(f\"  Architecture: {arch}\")\n",
        "    print(f\"  Layers: {num_layers}, Heads: {num_heads}, KV: {num_kv_heads}\")\n",
        "    print(f\"  d_head: {d_head}\")\n",
        "    print(f\"  rho_head: {rho_head:.4f}, rho_kv: {rho_kv:.4f}\")\n",
        "\n",
        "    # Multi-seed SI measurement\n",
        "    si_values = []\n",
        "    corr_values = []\n",
        "\n",
        "    for seed in seeds:\n",
        "        set_seed(seed)\n",
        "        act = extract_head_activations(\n",
        "            model, tokenizer, prompts, MAX_LENGTH,\n",
        "            use_chat_template=active_chat\n",
        "        )\n",
        "        ent = compute_head_entropy_profiles(\n",
        "            act['attention_patterns'], act['valid_lengths']\n",
        "        )\n",
        "        si, corr = compute_si(ent)\n",
        "        si_values.append(si)\n",
        "        corr_values.append(corr)\n",
        "\n",
        "    si_mean = np.mean(si_values)\n",
        "    si_std = np.std(si_values)\n",
        "    corr_mean = np.mean(corr_values)\n",
        "\n",
        "    print(f\"  SI: {si_mean:.4f} +/- {si_std:.4f}\")\n",
        "    print(f\"  Correlation: {corr_mean:.4f}\")\n",
        "\n",
        "    # Cleanup\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return {\n",
        "        'model': model_config['name'],\n",
        "        'display': model_config['display'],\n",
        "        'size': model_config['size'],\n",
        "        'architecture': arch,\n",
        "        'num_layers': num_layers,\n",
        "        'num_heads': num_heads,\n",
        "        'num_kv_heads': num_kv_heads,\n",
        "        'hidden_size': hidden_size,\n",
        "        'd_head': d_head,\n",
        "        'rho_head': rho_head,\n",
        "        'rho_kv': rho_kv,\n",
        "        'sanity_override': {\n",
        "            'use_chat_template': active_chat,\n",
        "            'dtype': str(active_dtype),\n",
        "            'label': SANITY_OVERRIDE['label']\n",
        "        },\n",
        "        'si_mean': si_mean,\n",
        "        'si_std': si_std,\n",
        "        'si_values': si_values,\n",
        "        'corr_mean': corr_mean\n",
        "    }\n",
        "\n",
        "print(\"Test function loaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: Run All Models\n",
        "\n",
        "print(f\"\\n{'#'*70}\")\n",
        "print(f\"# E08-Phi-Size-Ladder: B11 Size Confound Validation\")\n",
        "print(f\"# Testing Phi-1.5 (1.3B) and Phi-2 (2.7B)\")\n",
        "print(f\"{'#'*70}\")\n",
        "\n",
        "all_results = []\n",
        "\n",
        "for model_config in MODEL_LADDER:\n",
        "    try:\n",
        "        result = measure_model_si(model_config, STANDARD_PROMPTS, SEEDS)\n",
        "        all_results.append(result)\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR on {model_config['display']}: {e}\")\n",
        "        all_results.append({\n",
        "            'model': model_config['name'],\n",
        "            'display': model_config['display'],\n",
        "            'size': model_config['size'],\n",
        "            'error': str(e)\n",
        "        })\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"ALL MODELS TESTED!\")\n",
        "print(f\"{'='*70}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6: Hypothesis Test - Size vs Heritage\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"B11 SIZE CONFOUND HYPOTHESIS TEST\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "print(f\"\\nH0 (Heritage): SI ~ 0.33 regardless of size (Textbook Quality effect)\")\n",
        "print(f\"H1 (Size): SI varies with size (smaller = different SI)\")\n",
        "print(f\"\\nTarget SI: {PHI3_SI_TARGET} +/- {SI_TOLERANCE}\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"COMPLETE PHI FAMILY SIZE LADDER\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "print(f\"\\n{'Model':<25} {'Size':<8} {'SI':<12} {'Within Target?':<15} {'Arch':<10}\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "# New results\n",
        "hypothesis_support = []\n",
        "for r in all_results:\n",
        "    if 'error' not in r:\n",
        "        within_target = abs(r['si_mean'] - PHI3_SI_TARGET) <= SI_TOLERANCE\n",
        "        status = '\\u2705 YES' if within_target else '\\u274c NO'\n",
        "        hypothesis_support.append(within_target)\n",
        "        print(f\"{r['display']:<25} {r['size']:<8} {r['si_mean']:.4f}+/-{r['si_std']:.3f}  {status:<15} {r['architecture']:<10}\")\n",
        "    else:\n",
        "        print(f\"{r['display']:<25} {r['size']:<8} ERROR: {r['error'][:30]}\")\n",
        "        hypothesis_support.append(None)\n",
        "\n",
        "# Reference Phi-3 values\n",
        "print(\"-\"*70)\n",
        "print(\"Reference (E08-Phi3):\")\n",
        "for name, data in PHI3_REFERENCE.items():\n",
        "    within = abs(data['si'] - PHI3_SI_TARGET) <= SI_TOLERANCE\n",
        "    hypothesis_support.append(within)\n",
        "    status = '\\u2705 YES' if within else '\\u274c NO'\n",
        "    print(f\"{name:<25} {data['size_b']:.1f}B    {data['si']:.4f}          {status:<15} {data['arch']:<10}\")\n",
        "\n",
        "# Verdict\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"VERDICT\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "valid_tests = len([h for h in hypothesis_support if h is not None])\n",
        "supporting = sum([h for h in hypothesis_support if h])\n",
        "\n",
        "print(f\"\\nModels within target: {supporting}/{valid_tests}\")\n",
        "\n",
        "if supporting == valid_tests and valid_tests >= 3:\n",
        "    print(f\"\\n>>> \\u2705 HERITAGE HYPOTHESIS CONFIRMED <<<\")\n",
        "    print(f\"    All {valid_tests} Phi models have SI ~ 0.33\")\n",
        "    print(f\"    Size does NOT affect SI in Microsoft heritage\")\n",
        "    print(f\"    B11 -> A-Tier RECOMMENDED\")\n",
        "elif supporting < valid_tests:\n",
        "    outliers = valid_tests - supporting\n",
        "    print(f\"\\n>>> \\u274c SIZE CONFOUND DETECTED <<<\")\n",
        "    print(f\"    {outliers}/{valid_tests} models outside target SI\")\n",
        "    print(f\"    Size IS a factor in SI variation\")\n",
        "    print(f\"    B11 stays B-Tier\")\n",
        "else:\n",
        "    print(f\"\\n>>> \\u26a0\\ufe0f INCONCLUSIVE <<<\")\n",
        "    print(f\"    Not enough data points\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 7: Visualization\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# Plot 1: Complete Phi Size Ladder\n",
        "ax1 = axes[0]\n",
        "\n",
        "# Combine new results with reference\n",
        "all_phi = []\n",
        "for r in all_results:\n",
        "    if 'error' not in r:\n",
        "        all_phi.append({'name': r['display'], 'size_b': float(r['size'].replace('B', '')), 'si': r['si_mean'], 'std': r['si_std']})\n",
        "\n",
        "for name, data in PHI3_REFERENCE.items():\n",
        "    all_phi.append({'name': name, 'size_b': data['size_b'], 'si': data['si'], 'std': 0})\n",
        "\n",
        "# Sort by size\n",
        "all_phi = sorted(all_phi, key=lambda x: x['size_b'])\n",
        "\n",
        "sizes = [f\"{p['size_b']}B\" for p in all_phi]\n",
        "si_vals = [p['si'] for p in all_phi]\n",
        "si_stds = [p.get('std', 0) for p in all_phi]\n",
        "\n",
        "colors = ['#9b59b6' if p['size_b'] < 3.8 else '#3498db' for p in all_phi]\n",
        "\n",
        "bars = ax1.bar(sizes, si_vals, yerr=si_stds, color=colors, alpha=0.8, capsize=5, edgecolor='black')\n",
        "\n",
        "# Target zone\n",
        "ax1.axhline(y=PHI3_SI_TARGET, color='green', linestyle='--', linewidth=2, label=f'Target SI = {PHI3_SI_TARGET}')\n",
        "ax1.axhspan(PHI3_SI_TARGET - SI_TOLERANCE, PHI3_SI_TARGET + SI_TOLERANCE, alpha=0.2, color='green', label=f'+/- {SI_TOLERANCE} tolerance')\n",
        "\n",
        "ax1.set_ylabel('Specialization Index (SI)', fontsize=12)\n",
        "ax1.set_xlabel('Model Size', fontsize=12)\n",
        "ax1.set_title('Microsoft Phi Family: Complete Size Ladder', fontsize=14, fontweight='bold')\n",
        "ax1.set_ylim(0, 1)\n",
        "ax1.legend(loc='upper right')\n",
        "\n",
        "for bar, si in zip(bars, si_vals):\n",
        "    ax1.annotate(f'{si:.3f}', xy=(bar.get_x() + bar.get_width()/2, si + 0.03),\n",
        "                 ha='center', fontweight='bold', fontsize=10)\n",
        "\n",
        "# Plot 2: Size vs SI (scatter)\n",
        "ax2 = axes[1]\n",
        "\n",
        "# All Phi models\n",
        "phi_sizes = [p['size_b'] for p in all_phi]\n",
        "phi_sis = [p['si'] for p in all_phi]\n",
        "ax2.scatter(phi_sizes, phi_sis, s=150, c='#9b59b6', marker='s', \n",
        "            edgecolors='black', linewidths=2, label='Microsoft Phi', zorder=5)\n",
        "\n",
        "# Fit line if enough points\n",
        "if len(phi_sizes) >= 3:\n",
        "    z = np.polyfit(phi_sizes, phi_sis, 1)\n",
        "    p = np.poly1d(z)\n",
        "    x_line = np.linspace(min(phi_sizes), max(phi_sizes), 100)\n",
        "    ax2.plot(x_line, p(x_line), 'r--', alpha=0.5, label=f'Trend (slope={z[0]:.4f})')\n",
        "\n",
        "# Target zone\n",
        "ax2.axhline(y=PHI3_SI_TARGET, color='purple', linestyle='--', linewidth=2, alpha=0.7)\n",
        "ax2.axhspan(PHI3_SI_TARGET - SI_TOLERANCE, PHI3_SI_TARGET + SI_TOLERANCE, alpha=0.1, color='purple')\n",
        "\n",
        "ax2.set_xlabel('Model Size (B)', fontsize=12)\n",
        "ax2.set_ylabel('Specialization Index (SI)', fontsize=12)\n",
        "ax2.set_title('Size vs SI: Testing Size Confound', fontsize=14, fontweight='bold')\n",
        "ax2.legend(loc='best', fontsize=9)\n",
        "ax2.set_xlim(0, 16)\n",
        "ax2.set_ylim(0, 1)\n",
        "\n",
        "plt.suptitle(f'E08-Phi-Size-Ladder: B11 Size Confound Validation\\nSeeds: {SEEDS}', fontsize=14, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "\n",
        "fig_path = f'../figures/E08_phi_size_ladder_{TIMESTAMP}.png'\n",
        "plt.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nFigure saved: {fig_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 8: Save Results\n",
        "\n",
        "def convert_to_native(obj):\n",
        "    if isinstance(obj, dict):\n",
        "        return {k: convert_to_native(v) for k, v in obj.items()}\n",
        "    elif isinstance(obj, list):\n",
        "        return [convert_to_native(v) for v in obj]\n",
        "    elif isinstance(obj, (np.bool_, np.integer)):\n",
        "        return int(obj)\n",
        "    elif isinstance(obj, np.floating):\n",
        "        return float(obj)\n",
        "    elif isinstance(obj, np.ndarray):\n",
        "        return obj.tolist()\n",
        "    else:\n",
        "        return obj\n",
        "\n",
        "filename = f'../results/E08_phi_size_ladder_{TIMESTAMP}.json'\n",
        "\n",
        "# Determine verdict\n",
        "valid_results = [r for r in all_results if 'error' not in r]\n",
        "all_within_target = all(\n",
        "    abs(r['si_mean'] - PHI3_SI_TARGET) <= SI_TOLERANCE \n",
        "    for r in valid_results\n",
        ") if valid_results else False\n",
        "\n",
        "# Include Phi-3 reference in verdict\n",
        "phi3_within = all(\n",
        "    abs(data['si'] - PHI3_SI_TARGET) <= SI_TOLERANCE\n",
        "    for data in PHI3_REFERENCE.values()\n",
        ")\n",
        "\n",
        "total_models = len(valid_results) + len(PHI3_REFERENCE)\n",
        "total_within = sum(1 for r in valid_results if abs(r['si_mean'] - PHI3_SI_TARGET) <= SI_TOLERANCE)\n",
        "total_within += sum(1 for data in PHI3_REFERENCE.values() if abs(data['si'] - PHI3_SI_TARGET) <= SI_TOLERANCE)\n",
        "\n",
        "if total_within == total_models and total_models >= 4:\n",
        "    verdict = \"HERITAGE_CONFIRMED\"\n",
        "    b11_recommendation = \"A-Tier\"\n",
        "elif total_within < total_models:\n",
        "    verdict = \"SIZE_CONFOUND_DETECTED\"\n",
        "    b11_recommendation = \"B-Tier (unchanged)\"\n",
        "else:\n",
        "    verdict = \"INCONCLUSIVE\"\n",
        "    b11_recommendation = \"B-Tier (unchanged)\"\n",
        "\n",
        "output = {\n",
        "    'experiment': 'E08-Phi-Size-Ladder',\n",
        "    'purpose': 'B11 Size Confound Validation (Phi-1.5, Phi-2)',\n",
        "    'timestamp': TIMESTAMP,\n",
        "    'hypothesis': {\n",
        "        'H0': 'Heritage: SI ~ 0.33 regardless of size',\n",
        "        'H1': 'Size: SI varies with model size',\n",
        "        'target_si': PHI3_SI_TARGET,\n",
        "        'tolerance': SI_TOLERANCE\n",
        "    },\n",
        "    'methodology': {\n",
        "        'standard': 'E11-v3',\n",
        "        'seeds': SEEDS,\n",
        "        'prompts': PROMPT_VERSION,\n",
        "        'prompt_md5': PROMPT_MD5,\n",
        "        'max_length': MAX_LENGTH,\n",
        "        'padding': False,\n",
        "        'use_cache': False,\n",
        "        'chat_template': False,\n",
        "        'dtype': 'bfloat16 (sanity-validated)',\n",
        "        'sanity_check': 'PASSED',\n",
        "        'sanity_override': {\n",
        "            'use_chat_template': SANITY_OVERRIDE.get('use_chat_template'),\n",
        "            'dtype': str(SANITY_OVERRIDE.get('dtype')) if SANITY_OVERRIDE.get('dtype') else None,\n",
        "            'label': SANITY_OVERRIDE.get('label')\n",
        "        }\n",
        "    },\n",
        "    'sanity_result': convert_to_native(sanity_result),\n",
        "    'results': convert_to_native(all_results),\n",
        "    'reference_phi3': PHI3_REFERENCE,\n",
        "    'verdict': {\n",
        "        'conclusion': verdict,\n",
        "        'b11_recommendation': b11_recommendation,\n",
        "        'models_tested': len(valid_results),\n",
        "        'total_models_with_reference': total_models,\n",
        "        'models_within_target': total_within,\n",
        "        'all_within_target': total_within == total_models\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(filename, 'w') as f:\n",
        "    json.dump(output, f, indent=2)\n",
        "\n",
        "print(f\"Results saved: {filename}\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"FINAL VERDICT: {verdict}\")\n",
        "print(f\"B11 Recommendation: {b11_recommendation}\")\n",
        "print(f\"Models within target: {total_within}/{total_models}\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "# Auto-download\n",
        "try:\n",
        "    from google.colab import files\n",
        "    import shutil\n",
        "    import os\n",
        "    os.makedirs('download', exist_ok=True)\n",
        "    shutil.copy(filename, 'download/')\n",
        "    shutil.copy(fig_path, 'download/')\n",
        "    shutil.make_archive(f'E08_phi_size_ladder_{TIMESTAMP}', 'zip', 'download')\n",
        "    files.download(f'E08_phi_size_ladder_{TIMESTAMP}.zip')\n",
        "except:\n",
        "    print('Not in Colab')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary: E08-Phi-Size-Ladder\n",
        "\n",
        "### KRANZ Validation\n",
        "\n",
        "| Check | Status |\n",
        "|-------|--------|\n",
        "| Sanity Check | **REQUIRED** (Cell 3b) |\n",
        "| Seeds | 42, 123, 456 |\n",
        "| Prompts | Standard-10 v3 (MD5 verified) |\n",
        "| padding | FALSE |\n",
        "| use_cache | FALSE |\n",
        "| dtype | sanity-validated |\n",
        "\n",
        "### Decision Rule\n",
        "\n",
        "| Outcome | Phi-1.5/Phi-2 SI | Conclusion |\n",
        "|---------|------------------|------------|\n",
        "| **Heritage Confirmed** | ~ 0.33 (+/- 0.05) | B11 -> A-Tier |\n",
        "| **Size Confound** | != 0.33 | B11 stays B-Tier |\n",
        "\n",
        "### Methodological Notes\n",
        "\n",
        "1. **Sanity Check is MANDATORY** - older Phi models may have different quirks\n",
        "2. **padding=False** - consistent with Phi-3 methodology\n",
        "3. **use_cache=False** - critical for Phi models\n",
        "4. **No chat template** - Phi-1.5 and Phi-2 are base models\n",
        "\n",
        "---\n",
        "\n",
        "*Paper 4: Behavioral Sink Dynamics*  \n",
        "*E08-Phi-Size-Ladder: B11 Size Confound Validation*  \n",
        "*KRANZ-validated methodology*"
      ]
    }
  ]
}
