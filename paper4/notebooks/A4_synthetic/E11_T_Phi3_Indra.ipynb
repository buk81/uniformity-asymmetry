{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E11-T-Phi3-Indra: State-Dependency on Microsoft Heritage (E11-v3 Standard)\n",
    "\n",
    "**Paper 4: Behavioral Sink Dynamics**\n",
    "\n",
    "## Purpose: A2 Claim Validation (3rd GQA Heritage)\n",
    "\n",
    "A2 (Indra State-Dependency) has evidence from:\n",
    "- GQA (LLaMA-3.1): Gap 59pp\n",
    "- MHA (LLaMA-2): Gap 138pp\n",
    "\n",
    "This notebook tests **Microsoft Phi-3** (3rd Heritage) for A2 generalization.\n",
    "\n",
    "## Methodology (E11-v3 Standard)\n",
    "\n",
    "| Standard | Implementation |\n",
    "|----------|----------------|\n",
    "| Seeds | 42, 123, 456 (3-seed aggregation) |\n",
    "| Noise Injection | **PRE-ATTENTION** (affects attention weights) |\n",
    "| SI Measurement | **GLOBAL + LOCAL** (region-isolated) |\n",
    "| Attention Mask | **YES** (excludes padding from entropy) |\n",
    "| Chat Template | **YES** for Instruct model |\n",
    "| dtype | **bfloat16** (stable on A100) |\n",
    "| Prompts | Standard-10 v3 (MD5: 715065ba) |\n",
    "\n",
    "## Phi-3 Architecture\n",
    "\n",
    "| Property | Phi-3-mini | Phi-3-small | Phi-3-medium |\n",
    "|----------|------------|-------------|---------------|\n",
    "| Params | 3.8B | 7B | 14B |\n",
    "| Attention | GQA | GQA | GQA |\n",
    "| Heritage | Microsoft (synthetic data) | Microsoft | Microsoft |\n",
    "| Training | Heavy textbook + synthetic | Same | Same |\n",
    "\n",
    "## Hypothesis\n",
    "\n",
    "**If A2 is universal:** Phi-3 Instruct should show DAMAGE under noise (healthy model).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup + Seeds (E11-v3 STANDARD)\n",
    "!pip install -q transformers torch accelerate scipy matplotlib seaborn\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from scipy.stats import entropy as scipy_entropy\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# === REPRODUCIBILITY SEEDS (E11-v3 STANDARD) ===\n",
    "SEEDS = [42, 123, 456]\n",
    "PRIMARY_SEED = 42\n",
    "\n",
    "def set_seed(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(PRIMARY_SEED)\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "Path('../results').mkdir(parents=True, exist_ok=True)\n",
    "Path('../figures').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"E11-T-Phi3-Indra (E11-v3 Standard)\")\n",
    "print(f\"Timestamp: {TIMESTAMP}\")\n",
    "print(f\"Seeds: {SEEDS}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Configuration (E11-v3 STANDARD)\n",
    "\n",
    "# === MODEL CONFIGURATION ===\n",
    "# Phi-3 uses GQA architecture\n",
    "# Note: Base models may not be publicly available - testing Instruct only\n",
    "\n",
    "MODEL_CONFIGS = {\n",
    "    'instruct': {\n",
    "        'name': 'microsoft/Phi-3-mini-4k-instruct',\n",
    "        'display': 'Phi-3-Mini-4K-Instruct',\n",
    "        'state': 'HEALTHY',  # Instruct models are considered HEALTHY\n",
    "        'expected_effect': 'DAMAGE',  # Healthy models should be DAMAGED by noise\n",
    "        'use_chat_template': True,\n",
    "        'params': '3.8B'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Reference values from other architectures\n",
    "E11T_REFERENCES = {\n",
    "    'gqa_llama31': {\n",
    "        'model': 'LLaMA-3.1-8B',\n",
    "        'collapsed_heal': 28.6,\n",
    "        'healthy_damage': -30.5,\n",
    "        'gap_pp': 59.1\n",
    "    },\n",
    "    'mha_llama2': {\n",
    "        'model': 'LLaMA-2-7B',\n",
    "        'collapsed_heal': 114.05,\n",
    "        'healthy_damage': -24.02,\n",
    "        'gap_pp': 138.08\n",
    "    }\n",
    "}\n",
    "\n",
    "# === E11-v3 STANDARD PARAMETERS ===\n",
    "NOISE_LEVELS = [0.0, 0.01, 0.02, 0.05, 0.1, 0.2]\n",
    "MAX_LENGTH = 128\n",
    "\n",
    "# Standard-10 v3 Prompts (MD5: 715065bab181f46bf12ed471951141e2)\n",
    "STANDARD_PROMPTS = [\n",
    "    \"What is the capital of France and what is its population?\",\n",
    "    \"If all roses are flowers and some flowers fade quickly, can we conclude that some roses fade quickly? Explain step by step.\",\n",
    "    \"Calculate 47 multiplied by 23 and show your work.\",\n",
    "    \"Translate the following to German: 'The quick brown fox jumps over the lazy dog'.\",\n",
    "    \"Write a Python function that checks if a number is prime.\",\n",
    "    \"Summarize the main points: Machine learning is a subset of artificial intelligence that enables systems to learn from data. It uses algorithms to identify patterns and make decisions with minimal human intervention.\",\n",
    "    \"Statement A: 'All birds can fly.' Statement B: 'Penguins are birds that cannot fly.' Are these statements contradictory? Explain.\",\n",
    "    \"What are the safety considerations when using a kitchen knife?\",\n",
    "    \"Write a haiku about artificial intelligence.\",\n",
    "    \"Complete this sentence in a helpful way: 'The best approach to solving complex problems is'\",\n",
    "]\n",
    "\n",
    "print(f\"\\nConfiguration (E11-v3 Standard):\")\n",
    "print(f\"  Seeds: {SEEDS}\")\n",
    "print(f\"  Noise levels: {NOISE_LEVELS}\")\n",
    "print(f\"  MAX_LENGTH: {MAX_LENGTH}\")\n",
    "print(f\"  Prompts: Standard-10 v3\")\n",
    "print(f\"\\nModel:\")\n",
    "for key, cfg in MODEL_CONFIGS.items():\n",
    "    print(f\"  {key}: {cfg['display']} ({cfg['params']})\")\n",
    "    print(f\"         State: {cfg['state']}, Expected: {cfg['expected_effect']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 3: Specialization Metrics (E11-v3 STANDARD - PHI-3 FIX)\n# =============================================================================\n# FIX: padding=False statt padding='max_length'\n# Phi-3 returns degenerate attention when padded heavily\n# =============================================================================\n\ndef extract_head_activations(model, tokenizer, prompts, max_length=128, use_chat_template=False):\n    \"\"\"\n    Extract per-head attention patterns WITH attention masks.\n    E11-v3 STANDARD: Attention mask excludes padding from entropy.\n    \n    PHI-3 FIX: Use padding=False to avoid degenerate attention patterns!\n    \"\"\"\n    all_attention_patterns = []\n    all_valid_lengths = []\n    \n    for prompt in prompts:\n        # === CHAT TEMPLATE (E11-v3 STANDARD) ===\n        if use_chat_template and hasattr(tokenizer, 'apply_chat_template'):\n            try:\n                messages = [{\"role\": \"user\", \"content\": prompt}]\n                formatted = tokenizer.apply_chat_template(\n                    messages, \n                    tokenize=False, \n                    add_generation_prompt=True\n                )\n            except Exception as e:\n                print(f\"Chat template failed: {e}, using raw prompt\")\n                formatted = prompt\n        else:\n            formatted = prompt\n        \n        # PHI-3 FIX: NO PADDING - use actual sequence length\n        inputs = tokenizer(\n            formatted, \n            return_tensors='pt',\n            max_length=max_length,\n            truncation=True,\n            padding=False  # CRITICAL FIX: No padding!\n        ).to(model.device)\n        \n        valid_len = inputs['input_ids'].shape[1]\n        \n        with torch.no_grad():\n            # CRITICAL: use_cache=False for Phi-3 (DynamicCache bug)\n            outputs = model(**inputs, output_attentions=True, use_cache=False)\n        \n        # Stack attention layers: [layers, heads, seq, seq]\n        attn_stack = torch.stack([a.squeeze(0) for a in outputs.attentions], dim=0)\n        all_attention_patterns.append(attn_stack.cpu())\n        all_valid_lengths.append(valid_len)\n    \n    return {\n        'attention_patterns': all_attention_patterns,\n        'valid_lengths': all_valid_lengths,\n        'num_layers': len(outputs.attentions),\n        'num_heads': outputs.attentions[0].shape[1]\n    }\n\n\ndef compute_head_entropy_profiles(attention_patterns, valid_lengths):\n    \"\"\"\n    Compute normalized entropy (E11-v3 STANDARD).\n    PHI-3 FIX: Use valid_lengths instead of attention_mask.\n    \"\"\"\n    num_prompts = len(attention_patterns)\n    num_layers = attention_patterns[0].shape[0]\n    num_heads = attention_patterns[0].shape[1]\n    \n    all_entropies = np.zeros((num_prompts, num_layers, num_heads))\n    \n    for p_idx, attn in enumerate(attention_patterns):\n        valid_len = valid_lengths[p_idx]\n        \n        for layer in range(num_layers):\n            for head in range(num_heads):\n                attn_weights = attn[layer, head].float().cpu().numpy()\n                \n                # Already correctly sized (no padding), but slice just in case\n                attn_weights = attn_weights[:valid_len, :valid_len]\n                \n                # Average across query positions\n                attn_weights = attn_weights.mean(axis=0)\n                \n                # Normalize\n                attn_weights = attn_weights / (attn_weights.sum() + 1e-10)\n                attn_weights = attn_weights[attn_weights > 0]\n                \n                if len(attn_weights) > 1:\n                    h = scipy_entropy(attn_weights, base=2)\n                    h_max = np.log2(len(attn_weights))\n                    h_norm = h / h_max if h_max > 0 else 0\n                else:\n                    h_norm = 0\n                \n                all_entropies[p_idx, layer, head] = h_norm\n    \n    return all_entropies.mean(axis=0)\n\n\ndef compute_si_global(head_entropies):\n    \"\"\"Compute GLOBAL SI (all layers).\"\"\"\n    num_layers, num_heads = head_entropies.shape\n    head_profiles = head_entropies.T\n    head_corr_matrix = np.corrcoef(head_profiles)\n    upper_tri = head_corr_matrix[np.triu_indices(num_heads, k=1)]\n    mean_head_correlation = float(np.nanmean(upper_tri))\n    \n    return {\n        'specialization_index': 1.0 - mean_head_correlation,\n        'mean_head_correlation': mean_head_correlation,\n        'method': 'GLOBAL'\n    }\n\n\ndef compute_si_local(head_entropies, layer_start, layer_end):\n    \"\"\"Compute REGION-LOCAL SI.\"\"\"\n    local_entropies = head_entropies[layer_start:layer_end, :]\n    local_layers, num_heads = local_entropies.shape\n    \n    if local_layers < 2:\n        return {\n            'specialization_index': 0.0,\n            'mean_head_correlation': 1.0,\n            'method': 'LOCAL',\n            'layer_range': [layer_start, layer_end]\n        }\n    \n    head_profiles = local_entropies.T\n    head_corr_matrix = np.corrcoef(head_profiles)\n    upper_tri = head_corr_matrix[np.triu_indices(num_heads, k=1)]\n    mean_head_correlation = float(np.nanmean(upper_tri))\n    \n    return {\n        'specialization_index': 1.0 - mean_head_correlation,\n        'mean_head_correlation': mean_head_correlation,\n        'method': 'LOCAL',\n        'layer_range': [layer_start, layer_end]\n    }\n\nprint(\"Specialization metrics loaded (E11-v3 Standard + PHI-3 FIX).\")\nprint(\"  - Attention mask: NO (using valid_lengths)\")\nprint(\"  - Padding: FALSE (critical Phi-3 fix!)\")\nprint(\"  - Chat template: Configurable\")\nprint(\"  - SI methods: GLOBAL + LOCAL\")\nprint(\"  - use_cache: FALSE (Phi-3 DynamicCache fix)\")"
  },
  {
   "cell_type": "code",
   "source": "# Cell 3b: SANITY CHECK (PHI-3 FIXED VERSION)\n# =============================================================================\n# Uses padding=False fix - should now pass!\n# =============================================================================\n\ndef run_sanity_check(model_name, use_chat_template=True):\n    \"\"\"\n    Sanity check with PHI-3 FIX: No padding.\n    \"\"\"\n    print(f\"\\n{'='*70}\")\n    print(f\"SANITY CHECK (PHI-3 FIX): {model_name}\")\n    print(f\"{'='*70}\")\n    \n    # Load model\n    print(\"\\n1. Loading model...\")\n    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        torch_dtype=torch.bfloat16,\n        device_map='auto',\n        trust_remote_code=True,\n        attn_implementation=\"eager\"\n    )\n    model.eval()\n    \n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    \n    # Single prompt test\n    test_prompt = \"What is the capital of France and what is its population?\"\n    print(f\"\\n2. Test prompt: '{test_prompt[:50]}...'\")\n    \n    # Format with chat template\n    if use_chat_template and hasattr(tokenizer, 'apply_chat_template'):\n        messages = [{\"role\": \"user\", \"content\": test_prompt}]\n        formatted = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n        print(f\"   Chat template applied: {len(formatted)} chars\")\n    else:\n        formatted = test_prompt\n        print(f\"   Raw prompt (no chat template)\")\n    \n    # PHI-3 FIX: NO PADDING\n    inputs = tokenizer(\n        formatted, \n        return_tensors='pt',\n        max_length=MAX_LENGTH,\n        truncation=True,\n        padding=False  # CRITICAL FIX!\n    ).to(model.device)\n    \n    valid_len = inputs['input_ids'].shape[1]\n    print(f\"\\n3. Tokenization (PHI-3 FIX):\")\n    print(f\"   Sequence length: {valid_len}\")\n    print(f\"   Padding: FALSE\")\n    \n    assert valid_len > 5, f\"ABORT: valid_len={valid_len} too small\"\n    print(f\"   ✅ valid_len > 5: PASS\")\n    \n    # Forward pass\n    print(f\"\\n4. Forward pass...\")\n    with torch.no_grad():\n        outputs = model(**inputs, output_attentions=True, use_cache=False)\n    \n    assert outputs.attentions is not None, \"ABORT: outputs.attentions is None\"\n    print(f\"   ✅ outputs.attentions exists: PASS\")\n    \n    # Attention diagnostics\n    attn_layer0 = outputs.attentions[0].squeeze(0)\n    num_layers = len(outputs.attentions)\n    num_heads = attn_layer0.shape[0]\n    \n    print(f\"\\n5. Attention diagnostics:\")\n    print(f\"   Num layers: {num_layers}\")\n    print(f\"   Num heads: {num_heads}\")\n    print(f\"   Layer 0 shape: {attn_layer0.shape}\")\n    \n    attn_abs_mean = attn_layer0.abs().mean().item()\n    attn_std = attn_layer0.std().item()\n    \n    print(f\"   attn.abs().mean() = {attn_abs_mean:.6f}\")\n    print(f\"   attn.std() = {attn_std:.6f}\")\n    \n    assert attn_abs_mean > 0, \"ABORT: attn.abs().mean() = 0\"\n    print(f\"   ✅ attn.abs().mean() > 0: PASS\")\n    \n    assert torch.isfinite(attn_layer0).all(), \"ABORT: attention contains NaN/Inf\"\n    print(f\"   ✅ torch.isfinite(attn): PASS\")\n    \n    # Head diversity check\n    head0 = attn_layer0[0]\n    head1 = attn_layer0[1]\n    heads_identical = torch.allclose(head0, head1, atol=1e-4)\n    \n    print(f\"\\n6. Head diversity check:\")\n    print(f\"   Head 0 vs Head 1 identical? {heads_identical}\")\n    if heads_identical:\n        print(f\"   ⚠️ WARNING: Heads identical\")\n    else:\n        print(f\"   ✅ Heads are different: PASS\")\n    \n    # Compute SI using fixed functions\n    print(f\"\\n7. Computing baseline SI (PHI-3 FIX)...\")\n    act = extract_head_activations(\n        model, tokenizer, [test_prompt], MAX_LENGTH,\n        use_chat_template=use_chat_template\n    )\n    ent = compute_head_entropy_profiles(act['attention_patterns'], act['valid_lengths'])\n    si_result = compute_si_global(ent)\n    baseline_si = si_result['specialization_index']\n    mean_corr = si_result['mean_head_correlation']\n    \n    entropy_min = ent.min()\n    entropy_max = ent.max()\n    \n    print(f\"   Entropy range: [{entropy_min:.4f}, {entropy_max:.4f}]\")\n    print(f\"\\n8. BASELINE SI:\")\n    print(f\"   Mean head correlation: {mean_corr:.4f}\")\n    print(f\"   Specialization Index: {baseline_si:.4f}\")\n    \n    SI_THRESHOLD = 0.05\n    if baseline_si < SI_THRESHOLD:\n        print(f\"\\n   ❌ SANITY CHECK FAILED!\")\n        print(f\"   baseline_si = {baseline_si:.4f} < {SI_THRESHOLD}\")\n        del model\n        torch.cuda.empty_cache()\n        raise AssertionError(f\"ABORT: baseline_si={baseline_si:.4f} < {SI_THRESHOLD}\")\n    \n    print(f\"\\n   ✅ baseline_si > {SI_THRESHOLD}: PASS\")\n    \n    del model\n    torch.cuda.empty_cache()\n    \n    print(f\"\\n{'='*70}\")\n    print(f\"SANITY CHECK PASSED! ✅\")\n    print(f\"{'='*70}\")\n    \n    return {\n        'valid_len': valid_len,\n        'attn_abs_mean': attn_abs_mean,\n        'attn_std': attn_std,\n        'entropy_range': [entropy_min, entropy_max],\n        'baseline_si': baseline_si,\n        'heads_identical': heads_identical\n    }\n\n# RUN SANITY CHECK\nprint(\"Running sanity check (PHI-3 FIX: padding=False)...\")\nsanity_result = run_sanity_check(\n    MODEL_CONFIGS['instruct']['name'],\n    use_chat_template=MODEL_CONFIGS['instruct']['use_chat_template']\n)\nprint(f\"\\nSanity check result: {sanity_result}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: PRE-ATTENTION Noise Injector (E11-v3 STANDARD)\n",
    "\n",
    "class PreAttentionNoiseInjector:\n",
    "    \"\"\"\n",
    "    E11-v3 STANDARD: Inject noise BEFORE attention computation.\n",
    "    This affects the attention weights (SI measurement target).\n",
    "    \n",
    "    CRITICAL: Phi-3 uses different layer structure!\n",
    "    We need to hook into the correct module.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, target_range, noise_std=0.0):\n",
    "        self.model = model\n",
    "        self.target_start, self.target_end = target_range\n",
    "        self.noise_std = noise_std\n",
    "        self.hooks = []\n",
    "        \n",
    "        # Detect model architecture\n",
    "        self.layer_path = self._detect_layer_path()\n",
    "    \n",
    "    def _detect_layer_path(self):\n",
    "        \"\"\"Detect the correct layer path for different model architectures.\"\"\"\n",
    "        # Try common paths\n",
    "        if hasattr(self.model, 'model') and hasattr(self.model.model, 'layers'):\n",
    "            return 'model.layers'  # LLaMA, Mistral, Phi-3\n",
    "        elif hasattr(self.model, 'transformer') and hasattr(self.model.transformer, 'h'):\n",
    "            return 'transformer.h'  # GPT-2, Falcon\n",
    "        elif hasattr(self.model, 'gpt_neox') and hasattr(self.model.gpt_neox, 'layers'):\n",
    "            return 'gpt_neox.layers'  # Pythia\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model architecture: {type(self.model)}\")\n",
    "    \n",
    "    def _get_layers(self):\n",
    "        \"\"\"Get the transformer layers.\"\"\"\n",
    "        if self.layer_path == 'model.layers':\n",
    "            return self.model.model.layers\n",
    "        elif self.layer_path == 'transformer.h':\n",
    "            return self.model.transformer.h\n",
    "        elif self.layer_path == 'gpt_neox.layers':\n",
    "            return self.model.gpt_neox.layers\n",
    "    \n",
    "    def _make_pre_hook(self, layer_idx):\n",
    "        \"\"\"Create forward PRE-hook (before attention).\"\"\"\n",
    "        def hook(module, args):\n",
    "            if self.noise_std > 0 and self.target_start <= layer_idx < self.target_end:\n",
    "                hidden_states = args[0]\n",
    "                noise = torch.randn_like(hidden_states) * self.noise_std\n",
    "                noisy_hidden_states = hidden_states + noise\n",
    "                return (noisy_hidden_states,) + args[1:]\n",
    "            return args\n",
    "        return hook\n",
    "    \n",
    "    def attach(self):\n",
    "        \"\"\"Attach PRE-hooks to transformer layers.\"\"\"\n",
    "        layers = self._get_layers()\n",
    "        for idx, layer in enumerate(layers):\n",
    "            hook = layer.register_forward_pre_hook(self._make_pre_hook(idx))\n",
    "            self.hooks.append(hook)\n",
    "    \n",
    "    def detach(self):\n",
    "        \"\"\"Remove all hooks.\"\"\"\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks = []\n",
    "\n",
    "print(\"PRE-Attention noise injector loaded (E11-v3 Standard).\")\n",
    "print(\"  - Injection point: BEFORE attention\")\n",
    "print(\"  - Architecture detection: Auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 5: Run Indra Test on Phi-3 (PHI-3 FIX: valid_lengths)\n\ndef run_indra_test(model_config, noise_levels, prompts, seeds):\n    \"\"\"\n    Run full Indra test with multi-seed aggregation (E11-v3 Standard).\n    PHI-3 FIX: Uses valid_lengths instead of attention_masks.\n    \"\"\"\n    print(f\"\\n{'='*70}\")\n    print(f\"TESTING: {model_config['display']}\")\n    print(f\"State: {model_config['state']}, Expected: {model_config['expected_effect']}\")\n    print(f\"{'='*70}\")\n    \n    # Load model with bfloat16 (E11-v3 STANDARD)\n    print(f\"\\nLoading: {model_config['name']}\")\n    tokenizer = AutoTokenizer.from_pretrained(\n        model_config['name'],\n        trust_remote_code=True\n    )\n    model = AutoModelForCausalLM.from_pretrained(\n        model_config['name'],\n        torch_dtype=torch.bfloat16,\n        device_map='auto',\n        trust_remote_code=True,\n        attn_implementation=\"eager\"\n    )\n    model.eval()\n    \n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    \n    # Architecture info\n    config = model.config\n    num_layers = config.num_hidden_layers\n    num_heads = config.num_attention_heads\n    num_kv_heads = getattr(config, 'num_key_value_heads', num_heads)\n    hidden_size = config.hidden_size\n    d_head = hidden_size // num_heads\n    \n    rho_head = num_heads / math.sqrt(hidden_size)\n    rho_kv = num_kv_heads / num_layers\n    \n    if num_kv_heads == num_heads:\n        arch_type = \"MHA\"\n    elif num_kv_heads == 1:\n        arch_type = \"MQA\"\n    else:\n        arch_type = f\"GQA ({num_heads}:{num_kv_heads})\"\n    \n    print(f\"  Architecture: {arch_type}\")\n    print(f\"  Layers: {num_layers}, Heads: {num_heads}, KV Heads: {num_kv_heads}\")\n    print(f\"  d_head: {d_head}, hidden_size: {hidden_size}\")\n    print(f\"  rho_head: {rho_head:.4f}, rho_kv: {rho_kv:.4f}\")\n    \n    # Layer ranges (thirds)\n    third = num_layers // 3\n    layer_ranges = {\n        'early': (0, third),\n        'middle': (third, 2*third),\n        'late': (2*third, num_layers),\n        'all': (0, num_layers)\n    }\n    \n    # Baseline measurement (PHI-3 FIX: valid_lengths)\n    print(f\"\\n  Measuring baseline (no noise)...\")\n    set_seed(PRIMARY_SEED)\n    baseline_act = extract_head_activations(\n        model, tokenizer, prompts, MAX_LENGTH,\n        use_chat_template=model_config['use_chat_template']\n    )\n    baseline_ent = compute_head_entropy_profiles(\n        baseline_act['attention_patterns'],\n        baseline_act['valid_lengths']  # PHI-3 FIX\n    )\n    baseline_global = compute_si_global(baseline_ent)\n    \n    baseline_local = {}\n    for region, (start, end) in layer_ranges.items():\n        baseline_local[region] = compute_si_local(baseline_ent, start, end)\n    \n    print(f\"  Baseline Global SI: {baseline_global['specialization_index']:.4f}\")\n    print(f\"  Baseline Correlation: {baseline_global['mean_head_correlation']:.4f}\")\n    \n    # CRITICAL: Assert baseline SI > 0.05\n    if baseline_global['specialization_index'] < 0.05:\n        raise RuntimeError(f\"Baseline SI too low: {baseline_global['specialization_index']:.4f} - measurement failure!\")\n    \n    # Multi-seed treatment loop\n    all_seed_results = {}\n    \n    for seed in seeds:\n        print(f\"\\n  Seed {seed}:\")\n        seed_results = {'global': [], 'local': []}\n        \n        for region_name, (start, end) in layer_ranges.items():\n            region_global = {'region': region_name, 'tests': []}\n            region_local = {'region': region_name, 'tests': []}\n            \n            for noise_std in noise_levels:\n                set_seed(seed)\n                \n                injector = PreAttentionNoiseInjector(model, (start, end), noise_std)\n                injector.attach()\n                \n                treated_act = extract_head_activations(\n                    model, tokenizer, prompts, MAX_LENGTH,\n                    use_chat_template=model_config['use_chat_template']\n                )\n                treated_ent = compute_head_entropy_profiles(\n                    treated_act['attention_patterns'],\n                    treated_act['valid_lengths']  # PHI-3 FIX\n                )\n                \n                injector.detach()\n                \n                # Global SI\n                treated_global = compute_si_global(treated_ent)\n                si_before = baseline_global['specialization_index']\n                si_after = treated_global['specialization_index']\n                change_pct = ((si_after - si_before) / si_before * 100) if si_before > 0 else 0\n                \n                region_global['tests'].append({\n                    'noise': noise_std,\n                    'si': si_after,\n                    'change_pct': change_pct\n                })\n                \n                # Local SI\n                treated_local = compute_si_local(treated_ent, start, end)\n                si_before_local = baseline_local[region_name]['specialization_index']\n                si_after_local = treated_local['specialization_index']\n                change_pct_local = ((si_after_local - si_before_local) / si_before_local * 100) if si_before_local > 0 else 0\n                \n                region_local['tests'].append({\n                    'noise': noise_std,\n                    'si': si_after_local,\n                    'change_pct': change_pct_local\n                })\n            \n            # Best/Worst effect based on expected outcome\n            if model_config['expected_effect'] == 'HEAL':\n                region_global['best'] = max(region_global['tests'], key=lambda x: x['change_pct'])\n                region_local['best'] = max(region_local['tests'], key=lambda x: x['change_pct'])\n            else:  # DAMAGE expected\n                region_global['best'] = min(region_global['tests'], key=lambda x: x['change_pct'])\n                region_local['best'] = min(region_local['tests'], key=lambda x: x['change_pct'])\n            \n            seed_results['global'].append(region_global)\n            seed_results['local'].append(region_local)\n            \n            if seed == PRIMARY_SEED:\n                print(f\"    {region_name}: Global={region_global['best']['change_pct']:+.1f}%, Local={region_local['best']['change_pct']:+.1f}%\")\n        \n        all_seed_results[str(seed)] = seed_results\n    \n    # Aggregate across seeds\n    aggregated = {'global': {}, 'local': {}}\n    for si_type in ['global', 'local']:\n        for region_name in layer_ranges.keys():\n            values = []\n            for seed in seeds:\n                region_data = next(r for r in all_seed_results[str(seed)][si_type] if r['region'] == region_name)\n                values.append(region_data['best']['change_pct'])\n            aggregated[si_type][region_name] = {\n                'mean': float(np.mean(values)),\n                'std': float(np.std(values)),\n                'values': values\n            }\n    \n    # Cleanup\n    del model\n    torch.cuda.empty_cache()\n    \n    return {\n        'model_name': model_config['name'],\n        'display_name': model_config['display'],\n        'state': model_config['state'],\n        'expected_effect': model_config['expected_effect'],\n        'architecture': arch_type,\n        'rho_head': rho_head,\n        'rho_kv': rho_kv,\n        'num_layers': num_layers,\n        'num_heads': num_heads,\n        'num_kv_heads': num_kv_heads,\n        'd_head': d_head,\n        'baseline_global': baseline_global,\n        'baseline_local': {k: v for k, v in baseline_local.items()},\n        'all_seed_results': all_seed_results,\n        'aggregated': aggregated,\n        'layer_ranges': layer_ranges\n    }\n\nprint(\"Test function loaded (E11-v3 Standard + PHI-3 FIX).\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Run Test\n",
    "\n",
    "print(f\"\\n{'#'*70}\")\n",
    "print(f\"# E11-T-Phi3-Indra: Microsoft Heritage Test\")\n",
    "print(f\"# State-Dependency on 3rd GQA Family (E11-v3 Standard)\")\n",
    "print(f\"{'#'*70}\")\n",
    "\n",
    "results = run_indra_test(\n",
    "    model_config=MODEL_CONFIGS['instruct'],\n",
    "    noise_levels=NOISE_LEVELS,\n",
    "    prompts=STANDARD_PROMPTS,\n",
    "    seeds=SEEDS\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"TEST COMPLETE!\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Verdict Analysis\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"PHI-3 STATE-DEPENDENCY VERDICT\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Thresholds\n",
    "DAMAGE_THRESHOLD = -5.0\n",
    "\n",
    "# Get worst effect (most negative = most damage)\n",
    "worst_global = min(results['aggregated']['global'].values(), key=lambda x: x['mean'])\n",
    "worst_region = [k for k, v in results['aggregated']['global'].items() if v['mean'] == worst_global['mean']][0]\n",
    "effect = worst_global['mean']\n",
    "\n",
    "print(f\"\\n[INSTRUCT] Phi-3-Mini ({results['state']})\")\n",
    "print(f\"  Baseline SI: {results['baseline_global']['specialization_index']:.4f}\")\n",
    "print(f\"  Architecture: {results['architecture']}\")\n",
    "print(f\"  rho_kv: {results['rho_kv']:.4f}\")\n",
    "print(f\"  Expected: DAMAGE (-SI)\")\n",
    "print(f\"  Actual Effect: {effect:+.2f}% at {worst_region}\")\n",
    "print(f\"  Seeds: {worst_global['values']}\")\n",
    "\n",
    "if effect < DAMAGE_THRESHOLD:\n",
    "    verdict = \"DAMAGED\"\n",
    "    a2_impact = \"A2 CONFIRMED on 3rd Heritage!\"\n",
    "elif effect > 5.0:\n",
    "    verdict = \"UNEXPECTED_HEAL\"\n",
    "    a2_impact = \"A2 PARTIAL - Phi-3 behaves like COLLAPSED!\"\n",
    "else:\n",
    "    verdict = \"NO_EFFECT\"\n",
    "    a2_impact = \"A2 NEUTRAL - Effect too small\"\n",
    "\n",
    "print(f\"\\n  VERDICT: {verdict}\")\n",
    "print(f\"  IMPACT: {a2_impact}\")\n",
    "\n",
    "# Cross-architecture comparison\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"CROSS-ARCHITECTURE COMPARISON (Healthy Models)\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\n{'Model':<25} {'Heritage':<15} {'Effect':<15} {'Region':<10}\")\n",
    "print(\"-\"*65)\n",
    "print(f\"{'LLaMA-3.1-8B-Instruct':<25} {'Meta':<15} {'-30.5%':<15} {'middle':<10}\")\n",
    "print(f\"{'LLaMA-2-7B-Chat':<25} {'Meta':<15} {'-24.0%':<15} {'middle':<10}\")\n",
    "print(f\"{'Phi-3-Mini-Instruct':<25} {'Microsoft':<15} {effect:+.1f}%{'':>10} {worst_region:<10}\")\n",
    "\n",
    "# Store verdict\n",
    "verdict_data = {\n",
    "    'verdict': verdict,\n",
    "    'effect': effect,\n",
    "    'region': worst_region,\n",
    "    'a2_impact': a2_impact,\n",
    "    'baseline_si': results['baseline_global']['specialization_index'],\n",
    "    'comparison': {\n",
    "        'llama31_instruct': -30.5,\n",
    "        'llama2_chat': -24.0,\n",
    "        'phi3_mini': effect\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Visualization\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "colors = {'early': '#3498db', 'middle': '#2ecc71', 'late': '#e74c3c', 'all': '#9b59b6'}\n",
    "\n",
    "# Plot 1: Phi-3 Region Effects\n",
    "ax1 = axes[0]\n",
    "regions = list(results['aggregated']['global'].keys())\n",
    "means = [results['aggregated']['global'][r]['mean'] for r in regions]\n",
    "stds = [results['aggregated']['global'][r]['std'] for r in regions]\n",
    "\n",
    "bars = ax1.bar(regions, means, yerr=stds, color=[colors[r] for r in regions], alpha=0.8, capsize=5)\n",
    "ax1.axhline(y=0, color='black', linestyle='-')\n",
    "ax1.axhline(y=-5, color='red', linestyle=':', alpha=0.7, label='-5% threshold')\n",
    "ax1.set_ylabel('SI Change %')\n",
    "ax1.set_title(f'Phi-3-Mini-Instruct (HEALTHY)\\nExpected: DAMAGE | Verdict: {verdict}')\n",
    "ax1.legend()\n",
    "\n",
    "for bar, val in zip(bars, means):\n",
    "    ax1.annotate(f'{val:+.1f}%', xy=(bar.get_x() + bar.get_width()/2, val),\n",
    "                 xytext=(0, 5 if val > 0 else -12), textcoords='offset points',\n",
    "                 ha='center', fontweight='bold')\n",
    "\n",
    "# Plot 2: Cross-Architecture Comparison\n",
    "ax2 = axes[1]\n",
    "models = ['LLaMA-3.1\\n(Meta)', 'LLaMA-2\\n(Meta)', 'Phi-3\\n(Microsoft)']\n",
    "effects = [-30.5, -24.0, effect]\n",
    "bar_colors = ['#e74c3c', '#e74c3c', '#e74c3c' if effect < 0 else '#2ecc71']\n",
    "\n",
    "bars = ax2.bar(models, effects, color=bar_colors, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "ax2.axhline(y=0, color='black', linestyle='-', linewidth=2)\n",
    "ax2.axhline(y=-5, color='red', linestyle=':', alpha=0.7)\n",
    "ax2.set_ylabel('SI Change % (Healthy Models)')\n",
    "ax2.set_title('Cross-Heritage: Healthy Model Damage')\n",
    "\n",
    "for bar, eff in zip(bars, effects):\n",
    "    ax2.annotate(f'{eff:+.1f}%', xy=(bar.get_x() + bar.get_width()/2, eff),\n",
    "                 xytext=(0, -20 if eff < 0 else 10), textcoords='offset points',\n",
    "                 ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.suptitle(f'E11-T-Phi3-Indra: Microsoft Heritage Test\\nSeeds: {SEEDS}', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "\n",
    "fig_path = f'../figures/E11T_phi3_indra_{TIMESTAMP}.png'\n",
    "plt.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFigure saved: {fig_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Save Results\n",
    "\n",
    "def convert_to_native(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_to_native(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_native(v) for v in obj]\n",
    "    elif isinstance(obj, (np.bool_, np.integer)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "filename = f'../results/E11T_phi3_indra_{TIMESTAMP}.json'\n",
    "\n",
    "output = {\n",
    "    'experiment': 'E11-T-Phi3-Indra',\n",
    "    'purpose': 'A2 State-Dependency on Microsoft Heritage (3rd GQA Family)',\n",
    "    'timestamp': TIMESTAMP,\n",
    "    'methodology': {\n",
    "        'standard': 'E11-v3',\n",
    "        'seeds': SEEDS,\n",
    "        'noise_injection': 'PRE-ATTENTION',\n",
    "        'si_measurement': 'GLOBAL + LOCAL',\n",
    "        'attention_mask': True,\n",
    "        'chat_template': True,\n",
    "        'dtype': 'bfloat16',\n",
    "        'prompts': 'Standard-10 v3',\n",
    "        'max_length': MAX_LENGTH\n",
    "    },\n",
    "    'references': E11T_REFERENCES,\n",
    "    'noise_levels': NOISE_LEVELS,\n",
    "    'num_prompts': len(STANDARD_PROMPTS),\n",
    "    'results': convert_to_native(results),\n",
    "    'verdict': convert_to_native(verdict_data)\n",
    "}\n",
    "\n",
    "with open(filename, 'w') as f:\n",
    "    json.dump(output, f, indent=2)\n",
    "\n",
    "print(f\"Results saved: {filename}\")\n",
    "\n",
    "# Auto-download\n",
    "try:\n",
    "    from google.colab import files\n",
    "    import shutil\n",
    "    import os\n",
    "    os.makedirs('download', exist_ok=True)\n",
    "    shutil.copy(filename, 'download/')\n",
    "    shutil.copy(fig_path, 'download/')\n",
    "    shutil.make_archive(f'E11T_phi3_indra_{TIMESTAMP}', 'zip', 'download')\n",
    "    files.download(f'E11T_phi3_indra_{TIMESTAMP}.zip')\n",
    "    print('Downloaded!')\n",
    "except:\n",
    "    print('Not in Colab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: E11-T-Phi3-Indra\n",
    "\n",
    "### Methodology (E11-v3 Standard)\n",
    "\n",
    "| Standard | Implementation |\n",
    "|----------|----------------|\n",
    "| Seeds | 42, 123, 456 |\n",
    "| Noise | PRE-ATTENTION |\n",
    "| SI | GLOBAL + LOCAL |\n",
    "| Mask | YES |\n",
    "| Chat Template | YES |\n",
    "| dtype | bfloat16 |\n",
    "| Prompts | Standard-10 v3 |\n",
    "\n",
    "### Expected Outcomes\n",
    "\n",
    "| Model | State | Expected | If Confirmed |\n",
    "|-------|-------|----------|---------------|\n",
    "| Phi-3-Mini-Instruct | HEALTHY | DAMAGE (-SI) | A2 on 3rd Heritage |\n",
    "\n",
    "### Cross-Architecture Target\n",
    "\n",
    "| Heritage | Model | Healthy Damage |\n",
    "|----------|-------|----------------|\n",
    "| Meta | LLaMA-3.1 | -30.5% |\n",
    "| Meta | LLaMA-2 | -24.0% |\n",
    "| **Microsoft** | **Phi-3** | **???** |\n",
    "\n",
    "---\n",
    "\n",
    "*Paper 4: Behavioral Sink Dynamics*  \n",
    "*E11-T-Phi3-Indra: Microsoft Heritage Test (E11-v3 Standard)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}