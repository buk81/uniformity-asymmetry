{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "A100"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell-0"
   },
   "source": [
    "# E04b: Heritage Expansion - MHA + MQA Architectures\n",
    "\n",
    "**Paper 4: Behavioral Sink Dynamics**\n",
    "\n",
    "## Purpose: Expand A3 Claim to 5 Families, 3 Architectures\n",
    "\n",
    "**Current A3 Evidence (3 families, 1 architecture):**\n",
    "\n",
    "| Family | Architecture | Early Î” | Status |\n",
    "|--------|-------------|---------|--------|\n",
    "| Gemma-27B | GQA | +150% | âœ… E11-Indra |\n",
    "| LLaMA-3.1-8B | GQA | +51% | âœ… E04-LLaMA31 |\n",
    "| Qwen2-7B | GQA | +117% | âœ… E04-Qwen |\n",
    "\n",
    "## Gap: All families use GQA!\n",
    "\n",
    "We need to test:\n",
    "1. **MHA (Multi-Head Attention):** LLaMA-2-7B\n",
    "2. **MQA (Multi-Query Attention):** Falcon-7B\n",
    "\n",
    "## Staged Approach (per Codex)\n",
    "\n",
    "**Phase 1: Screening**\n",
    "- 1 seed (42)\n",
    "- 3 noise levels (0.0, 0.05, 0.1)\n",
    "- Quick test to detect signal\n",
    "\n",
    "**Phase 2: Confirm (only if signal found)**\n",
    "- 3 seeds (42, 123, 456)\n",
    "- 6 noise levels (full range)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cell-1"
   },
   "source": [
    "# Cell 1: Setup + Seeds (E11-v3 STANDARD)\n",
    "!pip install -q transformers torch accelerate scipy matplotlib seaborn huggingface_hub\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from scipy.stats import entropy as scipy_entropy\n",
    "import json\n",
    "import hashlib\n",
    "import warnings\n",
    "import gc\n",
    "import traceback\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# ==============================================================================\n",
    "# E11-v3 STANDARD: 3-Seed Reproducibility\n",
    "# ==============================================================================\n",
    "SEEDS = [42, 123, 456]\n",
    "SEED = 42  # Initial seed for setup\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "Path('results').mkdir(parents=True, exist_ok=True)\n",
    "Path('figures').mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Timestamp: {TIMESTAMP}\")\n",
    "print(f\"E11-v3 Standard: Seeds {SEEDS}\")\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"VRAM: {vram_gb:.1f} GB\")\n",
    "\n",
    "# === HF LOGIN (Required for gated models like LLaMA-2) ===\n",
    "from huggingface_hub import login, HfFolder\n",
    "\n",
    "\n",
    "def get_hf_token():\n",
    "    token = None\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        token = userdata.get('HF_TOKEN')\n",
    "    except Exception:\n",
    "        pass\n",
    "    if not token:\n",
    "        token = os.environ.get('HF_TOKEN') or os.environ.get('HUGGINGFACE_TOKEN') or os.environ.get('HUGGING_FACE_HUB_TOKEN')\n",
    "    if not token:\n",
    "        token = HfFolder.get_token()\n",
    "    return token\n",
    "\n",
    "\n",
    "HF_TOKEN = get_hf_token()\n",
    "if HF_TOKEN:\n",
    "    try:\n",
    "        login(token=HF_TOKEN)\n",
    "        print(\"HF Login: SUCCESS (required for gated models)\")\n",
    "    except Exception as e:\n",
    "        print(f\"HF Login failed: {e}\")\n",
    "else:\n",
    "    print(\"WARNING: No HF_TOKEN found! LLaMA requires authentication.\")\n",
    "    print(\"Colab: Runtime -> Secrets -> Add HF_TOKEN\")\n",
    "    print(\"Local: run `huggingface-cli login` or set HF_TOKEN env var\")\n",
    "\n",
    "TOKEN_KWARGS = {'token': HF_TOKEN} if HF_TOKEN else {}\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cell-2"
   },
   "source": "# Cell 2: Configuration - Target Families (E11-v3 STANDARD)\n\n# ==============================================================================\n# E11-v3 STANDARD PARAMETERS\n# ==============================================================================\nMAX_LENGTH = 128\nDTYPE = torch.bfloat16  # E11-v3: bfloat16 (NOT float16!)\nUSE_CHAT_TEMPLATE_BASE = False  # Base models: no chat template\nUSE_CHAT_TEMPLATE_INSTRUCT = True  # Instruct models: use chat template if available\nEXPECTED_MD5 = \"715065bab181f46bf12ed471951141e2\"\nDELTA_PCT_EPSILON = 1e-6  # Prevent division by near-zero in Î”% calculation\n\n# ============================================================\n# TARGET MODELS - Architecture Diversity\n# ============================================================\n\nMODEL_PAIRS = {\n    'LLaMA-2': {\n        'base': 'meta-llama/Llama-2-7b-hf',\n        'instruct': 'meta-llama/Llama-2-7b-chat-hf',\n        'expected_arch': 'MHA',\n        'family': 'LLaMA-2',\n        'vendor': 'Meta',\n        'params_B': 7,\n        'hypothesis': 'MHA may have different fragility signature than GQA'\n    },\n    'Falcon': {\n        'base': 'tiiuae/falcon-7b',\n        'instruct': 'tiiuae/falcon-7b-instruct',\n        'expected_arch': 'MQA',\n        'family': 'Falcon',\n        'vendor': 'TII',\n        'params_B': 7,\n        'hypothesis': 'MQA has different head correlation dynamics'\n    }\n}\n\n# Phase 1: Screening (quick)\nPHASE1_CONFIG = {\n    'seeds': [42],  # Single seed\n    'noise_levels': [0.0, 0.05, 0.1],  # 3 levels\n    'prompts_subset': 5  # First 5 prompts only\n}\n\n# Phase 2: Confirmation (E11-v3 compliant)\nPHASE2_CONFIG = {\n    'seeds': SEEDS,  # 3-seed E11-v3 standard\n    'noise_levels': [0.0, 0.01, 0.02, 0.05, 0.1, 0.2],\n    'prompts_subset': 10  # All 10 prompts\n}\n\n# Use Phase 2 by default for E11-v3 compliance\nACTIVE_CONFIG = PHASE2_CONFIG\n\n# ==============================================================================\n# CANONICAL Standard-10 Prompt Set (MD5: 715065bab181f46bf12ed471951141e2)\n# ==============================================================================\nSTANDARD_PROMPTS = [\n    \"What is the capital of France and what is its population?\",\n    \"If all roses are flowers and some flowers fade quickly, can we conclude that some roses fade quickly? Explain step by step.\",\n    \"Calculate 47 multiplied by 23 and show your work.\",\n    \"Translate the following to German: 'The quick brown fox jumps over the lazy dog'.\",\n    \"Write a Python function that checks if a number is prime.\",\n    \"Summarize the main points: Machine learning is a subset of artificial intelligence that enables systems to learn from data. It uses algorithms to identify patterns and make decisions with minimal human intervention.\",\n    \"Statement A: 'All birds can fly.' Statement B: 'Penguins are birds that cannot fly.' Are these statements contradictory? Explain.\",\n    \"What are the safety considerations when using a kitchen knife?\",\n    \"Write a haiku about artificial intelligence.\",\n    \"Complete this sentence in a helpful way: 'The best approach to solving complex problems is'\",\n]\n\n# ==============================================================================\n# E11-v3 PROMPT VERIFICATION\n# ==============================================================================\ndef verify_prompts():\n    \"\"\"Verify Standard-10 prompts haven't been modified.\"\"\"\n    prompt_string = '|||'.join(STANDARD_PROMPTS)  # Canonical delimiter for MD5\n    actual_md5 = hashlib.md5(prompt_string.encode()).hexdigest()\n    return actual_md5, actual_md5 == EXPECTED_MD5\n\nactual_md5, prompts_ok = verify_prompts()\nprint(f\"E11-v3 Prompt Verification:\")\nprint(f\"  Expected MD5: {EXPECTED_MD5}\")\nprint(f\"  Actual MD5:   {actual_md5}\")\nprint(f\"  Status:       {'âœ“ VERIFIED' if prompts_ok else 'âœ— MISMATCH - STOP!'}\")\n\nif not prompts_ok:\n    raise ValueError(f\"Prompt MD5 mismatch! Expected {EXPECTED_MD5}, got {actual_md5}\")\n\nprint(f\"\\nE04b: Heritage Expansion - MHA + MQA (E11-v3)\")\nprint(f\"\\n{'='*60}\")\nprint(f\"TARGET FAMILIES:\")\nfor name, info in MODEL_PAIRS.items():\n    print(f\"  {name}: {info['expected_arch']} ({info['vendor']})\")\nprint(f\"\\nACTIVE PHASE: {'Phase 1 (Screening)' if ACTIVE_CONFIG == PHASE1_CONFIG else 'Phase 2 (E11-v3 Full)'}\")\nprint(f\"  Seeds: {ACTIVE_CONFIG['seeds']}\")\nprint(f\"  Noise levels: {ACTIVE_CONFIG['noise_levels']}\")\nprint(f\"  Prompts: {ACTIVE_CONFIG['prompts_subset']}/10\")\nprint(f\"{'='*60}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cell-3"
   },
   "source": [
    "# Cell 3: Fragility Metrics (FIXED v3 - use_cache=False for Falcon compatibility)\n",
    "\n",
    "def compute_repetition_score(model, tokenizer, prompts, max_length=128, use_chat_template=False):\n",
    "    \"\"\"\n",
    "    Compute repetition score (proxy for degradation).\n",
    "    Higher = more repetitive = more degraded.\n",
    "\n",
    "    FIXED v3: padding + attention_mask for stable input length.\n",
    "    use_cache=False to avoid Falcon KV cache errors.\n",
    "    \"\"\"\n",
    "    total_rep = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    for prompt in prompts:\n",
    "        if use_chat_template and hasattr(tokenizer, 'apply_chat_template'):\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "            try:\n",
    "                formatted = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "            except Exception:\n",
    "                formatted = prompt  # Fallback\n",
    "        else:\n",
    "            formatted = prompt\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            formatted,\n",
    "            return_tensors='pt',\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length'\n",
    "        )\n",
    "        if 'input_ids' not in inputs or inputs['input_ids'] is None:\n",
    "            raise ValueError('Tokenizer returned no input_ids')\n",
    "        inputs = inputs.to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_length,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,\n",
    "                use_cache=False  # CRITICAL: Disable KV cache for Falcon compatibility\n",
    "            )\n",
    "\n",
    "        if outputs is None:\n",
    "            raise RuntimeError('Generation returned None')\n",
    "\n",
    "        input_len = inputs['input_ids'].shape[1]\n",
    "        if input_len == 0:\n",
    "            continue\n",
    "\n",
    "        generated = outputs[0][input_len:]\n",
    "        tokens = generated.tolist()\n",
    "\n",
    "        if len(tokens) > 1:\n",
    "            reps = sum(1 for i in range(1, len(tokens)) if tokens[i] == tokens[i-1])\n",
    "            total_rep += reps\n",
    "            total_tokens += len(tokens) - 1\n",
    "\n",
    "    return total_rep / total_tokens if total_tokens > 0 else 0\n",
    "\n",
    "\n",
    "def compute_fragility_curve(model, tokenizer, prompts, noise_levels, layer_range, \n",
    "                           max_length=128, use_chat_template=False, seed=42):\n",
    "    \"\"\"\n",
    "    Compute fragility curve: repetition score vs noise level.\n",
    "    Returns fragility score (slope of degradation).\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    \n",
    "    for noise_std in noise_levels:\n",
    "        # Set seed\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        \n",
    "        if noise_std > 0:\n",
    "            injector = AttentionNoiseInjector(model, layer_range, noise_std)\n",
    "            injector.attach()\n",
    "        \n",
    "        rep_score = compute_repetition_score(model, tokenizer, prompts, max_length, use_chat_template)\n",
    "        scores.append((noise_std, rep_score))\n",
    "        \n",
    "        if noise_std > 0:\n",
    "            injector.detach()\n",
    "    \n",
    "    # Compute fragility as slope\n",
    "    if len(scores) >= 2:\n",
    "        x = np.array([s[0] for s in scores])\n",
    "        y = np.array([s[1] for s in scores])\n",
    "        if np.std(x) > 0:\n",
    "            fragility = np.polyfit(x, y, 1)[0]\n",
    "        else:\n",
    "            fragility = 0\n",
    "    else:\n",
    "        fragility = 0\n",
    "    \n",
    "    return {\n",
    "        'curve': scores,\n",
    "        'fragility': float(fragility)\n",
    "    }\n",
    "\n",
    "print(\"Fragility metrics loaded (FIXED v3: padding + use_cache=False for Falcon)\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cell-4"
   },
   "source": [
    "# Cell 4: Layer-Targeted Noise Injector\n",
    "\n",
    "class AttentionNoiseInjector:\n",
    "    \"\"\"\n",
    "    Inject Gaussian noise into attention outputs of SPECIFIC layer ranges.\n",
    "    Supports: LLaMA (model.layers), Falcon/GPT-2 (transformer.h), NeoX (gpt_neox.layers)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, target_range, noise_std=0.0):\n",
    "        self.model = model\n",
    "        self.target_start, self.target_end = target_range\n",
    "        self.noise_std = noise_std\n",
    "        self.hooks = []\n",
    "\n",
    "    def _get_layers(self):\n",
    "        if hasattr(self.model, 'model') and hasattr(self.model.model, 'layers'):\n",
    "            return self.model.model.layers\n",
    "        if hasattr(self.model, 'transformer') and hasattr(self.model.transformer, 'h'):\n",
    "            return self.model.transformer.h\n",
    "        if hasattr(self.model, 'gpt_neox') and hasattr(self.model.gpt_neox, 'layers'):\n",
    "            return self.model.gpt_neox.layers\n",
    "        raise ValueError('Unsupported model architecture for noise injection')\n",
    "\n",
    "    def _get_attn_module(self, layer):\n",
    "        return (\n",
    "            getattr(layer, 'self_attn', None)\n",
    "            or getattr(layer, 'self_attention', None)\n",
    "            or getattr(layer, 'attn', None)\n",
    "            or getattr(layer, 'attention', None)\n",
    "        )\n",
    "\n",
    "    def _make_hook(self, layer_idx):\n",
    "        def hook(module, input, output):\n",
    "            if self.noise_std > 0 and self.target_start <= layer_idx < self.target_end:\n",
    "                if isinstance(output, tuple):\n",
    "                    attn_output = output[0]\n",
    "                    noise = torch.randn_like(attn_output) * self.noise_std\n",
    "                    return (attn_output + noise,) + output[1:]\n",
    "                else:\n",
    "                    noise = torch.randn_like(output) * self.noise_std\n",
    "                    return output + noise\n",
    "            return output\n",
    "        return hook\n",
    "\n",
    "    def attach(self):\n",
    "        layers = self._get_layers()\n",
    "        for idx, layer in enumerate(layers):\n",
    "            attn = self._get_attn_module(layer)\n",
    "            if attn is None:\n",
    "                continue\n",
    "            hook = attn.register_forward_hook(self._make_hook(idx))\n",
    "            self.hooks.append(hook)\n",
    "        if not self.hooks:\n",
    "            raise RuntimeError('No attention modules found to hook')\n",
    "\n",
    "    def detach(self):\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks = []\n",
    "\n",
    "    def set_noise(self, std):\n",
    "        self.noise_std = std\n",
    "\n",
    "print(\"Attention noise injector ready.\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cell-5"
   },
   "source": "# Cell 5: Twin Test Function (E11-v3 compliant)\n\ndef load_model_safe(model_name):\n    \"\"\"Load model with bf16â†’fp16 fallback. Returns (model, dtype_str) or raises.\"\"\"\n    # Try bf16 first (E11-v3 standard, optimal for A100/H100)\n    try:\n        model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            **TOKEN_KWARGS,\n            torch_dtype=DTYPE,  # E11-v3: bfloat16\n            device_map='auto',\n            trust_remote_code=True,\n            attn_implementation=\"eager\"\n        )\n        return model, 'bfloat16'\n    except Exception as e:\n        print(f\"    bf16 failed: {e}\")\n        print(f\"    Trying fp16 fallback...\")\n    \n    # Fallback to fp16\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        **TOKEN_KWARGS,\n        torch_dtype=torch.float16,\n        device_map='auto',\n        trust_remote_code=True,\n        attn_implementation=\"eager\"\n    )\n    return model, 'float16'\n\n\ndef detect_architecture(config):\n    \"\"\"\n    Detect attention architecture from model config.\n    Returns: (arch_name, num_kv_heads)\n\n    FIXED v4: multi_query flag has PRIORITY (Falcon-specific).\n    Falcon configs may have num_key_value_heads=71 but multi_query=True,\n    which means it's actually MQA with 1 shared KV head.\n    \"\"\"\n    num_query_heads = config.num_attention_heads\n\n    # Method 1: Check multi_query flag FIRST (Falcon uses this!)\n    # CRITICAL: This must be checked before num_key_value_heads!\n    multi_query = getattr(config, 'multi_query', False)\n\n    # Method 2: Check explicit num_key_value_heads (LLaMA, Qwen, etc.)\n    num_kv_heads = getattr(config, 'num_key_value_heads', None)\n\n    # Method 3: Check n_head_kv (older Falcon configs)\n    n_head_kv = getattr(config, 'n_head_kv', None)\n\n    # Determine actual KV heads - multi_query has PRIORITY!\n    if multi_query:\n        # Falcon MQA: multi_query=true means KV heads = 1\n        # This takes precedence over any num_key_value_heads setting\n        effective_kv_heads = 1\n    elif num_kv_heads is not None:\n        effective_kv_heads = num_kv_heads\n    elif n_head_kv is not None:\n        effective_kv_heads = n_head_kv\n    else:\n        # Default: same as query heads (MHA)\n        effective_kv_heads = num_query_heads\n\n    # Classify architecture\n    if effective_kv_heads == num_query_heads:\n        arch = \"MHA\"\n    elif effective_kv_heads == 1:\n        arch = \"MQA\"\n    else:\n        arch = f\"GQA ({num_query_heads}:{effective_kv_heads})\"\n\n    return arch, effective_kv_heads\n\n\ndef run_twin_test(model_pair_info, config, prompts):\n    \"\"\"\n    Run complete twin test for a model pair.\n    Returns results dict with fragility measurements.\n    E11-v3 compliant: 3-seed averaging when config uses SEEDS.\n    \"\"\"\n    family_name = model_pair_info['family']\n    base_name = model_pair_info['base']\n    instruct_name = model_pair_info['instruct']\n    \n    print(f\"\\n{'#'*70}\")\n    print(f\"TESTING: {family_name} ({model_pair_info['expected_arch']})\")\n    print(f\"{'#'*70}\")\n    \n    # Initialize result structure\n    results = {\n        'family': family_name,\n        'vendor': model_pair_info['vendor'],\n        'expected_arch': model_pair_info['expected_arch'],\n        'base_model': base_name,\n        'instruct_model': instruct_name,\n        'base': {'seed_results': {}},\n        'instruct': {'seed_results': {}},\n        'status': 'running',\n        'detected_arch': 'unknown',\n        'model_config': {},\n        'layer_ranges': {}\n    }\n    \n    # Initialize variables\n    base_model = None\n    base_tokenizer = None\n    instruct_model = None\n    instruct_tokenizer = None\n    used_dtype = 'unknown'\n    \n    try:\n        # === LOAD BASE MODEL ===\n        print(f\"\\nLoading BASE: {base_name}\")\n        base_tokenizer = AutoTokenizer.from_pretrained(base_name, trust_remote_code=True, **TOKEN_KWARGS)\n        base_model, used_dtype = load_model_safe(base_name)\n        base_model.eval()\n        print(f\"  Loaded with dtype: {used_dtype}\")\n        \n        if base_tokenizer.pad_token is None:\n            base_tokenizer.pad_token = base_tokenizer.eos_token\n        \n        # Architecture detection\n        cfg = base_model.config\n        num_layers = cfg.num_hidden_layers\n        num_query_heads = cfg.num_attention_heads\n        hidden_size = cfg.hidden_size\n        d_head = hidden_size // num_query_heads\n        rho = num_query_heads / (hidden_size ** 0.5)\n        \n        detected_arch, effective_kv_heads = detect_architecture(cfg)\n        \n        results['detected_arch'] = detected_arch\n        results['model_config'] = {\n            'num_layers': num_layers,\n            'num_query_heads': num_query_heads,\n            'num_kv_heads': effective_kv_heads,\n            'd_head': d_head,\n            'rho': float(rho),\n            'dtype': used_dtype,\n            'multi_query_flag': getattr(cfg, 'multi_query', False)\n        }\n        \n        print(f\"  Detected: {detected_arch} (L={num_layers}, H={num_query_heads}, KV={effective_kv_heads})\")\n        print(f\"  Ï = {rho:.4f}\")\n        \n        # Layer ranges\n        third = num_layers // 3\n        layer_ranges = {\n            'early': (0, third),\n            'middle': (third, 2*third),\n            'late': (2*third, num_layers)\n        }\n        results['layer_ranges'] = {k: list(v) for k, v in layer_ranges.items()}\n        \n        # === TEST BASE MODEL ===\n        print(f\"\\n  Testing BASE fragility ({len(config['seeds'])} seeds)...\")\n        base_seed_results = {}\n        for seed in config['seeds']:\n            seed_results = {}\n            for region_name, layer_range in layer_ranges.items():\n                result = compute_fragility_curve(\n                    base_model, base_tokenizer, prompts[:config['prompts_subset']],\n                    config['noise_levels'], layer_range,\n                    MAX_LENGTH, USE_CHAT_TEMPLATE_BASE, seed\n                )\n                seed_results[region_name] = result\n            base_seed_results[seed] = seed_results\n            print(f\"    Seed {seed}: Early={seed_results['early']['fragility']:+.3f}, Mid={seed_results['middle']['fragility']:+.3f}, Late={seed_results['late']['fragility']:+.3f}\")\n        \n        results['base']['seed_results'] = base_seed_results\n        \n        # Aggregate base\n        for region_name in layer_ranges.keys():\n            frags = [base_seed_results[s][region_name]['fragility'] for s in config['seeds']]\n            results['base'][region_name] = {\n                'mean': float(np.mean(frags)),\n                'std': float(np.std(frags)) if len(frags) > 1 else 0.0\n            }\n        \n        # === UNLOAD BASE, LOAD INSTRUCT ===\n        print(f\"\\n  Unloading BASE, loading INSTRUCT...\")\n        del base_model\n        del base_tokenizer\n        base_model = None\n        base_tokenizer = None\n        gc.collect()\n        torch.cuda.empty_cache()\n        \n        print(f\"  Loading INSTRUCT: {instruct_name}\")\n        instruct_tokenizer = AutoTokenizer.from_pretrained(instruct_name, trust_remote_code=True, **TOKEN_KWARGS)\n        instruct_model, instruct_dtype = load_model_safe(instruct_name)\n        instruct_model.eval()\n        print(f\"  Loaded with dtype: {instruct_dtype}\")\n        \n        if instruct_tokenizer.pad_token is None:\n            instruct_tokenizer.pad_token = instruct_tokenizer.eos_token\n        \n        # === TEST INSTRUCT MODEL ===\n        print(f\"\\n  Testing INSTRUCT fragility ({len(config['seeds'])} seeds)...\")\n        instruct_seed_results = {}\n        for seed in config['seeds']:\n            seed_results = {}\n            for region_name, layer_range in layer_ranges.items():\n                result = compute_fragility_curve(\n                    instruct_model, instruct_tokenizer, prompts[:config['prompts_subset']],\n                    config['noise_levels'], layer_range,\n                    MAX_LENGTH, USE_CHAT_TEMPLATE_INSTRUCT, seed\n                )\n                seed_results[region_name] = result\n            instruct_seed_results[seed] = seed_results\n            print(f\"    Seed {seed}: Early={seed_results['early']['fragility']:+.3f}, Mid={seed_results['middle']['fragility']:+.3f}, Late={seed_results['late']['fragility']:+.3f}\")\n        \n        results['instruct']['seed_results'] = instruct_seed_results\n        \n        # Aggregate instruct\n        for region_name in layer_ranges.keys():\n            frags = [instruct_seed_results[s][region_name]['fragility'] for s in config['seeds']]\n            results['instruct'][region_name] = {\n                'mean': float(np.mean(frags)),\n                'std': float(np.std(frags)) if len(frags) > 1 else 0.0\n            }\n        \n        # === COMPUTE DELTA ===\n        print(f\"\\n  Computing Heritage Damage...\")\n        deltas = {}\n        for region_name in layer_ranges.keys():\n            base_frag = results['base'][region_name]['mean']\n            inst_frag = results['instruct'][region_name]['mean']\n            delta = inst_frag - base_frag\n            \n            if abs(base_frag) < DELTA_PCT_EPSILON:\n                delta_pct = float('nan')\n                pct_status = \"(baseâ‰ˆ0, Î”% undefined)\"\n            else:\n                delta_pct = (delta / abs(base_frag)) * 100\n                pct_status = f\"({delta_pct:+.1f}%)\"\n            \n            deltas[region_name] = {\n                'absolute': float(delta),\n                'percent': float(delta_pct),\n                'base_near_zero': abs(base_frag) < DELTA_PCT_EPSILON\n            }\n            print(f\"    {region_name}: Î”={delta:+.3f} {pct_status}\")\n        \n        results['deltas'] = deltas\n        \n        # === HERITAGE VERDICT ===\n        def get_verdict_value(delta_info):\n            if np.isnan(delta_info['percent']):\n                return delta_info['absolute'] * 100\n            return delta_info['percent']\n        \n        early_v = get_verdict_value(deltas['early'])\n        middle_v = get_verdict_value(deltas['middle'])\n        late_v = get_verdict_value(deltas['late'])\n        \n        if early_v > 30 and abs(middle_v) < 20 and abs(late_v) < 20:\n            verdict = \"HERITAGE_DAMAGED_EARLY\"\n            verdict_detail = \"RLHF damages Early layers, Middle/Late immune (A3 pattern confirmed)\"\n        elif early_v > 30:\n            verdict = \"HERITAGE_DAMAGED\"\n            verdict_detail = \"RLHF damages Early layers\"\n        elif abs(early_v) < 20:\n            verdict = \"HERITAGE_PROTECTED\"\n            verdict_detail = \"Heritage protects against RLHF damage\"\n        else:\n            verdict = \"HERITAGE_PARTIAL\"\n            verdict_detail = \"Mixed results\"\n        \n        results['verdict'] = verdict\n        results['verdict_detail'] = verdict_detail\n        results['status'] = 'success'\n        \n        print(f\"\\n  VERDICT: {verdict}\")\n        print(f\"  {verdict_detail}\")\n        \n    except Exception as e:\n        results['status'] = 'error'\n        results['error'] = str(e)\n        results['traceback'] = traceback.format_exc()\n        print(f\"\\n  ERROR: {e}\")\n        print(traceback.format_exc())\n    \n    finally:\n        if base_model is not None:\n            del base_model\n        if base_tokenizer is not None:\n            del base_tokenizer\n        if instruct_model is not None:\n            del instruct_model\n        if instruct_tokenizer is not None:\n            del instruct_tokenizer\n        gc.collect()\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n    \n    return results\n\nprint(\"Twin test function defined (E11-v3 compliant, FIXED v4: Falcon MQA detection)\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cell-6"
   },
   "source": [
    "# Cell 6: Run Phase 1 - LLaMA-2 (MHA)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"PHASE 1: SCREENING - LLaMA-2 (MHA)\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "llama2_results = run_twin_test(\n",
    "    MODEL_PAIRS['LLaMA-2'],\n",
    "    ACTIVE_CONFIG,\n",
    "    STANDARD_PROMPTS\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"LLaMA-2 TEST COMPLETE\")\n",
    "print(f\"{'='*70}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cell-7"
   },
   "source": [
    "# Cell 7: Run Phase 1 - Falcon (MQA)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"PHASE 1: SCREENING - Falcon (MQA)\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "falcon_results = run_twin_test(\n",
    "    MODEL_PAIRS['Falcon'],\n",
    "    ACTIVE_CONFIG,\n",
    "    STANDARD_PROMPTS\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Falcon TEST COMPLETE\")\n",
    "print(f\"{'='*70}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cell-8"
   },
   "source": [
    "# Cell 8: Analysis - Cross-Architecture Summary\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"E04b RESULTS: CROSS-ARCHITECTURE HERITAGE ANALYSIS\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "all_results = {\n",
    "    'LLaMA-2': llama2_results,\n",
    "    'Falcon': falcon_results\n",
    "}\n",
    "\n",
    "# Summary table\n",
    "print(f\"\\n\" + \"-\"*90)\n",
    "print(f\"{'Family':<12} {'Arch':<8} {'Early Î”':<12} {'Middle Î”':<12} {'Late Î”':<12} {'Verdict':<20}\")\n",
    "print(\"-\"*90)\n",
    "\n",
    "for family_name, res in all_results.items():\n",
    "    if res['status'] == 'success':\n",
    "        arch = res['detected_arch']\n",
    "        early = f\"{res['deltas']['early']['percent']:+.1f}%\"\n",
    "        middle = f\"{res['deltas']['middle']['percent']:+.1f}%\"\n",
    "        late = f\"{res['deltas']['late']['percent']:+.1f}%\"\n",
    "        verdict = res['verdict']\n",
    "    else:\n",
    "        arch = res.get('expected_arch', '?')\n",
    "        early = middle = late = 'ERROR'\n",
    "        verdict = res.get('error', 'Unknown error')[:20]\n",
    "    \n",
    "    print(f\"{family_name:<12} {arch:<8} {early:<12} {middle:<12} {late:<12} {verdict:<20}\")\n",
    "\n",
    "print(\"-\"*90)\n",
    "\n",
    "# Compare with prior GQA results\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"COMPARISON WITH PRIOR GQA RESULTS (A3 Claim)\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "prior_gqa = {\n",
    "    'Gemma-27B': {'arch': 'GQA', 'early': '+150%', 'verdict': 'DAMAGED_EARLY'},\n",
    "    'LLaMA-3.1-8B': {'arch': 'GQA', 'early': '+51%', 'verdict': 'DAMAGED_EARLY'},\n",
    "    'Qwen2-7B': {'arch': 'GQA', 'early': '+117%', 'verdict': 'DAMAGED_EARLY'}\n",
    "}\n",
    "\n",
    "print(f\"\\n\" + \"-\"*70)\n",
    "print(f\"{'Family':<15} {'Arch':<8} {'Early Î”':<12} {'Source':<15}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for family, info in prior_gqa.items():\n",
    "    print(f\"{family:<15} {info['arch']:<8} {info['early']:<12} Prior (E04/E11)\")\n",
    "\n",
    "for family_name, res in all_results.items():\n",
    "    if res['status'] == 'success':\n",
    "        arch = res['detected_arch']\n",
    "        early = f\"{res['deltas']['early']['percent']:+.1f}%\"\n",
    "        print(f\"{family_name:<15} {arch:<8} {early:<12} NEW (E04b)\")\n",
    "\n",
    "print(\"-\"*70)\n",
    "\n",
    "# A3 upgrade check\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"A3 CLAIM UPGRADE CHECK\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "a3_pattern_count = 0\n",
    "total_tested = 0\n",
    "\n",
    "for family_name, res in all_results.items():\n",
    "    if res['status'] == 'success':\n",
    "        total_tested += 1\n",
    "        if 'DAMAGED_EARLY' in res['verdict'] or 'DAMAGED' in res['verdict']:\n",
    "            a3_pattern_count += 1\n",
    "            print(f\"  {family_name}: âœ… A3 pattern (Early damaged)\")\n",
    "        elif 'PROTECTED' in res['verdict']:\n",
    "            print(f\"  {family_name}: âŒ A3 refuted (Heritage protects)\")\n",
    "        else:\n",
    "            print(f\"  {family_name}: âš ï¸ Unclear ({res['verdict']})\")\n",
    "\n",
    "if total_tested > 0:\n",
    "    print(f\"\\nA3 Pattern: {a3_pattern_count}/{total_tested} new families\")\n",
    "    if a3_pattern_count == total_tested:\n",
    "        print(f\"\\nðŸŽ¯ A3 CLAIM STRENGTHENED: Now 5 families, 3 architectures!\")\n",
    "    elif a3_pattern_count > 0:\n",
    "        print(f\"\\nâš ï¸ A3 CLAIM PARTIALLY EXTENDED: {a3_pattern_count} new families confirm pattern\")\n",
    "    else:\n",
    "        print(f\"\\nâŒ A3 CLAIM NOT EXTENDED: No new families show pattern\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cell-9"
   },
   "source": [
    "# Cell 9: Visualization\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "families = list(all_results.keys())\n",
    "colors = {'early': '#e74c3c', 'middle': '#3498db', 'late': '#2ecc71'}\n",
    "\n",
    "# Plot 1: Early Layer Fragility Comparison\n",
    "ax1 = axes[0]\n",
    "early_deltas = []\n",
    "family_labels = []\n",
    "for family_name, res in all_results.items():\n",
    "    if res['status'] == 'success':\n",
    "        early_deltas.append(res['deltas']['early']['percent'])\n",
    "        family_labels.append(f\"{family_name}\\n({res['detected_arch']})\")\n",
    "\n",
    "if early_deltas:\n",
    "    bars = ax1.bar(family_labels, early_deltas, color='#e74c3c', alpha=0.7)\n",
    "    ax1.axhline(y=0, color='black', linestyle='--')\n",
    "    ax1.axhline(y=30, color='orange', linestyle=':', label='A3 threshold (30%)')\n",
    "    ax1.set_ylabel('Early Layer Î” (%)')\n",
    "    ax1.set_title('Early Layer Fragility Change\\n(Instruct vs Base)')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, val in zip(bars, early_deltas):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2, \n",
    "                f'{val:+.0f}%', ha='center', va='bottom')\n",
    "\n",
    "# Plot 2: All Regions Comparison\n",
    "ax2 = axes[1]\n",
    "x = np.arange(len(family_labels))\n",
    "width = 0.25\n",
    "\n",
    "for i, (family_name, res) in enumerate(all_results.items()):\n",
    "    if res['status'] == 'success':\n",
    "        if i == 0:\n",
    "            early_vals = [res['deltas']['early']['percent']]\n",
    "            mid_vals = [res['deltas']['middle']['percent']]\n",
    "            late_vals = [res['deltas']['late']['percent']]\n",
    "        else:\n",
    "            early_vals.append(res['deltas']['early']['percent'])\n",
    "            mid_vals.append(res['deltas']['middle']['percent'])\n",
    "            late_vals.append(res['deltas']['late']['percent'])\n",
    "\n",
    "if family_labels:\n",
    "    ax2.bar(x - width, early_vals, width, label='Early', color=colors['early'], alpha=0.7)\n",
    "    ax2.bar(x, mid_vals, width, label='Middle', color=colors['middle'], alpha=0.7)\n",
    "    ax2.bar(x + width, late_vals, width, label='Late', color=colors['late'], alpha=0.7)\n",
    "    ax2.axhline(y=0, color='black', linestyle='--')\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(family_labels)\n",
    "    ax2.set_ylabel('Î” Fragility (%)')\n",
    "    ax2.set_title('Layer-Wise Fragility Change\\nby Architecture')\n",
    "    ax2.legend()\n",
    "\n",
    "# Plot 3: Architecture Comparison (including prior GQA)\n",
    "ax3 = axes[2]\n",
    "\n",
    "# Combine prior GQA with new results\n",
    "all_families = ['Gemma-27B', 'LLaMA-3.1-8B', 'Qwen2-7B']\n",
    "all_early = [150, 51, 117]  # Prior GQA results\n",
    "all_archs = ['GQA', 'GQA', 'GQA']\n",
    "\n",
    "for family_name, res in all_results.items():\n",
    "    if res['status'] == 'success':\n",
    "        all_families.append(family_name)\n",
    "        all_early.append(res['deltas']['early']['percent'])\n",
    "        all_archs.append(res['detected_arch'])\n",
    "\n",
    "# Color by architecture\n",
    "arch_colors = {'GQA': '#3498db', 'MHA': '#e74c3c', 'MQA': '#2ecc71'}\n",
    "bar_colors = [arch_colors.get(a, '#95a5a6') for a in all_archs]\n",
    "\n",
    "bars = ax3.bar(range(len(all_families)), all_early, color=bar_colors, alpha=0.7)\n",
    "ax3.axhline(y=0, color='black', linestyle='--')\n",
    "ax3.axhline(y=30, color='orange', linestyle=':', label='A3 threshold')\n",
    "ax3.set_xticks(range(len(all_families)))\n",
    "ax3.set_xticklabels(all_families, rotation=45, ha='right')\n",
    "ax3.set_ylabel('Early Layer Î” (%)')\n",
    "ax3.set_title('Cross-Architecture Comparison\\n(All 5 Families)')\n",
    "\n",
    "# Add architecture legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor=c, label=a, alpha=0.7) for a, c in arch_colors.items()]\n",
    "ax3.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "fig_path = f'figures/E04b_heritage_expansion_{TIMESTAMP}.png'\n",
    "plt.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFigure saved: {fig_path}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cell-10"
   },
   "source": [
    "# Cell 10: Save Results\n",
    "\n",
    "def convert_to_native(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_to_native(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_native(v) for v in obj]\n",
    "    elif isinstance(obj, tuple):\n",
    "        return tuple(convert_to_native(v) for v in obj)\n",
    "    elif isinstance(obj, (np.bool_, np.integer)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "filename = f'results/E04b_heritage_expansion_{TIMESTAMP}.json'\n",
    "\n",
    "# Count successful tests\n",
    "success_count = sum(1 for r in all_results.values() if r['status'] == 'success')\n",
    "a3_count = sum(1 for r in all_results.values() if r['status'] == 'success' and 'DAMAGED' in r.get('verdict', ''))\n",
    "\n",
    "output = {\n",
    "    'experiment': 'E04b-Heritage-Expansion',\n",
    "    'purpose': 'Extend A3 claim to MHA + MQA architectures',\n",
    "    'timestamp': TIMESTAMP,\n",
    "    \n",
    "    # E11-v3 Methodology Block (REQUIRED)\n",
    "    'methodology': {\n",
    "        'standard': 'E11-v3',\n",
    "        'seeds': ACTIVE_CONFIG['seeds'],\n",
    "        'max_length': MAX_LENGTH,\n",
    "        'dtype': str(DTYPE),\n",
    "        'prompt_md5': actual_md5,\n",
    "        'prompt_md5_verified': prompts_ok,\n",
    "        'num_prompts': ACTIVE_CONFIG['prompts_subset'],\n",
    "        'total_prompts': len(STANDARD_PROMPTS),\n",
    "        'prompt_set': 'Standard-10',\n",
    "        'noise_levels': ACTIVE_CONFIG['noise_levels'],\n",
    "        'use_chat_template_base': USE_CHAT_TEMPLATE_BASE,\n",
    "        'use_chat_template_instruct': USE_CHAT_TEMPLATE_INSTRUCT,\n",
    "        'attention_masked': True\n",
    "    },\n",
    "    \n",
    "    'phase': 'Phase 1 (Screening)' if ACTIVE_CONFIG == PHASE1_CONFIG else 'Phase 2 (E11-v3 Full)',\n",
    "    'model_pairs': {k: {'base': v['base'], 'instruct': v['instruct'], 'expected_arch': v['expected_arch']} \n",
    "                   for k, v in MODEL_PAIRS.items()},\n",
    "    'results': convert_to_native(all_results),\n",
    "    'summary': {\n",
    "        'total_tested': len(all_results),\n",
    "        'successful': success_count,\n",
    "        'a3_pattern_found': a3_count,\n",
    "        'architectures_tested': list(set(r.get('detected_arch', r.get('expected_arch', '?')) \n",
    "                                        for r in all_results.values())),\n",
    "        'conclusion': 'A3 extended' if a3_count == success_count and success_count > 0 else \n",
    "                     'Partial extension' if a3_count > 0 else 'No extension'\n",
    "    },\n",
    "    'prior_gqa_reference': {\n",
    "        'Gemma-27B': {'early_delta': '+150%', 'source': 'E11-Indra'},\n",
    "        'LLaMA-3.1-8B': {'early_delta': '+51%', 'source': 'E04-LLaMA31'},\n",
    "        'Qwen2-7B': {'early_delta': '+117%', 'source': 'E04-Qwen'}\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(filename, 'w') as f:\n",
    "    json.dump(output, f, indent=2)\n",
    "\n",
    "print(f\"Results saved: {filename}\")\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"  Total tested: {output['summary']['total_tested']}\")\n",
    "print(f\"  Successful: {output['summary']['successful']}\")\n",
    "print(f\"  A3 pattern: {output['summary']['a3_pattern_found']}\")\n",
    "print(f\"  Conclusion: {output['summary']['conclusion']}\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cell-11"
   },
   "source": [
    "# Cell 11: Auto-Download Results\n",
    "\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "def auto_download_results():\n",
    "    try:\n",
    "        from google.colab import files\n",
    "    except ImportError:\n",
    "        print('Not in Colab - skipping auto-download')\n",
    "        return\n",
    "    \n",
    "    print('=' * 60)\n",
    "    print('AUTO-DOWNLOADING RESULTS...')\n",
    "    print('=' * 60)\n",
    "    \n",
    "    json_files = glob.glob('results/*.json') + glob.glob('figures/*.json')\n",
    "    png_files = glob.glob('results/*.png') + glob.glob('figures/*.png')\n",
    "    all_files = json_files + png_files\n",
    "    \n",
    "    if not all_files:\n",
    "        print('WARNING: No result files found!')\n",
    "        return\n",
    "    \n",
    "    print(f'Found {len(all_files)} files')\n",
    "    \n",
    "    import os\n",
    "    zip_name = f'E04b_heritage_expansion_results_{TIMESTAMP}'\n",
    "    \n",
    "    os.makedirs('download_package', exist_ok=True)\n",
    "    for f in all_files:\n",
    "        shutil.copy(f, 'download_package/')\n",
    "    \n",
    "    shutil.make_archive(zip_name, 'zip', 'download_package')\n",
    "    print(f'Downloading: {zip_name}.zip')\n",
    "    files.download(f'{zip_name}.zip')\n",
    "    print('DOWNLOAD COMPLETE!')\n",
    "\n",
    "auto_download_results()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cell-12"
   },
   "source": [
    "---\n",
    "\n",
    "## Summary: E04b Heritage Expansion\n",
    "\n",
    "### Purpose\n",
    "Extend A3 claim (Heritage > Scale) from 3 GQA families to 5 families across 3 architectures.\n",
    "\n",
    "### New Architectures Tested\n",
    "\n",
    "| Architecture | Family | What it tests |\n",
    "|-------------|--------|---------------|\n",
    "| **MHA** | LLaMA-2-7B | Classic attention (all heads independent) |\n",
    "| **MQA** | Falcon-7B | Single KV head (extreme sharing) |\n",
    "\n",
    "### Expected Outcomes\n",
    "\n",
    "If A3 pattern (Early damaged, Middle/Late immune) holds:\n",
    "- **A3 upgraded to A-Tier** with 5 families, 3 architectures\n",
    "- Pattern is **architecture-invariant** (fundamental to RLHF)\n",
    "\n",
    "If pattern breaks:\n",
    "- A3 becomes **architecture-conditional**\n",
    "- MHA/MQA may have different fragility dynamics\n",
    "\n",
    "### Phase Progression\n",
    "\n",
    "1. **Phase 1 (This notebook):** Quick screening (1 seed, 3 noise levels)\n",
    "2. **Phase 2 (If signal found):** Full validation (3 seeds, 6 noise levels)\n",
    "\n",
    "---\n",
    "\n",
    "*Paper 4: Behavioral Sink Dynamics*  \n",
    "*E04b: Heritage Expansion*"
   ]
  }
 ]
}