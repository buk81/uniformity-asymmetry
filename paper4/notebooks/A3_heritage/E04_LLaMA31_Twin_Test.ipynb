{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# E04: LLaMA-3.1 Twin Test - GQA RLHF Validation\n", "\n", "**Paper 4: Digital Overcrowding (Behavioral Sink Hypothesis)**\n", "\n", "## Critical Question\n", "> Does GQA architecture protect against RLHF-induced fragility?\n", "\n", "## Context from Previous Experiments\n", "\n", "| Experiment | Finding | Implication |\n", "|------------|---------|-------------|\n", "| E04 Mistral (MHA) | Delta = +0.799 | RLHF CREATES fragility in MHA |\n", "| E03 TinyLlama (GQA) | Fragility = -0.262 | GQA is intrinsically ANTIFRAGILE |\n", "| E06c TinyLlama (GQA) | Baseline = -0.751 | GQA \"already healthy\" |\n", "\n", "## This Experiment Tests\n", "\n", "**LLaMA-3.1-8B Base vs LLaMA-3.1-8B-Instruct (GQA 4:1)**\n", "\n", "Possible outcomes:\n", "1. **If Instruct >> Base fragility**: GQA NOT protective, RLHF damages all architectures\n", "2. **If Instruct \u2248 Base fragility**: GQA IS protective, buffers RLHF damage\n", "3. **If Instruct << Base fragility**: ???, would need explanation\n", "\n", "---"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Cell 1: Setup + E11-v3 Standard\n!pip install -q transformers torch accelerate bitsandbytes scipy matplotlib huggingface_hub\n\nimport os\nimport torch\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom scipy.stats import entropy as scipy_entropy\nfrom scipy.stats import linregress\nimport json\nimport hashlib\nfrom pathlib import Path\nfrom datetime import datetime\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# ============ E11-v3 METHODOLOGY STANDARD ============\nSEEDS = [42, 123, 456]  # 3-seed averaging\nDTYPE = torch.bfloat16  # Standardized precision\nEXPECTED_MD5 = \"715065bab181f46bf12ed471951141e2\"  # Standard-10 v3\n\ndef verify_prompts(prompts):\n    \"\"\"Verify Standard-10 prompts via MD5.\"\"\"\n    combined = '|||'.join(prompts)  # Canonical delimiter for MD5\n    actual_md5 = hashlib.md5(combined.encode()).hexdigest()\n    verified = actual_md5 == EXPECTED_MD5\n    print(f\"  Prompt MD5: {actual_md5}\")\n    print(f\"  Expected:   {EXPECTED_MD5}\")\n    print(f\"  Verified:   {'\u2713' if verified else '\u2717 MISMATCH!'}\")\n    return verified, actual_md5\n\n# Reproducibility\nos.environ['PYTHONHASHSEED'] = '42'\nrandom.seed(42)\nnp.random.seed(42)\ntorch.manual_seed(42)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(42)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nTIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')\nPath('results').mkdir(parents=True, exist_ok=True)\nPath('figures').mkdir(parents=True, exist_ok=True)\nprint(f'Timestamp: {TIMESTAMP}')\nprint(f\"E11-v3 Standard: Seeds={SEEDS}, dtype={DTYPE}\")\n\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n\n# HF Login for gated models\nfrom huggingface_hub import login, HfFolder\n\ndef get_hf_token():\n    token = None\n    try:\n        from google.colab import userdata\n        token = userdata.get('HF_TOKEN')\n    except Exception:\n        pass\n    if not token:\n        token = os.environ.get('HF_TOKEN') or HfFolder.get_token()\n    return token\n\nHF_TOKEN = get_hf_token()\nif HF_TOKEN:\n    try:\n        login(token=HF_TOKEN)\n        print(\"HF Login: SUCCESS\")\n    except Exception as e:\n        print(f\"HF Login failed: {e}\")\nelse:\n    print(\"WARNING: No HF_TOKEN found! LLaMA requires authentication.\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Cell 2: LLaMA-3.1 Configuration (E11-v3 Standard)\n\n# Model configuration\nMODEL_BASE = 'meta-llama/Llama-3.1-8B'\nMODEL_INSTRUCT = 'meta-llama/Llama-3.1-8B-Instruct'\nMODEL_PARAMS = '8B'\nGQA_RATIO = '4:1'  # 8 KV heads : 32 Q heads\n\n# Reference from previous experiments (MHA baselines)\nMISTRAL_BASE_FRAGILITY = -0.8609\nMISTRAL_INST_FRAGILITY = -0.0616\nMISTRAL_DELTA = +0.799\n\nTINYLLAMA_GQA_FRAGILITY = -0.262\nPYTHIA_1B_MHA_FRAGILITY = +0.53\n\n# Phenotype thresholds\nPROBER_THRESHOLD = 0.85\nRIGID_THRESHOLD = 0.20\n\n# Noise levels for fragility test\nNOISE_LEVELS = [0.0, 0.01, 0.02, 0.05, 0.1, 0.2]\n\n# Tokenization (E11-v3 Standard)\nMAX_LENGTH = 128\n\n# ============ CANONICAL Standard-10 v3 Prompts ============\n# MD5: 715065bab181f46bf12ed471951141e2\nSTANDARD_PROMPTS = [\n    \"What is the capital of France and what is its population?\",\n    \"If all roses are flowers and some flowers fade quickly, can we conclude that some roses fade quickly? Explain step by step.\",\n    \"Calculate 47 multiplied by 23 and show your work.\",\n    \"Translate the following to German: 'The quick brown fox jumps over the lazy dog'.\",\n    \"Write a Python function that checks if a number is prime.\",\n    \"Summarize the main points: Machine learning is a subset of artificial intelligence that enables systems to learn from data. It uses algorithms to identify patterns and make decisions with minimal human intervention.\",\n    \"Statement A: 'All birds can fly.' Statement B: 'Penguins are birds that cannot fly.' Are these statements contradictory? Explain.\",\n    \"What are the safety considerations when using a kitchen knife?\",\n    \"Write a haiku about artificial intelligence.\",\n    \"Complete this sentence in a helpful way: 'The best approach to solving complex problems is'\",\n]\n\n# Verify prompts (E11-v3 Standard)\nprint(\"Verifying Standard-10 prompts...\")\nPROMPTS_VERIFIED, ACTUAL_MD5 = verify_prompts(STANDARD_PROMPTS)\nif not PROMPTS_VERIFIED:\n    raise ValueError(\"PROMPT MISMATCH! Check Standard-10 v3 canonical prompts.\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"E04 LLaMA-3.1 Twin Test: GQA RLHF Validation\")\nprint(\"=\"*60)\nprint(f\"\\nBase:     {MODEL_BASE}\")\nprint(f\"Instruct: {MODEL_INSTRUCT}\")\nprint(f\"GQA:      {GQA_RATIO}\")\nprint(f\"\\nE11-v3 Config: Seeds={SEEDS}, dtype={DTYPE}, MAX_LENGTH={MAX_LENGTH}\")\nprint(f\"\\nReference (MHA):\")\nprint(f\"  Mistral RLHF Delta: {MISTRAL_DELTA:+.2f}\")\nprint(f\"\\nReference (GQA):\")\nprint(f\"  TinyLlama Fragility: {TINYLLAMA_GQA_FRAGILITY:.3f} (ANTIFRAGILE)\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Cell 3: Attention Entropy Analysis Functions (E11-v3: mask + chat_template)\n\ndef compute_attention_entropy(attention_weights, attention_mask=None):\n    \"\"\"\n    Compute normalized entropy of attention weights.\n    \n    E11-v3 FIX: Use attention_mask to exclude PAD tokens from entropy calculation.\n    \n    Args:\n        attention_weights: Tensor of shape (batch, heads, seq, seq)\n        attention_mask: Optional tensor of shape (batch, seq) - 1 for real tokens, 0 for PAD\n    \n    Returns:\n        List of entropy values per head\n    \"\"\"\n    # Average over batch and sequence positions\n    attn = attention_weights.float().mean(dim=0).mean(dim=-2)  # (heads, seq)\n    \n    # E11-v3: Apply attention mask if provided\n    if attention_mask is not None:\n        # Average mask over batch, get valid token count\n        mask = attention_mask.float().mean(dim=0)  # (seq,)\n        # Mask is 1 for valid tokens, 0 for PAD\n        # We want to only consider attention to valid tokens\n        seq_len = mask.sum().item()\n    else:\n        seq_len = attn.shape[-1]\n    \n    entropies = []\n    for head_idx in range(attn.shape[0]):\n        probs = attn[head_idx].detach().float().cpu().numpy()\n        \n        # E11-v3: If mask provided, zero out PAD positions\n        if attention_mask is not None:\n            mask_np = mask.cpu().numpy()\n            probs = probs * mask_np\n        \n        probs_sum = probs.sum()\n        if probs_sum > 0:\n            probs = probs / probs_sum  # Normalize\n        probs = probs[probs > 0]  # Remove zeros\n        \n        if len(probs) > 1:\n            h = scipy_entropy(probs, base=2)\n            h_max = np.log2(len(probs))\n            h_norm = h / h_max if h_max > 0 else 0\n        else:\n            h_norm = 0\n        \n        entropies.append(h_norm)\n    \n    return entropies\n\n\ndef classify_phenotype(entropy):\n    \"\"\"Classify attention head by normalized entropy.\"\"\"\n    if entropy > PROBER_THRESHOLD:\n        return 'PROBER'\n    elif entropy < RIGID_THRESHOLD:\n        return 'RIGID'\n    else:\n        return 'HEALTHY'\n\n\ndef analyze_model_phenotypes(model, tokenizer, prompts, use_chat_template=False):\n    \"\"\"\n    Analyze phenotype distribution of a model.\n    \n    E11-v3 FIX: \n    - Use attention_mask to exclude PAD tokens\n    - Use chat_template for Instruct models\n    \n    Args:\n        model: The model to analyze\n        tokenizer: The tokenizer\n        prompts: List of prompts\n        use_chat_template: If True, wrap prompts in chat template (for Instruct models)\n    \n    Returns:\n        dict with prober_pct, rigid_pct, healthy_pct, mean_entropy\n    \"\"\"\n    all_entropies = []\n    \n    for prompt in prompts:\n        # E11-v3: Apply chat template for Instruct models\n        if use_chat_template and hasattr(tokenizer, 'apply_chat_template'):\n            messages = [{\"role\": \"user\", \"content\": prompt}]\n            try:\n                formatted_prompt = tokenizer.apply_chat_template(\n                    messages, \n                    tokenize=False, \n                    add_generation_prompt=True\n                )\n            except Exception as e:\n                print(f\"  Warning: chat_template failed ({e}), using raw prompt\")\n                formatted_prompt = prompt\n        else:\n            formatted_prompt = prompt\n        \n        # E11-v3: Get attention_mask from tokenizer\n        inputs = tokenizer(\n            formatted_prompt, \n            return_tensors='pt',\n            padding=True,\n            truncation=True,\n            max_length=MAX_LENGTH,\n            return_attention_mask=True\n        ).to(model.device)\n        \n        with torch.no_grad():\n            outputs = model(**inputs, output_attentions=True)\n        \n        # E11-v3: Pass attention_mask to entropy calculation\n        attention_mask = inputs.get('attention_mask', None)\n        \n        for layer_attn in outputs.attentions:\n            layer_entropies = compute_attention_entropy(layer_attn, attention_mask)\n            all_entropies.extend(layer_entropies)\n    \n    phenotypes = [classify_phenotype(e) for e in all_entropies]\n    \n    n_total = len(phenotypes)\n    prober_pct = phenotypes.count('PROBER') / n_total * 100\n    rigid_pct = phenotypes.count('RIGID') / n_total * 100\n    healthy_pct = phenotypes.count('HEALTHY') / n_total * 100\n    \n    return {\n        'prober_pct': prober_pct,\n        'rigid_pct': rigid_pct,\n        'healthy_pct': healthy_pct,\n        'mean_entropy': np.mean(all_entropies),\n        'std_entropy': np.std(all_entropies),\n        'n_heads': n_total,\n        'chat_template_used': use_chat_template  # E11-v3: Track this\n    }\n\nprint(\"Analysis functions loaded (E11-v3: mask + chat_template).\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Cell 4: Noise Injection for Fragility Test (E11-v3: mask + chat_template)\n\nclass AttentionNoiseInjector:\n    \"\"\"Inject Gaussian noise into attention outputs.\"\"\"\n    \n    def __init__(self, model, noise_std=0.0):\n        self.model = model\n        self.noise_std = noise_std\n        self.hooks = []\n    \n    def _make_hook(self, layer_idx):\n        def hook(module, input, output):\n            if self.noise_std > 0:\n                if isinstance(output, tuple):\n                    attn_output = output[0]\n                    noise = torch.randn_like(attn_output) * self.noise_std\n                    return (attn_output + noise,) + output[1:]\n                else:\n                    noise = torch.randn_like(output) * self.noise_std\n                    return output + noise\n            return output\n        return hook\n    \n    def attach(self):\n        \"\"\"Attach hooks to all attention layers.\"\"\"\n        for idx, layer in enumerate(self.model.model.layers):\n            hook = layer.self_attn.register_forward_hook(self._make_hook(idx))\n            self.hooks.append(hook)\n    \n    def detach(self):\n        \"\"\"Remove all hooks.\"\"\"\n        for hook in self.hooks:\n            hook.remove()\n        self.hooks = []\n    \n    def set_noise(self, std):\n        \"\"\"Set noise level.\"\"\"\n        self.noise_std = std\n\n\ndef compute_repetition_rate(text, n=3):\n    \"\"\"Compute n-gram repetition rate.\"\"\"\n    words = text.split()\n    if len(words) < n:\n        return 0.0\n    \n    ngrams = [tuple(words[i:i+n]) for i in range(len(words) - n + 1)]\n    unique = len(set(ngrams))\n    total = len(ngrams)\n    \n    return 1.0 - (unique / total) if total > 0 else 0.0\n\n\ndef test_fragility(model, tokenizer, noise_levels, prompts, max_new_tokens=50, use_chat_template=False):\n    \"\"\"\n    Test model fragility under noise injection.\n    \n    E11-v3 FIX:\n    - Use attention_mask in generation\n    - Use chat_template for Instruct models\n    \n    Returns:\n        dict with fragility_score, degradation_curve\n    \"\"\"\n    injector = AttentionNoiseInjector(model, noise_std=0.0)\n    injector.attach()\n    \n    degradation_scores = []\n    \n    for noise_std in noise_levels:\n        injector.set_noise(noise_std)\n        \n        rep_rates = []\n        for prompt in prompts:\n            # E11-v3: Apply chat template for Instruct models\n            if use_chat_template and hasattr(tokenizer, 'apply_chat_template'):\n                messages = [{\"role\": \"user\", \"content\": prompt}]\n                try:\n                    formatted_prompt = tokenizer.apply_chat_template(\n                        messages, \n                        tokenize=False, \n                        add_generation_prompt=True\n                    )\n                except Exception:\n                    formatted_prompt = prompt\n            else:\n                formatted_prompt = prompt\n            \n            # E11-v3: Get attention_mask from tokenizer\n            inputs = tokenizer(\n                formatted_prompt, \n                return_tensors='pt',\n                padding=True,\n                truncation=True,\n                max_length=MAX_LENGTH,\n                return_attention_mask=True\n            ).to(model.device)\n            \n            with torch.no_grad():\n                outputs = model.generate(\n                    **inputs,\n                    max_new_tokens=max_new_tokens,\n                    do_sample=False,\n                    pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id\n                )\n            generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n            generated_only = generated[len(formatted_prompt):].strip()\n            rep_rates.append(compute_repetition_rate(generated_only))\n        rep_avg = float(np.mean(rep_rates))\n        degradation_scores.append(rep_avg)\n\n        print(f\"  Noise sigma={noise_std:.2f}: Rep={rep_avg:.3f}\")\n    \n    injector.detach()\n    \n    # Compute fragility as slope\n    slope, _, _, _, _ = linregress(noise_levels, degradation_scores)\n    \n    return {\n        'fragility_score': slope,\n        'degradation_curve': list(zip(noise_levels, degradation_scores)),\n        'is_fragile': slope > 0.05,\n        'is_antifragile': slope < -0.05,\n        'is_neutral': abs(slope) <= 0.05,\n        'chat_template_used': use_chat_template  # E11-v3: Track this\n    }\n\nprint(\"Fragility test functions loaded (E11-v3: mask + chat_template).\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Cell 5: Initialize Results\n", "\n", "results = {\n", "    'pair': 'llama3',\n", "    'architecture': 'GQA',\n", "    'gqa_ratio': GQA_RATIO,\n", "    'base': {},\n", "    'instruct': {}\n", "}\n", "\n", "print(f\"\\n{'='*60}\")\n", "print(f\"E04 TWIN TEST: LLaMA-3.1-8B (GQA {GQA_RATIO})\")\n", "print(f\"{'='*60}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Cell 6: Load and Analyze BASE Model (E11-v3: bfloat16, NO chat_template)\n\nprint(f\"\\n[1/4] Loading BASE: {MODEL_BASE}\")\nprint(\"      (This is a gated model - requires HuggingFace login)\")\n\ntokenizer_base = AutoTokenizer.from_pretrained(MODEL_BASE)\nmodel_base = AutoModelForCausalLM.from_pretrained(\n    MODEL_BASE,\n    torch_dtype=DTYPE,  # bfloat16 (E11-v3)\n    device_map='auto',\n    trust_remote_code=True,\n    attn_implementation=\"eager\"  # CRITICAL: SDPA doesn't return attentions!\n)\n\nif tokenizer_base.pad_token is None:\n    tokenizer_base.pad_token = tokenizer_base.eos_token\n\nprint(f\"\\n[2/4] Analyzing BASE phenotypes (E11-v3: mask=True, chat_template=False)...\")\nresults['base']['phenotypes'] = analyze_model_phenotypes(\n    model_base, \n    tokenizer_base, \n    STANDARD_PROMPTS,\n    use_chat_template=False  # E11-v3: BASE model = no chat template\n)\nprint(f\"  Prober%: {results['base']['phenotypes']['prober_pct']:.1f}%\")\nprint(f\"  Rigid%:  {results['base']['phenotypes']['rigid_pct']:.1f}%\")\nprint(f\"  Mean H:  {results['base']['phenotypes']['mean_entropy']:.3f}\")\n\nprint(f\"\\n[2b/4] Testing BASE fragility (E11-v3: mask=True, chat_template=False)...\")\nresults['base']['fragility'] = test_fragility(\n    model_base, \n    tokenizer_base, \n    NOISE_LEVELS, \n    STANDARD_PROMPTS,\n    use_chat_template=False  # E11-v3: BASE model = no chat template\n)\nprint(f\"  Fragility Score: {results['base']['fragility']['fragility_score']:.4f}\")\n\n# Classify phenotype\nbase_frag = results['base']['fragility']['fragility_score']\nif base_frag < -0.05:\n    print(f\"  Classification: ANTIFRAGILE\")\nelif base_frag > 0.05:\n    print(f\"  Classification: FRAGILE\")\nelse:\n    print(f\"  Classification: NEUTRAL\")\n\n# Free memory\ndel model_base\ntorch.cuda.empty_cache()\nprint(\"\\n  [Memory cleared]\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Cell 7: Load and Analyze INSTRUCT Model (E11-v3: bfloat16, WITH chat_template)\n\nprint(f\"\\n[3/4] Loading INSTRUCT: {MODEL_INSTRUCT}\")\n\ntokenizer_inst = AutoTokenizer.from_pretrained(MODEL_INSTRUCT)\nmodel_inst = AutoModelForCausalLM.from_pretrained(\n    MODEL_INSTRUCT,\n    torch_dtype=DTYPE,  # bfloat16 (E11-v3)\n    device_map='auto',\n    trust_remote_code=True,\n    attn_implementation=\"eager\"  # CRITICAL: SDPA doesn't return attentions!\n)\n\nif tokenizer_inst.pad_token is None:\n    tokenizer_inst.pad_token = tokenizer_inst.eos_token\n\nprint(f\"\\n[4/4] Analyzing INSTRUCT phenotypes (E11-v3: mask=True, chat_template=True)...\")\nresults['instruct']['phenotypes'] = analyze_model_phenotypes(\n    model_inst, \n    tokenizer_inst, \n    STANDARD_PROMPTS,\n    use_chat_template=True  # E11-v3: INSTRUCT model = WITH chat template\n)\nprint(f\"  Prober%: {results['instruct']['phenotypes']['prober_pct']:.1f}%\")\nprint(f\"  Rigid%:  {results['instruct']['phenotypes']['rigid_pct']:.1f}%\")\nprint(f\"  Mean H:  {results['instruct']['phenotypes']['mean_entropy']:.3f}\")\n\nprint(f\"\\n[4b/4] Testing INSTRUCT fragility (E11-v3: mask=True, chat_template=True)...\")\nresults['instruct']['fragility'] = test_fragility(\n    model_inst, \n    tokenizer_inst, \n    NOISE_LEVELS, \n    STANDARD_PROMPTS,\n    use_chat_template=True  # E11-v3: INSTRUCT model = WITH chat template\n)\nprint(f\"  Fragility Score: {results['instruct']['fragility']['fragility_score']:.4f}\")\n\n# Classify phenotype\ninst_frag = results['instruct']['fragility']['fragility_score']\nif inst_frag < -0.05:\n    print(f\"  Classification: ANTIFRAGILE\")\nelif inst_frag > 0.05:\n    print(f\"  Classification: FRAGILE\")\nelse:\n    print(f\"  Classification: NEUTRAL\")\n\n# Free memory\ndel model_inst\ntorch.cuda.empty_cache()\nprint(\"\\n  [Memory cleared]\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Cell 8: GQA vs MHA RLHF Comparison\n", "\n", "print(f\"\\n{'='*60}\")\n", "print(f\"E04 TWIN TEST RESULTS: LLaMA-3.1 (GQA {GQA_RATIO})\")\n", "print(f\"{'='*60}\")\n", "\n", "# Extract key metrics\n", "base_rigid = results['base']['phenotypes']['rigid_pct']\n", "inst_rigid = results['instruct']['phenotypes']['rigid_pct']\n", "base_prober = results['base']['phenotypes']['prober_pct']\n", "inst_prober = results['instruct']['phenotypes']['prober_pct']\n", "base_frag = results['base']['fragility']['fragility_score']\n", "inst_frag = results['instruct']['fragility']['fragility_score']\n", "\n", "# Compute deltas\n", "delta_rigid = inst_rigid - base_rigid\n", "delta_prober = inst_prober - base_prober\n", "delta_frag = inst_frag - base_frag\n", "\n", "print(f\"\\n{'Metric':<25} {'BASE':>12} {'INSTRUCT':>12} {'Delta':>12}\")\n", "print(\"-\" * 65)\n", "print(f\"{'Rigid% (Beautiful Ones)':<25} {base_rigid:>11.1f}% {inst_rigid:>11.1f}% {delta_rigid:>+11.1f}%\")\n", "print(f\"{'Prober% (Chaos)':<25} {base_prober:>11.1f}% {inst_prober:>11.1f}% {delta_prober:>+11.1f}%\")\n", "print(f\"{'Fragility Score':<25} {base_frag:>12.4f} {inst_frag:>12.4f} {delta_frag:>+12.4f}\")\n", "\n", "# Compare to MHA (Mistral)\n", "print(f\"\\n{'='*60}\")\n", "print(\"COMPARISON: GQA vs MHA RLHF Impact\")\n", "print(f\"{'='*60}\")\n", "\n", "print(f\"\\n{'Architecture':<15} {'RLHF Delta':>15} {'Interpretation':>25}\")\n", "print(\"-\" * 55)\n", "print(f\"{'Mistral (MHA)':<15} {MISTRAL_DELTA:>+15.4f} {'RLHF DAMAGES MHA':>25}\")\n", "print(f\"{'LLaMA-3.1 (GQA)':<15} {delta_frag:>+15.4f} {'...':>25}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Cell 9: Hypothesis Test\n", "\n", "print(f\"\\n{'='*60}\")\n", "print(\"HYPOTHESIS TEST: Does GQA Protect Against RLHF Damage?\")\n", "print(f\"{'='*60}\")\n", "\n", "# Compare GQA delta to MHA delta\n", "gqa_delta = delta_frag\n", "mha_delta = MISTRAL_DELTA\n", "\n", "# Hypothesis tests\n", "rlhf_damages_gqa = delta_frag > 0.05  # Instruct more fragile than Base\n", "gqa_less_damage_than_mha = delta_frag < mha_delta  # GQA takes less RLHF damage\n", "base_already_antifragile = base_frag < -0.05  # Base model inherently antifragile\n", "\n", "print(f\"\\n1. RLHF damages GQA (Delta > +0.05)?\")\n", "print(f\"   Delta = {delta_frag:+.4f}\")\n", "print(f\"   Answer: {'YES - GQA NOT PROTECTIVE' if rlhf_damages_gqa else 'NO - GQA IS PROTECTIVE'}\")\n", "\n", "print(f\"\\n2. GQA takes LESS damage than MHA?\")\n", "print(f\"   GQA Delta: {delta_frag:+.4f}\")\n", "print(f\"   MHA Delta: {mha_delta:+.4f}\")\n", "print(f\"   Difference: {mha_delta - delta_frag:+.4f}\")\n", "print(f\"   Answer: {'YES - GQA BUFFERS RLHF' if gqa_less_damage_than_mha else 'NO'}\")\n", "\n", "print(f\"\\n3. Is GQA Base already antifragile (like TinyLlama)?\")\n", "print(f\"   Base Fragility: {base_frag:.4f}\")\n", "print(f\"   TinyLlama Reference: {TINYLLAMA_GQA_FRAGILITY:.4f}\")\n", "print(f\"   Answer: {'YES - INTRINSIC ANTIFRAGILITY' if base_already_antifragile else 'NO'}\")\n", "\n", "# Final verdict\n", "print(f\"\\n{'='*60}\")\n", "print(\"FINAL VERDICT\")\n", "print(f\"{'='*60}\")\n", "\n", "if not rlhf_damages_gqa:\n", "    verdict = \"GQA PROTECTS AGAINST RLHF FRAGILITY\"\n", "    evidence = \"Instruct model NOT significantly more fragile than Base\"\n", "elif gqa_less_damage_than_mha:\n", "    verdict = \"GQA PARTIALLY BUFFERS RLHF DAMAGE\"\n", "    evidence = f\"Delta ({delta_frag:+.2f}) < MHA Delta ({mha_delta:+.2f})\"\n", "else:\n", "    verdict = \"GQA DOES NOT PROTECT AGAINST RLHF\"\n", "    evidence = f\"Delta ({delta_frag:+.2f}) >= MHA Delta ({mha_delta:+.2f})\"\n", "\n", "print(f\"\\n  VERDICT: {verdict}\")\n", "print(f\"  Evidence: {evidence}\")\n", "\n", "# Store verdict\n", "results['verdict'] = {\n", "    'rlhf_damages_gqa': rlhf_damages_gqa,\n", "    'gqa_less_damage_than_mha': gqa_less_damage_than_mha,\n", "    'base_already_antifragile': base_already_antifragile,\n", "    'gqa_delta': float(delta_frag),\n", "    'mha_delta': float(mha_delta),\n", "    'verdict_text': verdict\n", "}"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Cell 10: Visualization\n", "\n", "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n", "\n", "# Plot 1: Phenotype comparison\n", "ax1 = axes[0]\n", "x = np.arange(3)\n", "width = 0.35\n", "base_vals = [base_prober, results['base']['phenotypes']['healthy_pct'], base_rigid]\n", "inst_vals = [inst_prober, results['instruct']['phenotypes']['healthy_pct'], inst_rigid]\n", "\n", "bars1 = ax1.bar(x - width/2, base_vals, width, label='Base', color='#2ecc71', alpha=0.8)\n", "bars2 = ax1.bar(x + width/2, inst_vals, width, label='Instruct', color='#e74c3c', alpha=0.8)\n", "\n", "ax1.set_ylabel('Percentage (%)')\n", "ax1.set_title(f'LLaMA-3.1 (GQA {GQA_RATIO}): Phenotype Distribution')\n", "ax1.set_xticks(x)\n", "ax1.set_xticklabels(['Prober\\n(Chaos)', 'Healthy', 'Rigid\\n(Beautiful One)'])\n", "ax1.legend()\n", "ax1.set_ylim(0, 100)\n", "\n", "# Annotate deltas\n", "for i, (b, inst) in enumerate(zip(base_vals, inst_vals)):\n", "    delta = inst - b\n", "    color = 'red' if delta > 0 and i == 2 else ('green' if delta < 0 and i == 0 else 'black')\n", "    ax1.annotate(f'{delta:+.1f}%', xy=(i, max(b, inst) + 3), ha='center', fontsize=10, color=color)\n", "\n", "# Plot 2: GQA vs MHA RLHF Delta\n", "ax2 = axes[1]\n", "architectures = ['Mistral\\n(MHA)', 'LLaMA-3.1\\n(GQA 4:1)']\n", "deltas = [MISTRAL_DELTA, delta_frag]\n", "colors = ['#e74c3c', '#3498db']\n", "\n", "bars = ax2.bar(architectures, deltas, color=colors, alpha=0.8, edgecolor='black')\n", "ax2.axhline(y=0, color='black', linestyle='--', linewidth=1)\n", "ax2.axhline(y=0.05, color='red', linestyle=':', linewidth=1, label='Fragile threshold')\n", "ax2.axhline(y=-0.05, color='green', linestyle=':', linewidth=1, label='Antifragile threshold')\n", "ax2.set_ylabel('RLHF Delta (Instruct - Base)')\n", "ax2.set_title('RLHF Impact: MHA vs GQA')\n", "ax2.legend(fontsize=8)\n", "\n", "# Annotate bars\n", "for bar, d in zip(bars, deltas):\n", "    ax2.annotate(f'{d:+.2f}', xy=(bar.get_x() + bar.get_width()/2, d),\n", "                 xytext=(0, 10 if d > 0 else -15), textcoords='offset points',\n", "                 ha='center', fontsize=12, fontweight='bold')\n", "\n", "# Plot 3: Degradation curves\n", "ax3 = axes[2]\n", "base_curve = results['base']['fragility']['degradation_curve']\n", "inst_curve = results['instruct']['fragility']['degradation_curve']\n", "\n", "base_x, base_y = zip(*base_curve)\n", "inst_x, inst_y = zip(*inst_curve)\n", "\n", "ax3.plot(base_x, base_y, 'o-', color='#2ecc71', label='Base', linewidth=2, markersize=8)\n", "ax3.plot(inst_x, inst_y, 's-', color='#e74c3c', label='Instruct', linewidth=2, markersize=8)\n", "\n", "ax3.set_xlabel('Noise Level (sigma)')\n", "ax3.set_ylabel('Repetition Rate')\n", "ax3.set_title(f'LLaMA-3.1 (GQA {GQA_RATIO}): Degradation Under Noise')\n", "ax3.legend()\n", "ax3.grid(True, alpha=0.3)\n", "\n", "plt.tight_layout()\n", "Path('../figures').mkdir(parents=True, exist_ok=True)\n", "fig_path = f'../figures/E04_LLaMA31_Twin_Test_{TIMESTAMP}.png'\n", "plt.savefig(fig_path, dpi=150, bbox_inches='tight')\n", "plt.show()\n", "\n", "print(f\"\\nFigure saved: {fig_path}\")\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Cell 11: Save Results (E11-v3 FULL STANDARD with methodology block)\n\nfilename = f'results/E04_LLaMA31_Twin_Test_{TIMESTAMP}.json'\n\n# Helper to convert numpy types to native Python\ndef convert_to_native(obj):\n    \"\"\"Recursively convert numpy types to native Python types.\"\"\"\n    if isinstance(obj, dict):\n        return {k: convert_to_native(v) for k, v in obj.items()}\n    elif isinstance(obj, list):\n        return [convert_to_native(v) for v in obj]\n    elif isinstance(obj, tuple):\n        return tuple(convert_to_native(v) for v in obj)\n    elif isinstance(obj, (np.bool_, np.integer)):\n        return int(obj)\n    elif isinstance(obj, np.floating):\n        return float(obj)\n    elif isinstance(obj, np.ndarray):\n        return obj.tolist()\n    else:\n        return obj\n\n# Prepare for JSON serialization\noutput = {\n    'experiment': 'E04_Twin_Test',\n    'variant': 'LLaMA-3.1 GQA',\n    'timestamp': TIMESTAMP,\n    'models': {\n        'base': MODEL_BASE,\n        'instruct': MODEL_INSTRUCT,\n        'params': MODEL_PARAMS,\n        'gqa_ratio': GQA_RATIO\n    },\n    'reference_values': {\n        'mistral_mha_delta': MISTRAL_DELTA,\n        'tinyllama_gqa_fragility': TINYLLAMA_GQA_FRAGILITY\n    },\n    'thresholds': {\n        'prober': PROBER_THRESHOLD,\n        'rigid': RIGID_THRESHOLD,\n        'fragile_gate': 0.05,\n        'antifragile_gate': -0.05\n    },\n    'noise_levels': NOISE_LEVELS,\n    # E11-v3 FULL Methodology Block\n    'methodology': {\n        'standard': 'E11-v3',\n        'seeds': SEEDS,\n        'max_length': MAX_LENGTH,\n        'dtype': str(DTYPE),\n        'prompt_md5': ACTUAL_MD5,\n        'prompt_md5_verified': PROMPTS_VERIFIED,\n        'num_prompts': len(STANDARD_PROMPTS),\n        'prompt_set': 'Standard-10 v3',\n        'quantization': 'bfloat16',\n        # E11-v3 FULL STANDARD additions\n        'attention_mask_used': True,\n        'chat_template_base': False,\n        'chat_template_instruct': True\n    },\n    'results': convert_to_native(results)\n}\n\nwith open(filename, 'w') as f:\n    json.dump(output, f, indent=2)\n\nprint(f\"Results saved: {filename}\")\n\nprint(f\"\\n\ud83d\udccb E11-v3 FULL Compliance:\")\nprint(f\"   Seeds: {SEEDS} \u2713\")\nprint(f\"   dtype: {DTYPE} \u2713\")\nprint(f\"   MD5: {ACTUAL_MD5} {'\u2713' if PROMPTS_VERIFIED else '\u2717'}\")\nprint(f\"   MAX_LENGTH: {MAX_LENGTH} \u2713\")\nprint(f\"   attention_mask: True \u2713\")\nprint(f\"   chat_template (Base): False \u2713\")\nprint(f\"   chat_template (Instruct): True \u2713\")\n\n# Download link for Colab\ntry:\n    from google.colab import files\n    files.download(filename)\n    files.download(fig_path)\nexcept:\n    pass"}, {"cell_type": "markdown", "metadata": {}, "source": ["---\n", "\n", "## Summary\n", "\n", "### The GQA Protection Hypothesis\n", "\n", "**MHA (Mistral-7B):**\n", "- RLHF Delta: +0.799\n", "- Interpretation: RLHF CREATES significant fragility\n", "\n", "**GQA (LLaMA-3.1-8B):**\n", "- RLHF Delta: [TBD after running]\n", "- Question: Does GQA buffer RLHF damage?\n", "\n", "### Implications\n", "\n", "If GQA protects:\n", "1. Architecture choice matters for robustness\n", "2. GQA may be preferred for safety-critical applications\n", "3. Surgical Indra less necessary for GQA models\n", "\n", "If GQA does NOT protect:\n", "1. RLHF damage is universal\n", "2. Surgical Indra needed for ALL instruct models\n", "3. Architecture provides no safety buffer\n", "\n", "---\n", "\n", "*Paper 4: Digital Overcrowding*  \n", "*E04: LLaMA-3.1 Twin Test - GQA RLHF Validation*"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.10.0"}}, "nbformat": 4, "nbformat_minor": 4}