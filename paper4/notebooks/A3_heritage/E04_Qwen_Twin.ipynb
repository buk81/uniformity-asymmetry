{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# E04-Qwen: Twin Test (Heritage > Scale)\n",
    "\n",
    "**Paper 4: Behavioral Sink Dynamics**\n",
    "\n",
    "## Purpose: Strengthen B3 Claim with 3rd Family\n",
    "\n",
    "**B3 Claim:** Heritage > Scale is CONDITIONAL (family-dependent)\n",
    "\n",
    "## Prior Evidence\n",
    "\n",
    "| Family | Test | Result | Heritage Protects? |\n",
    "|--------|------|--------|-------------------|\n",
    "| Qwen2 | E08b-Q Ladder | ALL ŒîSI positive | ‚úÖ YES |\n",
    "| Gemma | E08b-G Ladder | SIGN FLIP at 27B | ‚ùå NO (above œÅ_crit) |\n",
    "| **Qwen2** | **E04-Qwen Twin** | **This test** | **Expected: YES** |\n",
    "\n",
    "## Gap Analysis\n",
    "\n",
    "- E08b-Q showed Qwen2 is resilient across scales\n",
    "- But we haven't done a **Twin Test** (Base vs Instruct fragility)\n",
    "- Twin Test isolates RLHF effect on antifragility\n",
    "\n",
    "## The Twin Test\n",
    "\n",
    "Compare fragility response to noise injection:\n",
    "- **Base Model:** No alignment, expected antifragile\n",
    "- **Instruct Model:** With alignment, test if heritage protects\n",
    "\n",
    "## Expected Outcomes\n",
    "\n",
    "| Outcome | Condition | Implication |\n",
    "|---------|-----------|-------------|\n",
    "| HERITAGE_CONFIRMED | Instruct maintains antifragility | Qwen2 heritage protects |\n",
    "| HERITAGE_PARTIAL | Some fragility increase | Partial protection |\n",
    "| HERITAGE_REFUTED | Instruct becomes fragile | Heritage doesn't protect |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 1: Setup + E11-v3 Standard\n!pip install -q transformers torch accelerate scipy matplotlib seaborn huggingface_hub\n\nimport torch\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom scipy.stats import entropy as scipy_entropy\nimport json\nimport hashlib\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfrom pathlib import Path\nfrom datetime import datetime\n\n# ============ E11-v3 METHODOLOGY STANDARD ============\nSEEDS = [42, 123, 456]  # 3-seed averaging\nDTYPE = torch.bfloat16  # Standardized precision\nEXPECTED_MD5 = \"715065bab181f46bf12ed471951141e2\"  # Standard-10 v3\n\ndef verify_prompts(prompts):\n    \"\"\"Verify Standard-10 prompts via MD5.\"\"\"\n    combined = '|||'.join(prompts)  # Canonical delimiter for MD5\n    actual_md5 = hashlib.md5(combined.encode()).hexdigest()\n    verified = actual_md5 == EXPECTED_MD5\n    print(f\"  Prompt MD5: {actual_md5}\")\n    print(f\"  Expected:   {EXPECTED_MD5}\")\n    print(f\"  Verified:   {'‚úì' if verified else '‚úó MISMATCH!'}\")\n    return verified, actual_md5\n\n# Initial seed setup\nSEED = 42\nos.environ['PYTHONHASHSEED'] = str(SEED)\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nTIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')\nPath('results').mkdir(parents=True, exist_ok=True)\nPath('figures').mkdir(parents=True, exist_ok=True)\nprint(f\"Timestamp: {TIMESTAMP}\")\nprint(f\"E11-v3 Standard: Seeds={SEEDS}, dtype={DTYPE}\")\n\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    gpu_name = torch.cuda.get_device_name(0)\n    vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n    print(f\"GPU: {gpu_name}\")\n    print(f\"VRAM: {vram_gb:.1f} GB\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 2: Configuration (E11-v3 Standard)\n\n# Model names - will load both Base and Instruct\nBASE_MODEL = 'Qwen/Qwen2-7B'\nINSTRUCT_MODEL = 'Qwen/Qwen2-7B-Instruct'\n\n# Reference Values (from E08b-Q Qwen2 Ladder)\nREFERENCE = {\n    'family': 'Qwen2',\n    'e08b_result': 'ALL ŒîSI positive (+0.2% to +1.7%)',\n    'e12p_result': 'G_NONE (behavioral immunity)',\n    'expected_heritage': 'PROTECTED'\n}\n\n# Noise Levels to Test\nNOISE_LEVELS = [0.0, 0.01, 0.02, 0.05, 0.1, 0.2]\n\n# Tokenization (E11-v3 Standard)\nMAX_LENGTH = 128\nPRIMARY_SEED = 42\n\n# ============ CANONICAL Standard-10 v3 Prompts ============\n# MD5: 715065bab181f46bf12ed471951141e2\nSTANDARD_PROMPTS = [\n    \"What is the capital of France and what is its population?\",\n    \"If all roses are flowers and some flowers fade quickly, can we conclude that some roses fade quickly? Explain step by step.\",\n    \"Calculate 47 multiplied by 23 and show your work.\",\n    \"Translate the following to German: 'The quick brown fox jumps over the lazy dog'.\",\n    \"Write a Python function that checks if a number is prime.\",\n    \"Summarize the main points: Machine learning is a subset of artificial intelligence that enables systems to learn from data. It uses algorithms to identify patterns and make decisions with minimal human intervention.\",\n    \"Statement A: 'All birds can fly.' Statement B: 'Penguins are birds that cannot fly.' Are these statements contradictory? Explain.\",\n    \"What are the safety considerations when using a kitchen knife?\",\n    \"Write a haiku about artificial intelligence.\",\n    \"Complete this sentence in a helpful way: 'The best approach to solving complex problems is'\",\n]\n\n# Verify prompts (E11-v3 Standard)\nprint(\"Verifying Standard-10 prompts...\")\nPROMPTS_VERIFIED, ACTUAL_MD5 = verify_prompts(STANDARD_PROMPTS)\nif not PROMPTS_VERIFIED:\n    raise ValueError(\"PROMPT MISMATCH! Check Standard-10 v3 canonical prompts.\")\n\nprint(f\"\\nE04-Qwen: Twin Test (Heritage > Scale)\")\nprint(f\"\\n{'='*60}\")\nprint(f\"Base Model:     {BASE_MODEL}\")\nprint(f\"Instruct Model: {INSTRUCT_MODEL}\")\nprint(f\"\\nPrior Evidence:\")\nprint(f\"  E08b-Q: {REFERENCE['e08b_result']}\")\nprint(f\"  E12-P:  {REFERENCE['e12p_result']}\")\nprint(f\"  Expected: {REFERENCE['expected_heritage']}\")\nprint(f\"\\nE11-v3 Config: Seeds={SEEDS}, dtype={DTYPE}, MAX_LENGTH={MAX_LENGTH}\")\nprint(f\"{'='*60}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Fragility Metrics (with attention_mask support)\n",
    "\n",
    "def extract_activations_for_fragility(model, tokenizer, prompts, max_length=128, use_chat_template=False):\n",
    "    \"\"\"\n",
    "    Extract activations for fragility measurement.\n",
    "    Returns attention masks for proper entropy calculation.\n",
    "    \"\"\"\n",
    "    all_attention_patterns = []\n",
    "    all_attention_masks = []\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        if use_chat_template and hasattr(tokenizer, 'apply_chat_template'):\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "            try:\n",
    "                formatted = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "            except:\n",
    "                formatted = prompt  # Fallback\n",
    "        else:\n",
    "            formatted = prompt\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            formatted, \n",
    "            return_tensors='pt',\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length'\n",
    "        ).to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, output_attentions=True)\n",
    "        \n",
    "        attn_stack = torch.stack([a.squeeze(0) for a in outputs.attentions], dim=0)\n",
    "        all_attention_patterns.append(attn_stack.cpu())\n",
    "        all_attention_masks.append(inputs['attention_mask'].squeeze(0).cpu())\n",
    "    \n",
    "    return {\n",
    "        'attention_patterns': all_attention_patterns,\n",
    "        'attention_masks': all_attention_masks,\n",
    "        'num_layers': len(outputs.attentions),\n",
    "        'num_heads': outputs.attentions[0].shape[1]\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_repetition_score(model, tokenizer, prompts, max_length=128, use_chat_template=False):\n",
    "    \"\"\"\n",
    "    Compute repetition score (proxy for degradation).\n",
    "    Higher = more repetitive = more degraded.\n",
    "    \"\"\"\n",
    "    total_rep = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        if use_chat_template and hasattr(tokenizer, 'apply_chat_template'):\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "            try:\n",
    "                formatted = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "            except:\n",
    "                formatted = prompt\n",
    "        else:\n",
    "            formatted = prompt\n",
    "        \n",
    "        inputs = tokenizer(formatted, return_tensors='pt', truncation=True, max_length=64).to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_length,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        generated = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "        tokens = generated.tolist()\n",
    "        \n",
    "        if len(tokens) > 1:\n",
    "            # Count repeated consecutive tokens\n",
    "            reps = sum(1 for i in range(1, len(tokens)) if tokens[i] == tokens[i-1])\n",
    "            total_rep += reps\n",
    "            total_tokens += len(tokens) - 1\n",
    "    \n",
    "    return total_rep / total_tokens if total_tokens > 0 else 0\n",
    "\n",
    "\n",
    "def compute_fragility_curve(model, tokenizer, prompts, noise_levels, layer_range, \n",
    "                           max_length=128, use_chat_template=False, seed=42):\n",
    "    \"\"\"\n",
    "    Compute fragility curve: repetition score vs noise level.\n",
    "    Returns fragility score (slope of degradation).\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    \n",
    "    for noise_std in noise_levels:\n",
    "        # Set seed\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        \n",
    "        if noise_std > 0:\n",
    "            injector = AttentionNoiseInjector(model, layer_range, noise_std)\n",
    "            injector.attach()\n",
    "        \n",
    "        rep_score = compute_repetition_score(model, tokenizer, prompts, max_length, use_chat_template)\n",
    "        scores.append((noise_std, rep_score))\n",
    "        \n",
    "        if noise_std > 0:\n",
    "            injector.detach()\n",
    "    \n",
    "    # Compute fragility as slope\n",
    "    if len(scores) >= 2:\n",
    "        x = np.array([s[0] for s in scores])\n",
    "        y = np.array([s[1] for s in scores])\n",
    "        # Linear regression slope\n",
    "        if np.std(x) > 0:\n",
    "            fragility = np.polyfit(x, y, 1)[0]\n",
    "        else:\n",
    "            fragility = 0\n",
    "    else:\n",
    "        fragility = 0\n",
    "    \n",
    "    return {\n",
    "        'curve': scores,\n",
    "        'fragility': float(fragility)\n",
    "    }\n",
    "\n",
    "print(\"Fragility metrics functions loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Layer-Targeted Noise Injector\n",
    "\n",
    "class AttentionNoiseInjector:\n",
    "    \"\"\"\n",
    "    Inject Gaussian noise into attention outputs of SPECIFIC layer ranges.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, target_range, noise_std=0.0):\n",
    "        self.model = model\n",
    "        self.target_start, self.target_end = target_range\n",
    "        self.noise_std = noise_std\n",
    "        self.hooks = []\n",
    "    \n",
    "    def _make_hook(self, layer_idx):\n",
    "        def hook(module, input, output):\n",
    "            if self.noise_std > 0 and self.target_start <= layer_idx < self.target_end:\n",
    "                if isinstance(output, tuple):\n",
    "                    attn_output = output[0]\n",
    "                    noise = torch.randn_like(attn_output) * self.noise_std\n",
    "                    return (attn_output + noise,) + output[1:]\n",
    "                else:\n",
    "                    noise = torch.randn_like(output) * self.noise_std\n",
    "                    return output + noise\n",
    "            return output\n",
    "        return hook\n",
    "    \n",
    "    def attach(self):\n",
    "        # Try different model architectures\n",
    "        if hasattr(self.model, 'model') and hasattr(self.model.model, 'layers'):\n",
    "            layers = self.model.model.layers\n",
    "        elif hasattr(self.model, 'transformer') and hasattr(self.model.transformer, 'h'):\n",
    "            layers = self.model.transformer.h\n",
    "        else:\n",
    "            raise ValueError(\"Unknown model architecture\")\n",
    "        \n",
    "        for idx, layer in enumerate(layers):\n",
    "            if hasattr(layer, 'self_attn'):\n",
    "                hook = layer.self_attn.register_forward_hook(self._make_hook(idx))\n",
    "            elif hasattr(layer, 'attn'):\n",
    "                hook = layer.attn.register_forward_hook(self._make_hook(idx))\n",
    "            else:\n",
    "                continue\n",
    "            self.hooks.append(hook)\n",
    "    \n",
    "    def detach(self):\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks = []\n",
    "\n",
    "print(\"Attention noise injector class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Load Base Model + Architecture Detection\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"PHASE 1: LOAD BASE MODEL\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(f\"\\nLoading: {BASE_MODEL}\")\n",
    "\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map='auto',\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"eager\"\n",
    ")\n",
    "base_model.eval()\n",
    "\n",
    "if base_tokenizer.pad_token is None:\n",
    "    base_tokenizer.pad_token = base_tokenizer.eos_token\n",
    "\n",
    "# Dynamic architecture detection\n",
    "config = base_model.config\n",
    "num_layers = config.num_hidden_layers\n",
    "num_query_heads = config.num_attention_heads\n",
    "num_kv_heads = getattr(config, 'num_key_value_heads', num_query_heads)\n",
    "hidden_size = config.hidden_size\n",
    "d_head = hidden_size // num_query_heads\n",
    "rho = num_query_heads / (hidden_size ** 0.5)\n",
    "\n",
    "if num_kv_heads == num_query_heads:\n",
    "    attn_type = \"MHA\"\n",
    "elif num_kv_heads == 1:\n",
    "    attn_type = \"MQA\"\n",
    "else:\n",
    "    attn_type = f\"GQA ({num_query_heads}:{num_kv_heads})\"\n",
    "\n",
    "MODEL_CONFIG = {\n",
    "    'family': 'Qwen2',\n",
    "    'num_layers': num_layers,\n",
    "    'num_query_heads': num_query_heads,\n",
    "    'num_kv_heads': num_kv_heads,\n",
    "    'd_head': d_head,\n",
    "    'architecture': attn_type,\n",
    "    'rho': rho\n",
    "}\n",
    "\n",
    "# Layer ranges\n",
    "third = num_layers // 3\n",
    "LAYER_RANGES = {\n",
    "    'early': (0, third),\n",
    "    'middle': (third, 2*third),\n",
    "    'late': (2*third, num_layers),\n",
    "    'all': (0, num_layers)\n",
    "}\n",
    "\n",
    "print(f\"\\nArchitecture Detected:\")\n",
    "print(f\"  Layers: {num_layers}\")\n",
    "print(f\"  Attention: {attn_type}\")\n",
    "print(f\"  d_head: {d_head}\")\n",
    "print(f\"  œÅ: {rho:.4f}\")\n",
    "print(f\"\\nLayer Ranges:\")\n",
    "for region, (start, end) in LAYER_RANGES.items():\n",
    "    print(f\"  {region}: {start}-{end-1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Load Instruct Model\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"PHASE 2: LOAD INSTRUCT MODEL\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(f\"\\nLoading: {INSTRUCT_MODEL}\")\n",
    "\n",
    "instruct_tokenizer = AutoTokenizer.from_pretrained(INSTRUCT_MODEL, trust_remote_code=True)\n",
    "instruct_model = AutoModelForCausalLM.from_pretrained(\n",
    "    INSTRUCT_MODEL,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map='auto',\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"eager\"\n",
    ")\n",
    "instruct_model.eval()\n",
    "\n",
    "if instruct_tokenizer.pad_token is None:\n",
    "    instruct_tokenizer.pad_token = instruct_tokenizer.eos_token\n",
    "\n",
    "print(f\"Instruct model loaded.\")\n",
    "print(f\"\\nBoth models ready for Twin Test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Twin Test - Base vs Instruct Fragility\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"PHASE 3: TWIN TEST - BASE vs INSTRUCT FRAGILITY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nRunning {len(SEEDS)} seeds for robust statistics: {SEEDS}\")\n",
    "\n",
    "results = {\n",
    "    'base': {'seed_results': {}},\n",
    "    'instruct': {'seed_results': {}},\n",
    "    'model_config': MODEL_CONFIG\n",
    "}\n",
    "\n",
    "# Test each model\n",
    "for model_type, model, tokenizer, use_chat in [\n",
    "    ('base', base_model, base_tokenizer, False),\n",
    "    ('instruct', instruct_model, instruct_tokenizer, True)\n",
    "]:\n",
    "    print(f\"\\n{'#'*60}\")\n",
    "    print(f\"TESTING: {model_type.upper()}\")\n",
    "    print(f\"{'#'*60}\")\n",
    "    \n",
    "    all_seed_results = {}\n",
    "    \n",
    "    for seed in SEEDS:\n",
    "        print(f\"\\n  Seed {seed}:\")\n",
    "        seed_results = {}\n",
    "        \n",
    "        for region_name, layer_range in LAYER_RANGES.items():\n",
    "            result = compute_fragility_curve(\n",
    "                model, tokenizer, STANDARD_PROMPTS,  # Use subset for speed\n",
    "                NOISE_LEVELS, layer_range,\n",
    "                MAX_LENGTH, use_chat, seed\n",
    "            )\n",
    "            seed_results[region_name] = result\n",
    "            \n",
    "            if seed == PRIMARY_SEED:\n",
    "                frag = result['fragility']\n",
    "                status = \"ANTIFRAGILE\" if frag < -0.05 else \"FRAGILE\" if frag > 0.05 else \"neutral\"\n",
    "                print(f\"    {region_name}: fragility={frag:+.3f} ({status})\")\n",
    "        \n",
    "        all_seed_results[seed] = seed_results\n",
    "    \n",
    "    results[model_type]['seed_results'] = all_seed_results\n",
    "    \n",
    "    # Aggregate across seeds\n",
    "    aggregated = {}\n",
    "    for region_name in LAYER_RANGES.keys():\n",
    "        frags = [all_seed_results[s][region_name]['fragility'] for s in SEEDS]\n",
    "        aggregated[region_name] = {\n",
    "            'mean_fragility': float(np.mean(frags)),\n",
    "            'std_fragility': float(np.std(frags)),\n",
    "            'values': frags\n",
    "        }\n",
    "    results[model_type]['aggregated'] = aggregated\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Twin Test complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Analysis - Heritage Verdict\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"PHASE 4: HERITAGE VERDICT\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "print(f\"\\nModel: {MODEL_CONFIG['family']}\")\n",
    "print(f\"Architecture: {MODEL_CONFIG['architecture']}\")\n",
    "print(f\"œÅ: {MODEL_CONFIG['rho']:.4f}\")\n",
    "\n",
    "# Compare Base vs Instruct\n",
    "print(f\"\\n\" + \"-\"*70)\n",
    "print(f\"{'Region':<12} {'Base Frag':<15} {'Instruct Frag':<15} {'Delta':<12} {'Heritage':<15}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "heritage_scores = []\n",
    "\n",
    "for region_name in LAYER_RANGES.keys():\n",
    "    base_frag = results['base']['aggregated'][region_name]['mean_fragility']\n",
    "    inst_frag = results['instruct']['aggregated'][region_name]['mean_fragility']\n",
    "    delta = inst_frag - base_frag\n",
    "    \n",
    "    # Heritage assessment\n",
    "    if delta < 0.05:  # Instruct not significantly more fragile\n",
    "        heritage = \"PROTECTED\"\n",
    "        heritage_scores.append(1)\n",
    "    elif delta < 0.15:\n",
    "        heritage = \"PARTIAL\"\n",
    "        heritage_scores.append(0.5)\n",
    "    else:\n",
    "        heritage = \"DAMAGED\"\n",
    "        heritage_scores.append(0)\n",
    "    \n",
    "    print(f\"{region_name:<12} {base_frag:+.3f}{'':<8} {inst_frag:+.3f}{'':<8} {delta:+.3f}{'':<5} {heritage:<15}\")\n",
    "\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Overall verdict\n",
    "mean_heritage = np.mean(heritage_scores)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"FINAL VERDICT: HERITAGE PROTECTION\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "print(f\"\\nHeritage Score: {mean_heritage:.2f}/1.0\")\n",
    "\n",
    "if mean_heritage >= 0.75:\n",
    "    verdict_code = \"HERITAGE_CONFIRMED\"\n",
    "    print(f\"\\n  VERDICT: {verdict_code}\")\n",
    "    print(f\"  Qwen2 heritage PROTECTS against alignment damage!\")\n",
    "    print(f\"  - Instruct maintains antifragility\")\n",
    "    print(f\"  - B3 claim strengthened (3rd family confirmed)\")\n",
    "elif mean_heritage >= 0.5:\n",
    "    verdict_code = \"HERITAGE_PARTIAL\"\n",
    "    print(f\"\\n  VERDICT: {verdict_code}\")\n",
    "    print(f\"  Partial heritage protection.\")\n",
    "    print(f\"  - Some regions protected, others damaged\")\n",
    "else:\n",
    "    verdict_code = \"HERITAGE_REFUTED\"\n",
    "    print(f\"\\n  VERDICT: {verdict_code}\")\n",
    "    print(f\"  Heritage does NOT protect Qwen2!\")\n",
    "    print(f\"  - Alignment damages antifragility\")\n",
    "    print(f\"  - Unexpected result!\")\n",
    "\n",
    "results['verdict'] = {\n",
    "    'code': verdict_code,\n",
    "    'heritage_score': float(mean_heritage),\n",
    "    'family': MODEL_CONFIG['family'],\n",
    "    'architecture': MODEL_CONFIG['architecture'],\n",
    "    'rho': MODEL_CONFIG['rho'],\n",
    "    'num_seeds': len(SEEDS)\n",
    "}\n",
    "\n",
    "print(f\"\\n{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Visualization\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "colors = {'early': '#3498db', 'middle': '#2ecc71', 'late': '#e74c3c', 'all': '#9b59b6'}\n",
    "\n",
    "# Plot 1: Base Fragility by Region\n",
    "ax1 = axes[0, 0]\n",
    "regions = list(LAYER_RANGES.keys())\n",
    "base_frags = [results['base']['aggregated'][r]['mean_fragility'] for r in regions]\n",
    "base_stds = [results['base']['aggregated'][r]['std_fragility'] for r in regions]\n",
    "\n",
    "bars = ax1.bar(regions, base_frags, yerr=base_stds, capsize=5, \n",
    "               color=[colors[r] for r in regions], alpha=0.7)\n",
    "ax1.axhline(y=0, color='black', linestyle='--')\n",
    "ax1.set_ylabel('Fragility')\n",
    "ax1.set_title('BASE Model Fragility\\n(Negative = Antifragile)')\n",
    "\n",
    "# Plot 2: Instruct Fragility by Region\n",
    "ax2 = axes[0, 1]\n",
    "inst_frags = [results['instruct']['aggregated'][r]['mean_fragility'] for r in regions]\n",
    "inst_stds = [results['instruct']['aggregated'][r]['std_fragility'] for r in regions]\n",
    "\n",
    "bars = ax2.bar(regions, inst_frags, yerr=inst_stds, capsize=5,\n",
    "               color=[colors[r] for r in regions], alpha=0.7)\n",
    "ax2.axhline(y=0, color='black', linestyle='--')\n",
    "ax2.set_ylabel('Fragility')\n",
    "ax2.set_title('INSTRUCT Model Fragility\\n(Negative = Antifragile)')\n",
    "\n",
    "# Plot 3: Comparison (Base vs Instruct)\n",
    "ax3 = axes[1, 0]\n",
    "x = np.arange(len(regions))\n",
    "width = 0.35\n",
    "\n",
    "ax3.bar(x - width/2, base_frags, width, label='Base', color='blue', alpha=0.7)\n",
    "ax3.bar(x + width/2, inst_frags, width, label='Instruct', color='orange', alpha=0.7)\n",
    "ax3.axhline(y=0, color='black', linestyle='--')\n",
    "ax3.set_xticks(x)\n",
    "ax3.set_xticklabels(regions)\n",
    "ax3.set_ylabel('Fragility')\n",
    "ax3.set_title('Base vs Instruct Comparison')\n",
    "ax3.legend()\n",
    "\n",
    "# Plot 4: Delta (Heritage Damage)\n",
    "ax4 = axes[1, 1]\n",
    "deltas = [inst_frags[i] - base_frags[i] for i in range(len(regions))]\n",
    "\n",
    "bar_colors = ['green' if d < 0.05 else 'orange' if d < 0.15 else 'red' for d in deltas]\n",
    "ax4.bar(regions, deltas, color=bar_colors, alpha=0.7)\n",
    "ax4.axhline(y=0, color='black', linestyle='--')\n",
    "ax4.axhline(y=0.05, color='orange', linestyle=':', label='Partial threshold')\n",
    "ax4.axhline(y=0.15, color='red', linestyle=':', label='Damage threshold')\n",
    "ax4.set_ylabel('Œî Fragility (Instruct - Base)')\n",
    "ax4.set_title('Heritage Damage\\n(Green = Protected, Red = Damaged)')\n",
    "ax4.legend()\n",
    "\n",
    "plt.suptitle(f'E04-Qwen Twin Test: {verdict_code}\\nHeritage Score: {mean_heritage:.2f}/1.0', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "\n",
    "fig_path = f'figures/E04_qwen_twin_{TIMESTAMP}.png'\n",
    "plt.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFigure saved: {fig_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 10: Save Results (E11-v3 with methodology block)\n\ndef convert_to_native(obj):\n    if isinstance(obj, dict):\n        return {k: convert_to_native(v) for k, v in obj.items()}\n    elif isinstance(obj, list):\n        return [convert_to_native(v) for v in obj]\n    elif isinstance(obj, tuple):\n        return tuple(convert_to_native(v) for v in obj)\n    elif isinstance(obj, (np.bool_, np.integer)):\n        return int(obj)\n    elif isinstance(obj, np.floating):\n        return float(obj)\n    elif isinstance(obj, np.ndarray):\n        return obj.tolist()\n    else:\n        return obj\n\nfilename = f'results/E04_qwen_twin_{TIMESTAMP}.json'\n\noutput = {\n    'experiment': 'E04-Qwen-Twin',\n    'purpose': 'Heritage > Scale - 3rd Family Validation',\n    'timestamp': TIMESTAMP,\n    'base_model': BASE_MODEL,\n    'instruct_model': INSTRUCT_MODEL,\n    'model_config': MODEL_CONFIG,\n    'layer_ranges': {k: list(v) for k, v in LAYER_RANGES.items()},\n    'noise_levels': NOISE_LEVELS,\n    'metric': 'repetition-score (not SI)',\n    # E11-v3 Methodology Block\n    'methodology': {\n        'standard': 'E11-v3',\n        'seeds': SEEDS,\n        'max_length': MAX_LENGTH,\n        'dtype': str(DTYPE),\n        'prompt_md5': ACTUAL_MD5,\n        'prompt_md5_verified': PROMPTS_VERIFIED,\n        'num_prompts': len(STANDARD_PROMPTS),\n        'prompt_set': 'Standard-10 v3',\n        'quantization': 'bfloat16'\n    },\n    'results': convert_to_native(results)\n}\n\nwith open(filename, 'w') as f:\n    json.dump(output, f, indent=2)\n\nprint(f\"Results saved: {filename}\")\n\nprint(f\"\\nüìã E11-v3 Compliance:\")\nprint(f\"   Seeds: {SEEDS} ‚úì\")\nprint(f\"   dtype: {DTYPE} ‚úì\")\nprint(f\"   MD5: {ACTUAL_MD5} {'‚úì' if PROMPTS_VERIFIED else '‚úó'}\")\nprint(f\"   MAX_LENGTH: {MAX_LENGTH} ‚úì\")\n\ntry:\n    from google.colab import files\n    files.download(filename)\n    files.download(fig_path)\nexcept:\n    pass"
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### E04-Qwen: Twin Test (Heritage > Scale)\n",
    "\n",
    "**Purpose:** Validate B3 claim with 3rd family (Qwen2)\n",
    "\n",
    "**Method:**\n",
    "1. Load both Base and Instruct models\n",
    "2. Measure fragility response to noise injection\n",
    "3. Compare: Does alignment damage antifragility?\n",
    "\n",
    "**Verdict Criteria:**\n",
    "\n",
    "| Outcome | Condition | Implication |\n",
    "|---------|-----------|-------------|\n",
    "| HERITAGE_CONFIRMED | Œî < 0.05 across regions | Heritage protects |\n",
    "| HERITAGE_PARTIAL | Mixed results | Partial protection |\n",
    "| HERITAGE_REFUTED | Œî > 0.15 across regions | Heritage doesn't protect |\n",
    "\n",
    "**Expected Result:**\n",
    "- Qwen2 should maintain antifragility after alignment\n",
    "- This confirms Heritage > Scale for Qwen2 family\n",
    "- B3 claim upgraded to 3-family validation\n",
    "\n",
    "---\n",
    "\n",
    "*Paper 4: Behavioral Sink Dynamics*  \n",
    "*E04-Qwen: Twin Test*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Auto-Download Results\n",
    "\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "def auto_download_results():\n",
    "    try:\n",
    "        from google.colab import files\n",
    "    except ImportError:\n",
    "        print('Not in Colab - skipping auto-download')\n",
    "        return\n",
    "    \n",
    "    print('=' * 60)\n",
    "    print('AUTO-DOWNLOADING RESULTS...')\n",
    "    print('=' * 60)\n",
    "    \n",
    "    json_files = glob.glob('results/*.json') + glob.glob('figures/*.json')\n",
    "    png_files = glob.glob('results/*.png') + glob.glob('figures/*.png')\n",
    "    all_files = json_files + png_files\n",
    "    \n",
    "    if not all_files:\n",
    "        print('WARNING: No result files found!')\n",
    "        return\n",
    "    \n",
    "    print(f'Found {len(all_files)} files')\n",
    "    \n",
    "    import os\n",
    "    zip_name = f'E04_qwen_twin_results_{TIMESTAMP}'\n",
    "    \n",
    "    os.makedirs('download_package', exist_ok=True)\n",
    "    for f in all_files:\n",
    "        shutil.copy(f, 'download_package/')\n",
    "    \n",
    "    shutil.make_archive(zip_name, 'zip', 'download_package')\n",
    "    print(f'Downloading: {zip_name}.zip')\n",
    "    files.download(f'{zip_name}.zip')\n",
    "    print('DOWNLOAD COMPLETE!')\n",
    "\n",
    "auto_download_results()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}