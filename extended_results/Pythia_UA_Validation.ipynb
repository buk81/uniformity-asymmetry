{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uniformity Asymmetry Validation: Pythia Suite\n",
    "\n",
    "**Purpose:** Test the Uniformity Asymmetry (UA) metric on EleutherAI's Pythia models.\n",
    "\n",
    "**Research Question:** Do base models (without RLHF) show the same embedding-output disconnect as RLHF'd models like Llama-3?\n",
    "\n",
    "**Hypothesis:** If RLHF creates \"surface alignment\" (neutral embeddings, biased outputs), then Pythia (no RLHF) should show:\n",
    "- Either: Both embeddings AND outputs biased (aligned)\n",
    "- Or: Both embeddings AND outputs neutral (aligned)\n",
    "- NOT: Neutral embeddings with biased outputs (disconnect)\n",
    "\n",
    "**Models Tested:**\n",
    "- Pythia-1.4B (comparable to smaller models)\n",
    "- Pythia-6.9B (comparable to Llama-8B, Gemma-9B)\n",
    "\n",
    "**Reference:** This extends the work in \"Uniformity Asymmetry: An Exploratory Metric for Detecting Representational Preferences in LLM Embeddings\"\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Davide D'Elia  \n",
    "**Date:** 2026-01-02  \n",
    "**For:** EleutherAI Community Contribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers accelerate torch numpy scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy import stats\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "N_BOOTSTRAP = 10000\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration\n",
    "\n",
    "**Pythia Models:**\n",
    "- No RLHF, no instruction tuning\n",
    "- Trained on The Pile\n",
    "- Uses GPT-NeoX architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select which Pythia model to test\n",
    "# Options: \"1.4b\", \"2.8b\", \"6.9b\", \"12b\"\n",
    "PYTHIA_SIZE = \"6.9b\"  # @param [\"1.4b\", \"2.8b\", \"6.9b\", \"12b\"]\n",
    "\n",
    "PYTHIA_CONFIGS = {\n",
    "    \"1.4b\": {\n",
    "        \"hf_name\": \"EleutherAI/pythia-1.4b\",\n",
    "        \"display\": \"Pythia-1.4B\",\n",
    "        \"dtype\": torch.float16,\n",
    "        \"ram_gb\": 4\n",
    "    },\n",
    "    \"2.8b\": {\n",
    "        \"hf_name\": \"EleutherAI/pythia-2.8b\",\n",
    "        \"display\": \"Pythia-2.8B\",\n",
    "        \"dtype\": torch.float16,\n",
    "        \"ram_gb\": 7\n",
    "    },\n",
    "    \"6.9b\": {\n",
    "        \"hf_name\": \"EleutherAI/pythia-6.9b\",\n",
    "        \"display\": \"Pythia-6.9B\",\n",
    "        \"dtype\": torch.float16,\n",
    "        \"ram_gb\": 15\n",
    "    },\n",
    "    \"12b\": {\n",
    "        \"hf_name\": \"EleutherAI/pythia-12b\",\n",
    "        \"display\": \"Pythia-12B\",\n",
    "        \"dtype\": torch.float16,\n",
    "        \"ram_gb\": 26\n",
    "    }\n",
    "}\n",
    "\n",
    "config = PYTHIA_CONFIGS[PYTHIA_SIZE]\n",
    "print(f\"Selected: {config['display']}\")\n",
    "print(f\"HuggingFace: {config['hf_name']}\")\n",
    "print(f\"Estimated GPU RAM: ~{config['ram_gb']} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "230 statement pairs across 6 categories:\n",
    "- Ground Truth Numeric (30)\n",
    "- Ground Truth Non-Numeric (20)\n",
    "- Tech Philosophy (50)\n",
    "- Lifestyle (50)\n",
    "- Business (50)\n",
    "- Scientific Facts (30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from GitHub\n",
    "!wget -q https://raw.githubusercontent.com/buk81/uniformity-asymmetry/main/dataset.json\n",
    "\n",
    "with open('dataset.json', 'r') as f:\n",
    "    DATASET = json.load(f)\n",
    "\n",
    "total_pairs = sum(len(cat['pairs']) for cat in DATASET.values())\n",
    "print(f\"Loaded {total_pairs} statement pairs across {len(DATASET)} categories\")\n",
    "for cat_name, cat_data in DATASET.items():\n",
    "    print(f\"  - {cat_name}: {len(cat_data['pairs'])} pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text: str, model, tokenizer) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Extract embedding via mean pooling over last hidden layer (skip BOS token).\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "        hidden_states = outputs.hidden_states[-1]\n",
    "        # Mean pooling: skip BOS token (position 0)\n",
    "        embedding = hidden_states[0, 1:, :].mean(dim=0)\n",
    "    \n",
    "    return embedding.cpu().numpy().astype(np.float32)\n",
    "\n",
    "\n",
    "def get_output_preference(text_a: str, text_b: str, model, tokenizer) -> float:\n",
    "    \"\"\"\n",
    "    Calculate output preference as NLL(B) - NLL(A).\n",
    "    Positive = prefers A, Negative = prefers B.\n",
    "    \"\"\"\n",
    "    def get_nll(text):\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "            return outputs.loss.item()\n",
    "    \n",
    "    nll_a = get_nll(text_a)\n",
    "    nll_b = get_nll(text_b)\n",
    "    \n",
    "    return nll_b - nll_a  # Positive = prefers A\n",
    "\n",
    "\n",
    "def uniformity_score(embeddings: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculate average pairwise cosine similarity (uniformity).\n",
    "    Higher = more uniform/collapsed representations.\n",
    "    \"\"\"\n",
    "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    normalized = embeddings / (norms + 1e-10)\n",
    "    kernel = normalized @ normalized.T\n",
    "    n = kernel.shape[0]\n",
    "    idx = np.triu_indices(n, k=1)\n",
    "    return float(np.mean(kernel[idx]))\n",
    "\n",
    "\n",
    "def bootstrap_ci(data: np.ndarray, n_bootstrap: int = N_BOOTSTRAP,\n",
    "                 ci: float = 0.95, seed: int = RANDOM_SEED) -> Tuple[float, float]:\n",
    "    \"\"\"Calculate bootstrap confidence interval for mean.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    bootstrap_means = [np.mean(np.random.choice(data, size=len(data), replace=True))\n",
    "                       for _ in range(n_bootstrap)]\n",
    "    alpha = 1 - ci\n",
    "    return (float(np.percentile(bootstrap_means, alpha/2 * 100)),\n",
    "            float(np.percentile(bootstrap_means, (1 - alpha/2) * 100)))\n",
    "\n",
    "\n",
    "def cohens_d(data: np.ndarray, null_value: float = 0) -> float:\n",
    "    \"\"\"Calculate Cohen's d effect size.\"\"\"\n",
    "    return float((np.mean(data) - null_value) / (np.std(data) + 1e-10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading {config['display']}...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config[\"hf_name\"])\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config[\"hf_name\"],\n",
    "    torch_dtype=config[\"dtype\"],\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(f\"Model loaded on: {model.device}\")\n",
    "print(f\"Model dtype: {next(model.parameters()).dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_validation(model, tokenizer, dataset: dict, verbose: bool = True) -> dict:\n",
    "    \"\"\"\n",
    "    Run UA + Output Preference validation on all statement pairs.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    all_asymmetries = []\n",
    "    all_preferences = []\n",
    "    \n",
    "    total_pairs = sum(len(cat[\"pairs\"]) for cat in dataset.values())\n",
    "    processed = 0\n",
    "    \n",
    "    for category_name, category_data in dataset.items():\n",
    "        if verbose:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Category: {category_name.upper()}\")\n",
    "            print(f\"{'='*60}\")\n",
    "        \n",
    "        pairs = category_data[\"pairs\"]\n",
    "        n_pairs = len(pairs)\n",
    "        \n",
    "        embeddings_a = []\n",
    "        embeddings_b = []\n",
    "        preferences = []\n",
    "        \n",
    "        for i, (stmt_a, stmt_b) in enumerate(pairs):\n",
    "            processed += 1\n",
    "            if verbose and processed % 20 == 0:\n",
    "                print(f\"  [{processed:03d}/{total_pairs}] Processing...\")\n",
    "            \n",
    "            # Get embeddings\n",
    "            emb_a = get_embedding(stmt_a, model, tokenizer)\n",
    "            emb_b = get_embedding(stmt_b, model, tokenizer)\n",
    "            embeddings_a.append(emb_a)\n",
    "            embeddings_b.append(emb_b)\n",
    "            \n",
    "            # Get output preference\n",
    "            pref = get_output_preference(stmt_a, stmt_b, model, tokenizer)\n",
    "            preferences.append(pref)\n",
    "        \n",
    "        embeddings_a = np.array(embeddings_a)\n",
    "        embeddings_b = np.array(embeddings_b)\n",
    "        \n",
    "        # Calculate uniformity asymmetry\n",
    "        u_a = uniformity_score(embeddings_a)\n",
    "        u_b = uniformity_score(embeddings_b)\n",
    "        asymmetry = u_a - u_b\n",
    "        mean_pref = float(np.mean(preferences))\n",
    "        \n",
    "        all_asymmetries.append(asymmetry)\n",
    "        all_preferences.append(mean_pref)\n",
    "        \n",
    "        results[category_name] = {\n",
    "            \"n_pairs\": n_pairs,\n",
    "            \"uniformity_a\": u_a,\n",
    "            \"uniformity_b\": u_b,\n",
    "            \"asymmetry\": asymmetry,\n",
    "            \"output_preference\": mean_pref,\n",
    "            \"pct_prefer_a\": float(np.mean([p > 0 for p in preferences]) * 100)\n",
    "        }\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  UA: {asymmetry:+.4f} | Output Pref: {mean_pref:+.3f} | % A: {results[category_name]['pct_prefer_a']:.1f}%\")\n",
    "    \n",
    "    # Global statistics\n",
    "    all_asymmetries = np.array(all_asymmetries)\n",
    "    all_preferences = np.array(all_preferences)\n",
    "    \n",
    "    # Correlation between UA and Output Preference\n",
    "    correlation = np.corrcoef(all_asymmetries, all_preferences)[0, 1]\n",
    "    \n",
    "    results[\"_global_stats\"] = {\n",
    "        \"total_pairs\": total_pairs,\n",
    "        \"mean_asymmetry\": float(np.mean(all_asymmetries)),\n",
    "        \"std_asymmetry\": float(np.std(all_asymmetries)),\n",
    "        \"bootstrap_95ci\": bootstrap_ci(all_asymmetries),\n",
    "        \"cohens_d\": cohens_d(all_asymmetries),\n",
    "        \"ua_output_correlation\": float(correlation),\n",
    "        \"mean_output_preference\": float(np.mean(all_preferences))\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(f\"Running validation on {config['display']}...\")\n",
    "print(f\"Bootstrap resamples: {N_BOOTSTRAP:,}\")\n",
    "print()\n",
    "\n",
    "results = run_validation(model, tokenizer, DATASET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\" UNIFORMITY ASYMMETRY VALIDATION: {config['display']}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n--- PER-CATEGORY RESULTS ---\")\n",
    "print(f\"{'Category':<25} {'UA':<10} {'Output Pref':<12} {'% A':<8} {'Aligned?'}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for cat_name, cat_data in results.items():\n",
    "    if cat_name.startswith(\"_\"):\n",
    "        continue\n",
    "    \n",
    "    ua = cat_data['asymmetry']\n",
    "    pref = cat_data['output_preference']\n",
    "    pct_a = cat_data['pct_prefer_a']\n",
    "    \n",
    "    # Check alignment: same sign = aligned\n",
    "    aligned = \"YES\" if (ua > 0 and pref > 0) or (ua < 0 and pref < 0) or (abs(ua) < 0.02 and abs(pref) < 0.1) else \"NO\"\n",
    "    \n",
    "    print(f\"{cat_name:<25} {ua:+.4f}     {pref:+.3f}        {pct_a:>5.1f}%   {aligned}\")\n",
    "\n",
    "gs = results[\"_global_stats\"]\n",
    "\n",
    "print(\"\\n--- GLOBAL STATISTICS ---\")\n",
    "print(f\"Mean UA:              {gs['mean_asymmetry']:+.4f}\")\n",
    "print(f\"95% CI:               [{gs['bootstrap_95ci'][0]:.4f}, {gs['bootstrap_95ci'][1]:.4f}]\")\n",
    "print(f\"Cohen's d:            {gs['cohens_d']:.3f}\")\n",
    "print(f\"UA-Output Correlation: r = {gs['ua_output_correlation']:.3f}\")\n",
    "\n",
    "print(\"\\n--- INTERPRETATION ---\")\n",
    "r = gs['ua_output_correlation']\n",
    "if r > 0.5:\n",
    "    print(f\"ALIGNED: Embedding asymmetry correlates with output preference (r={r:.2f})\")\n",
    "    print(\"         This suggests embeddings and outputs are coupled (no RLHF masking).\")\n",
    "elif r < -0.3:\n",
    "    print(f\"INVERTED: Embedding asymmetry anti-correlates with output preference (r={r:.2f})\")\n",
    "    print(\"          This is an unexpected pattern - needs investigation.\")\n",
    "else:\n",
    "    print(f\"DECOUPLED: Weak correlation between embeddings and outputs (r={r:.2f})\")\n",
    "    print(\"           Similar to Llama-3 pattern - possible RLHF-like effect.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with Other Models\n",
    "\n",
    "Reference values from the paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference results from the paper (Scientific Facts category)\n",
    "reference_models = {\n",
    "    \"Llama-3.1-8B\": {\"ua\": -0.013, \"pref\": 1.165, \"interpretation\": \"MASKED (RLHF)\"},\n",
    "    \"Gemma-2-9B\": {\"ua\": 0.021, \"pref\": 1.138, \"interpretation\": \"Weak\"},\n",
    "    \"Mistral-7B\": {\"ua\": 0.010, \"pref\": 0.972, \"interpretation\": \"MASKED (RLHF)\"},\n",
    "    \"Apertus-8B\": {\"ua\": 0.109, \"pref\": 1.278, \"interpretation\": \"ALIGNED (no RLHF)\"}\n",
    "}\n",
    "\n",
    "# Get Pythia's Scientific Facts results\n",
    "pythia_sf = results.get(\"scientific_facts\", {})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" CROSS-MODEL COMPARISON (Scientific Facts Category)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Model':<20} {'UA':<10} {'Output Pref':<12} {'Interpretation'}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for model_name, data in reference_models.items():\n",
    "    print(f\"{model_name:<20} {data['ua']:+.3f}      {data['pref']:+.3f}        {data['interpretation']}\")\n",
    "\n",
    "if pythia_sf:\n",
    "    pythia_interp = \"ALIGNED\" if abs(pythia_sf['asymmetry']) > 0.05 else \"NEUTRAL\"\n",
    "    print(f\"{config['display']:<20} {pythia_sf['asymmetry']:+.3f}      {pythia_sf['output_preference']:+.3f}        {pythia_interp} (Base Model)\")\n",
    "\n",
    "print(\"\\n--- KEY QUESTION ---\")\n",
    "print(\"Does Pythia (no RLHF) show the same disconnect as Llama-3 (heavy RLHF)?\")\n",
    "if pythia_sf:\n",
    "    if abs(pythia_sf['asymmetry']) < 0.02 and abs(pythia_sf['output_preference']) > 0.5:\n",
    "        print(\"RESULT: YES - Pythia shows similar disconnect (surprising for base model!)\")\n",
    "    elif abs(pythia_sf['asymmetry']) > 0.05:\n",
    "        print(\"RESULT: NO - Pythia embeddings show asymmetry (expected for base model)\")\n",
    "    else:\n",
    "        print(\"RESULT: UNCLEAR - Need to analyze further\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add metadata\n",
    "results[\"_metadata\"] = {\n",
    "    \"model\": config[\"hf_name\"],\n",
    "    \"model_display\": config[\"display\"],\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"n_bootstrap\": N_BOOTSTRAP,\n",
    "    \"random_seed\": RANDOM_SEED,\n",
    "    \"purpose\": \"EleutherAI community contribution - Base model comparison\"\n",
    "}\n",
    "\n",
    "# Save to JSON\n",
    "output_file = f\"pythia_{PYTHIA_SIZE}_ua_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to: {output_file}\")\n",
    "\n",
    "# Download link\n",
    "from google.colab import files\n",
    "files.download(output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare data for plotting\n",
    "categories = [k for k in results.keys() if not k.startswith(\"_\")]\n",
    "ua_values = [results[k]['asymmetry'] for k in categories]\n",
    "pref_values = [results[k]['output_preference'] for k in categories]\n",
    "\n",
    "# Create scatter plot\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "# Plot Pythia results\n",
    "ax.scatter([u * 10 for u in ua_values], pref_values, s=150, c='purple', \n",
    "           label=config['display'], edgecolors='black', linewidths=1.5, zorder=5)\n",
    "\n",
    "# Add category labels\n",
    "for i, cat in enumerate(categories):\n",
    "    ax.annotate(cat.replace('_', '\\n'), (ua_values[i] * 10, pref_values[i]),\n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "\n",
    "# Add reference models (Scientific Facts only)\n",
    "ref_colors = {'Llama-3.1-8B': 'blue', 'Apertus-8B': 'red', 'Gemma-2-9B': 'orange', 'Mistral-7B': 'green'}\n",
    "for model_name, data in reference_models.items():\n",
    "    ax.scatter(data['ua'] * 10, data['pref'], s=100, c=ref_colors[model_name],\n",
    "               marker='x', label=f\"{model_name} (ref)\", linewidths=2)\n",
    "\n",
    "# Reference lines\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "ax.set_xlabel('Uniformity Asymmetry (UA × 10)', fontsize=11, fontweight='bold')\n",
    "ax.set_ylabel('Output Preference (mean ΔNLL)', fontsize=11, fontweight='bold')\n",
    "ax.set_title(f'{config[\"display\"]}: UA vs Output Preference by Category', fontsize=13, fontweight='bold')\n",
    "ax.legend(loc='best', fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'pythia_{PYTHIA_SIZE}_ua_scatter.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nPlot saved to: pythia_{PYTHIA_SIZE}_ua_scatter.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary for EleutherAI Discord\n",
    "\n",
    "Copy this summary to share results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = results[\"_global_stats\"]\n",
    "sf = results.get(\"scientific_facts\", {})\n",
    "\n",
    "summary = f\"\"\"\n",
    "## Pythia UA Validation Results\n",
    "\n",
    "**Model:** {config['display']}\n",
    "**Dataset:** 230 statement pairs across 6 categories\n",
    "**Method:** Uniformity Asymmetry (embedding clustering) + Output Preference (NLL)\n",
    "\n",
    "### Key Metrics\n",
    "- Mean UA: {gs['mean_asymmetry']:+.4f}\n",
    "- 95% CI: [{gs['bootstrap_95ci'][0]:.4f}, {gs['bootstrap_95ci'][1]:.4f}]\n",
    "- UA-Output Correlation: r = {gs['ua_output_correlation']:.3f}\n",
    "\n",
    "### Scientific Facts Category (for comparison with paper)\n",
    "- UA: {sf.get('asymmetry', 'N/A')}\n",
    "- Output Preference: {sf.get('output_preference', 'N/A')}\n",
    "\n",
    "### Interpretation\n",
    "{'Pythia shows embedding-output alignment (expected for base model)' if gs['ua_output_correlation'] > 0.3 else 'Pythia shows weak correlation - similar to RLHF models (unexpected)'}\n",
    "\n",
    "Full results: [JSON file attached]\n",
    "Repo: github.com/buk81/uniformity-asymmetry\n",
    "\"\"\"\n",
    "\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
