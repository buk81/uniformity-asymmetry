{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer-wise UA Analysis: Finding the Point of Divergence\n",
    "\n",
    "**Purpose:** Test where in the transformer the embedding-output correlation changes.\n",
    "\n",
    "**Research Question:** Does the correlation r(UA, Output) change across layers?\n",
    "\n",
    "**Hypotheses:**\n",
    "1. **Complexity Penalty:** High UA = high output entropy\n",
    "2. **Late-Stage Corruption:** r flips from positive to negative in late layers\n",
    "3. **Superposition:** High UA in ambiguous pairs\n",
    "\n",
    "**Based on:** KV Cache Paper (arXiv:2511.12752) showing early layers encode \"topic trajectory\" while late layers encode \"local discourse.\"\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Davide D'Elia  \n",
    "**Date:** 2026-01-03  \n",
    "**Model:** Pythia-6.9B (strongest effect: r = -0.87)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers accelerate torch numpy scipy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "MODEL_NAME = \"EleutherAI/pythia-6.9b\"\n",
    "MODEL_DISPLAY = \"Pythia-6.9B\"\n",
    "\n",
    "print(f\"Loading {MODEL_DISPLAY}...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    output_hidden_states=True  # IMPORTANT: We need all layer outputs\n",
    ")\n",
    "\n",
    "print(f\"Model loaded on: {model.device}\")\n",
    "print(f\"Number of layers: {model.config.num_hidden_layers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "!wget -q https://raw.githubusercontent.com/buk81/uniformity-asymmetry/main/dataset.json\n",
    "\n",
    "with open('dataset.json', 'r') as f:\n",
    "    DATASET = json.load(f)\n",
    "\n",
    "total_pairs = sum(len(cat['pairs']) for cat in DATASET.values())\n",
    "print(f\"Loaded {total_pairs} statement pairs across {len(DATASET)} categories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Functions: Multiple Embedding Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_layer_embeddings(text: str, model, tokenizer) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Extract embeddings using multiple strategies:\n",
    "    - mean_pooled: Mean over all tokens (skip BOS), final layer\n",
    "    - last_token: Last non-pad token, final layer\n",
    "    - layer_X: Mean-pooled embedding from layer X\n",
    "    \n",
    "    Returns dict with all embeddings.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "        hidden_states = outputs.hidden_states  # Tuple of (n_layers + 1) tensors\n",
    "        \n",
    "    # Number of layers (excluding embedding layer)\n",
    "    n_layers = len(hidden_states) - 1\n",
    "    \n",
    "    # Attention mask for last-token detection\n",
    "    attention_mask = inputs['attention_mask']\n",
    "    last_idx = (attention_mask[0].sum() - 1).item()\n",
    "    \n",
    "    embeddings = {}\n",
    "    \n",
    "    # 1. Mean-pooled, final layer (current method)\n",
    "    final_hidden = hidden_states[-1]\n",
    "    embeddings['mean_pooled'] = final_hidden[0, 1:, :].mean(dim=0).cpu().numpy().astype(np.float32)\n",
    "    \n",
    "    # 2. Last-token, final layer (robust against EOS/Pad)\n",
    "    embeddings['last_token'] = final_hidden[0, last_idx, :].cpu().numpy().astype(np.float32)\n",
    "    \n",
    "    # 3. First-token (BOS), final layer\n",
    "    embeddings['first_token'] = final_hidden[0, 0, :].cpu().numpy().astype(np.float32)\n",
    "    \n",
    "    # 4. Layer-by-layer embeddings (every 4th layer)\n",
    "    for layer_idx in range(0, n_layers + 1, 4):\n",
    "        layer_hidden = hidden_states[layer_idx]\n",
    "        # Mean-pooled for this layer\n",
    "        embeddings[f'layer_{layer_idx}'] = layer_hidden[0, 1:, :].mean(dim=0).cpu().numpy().astype(np.float32)\n",
    "    \n",
    "    # Make sure we have the final layer\n",
    "    if f'layer_{n_layers}' not in embeddings:\n",
    "        embeddings[f'layer_{n_layers}'] = hidden_states[-1][0, 1:, :].mean(dim=0).cpu().numpy().astype(np.float32)\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def get_output_preference(text_a: str, text_b: str, model, tokenizer) -> float:\n",
    "    \"\"\"\n",
    "    Calculate output preference as NLL(B) - NLL(A).\n",
    "    Positive = prefers A, Negative = prefers B.\n",
    "    \"\"\"\n",
    "    def get_nll(text):\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "            return outputs.loss.item()\n",
    "    \n",
    "    nll_a = get_nll(text_a)\n",
    "    nll_b = get_nll(text_b)\n",
    "    \n",
    "    return nll_b - nll_a\n",
    "\n",
    "\n",
    "def uniformity_score(embeddings: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculate average pairwise cosine similarity (uniformity).\n",
    "    \"\"\"\n",
    "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    normalized = embeddings / (norms + 1e-10)\n",
    "    kernel = normalized @ normalized.T\n",
    "    n = kernel.shape[0]\n",
    "    idx = np.triu_indices(n, k=1)\n",
    "    return float(np.mean(kernel[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Layer-wise Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_layer_analysis(model, tokenizer, dataset: dict, verbose: bool = True) -> dict:\n",
    "    \"\"\"\n",
    "    Run UA analysis for multiple embedding strategies.\n",
    "    \n",
    "    Returns dict with results for each strategy.\n",
    "    \"\"\"\n",
    "    # Collect all embeddings and preferences\n",
    "    all_embeddings_a = {}  # strategy -> list of embeddings\n",
    "    all_embeddings_b = {}\n",
    "    all_preferences = []\n",
    "    category_indices = []  # Track which category each pair belongs to\n",
    "    \n",
    "    total_pairs = sum(len(cat[\"pairs\"]) for cat in dataset.values())\n",
    "    processed = 0\n",
    "    \n",
    "    for category_name, category_data in dataset.items():\n",
    "        if verbose:\n",
    "            print(f\"\\nProcessing: {category_name}\")\n",
    "        \n",
    "        pairs = category_data[\"pairs\"]\n",
    "        \n",
    "        for stmt_a, stmt_b in pairs:\n",
    "            processed += 1\n",
    "            if verbose and processed % 20 == 0:\n",
    "                print(f\"  [{processed:03d}/{total_pairs}]\")\n",
    "            \n",
    "            # Get embeddings for all strategies\n",
    "            embs_a = get_all_layer_embeddings(stmt_a, model, tokenizer)\n",
    "            embs_b = get_all_layer_embeddings(stmt_b, model, tokenizer)\n",
    "            \n",
    "            # Store embeddings\n",
    "            for strategy in embs_a.keys():\n",
    "                if strategy not in all_embeddings_a:\n",
    "                    all_embeddings_a[strategy] = []\n",
    "                    all_embeddings_b[strategy] = []\n",
    "                all_embeddings_a[strategy].append(embs_a[strategy])\n",
    "                all_embeddings_b[strategy].append(embs_b[strategy])\n",
    "            \n",
    "            # Get output preference\n",
    "            pref = get_output_preference(stmt_a, stmt_b, model, tokenizer)\n",
    "            all_preferences.append(pref)\n",
    "            category_indices.append(category_name)\n",
    "    \n",
    "    # Calculate UA and correlation for each strategy\n",
    "    results = {}\n",
    "    strategies = list(all_embeddings_a.keys())\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"RESULTS: Correlation by Embedding Strategy\")\n",
    "        print(f\"{'='*60}\")\n",
    "    \n",
    "    for strategy in strategies:\n",
    "        embs_a = np.array(all_embeddings_a[strategy])\n",
    "        embs_b = np.array(all_embeddings_b[strategy])\n",
    "        \n",
    "        # Calculate per-category UA\n",
    "        category_uas = []\n",
    "        category_prefs = []\n",
    "        \n",
    "        for cat_name in dataset.keys():\n",
    "            cat_mask = [c == cat_name for c in category_indices]\n",
    "            cat_embs_a = embs_a[cat_mask]\n",
    "            cat_embs_b = embs_b[cat_mask]\n",
    "            cat_prefs = np.array(all_preferences)[cat_mask]\n",
    "            \n",
    "            u_a = uniformity_score(cat_embs_a)\n",
    "            u_b = uniformity_score(cat_embs_b)\n",
    "            ua = u_a - u_b\n",
    "            \n",
    "            category_uas.append(ua)\n",
    "            category_prefs.append(np.mean(cat_prefs))\n",
    "        \n",
    "        # Correlation\n",
    "        r, p = stats.pearsonr(category_uas, category_prefs)\n",
    "        \n",
    "        results[strategy] = {\n",
    "            'correlation': float(r),\n",
    "            'p_value': float(p),\n",
    "            'category_uas': category_uas,\n",
    "            'category_prefs': category_prefs\n",
    "        }\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"{strategy:<20} r = {r:+.3f}  (p = {p:.4f})\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(f\"Starting layer-wise analysis on {MODEL_DISPLAY}...\")\n",
    "print(f\"This will take ~30-45 minutes on A100.\\n\")\n",
    "\n",
    "results = run_layer_analysis(model, tokenizer, DATASET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize: Layer Correlation Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract layer-specific results\n",
    "layer_results = [(k, v) for k, v in results.items() if k.startswith('layer_')]\n",
    "layer_results.sort(key=lambda x: int(x[0].split('_')[1]))\n",
    "\n",
    "layer_nums = [int(k.split('_')[1]) for k, v in layer_results]\n",
    "layer_corrs = [v['correlation'] for k, v in layer_results]\n",
    "\n",
    "# Create figure\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Layer correlation curve\n",
    "ax1 = axes[0]\n",
    "ax1.plot(layer_nums, layer_corrs, 'o-', linewidth=2, markersize=8, color='blue')\n",
    "ax1.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax1.set_xlabel('Layer', fontsize=12)\n",
    "ax1.set_ylabel('r(UA, Output Preference)', fontsize=12)\n",
    "ax1.set_title('Correlation by Layer: Point of Divergence', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Mark the divergence point (where r crosses 0 or changes sign)\n",
    "for i in range(len(layer_corrs) - 1):\n",
    "    if layer_corrs[i] * layer_corrs[i+1] < 0:  # Sign change\n",
    "        ax1.axvline(x=(layer_nums[i] + layer_nums[i+1])/2, color='red', linestyle=':', linewidth=2, label='Divergence')\n",
    "        ax1.legend()\n",
    "\n",
    "# Plot 2: Compare strategies\n",
    "ax2 = axes[1]\n",
    "strategies = ['mean_pooled', 'last_token', 'first_token']\n",
    "strategy_corrs = [results[s]['correlation'] for s in strategies]\n",
    "colors = ['blue', 'green', 'orange']\n",
    "\n",
    "bars = ax2.bar(strategies, strategy_corrs, color=colors, edgecolor='black')\n",
    "ax2.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax2.set_ylabel('r(UA, Output Preference)', fontsize=12)\n",
    "ax2.set_title('Correlation by Pooling Strategy', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars, strategy_corrs):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "             f'{val:.3f}', ha='center', va='bottom', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('layer_analysis_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nPlot saved to: layer_analysis_results.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\" LAYER-WISE UA ANALYSIS: {MODEL_DISPLAY}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n--- POOLING STRATEGY COMPARISON ---\")\n",
    "print(f\"{'Strategy':<20} {'r(UA, Output)':<15} {'Interpretation'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for strategy in ['mean_pooled', 'last_token', 'first_token']:\n",
    "    r = results[strategy]['correlation']\n",
    "    if r < -0.5:\n",
    "        interp = \"INVERSE (strong negative)\"\n",
    "    elif r < -0.2:\n",
    "        interp = \"Weak negative\"\n",
    "    elif r < 0.2:\n",
    "        interp = \"DECOUPLED (near zero)\"\n",
    "    elif r < 0.5:\n",
    "        interp = \"Weak positive\"\n",
    "    else:\n",
    "        interp = \"ALIGNED (strong positive)\"\n",
    "    print(f\"{strategy:<20} {r:+.3f}           {interp}\")\n",
    "\n",
    "print(\"\\n--- LAYER-BY-LAYER CORRELATION ---\")\n",
    "print(f\"{'Layer':<10} {'r(UA, Output)':<15}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "for layer, corr in zip(layer_nums, layer_corrs):\n",
    "    print(f\"Layer {layer:<4} {corr:+.3f}\")\n",
    "\n",
    "# Find divergence point\n",
    "print(\"\\n--- DIVERGENCE ANALYSIS ---\")\n",
    "divergence_found = False\n",
    "for i in range(len(layer_corrs) - 1):\n",
    "    if layer_corrs[i] * layer_corrs[i+1] < 0:\n",
    "        print(f\"Sign change between Layer {layer_nums[i]} and Layer {layer_nums[i+1]}\")\n",
    "        print(f\"  Layer {layer_nums[i]}: r = {layer_corrs[i]:+.3f}\")\n",
    "        print(f\"  Layer {layer_nums[i+1]}: r = {layer_corrs[i+1]:+.3f}\")\n",
    "        divergence_found = True\n",
    "\n",
    "if not divergence_found:\n",
    "    if all(c < 0 for c in layer_corrs):\n",
    "        print(\"No sign change: Correlation is NEGATIVE across all layers\")\n",
    "    elif all(c > 0 for c in layer_corrs):\n",
    "        print(\"No sign change: Correlation is POSITIVE across all layers\")\n",
    "    else:\n",
    "        print(\"Pattern unclear - check individual layer values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare results for saving\n",
    "save_results = {\n",
    "    'model': MODEL_NAME,\n",
    "    'model_display': MODEL_DISPLAY,\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'strategies': {}\n",
    "}\n",
    "\n",
    "for strategy, data in results.items():\n",
    "    save_results['strategies'][strategy] = {\n",
    "        'correlation': data['correlation'],\n",
    "        'p_value': data['p_value']\n",
    "    }\n",
    "\n",
    "# Save to JSON\n",
    "output_file = f\"layer_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(save_results, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to: {output_file}\")\n",
    "\n",
    "# Download\n",
    "from google.colab import files\n",
    "files.download(output_file)\n",
    "files.download('layer_analysis_results.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary for Reddit/Discord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary text\n",
    "mean_r = results['mean_pooled']['correlation']\n",
    "last_r = results['last_token']['correlation']\n",
    "first_r = results['first_token']['correlation']\n",
    "\n",
    "summary = f\"\"\"\n",
    "## Layer-wise UA Analysis Results: {MODEL_DISPLAY}\n",
    "\n",
    "Tested different embedding strategies on the same dataset (230 pairs, 6 categories).\n",
    "\n",
    "### Pooling Strategy Comparison\n",
    "\n",
    "| Strategy | r(UA, Output) |\n",
    "|----------|---------------|\n",
    "| Mean-pooled (all tokens) | {mean_r:+.3f} |\n",
    "| Last-token only | {last_r:+.3f} |\n",
    "| First-token (BOS) | {first_r:+.3f} |\n",
    "\n",
    "### Layer-by-Layer Correlation\n",
    "\n",
    "| Layer | r |\n",
    "|-------|---|\n",
    "\"\"\"\n",
    "\n",
    "for layer, corr in zip(layer_nums, layer_corrs):\n",
    "    summary += f\"| {layer} | {corr:+.3f} |\\n\"\n",
    "\n",
    "# Interpretation\n",
    "if last_r > mean_r + 0.2:\n",
    "    interp = \"Last-token embeddings correlate MORE with output than mean-pooled.\"\n",
    "elif last_r < mean_r - 0.2:\n",
    "    interp = \"Last-token embeddings correlate LESS with output than mean-pooled.\"\n",
    "else:\n",
    "    interp = \"Pooling strategy has minimal effect on correlation.\"\n",
    "\n",
    "summary += f\"\\n### Interpretation\\n\\n{interp}\\n\"\n",
    "\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
