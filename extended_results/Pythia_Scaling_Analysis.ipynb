{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {"id": "header"},
      "source": [
        "# Pythia Scaling Analysis: Does the Layer-8 Peak Shift?\n",
        "\n",
        "**Research Question:**\n",
        "> Verschiebt sich der Layer-Bereich, in dem |r| maximal ist, mit der ModellgrÃ¶sse?\n",
        "\n",
        "**Method:** Run layer-wise UA analysis on Pythia family (410M, 1.4B, 6.9B, 12B)\n",
        "\n",
        "**Hypothesis:** Larger models may show different layer-wise correlation profiles.\n",
        "\n",
        "**Expected Output:** Clear yes/no on whether the \"divergence point\" is scale-dependent.\n",
        "\n",
        "---\n",
        "\n",
        "**Author:** Davide D'Elia  \n",
        "**Date:** 2026-01-03  \n",
        "**Runtime:** ~2-3 hours on A100 (all 4 models)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "setup-header"},
      "source": ["## 1. Setup"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "install"},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q transformers accelerate torch numpy scipy matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "imports"},
      "outputs": [],
      "source": [
        "import os\n",
        "import gc\n",
        "import json\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuration\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "config"},
      "outputs": [],
      "source": [
        "# Models to test (smallest to largest)\n",
        "MODELS = [\n",
        "    {\"name\": \"EleutherAI/pythia-410m\", \"display\": \"Pythia-410M\", \"layers\": 24},\n",
        "    {\"name\": \"EleutherAI/pythia-1.4b\", \"display\": \"Pythia-1.4B\", \"layers\": 24},\n",
        "    {\"name\": \"EleutherAI/pythia-6.9b\", \"display\": \"Pythia-6.9B\", \"layers\": 32},\n",
        "    {\"name\": \"EleutherAI/pythia-12b\", \"display\": \"Pythia-12B\", \"layers\": 36},\n",
        "]\n",
        "\n",
        "print(\"Models to analyze:\")\n",
        "for m in MODELS:\n",
        "    print(f\"  - {m['display']} ({m['layers']} layers)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "load-dataset"},
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "!wget -q https://raw.githubusercontent.com/buk81/uniformity-asymmetry/main/dataset.json\n",
        "\n",
        "with open('dataset.json', 'r') as f:\n",
        "    DATASET = json.load(f)\n",
        "\n",
        "total_pairs = sum(len(cat['pairs']) for cat in DATASET.values())\n",
        "print(f\"Loaded {total_pairs} statement pairs across {len(DATASET)} categories\")\n",
        "for cat_name, cat_data in DATASET.items():\n",
        "    print(f\"  - {cat_name}: {len(cat_data['pairs'])} pairs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "functions-header"},
      "source": ["## 2. Core Functions"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "core-functions"},
      "outputs": [],
      "source": [
        "def get_layer_embeddings(text: str, model, tokenizer, layer_step: int = 4) -> Dict[str, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Extract embeddings from every Nth layer.\n",
        "    \n",
        "    Args:\n",
        "        text: Input text\n",
        "        model: HuggingFace model\n",
        "        tokenizer: HuggingFace tokenizer\n",
        "        layer_step: Sample every Nth layer (default: 4)\n",
        "    \n",
        "    Returns:\n",
        "        Dict with 'layer_X' keys containing mean-pooled embeddings\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs, output_hidden_states=True)\n",
        "        hidden_states = outputs.hidden_states  # Tuple of (n_layers + 1) tensors\n",
        "        \n",
        "    n_layers = len(hidden_states) - 1  # Exclude embedding layer\n",
        "    \n",
        "    embeddings = {}\n",
        "    \n",
        "    # Sample every layer_step layers + final layer\n",
        "    layer_indices = list(range(0, n_layers + 1, layer_step))\n",
        "    if n_layers not in layer_indices:\n",
        "        layer_indices.append(n_layers)\n",
        "    \n",
        "    for layer_idx in layer_indices:\n",
        "        layer_hidden = hidden_states[layer_idx]\n",
        "        # Mean-pooled (skip BOS token)\n",
        "        embeddings[f'layer_{layer_idx}'] = layer_hidden[0, 1:, :].mean(dim=0).cpu().numpy().astype(np.float32)\n",
        "    \n",
        "    return embeddings\n",
        "\n",
        "\n",
        "def get_output_preference(text_a: str, text_b: str, model, tokenizer) -> float:\n",
        "    \"\"\"\n",
        "    Calculate output preference as NLL(B) - NLL(A).\n",
        "    Positive = prefers A, Negative = prefers B.\n",
        "    \"\"\"\n",
        "    def get_nll(text):\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
        "            return outputs.loss.item()\n",
        "    \n",
        "    nll_a = get_nll(text_a)\n",
        "    nll_b = get_nll(text_b)\n",
        "    \n",
        "    return nll_b - nll_a\n",
        "\n",
        "\n",
        "def uniformity_score(embeddings: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Calculate average pairwise cosine similarity (uniformity).\n",
        "    \"\"\"\n",
        "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
        "    normalized = embeddings / (norms + 1e-10)\n",
        "    kernel = normalized @ normalized.T\n",
        "    n = kernel.shape[0]\n",
        "    idx = np.triu_indices(n, k=1)\n",
        "    return float(np.mean(kernel[idx]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "analysis-function"},
      "outputs": [],
      "source": [
        "def run_layer_analysis_for_model(model, tokenizer, dataset: dict, model_display: str, \n",
        "                                  layer_step: int = 4) -> dict:\n",
        "    \"\"\"\n",
        "    Run complete layer-wise UA analysis for a single model.\n",
        "    \n",
        "    Returns dict with layer -> {correlation, p_value}\n",
        "    \"\"\"\n",
        "    # Collect all embeddings and preferences\n",
        "    all_embeddings_a = {}  # layer -> list of embeddings\n",
        "    all_embeddings_b = {}\n",
        "    all_preferences = []\n",
        "    category_indices = []\n",
        "    \n",
        "    total_pairs = sum(len(cat[\"pairs\"]) for cat in dataset.values())\n",
        "    processed = 0\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\" Processing: {model_display}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    for category_name, category_data in dataset.items():\n",
        "        pairs = category_data[\"pairs\"]\n",
        "        \n",
        "        for stmt_a, stmt_b in pairs:\n",
        "            processed += 1\n",
        "            if processed % 50 == 0:\n",
        "                print(f\"  [{processed:03d}/{total_pairs}]\")\n",
        "            \n",
        "            # Get layer embeddings\n",
        "            embs_a = get_layer_embeddings(stmt_a, model, tokenizer, layer_step)\n",
        "            embs_b = get_layer_embeddings(stmt_b, model, tokenizer, layer_step)\n",
        "            \n",
        "            # Store embeddings\n",
        "            for layer_key in embs_a.keys():\n",
        "                if layer_key not in all_embeddings_a:\n",
        "                    all_embeddings_a[layer_key] = []\n",
        "                    all_embeddings_b[layer_key] = []\n",
        "                all_embeddings_a[layer_key].append(embs_a[layer_key])\n",
        "                all_embeddings_b[layer_key].append(embs_b[layer_key])\n",
        "            \n",
        "            # Get output preference\n",
        "            pref = get_output_preference(stmt_a, stmt_b, model, tokenizer)\n",
        "            all_preferences.append(pref)\n",
        "            category_indices.append(category_name)\n",
        "    \n",
        "    # Calculate per-layer correlation\n",
        "    results = {}\n",
        "    \n",
        "    for layer_key in sorted(all_embeddings_a.keys(), key=lambda x: int(x.split('_')[1])):\n",
        "        embs_a = np.array(all_embeddings_a[layer_key])\n",
        "        embs_b = np.array(all_embeddings_b[layer_key])\n",
        "        \n",
        "        # Calculate per-category UA\n",
        "        category_uas = []\n",
        "        category_prefs = []\n",
        "        \n",
        "        for cat_name in dataset.keys():\n",
        "            cat_mask = [c == cat_name for c in category_indices]\n",
        "            cat_embs_a = embs_a[cat_mask]\n",
        "            cat_embs_b = embs_b[cat_mask]\n",
        "            cat_prefs = np.array(all_preferences)[cat_mask]\n",
        "            \n",
        "            u_a = uniformity_score(cat_embs_a)\n",
        "            u_b = uniformity_score(cat_embs_b)\n",
        "            ua = u_a - u_b\n",
        "            \n",
        "            category_uas.append(ua)\n",
        "            category_prefs.append(float(np.mean(cat_prefs)))\n",
        "        \n",
        "        # Correlation\n",
        "        r, p = stats.pearsonr(category_uas, category_prefs)\n",
        "        \n",
        "        layer_num = int(layer_key.split('_')[1])\n",
        "        results[layer_num] = {\n",
        "            'correlation': float(r),\n",
        "            'p_value': float(p)\n",
        "        }\n",
        "    \n",
        "    print(f\"\\n{model_display} - Layer Correlations:\")\n",
        "    for layer_num in sorted(results.keys()):\n",
        "        r = results[layer_num]['correlation']\n",
        "        print(f\"  Layer {layer_num:2d}: r = {r:+.3f}\")\n",
        "    \n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "run-header"},
      "source": ["## 3. Run Scaling Analysis"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "run-analysis"},
      "outputs": [],
      "source": [
        "# Store results for all models\n",
        "all_model_results = {}\n",
        "\n",
        "for model_config in MODELS:\n",
        "    model_name = model_config[\"name\"]\n",
        "    model_display = model_config[\"display\"]\n",
        "    \n",
        "    print(f\"\\n{'#'*70}\")\n",
        "    print(f\"# Loading: {model_display}\")\n",
        "    print(f\"{'#'*70}\")\n",
        "    \n",
        "    # Load model\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\",\n",
        "        output_hidden_states=True\n",
        "    )\n",
        "    \n",
        "    print(f\"Model loaded. Layers: {model.config.num_hidden_layers}\")\n",
        "    \n",
        "    # Run analysis\n",
        "    results = run_layer_analysis_for_model(model, tokenizer, DATASET, model_display)\n",
        "    \n",
        "    # Store results (convert keys to strings for JSON)\n",
        "    all_model_results[model_display] = {\n",
        "        \"model_name\": model_name,\n",
        "        \"n_layers\": int(model.config.num_hidden_layers),\n",
        "        \"layer_results\": {str(k): v for k, v in results.items()}\n",
        "    }\n",
        "    \n",
        "    # Clear GPU memory\n",
        "    del model\n",
        "    del tokenizer\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    print(f\"\\n{model_display} complete. GPU memory cleared.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\" ALL MODELS COMPLETE\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "viz-header"},
      "source": ["## 4. Visualization: Layer Curves Comparison"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "visualization"},
      "outputs": [],
      "source": [
        "# Create comparison plot\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Colors for each model\n",
        "colors = ['#2ecc71', '#3498db', '#9b59b6', '#e74c3c']  # green, blue, purple, red\n",
        "markers = ['o', 's', '^', 'D']\n",
        "\n",
        "# Plot 1: Absolute layer numbers\n",
        "ax1 = axes[0]\n",
        "\n",
        "for idx, (model_display, data) in enumerate(all_model_results.items()):\n",
        "    layer_results = data[\"layer_results\"]\n",
        "    layers = sorted([int(k) for k in layer_results.keys()])\n",
        "    correlations = [layer_results[str(l)]['correlation'] for l in layers]\n",
        "    \n",
        "    ax1.plot(layers, correlations, f'{markers[idx]}-', \n",
        "             color=colors[idx], linewidth=2, markersize=8, \n",
        "             label=model_display, alpha=0.8)\n",
        "\n",
        "ax1.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
        "ax1.set_xlabel('Layer Number', fontsize=12)\n",
        "ax1.set_ylabel('r(UA, Output Preference)', fontsize=12)\n",
        "ax1.set_title('Layer Correlation by Model Size (Absolute)', fontsize=14, fontweight='bold')\n",
        "ax1.legend(loc='lower left')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Normalized layer position (0-100%)\n",
        "ax2 = axes[1]\n",
        "\n",
        "for idx, (model_display, data) in enumerate(all_model_results.items()):\n",
        "    layer_results = data[\"layer_results\"]\n",
        "    n_layers = data[\"n_layers\"]\n",
        "    layers = sorted([int(k) for k in layer_results.keys()])\n",
        "    correlations = [layer_results[str(l)]['correlation'] for l in layers]\n",
        "    \n",
        "    # Normalize to percentage\n",
        "    normalized_layers = [l / n_layers * 100 for l in layers]\n",
        "    \n",
        "    ax2.plot(normalized_layers, correlations, f'{markers[idx]}-', \n",
        "             color=colors[idx], linewidth=2, markersize=8, \n",
        "             label=model_display, alpha=0.8)\n",
        "\n",
        "ax2.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
        "ax2.set_xlabel('Layer Position (% of total depth)', fontsize=12)\n",
        "ax2.set_ylabel('r(UA, Output Preference)', fontsize=12)\n",
        "ax2.set_title('Layer Correlation by Model Size (Normalized)', fontsize=14, fontweight='bold')\n",
        "ax2.legend(loc='lower left')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('pythia_scaling_comparison.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nPlot saved to: pythia_scaling_comparison.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "analysis-header"},
      "source": ["## 5. Analysis: Does the Peak Shift?"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "peak-analysis"},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\" SCALING ANALYSIS: Does the Divergence Point Shift?\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# For each model, find:\n",
        "# 1. Max positive r (and which layer)\n",
        "# 2. Max negative r (and which layer)\n",
        "# 3. Sign changes (divergence points)\n",
        "\n",
        "summary_data = []\n",
        "\n",
        "for model_display, data in all_model_results.items():\n",
        "    layer_results = data[\"layer_results\"]\n",
        "    n_layers = data[\"n_layers\"]\n",
        "    layers = sorted([int(k) for k in layer_results.keys()])\n",
        "    correlations = [layer_results[str(l)]['correlation'] for l in layers]\n",
        "    \n",
        "    print(f\"\\n--- {model_display} ({n_layers} layers) ---\")\n",
        "    \n",
        "    # Max positive\n",
        "    max_pos_r = max(correlations)\n",
        "    max_pos_layer = layers[correlations.index(max_pos_r)]\n",
        "    max_pos_pct = max_pos_layer / n_layers * 100\n",
        "    \n",
        "    # Max negative (most negative)\n",
        "    max_neg_r = min(correlations)\n",
        "    max_neg_layer = layers[correlations.index(max_neg_r)]\n",
        "    max_neg_pct = max_neg_layer / n_layers * 100\n",
        "    \n",
        "    # Final layer\n",
        "    final_layer = max(layers)\n",
        "    final_r = layer_results[str(final_layer)]['correlation']\n",
        "    \n",
        "    print(f\"  Max positive: r = {max_pos_r:+.3f} at Layer {max_pos_layer} ({max_pos_pct:.0f}%)\")\n",
        "    print(f\"  Max negative: r = {max_neg_r:+.3f} at Layer {max_neg_layer} ({max_neg_pct:.0f}%)\")\n",
        "    print(f\"  Final layer:  r = {final_r:+.3f}\")\n",
        "    \n",
        "    # Find sign changes\n",
        "    sign_changes = []\n",
        "    for i in range(len(correlations) - 1):\n",
        "        if correlations[i] * correlations[i+1] < 0:\n",
        "            midpoint = (layers[i] + layers[i+1]) / 2\n",
        "            midpoint_pct = midpoint / n_layers * 100\n",
        "            sign_changes.append({\n",
        "                'layer_before': int(layers[i]),\n",
        "                'layer_after': int(layers[i+1]),\n",
        "                'midpoint_pct': float(midpoint_pct)\n",
        "            })\n",
        "    \n",
        "    if sign_changes:\n",
        "        print(f\"  Sign changes:\")\n",
        "        for sc in sign_changes:\n",
        "            print(f\"    Between Layer {sc['layer_before']} and {sc['layer_after']} (~{sc['midpoint_pct']:.0f}% depth)\")\n",
        "    else:\n",
        "        print(f\"  No sign changes detected\")\n",
        "    \n",
        "    summary_data.append({\n",
        "        'model': model_display,\n",
        "        'n_layers': int(n_layers),\n",
        "        'max_pos_r': float(max_pos_r),\n",
        "        'max_pos_layer': int(max_pos_layer),\n",
        "        'max_pos_pct': float(max_pos_pct),\n",
        "        'max_neg_r': float(max_neg_r),\n",
        "        'max_neg_layer': int(max_neg_layer),\n",
        "        'max_neg_pct': float(max_neg_pct),\n",
        "        'final_r': float(final_r),\n",
        "        'sign_changes': sign_changes\n",
        "    })\n",
        "\n",
        "# Summary table\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\" SUMMARY TABLE\")\n",
        "print(\"=\"*70)\n",
        "print(f\"{'Model':<15} {'Layers':<8} {'Peak r':<12} {'Peak @':<10} {'Final r':<10}\")\n",
        "print(\"-\" * 60)\n",
        "for s in summary_data:\n",
        "    print(f\"{s['model']:<15} {s['n_layers']:<8} {s['max_pos_r']:+.3f}       {s['max_pos_pct']:.0f}%       {s['final_r']:+.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "answer-question"},
      "outputs": [],
      "source": [
        "# Answer the research question\n",
        "print(\"\\n\" + \"#\"*70)\n",
        "print(\"# RESEARCH QUESTION ANSWER\")\n",
        "print(\"#\"*70)\n",
        "\n",
        "# Extract peak positions\n",
        "peak_positions = [(s['model'], s['max_pos_pct']) for s in summary_data]\n",
        "\n",
        "# Check if peaks are at similar normalized positions\n",
        "peak_pcts = [p[1] for p in peak_positions]\n",
        "peak_std = float(np.std(peak_pcts))\n",
        "peak_mean = float(np.mean(peak_pcts))\n",
        "\n",
        "print(f\"\\nPeak positive correlation positions (% of model depth):\")\n",
        "for model, pct in peak_positions:\n",
        "    print(f\"  {model}: {pct:.0f}%\")\n",
        "\n",
        "print(f\"\\nMean: {peak_mean:.1f}%\")\n",
        "print(f\"Std:  {peak_std:.1f}%\")\n",
        "\n",
        "if peak_std < 10:\n",
        "    print(f\"\\n>>> ANSWER: The peak position is STABLE across scales (~{peak_mean:.0f}% depth)\")\n",
        "    print(f\"    This suggests an architectural constant, not emergent behavior.\")\n",
        "    peaks_stable = True\n",
        "else:\n",
        "    print(f\"\\n>>> ANSWER: The peak position SHIFTS with model size\")\n",
        "    print(f\"    This suggests scale-dependent processing regimes.\")\n",
        "    peaks_stable = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "save-header"},
      "source": ["## 6. Save Results"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "save-results"},
      "outputs": [],
      "source": [
        "# Prepare results for saving (all values explicitly converted to Python types)\n",
        "save_data = {\n",
        "    'timestamp': datetime.now().isoformat(),\n",
        "    'research_question': 'Does the layer correlation peak shift with model size?',\n",
        "    'models': all_model_results,\n",
        "    'summary': summary_data,\n",
        "    'peak_analysis': {\n",
        "        'mean_peak_pct': float(peak_mean),\n",
        "        'std_peak_pct': float(peak_std),\n",
        "        'peaks_stable': bool(peaks_stable)\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save to JSON\n",
        "output_file = f\"pythia_scaling_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
        "with open(output_file, 'w') as f:\n",
        "    json.dump(save_data, f, indent=2)\n",
        "\n",
        "print(f\"Results saved to: {output_file}\")\n",
        "\n",
        "# Download\n",
        "from google.colab import files\n",
        "files.download(output_file)\n",
        "files.download('pythia_scaling_comparison.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "interpretation-header"},
      "source": ["## 7. Interpretation Template"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "interpretation"},
      "outputs": [],
      "source": [
        "# Generate interpretation text\n",
        "interpretation = f\"\"\"\n",
        "## Pythia Scaling Analysis: Layer Correlation Across Model Sizes\n",
        "\n",
        "### Research Question\n",
        "> Does the layer-wise correlation pattern (where r flips from positive to negative) \n",
        "> shift with model size?\n",
        "\n",
        "### Models Tested\n",
        "\"\"\"\n",
        "\n",
        "for s in summary_data:\n",
        "    interpretation += f\"- **{s['model']}** ({s['n_layers']} layers): Peak r = {s['max_pos_r']:+.3f} at {s['max_pos_pct']:.0f}% depth\\n\"\n",
        "\n",
        "interpretation += f\"\"\"\n",
        "### Key Finding\n",
        "\n",
        "Peak position variability: {peak_std:.1f}% (std across models)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "if peaks_stable:\n",
        "    interpretation += f\"\"\"\n",
        "**The peak position is STABLE** across model sizes (~{peak_mean:.0f}% depth).\n",
        "\n",
        "This suggests:\n",
        "- The layer-wise correlation pattern is an architectural property, not emergent\n",
        "- The \"mid-layer regime\" appears at a consistent relative depth\n",
        "- Scale does not fundamentally change where the correlation sign flips\n",
        "\"\"\"\n",
        "else:\n",
        "    interpretation += f\"\"\"\n",
        "**The peak position SHIFTS** with model size.\n",
        "\n",
        "This suggests:\n",
        "- The correlation pattern is scale-dependent\n",
        "- Larger models may develop different processing regimes\n",
        "- Emergent behavior in layer specialization\n",
        "\"\"\"\n",
        "\n",
        "print(interpretation)"
      ]
    }
  ]
}
