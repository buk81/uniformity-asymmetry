{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grand Unified Thermodynamic Benchmark\n",
    "\n",
    "**Paper #3 Validation: P1 + P2 + P3 Combined**\n",
    "\n",
    "**Date:** 2026-01-05\n",
    "\n",
    "**Purpose:** Stabilize claims for Paper #3 (Topological Thermodynamics)\n",
    "\n",
    "---\n",
    "\n",
    "## What This Tests\n",
    "\n",
    "| Test | Description | Hypothesis |\n",
    "|------|-------------|------------|\n",
    "| **P1** | Cross-Model Input-Dependency | Different architectures have different BASE LEVELs |\n",
    "| **P2** | Expanded Prompt Set (n=25) | Statistical significance (p < 0.05) |\n",
    "| **P3** | Clich\u00e9 vs Novel | \"Plattit\u00fcden-Tunnel\" hypothesis |\n",
    "\n",
    "## Expected Results\n",
    "\n",
    "| Model | Base Level | Prediction |\n",
    "|-------|------------|------------|\n",
    "| LLaMA 3.1 | < 1.0 (D\u00e4mpfung) | Always braking, modulates 0.48 \u2192 0.80 |\n",
    "| Mistral | \u2248 1.0 (Inertia) | Neutral, modulates around 1.0 |\n",
    "| Gemma | > 1.0 (Instabil) | Tends to explode |\n",
    "| Pythia | < 1.0 (LayerNorm) | Control group |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup\n",
    "!pip install -q transformers accelerate scipy seaborn\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from scipy.stats import entropy, spearmanr, pearsonr, ttest_ind\n",
    "import gc\n",
    "import json\n",
    "from datetime import datetime\n",
    "from google.colab import userdata, files\n",
    "import os\n",
    "\n",
    "# Get HF Token\n",
    "HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "\n",
    "# Configure visualization\n",
    "plt.style.use('seaborn-v0_8-paper')\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "# Global timestamp for all files\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "print(f\"Session timestamp: {TIMESTAMP}\")\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: THE EXPANDED PROMPT SET (P2 + P3 Integration)\n",
    "# 25 prompts across 5 categories\n",
    "\n",
    "PROMPT_DATASET = {\n",
    "    \"Factual\": [\n",
    "        \"The capital city of France is\",\n",
    "        \"The atomic number of oxygen is\",\n",
    "        \"Water boils at a temperature of\",\n",
    "        \"The largest planet in our solar system is\",\n",
    "        \"The currency used in Japan is\"\n",
    "    ],\n",
    "    \"Syntactic\": [\n",
    "        \"The agreement, which, notwithstanding the fact that it was signed only yesterday, effectively binds all parties immediately, stipulates that\",\n",
    "        \"Although the weather was extremely cold, and despite the fact that they had no coats, the children decided to\",\n",
    "        \"The professor, having reviewed the complex derivation multiple times without finding the error, finally realized that\",\n",
    "        \"To imply that such a fundamental shift in policy could occur without significant public debate is to suggest that\",\n",
    "        \"Not only did the experiment fail to yield the expected results, but it also demonstrated that the initial hypothesis was\"\n",
    "    ],\n",
    "    \"Cliche\": [\n",
    "        \"The true meaning of happiness is often found in\",\n",
    "        \"Actions speak louder than\",\n",
    "        \"It is what it is, and we must simply\",\n",
    "        \"Time heals all\",\n",
    "        \"Life is a journey, not a\"\n",
    "    ],\n",
    "    \"Novel\": [\n",
    "        \"The epistemological implications of quantum decoherence suggest that the observer is\",\n",
    "        \"If consciousness creates reality, then the paradox of the unobserved electron implies\",\n",
    "        \"The intersection of baroque architecture and cybernetic theory creates a space where\",\n",
    "        \"Calculating the trajectory of a hyperspace jump requires factoring in the variability of\",\n",
    "        \"The symbiotic relationship between fungal mycelium and digital neural networks results in\"\n",
    "    ],\n",
    "    \"Nonsense\": [\n",
    "        \"Table sky run blue jump quickly under over\",\n",
    "        \"Purple idea furiously sleep colorless green\",\n",
    "        \"Clock river dance potato seven fast\",\n",
    "        \"Window eat loud tomorrow yellow under\",\n",
    "        \"Fish bicycle logic cloud mountain swim\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Category metadata for analysis\n",
    "CATEGORY_METADATA = {\n",
    "    \"Factual\": {\"expected_gain\": \"low\", \"expected_entropy\": \"medium\", \"complexity\": 1},\n",
    "    \"Syntactic\": {\"expected_gain\": \"high\", \"expected_entropy\": \"medium\", \"complexity\": 5},\n",
    "    \"Cliche\": {\"expected_gain\": \"medium\", \"expected_entropy\": \"low\", \"complexity\": 2},\n",
    "    \"Novel\": {\"expected_gain\": \"high\", \"expected_entropy\": \"high\", \"complexity\": 4},\n",
    "    \"Nonsense\": {\"expected_gain\": \"medium\", \"expected_entropy\": \"very_high\", \"complexity\": 3}\n",
    "}\n",
    "\n",
    "# Count prompts\n",
    "total_prompts = sum(len(v) for v in PROMPT_DATASET.values())\n",
    "print(f\"Total prompts: {total_prompts} (5 categories x 5 prompts)\")\n",
    "for cat, prompts in PROMPT_DATASET.items():\n",
    "    print(f\"  {cat}: {len(prompts)} prompts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: MODEL ZOO (P1 - Cross-Architecture)\n",
    "\n",
    "MODELS_TO_TEST = {\n",
    "    \"Pythia-6.9B\": {\n",
    "        \"hf_path\": \"EleutherAI/pythia-6.9b\",\n",
    "        \"norm_type\": \"LayerNorm\",\n",
    "        \"expected_base\": \"< 1.0\",\n",
    "        \"role\": \"Control (LayerNorm)\"\n",
    "    },\n",
    "    \"Gemma-7B\": {\n",
    "        \"hf_path\": \"google/gemma-7b\",\n",
    "        \"norm_type\": \"RMSNorm\",\n",
    "        \"expected_base\": \"> 1.0\",\n",
    "        \"role\": \"Exploder\"\n",
    "    },\n",
    "    \"Mistral-7B\": {\n",
    "        \"hf_path\": \"mistralai/Mistral-7B-v0.1\",\n",
    "        \"norm_type\": \"RMSNorm\",\n",
    "        \"expected_base\": \"~ 1.0\",\n",
    "        \"role\": \"Inertia\"\n",
    "    },\n",
    "    \"LLaMA-3.1-8B\": {\n",
    "        \"hf_path\": \"meta-llama/Llama-3.1-8B\",\n",
    "        \"norm_type\": \"RMSNorm\",\n",
    "        \"expected_base\": \"< 1.0\",\n",
    "        \"role\": \"Dampener\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Models to test:\")\n",
    "for name, info in MODELS_TO_TEST.items():\n",
    "    print(f\"  {name}: {info['norm_type']} | Expected: {info['expected_base']} | Role: {info['role']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: MEASUREMENT ENGINE\n",
    "\n",
    "def get_layer_list(model):\n",
    "    \"\"\"Get the transformer layers from different model architectures.\"\"\"\n",
    "    if hasattr(model, 'model') and hasattr(model.model, 'layers'):\n",
    "        return model.model.layers  # LLaMA, Mistral, Gemma\n",
    "    elif hasattr(model, 'gpt_neox') and hasattr(model.gpt_neox, 'layers'):\n",
    "        return model.gpt_neox.layers  # Pythia\n",
    "    elif hasattr(model, 'transformer') and hasattr(model.transformer, 'h'):\n",
    "        return model.transformer.h  # GPT-2 style\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model architecture: {type(model)}\")\n",
    "\n",
    "def measure_thermodynamics(model, tokenizer, text, device='cuda'):\n",
    "    \"\"\"Measure residual stream gain and output entropy for a given input.\"\"\"\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Hooks to capture residual stream norms\n",
    "    norms = []\n",
    "    \n",
    "    def get_norm_hook():\n",
    "        def hook(module, input, output):\n",
    "            if isinstance(output, tuple):\n",
    "                hidden_state = output[0]\n",
    "            else:\n",
    "                hidden_state = output\n",
    "            # Norm of last token\n",
    "            last_token_norm = torch.norm(hidden_state[0, -1]).item()\n",
    "            norms.append(last_token_norm)\n",
    "        return hook\n",
    "    \n",
    "    # Register hooks on all layers\n",
    "    handles = []\n",
    "    layers = get_layer_list(model)\n",
    "    for layer in layers:\n",
    "        handles.append(layer.register_forward_hook(get_norm_hook()))\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "    \n",
    "    # Cleanup hooks\n",
    "    for h in handles:\n",
    "        h.remove()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    if len(norms) >= 2:\n",
    "        # Last Layer Gain = Output of Last Layer / Output of Penultimate Layer\n",
    "        last_gain = norms[-1] / norms[-2] if norms[-2] > 0 else 1.0\n",
    "        # Total Amplification = Final / Initial\n",
    "        total_amp = norms[-1] / norms[0] if norms[0] > 0 else 1.0\n",
    "    else:\n",
    "        last_gain = 1.0\n",
    "        total_amp = 1.0\n",
    "    \n",
    "    # Output Entropy (next token distribution)\n",
    "    last_token_logits = logits[0, -1, :]\n",
    "    probs = torch.softmax(last_token_logits, dim=0).cpu().numpy()\n",
    "    ent = entropy(probs)\n",
    "    \n",
    "    # Top token and probability\n",
    "    top_idx = torch.argmax(last_token_logits).item()\n",
    "    top_prob = probs[top_idx]\n",
    "    top_token = tokenizer.decode([top_idx])\n",
    "    \n",
    "    return {\n",
    "        \"last_gain\": last_gain,\n",
    "        \"total_amp\": total_amp,\n",
    "        \"entropy\": ent,\n",
    "        \"top_token\": top_token,\n",
    "        \"top_prob\": top_prob,\n",
    "        \"n_layers\": len(norms),\n",
    "        \"all_norms\": norms\n",
    "    }\n",
    "\n",
    "print(\"Measurement engine ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: EXECUTION LOOP\n",
    "\n",
    "all_results = []\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"STARTING GRAND UNIFIED THERMODYNAMIC BENCHMARK\")\n",
    "print(f\"Models: {len(MODELS_TO_TEST)} | Prompts: {total_prompts}\")\n",
    "print(f\"Total measurements: {len(MODELS_TO_TEST) * total_prompts}\")\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "for model_name, model_info in MODELS_TO_TEST.items():\n",
    "    hf_path = model_info[\"hf_path\"]\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Loading {model_name} ({model_info['role']})...\")\n",
    "    print(f\"  Path: {hf_path}\")\n",
    "    print(f\"  Norm: {model_info['norm_type']}\")\n",
    "    print(f\"  Expected Base: {model_info['expected_base']}\")\n",
    "    \n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(hf_path, token=HF_TOKEN)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            hf_path,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            token=HF_TOKEN\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n  Testing {len(PROMPT_DATASET)} categories ({total_prompts} prompts)...\")\n",
    "        \n",
    "        for category, prompts in PROMPT_DATASET.items():\n",
    "            print(f\"    {category}: \", end=\"\")\n",
    "            for i, prompt in enumerate(prompts):\n",
    "                res = measure_thermodynamics(model, tokenizer, prompt)\n",
    "                \n",
    "                all_results.append({\n",
    "                    \"Model\": model_name,\n",
    "                    \"Norm_Type\": model_info[\"norm_type\"],\n",
    "                    \"Role\": model_info[\"role\"],\n",
    "                    \"Category\": category,\n",
    "                    \"Complexity\": CATEGORY_METADATA[category][\"complexity\"],\n",
    "                    \"Prompt\": prompt,\n",
    "                    \"Prompt_Short\": prompt[:40] + \"...\",\n",
    "                    \"Entropy\": res[\"entropy\"],\n",
    "                    \"Last_Gain\": res[\"last_gain\"],\n",
    "                    \"Total_Amp\": res[\"total_amp\"],\n",
    "                    \"Top_Token\": res[\"top_token\"],\n",
    "                    \"Top_Prob\": res[\"top_prob\"],\n",
    "                    \"N_Layers\": res[\"n_layers\"]\n",
    "                })\n",
    "                print(\".\", end=\"\", flush=True)\n",
    "            print(f\" Done (n={len(prompts)})\")\n",
    "        \n",
    "        # Cleanup\n",
    "        del model\n",
    "        del tokenizer\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        print(f\"\\n  {model_name} complete. Memory cleared.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n  FAILED: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"BENCHMARK COMPLETE\")\n",
    "print(f\"Total measurements: {len(all_results)}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: CREATE DATAFRAME & SAVE\n",
    "\n",
    "df = pd.DataFrame(all_results)\n",
    "\n",
    "# Define filenames with global timestamp\n",
    "CSV_FILE = f\"thermodynamic_benchmark_{TIMESTAMP}.csv\"\n",
    "JSON_FILE = f\"thermodynamic_benchmark_{TIMESTAMP}.json\"\n",
    "PNG_MAIN = f\"grand_unified_benchmark_{TIMESTAMP}.png\"\n",
    "PNG_DETAIL = f\"per_model_detail_{TIMESTAMP}.png\"\n",
    "\n",
    "# Save raw results\n",
    "df.to_csv(CSV_FILE, index=False)\n",
    "print(f\"Saved: {CSV_FILE}\")\n",
    "\n",
    "# Display summary\n",
    "print(\"\\nRESULTS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(df.groupby(['Model', 'Category'])[['Last_Gain', 'Entropy']].mean().round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: STATISTICAL ANALYSIS\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STATISTICAL ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. Per-Model Correlation: Entropy vs Gain\n",
    "print(\"\\n1. CORRELATION: Entropy vs Last_Gain (per model)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "correlation_results = []\n",
    "for model in df['Model'].unique():\n",
    "    subset = df[df['Model'] == model]\n",
    "    \n",
    "    # Spearman (rank correlation)\n",
    "    spearman_r, spearman_p = spearmanr(subset['Entropy'], subset['Last_Gain'])\n",
    "    \n",
    "    # Pearson (linear correlation)\n",
    "    pearson_r, pearson_p = pearsonr(subset['Entropy'], subset['Last_Gain'])\n",
    "    \n",
    "    sig_s = \"***\" if spearman_p < 0.001 else \"**\" if spearman_p < 0.01 else \"*\" if spearman_p < 0.05 else \"ns\"\n",
    "    sig_p = \"***\" if pearson_p < 0.001 else \"**\" if pearson_p < 0.01 else \"*\" if pearson_p < 0.05 else \"ns\"\n",
    "    \n",
    "    correlation_results.append({\n",
    "        \"Model\": model,\n",
    "        \"Spearman_r\": spearman_r,\n",
    "        \"Spearman_p\": spearman_p,\n",
    "        \"Pearson_r\": pearson_r,\n",
    "        \"Pearson_p\": pearson_p\n",
    "    })\n",
    "    \n",
    "    print(f\"{model:<20} | Spearman: r={spearman_r:+.3f} p={spearman_p:.4f} ({sig_s})\")\n",
    "    print(f\"{'':<20} | Pearson:  r={pearson_r:+.3f} p={pearson_p:.4f} ({sig_p})\")\n",
    "\n",
    "# 2. Per-Model Correlation: Complexity vs Gain  \n",
    "print(\"\\n2. CORRELATION: Complexity vs Last_Gain (per model)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "complexity_correlations = []\n",
    "for model in df['Model'].unique():\n",
    "    subset = df[df['Model'] == model]\n",
    "    spearman_r, spearman_p = spearmanr(subset['Complexity'], subset['Last_Gain'])\n",
    "    sig = \"***\" if spearman_p < 0.001 else \"**\" if spearman_p < 0.01 else \"*\" if spearman_p < 0.05 else \"ns\"\n",
    "    print(f\"{model:<20} | r={spearman_r:+.3f} p={spearman_p:.4f} ({sig})\")\n",
    "    complexity_correlations.append({\n",
    "        \"Model\": model,\n",
    "        \"Complexity_Spearman_r\": spearman_r,\n",
    "        \"Complexity_Spearman_p\": spearman_p\n",
    "    })\n",
    "\n",
    "# 3. Base Level Analysis (mean gain per model)\n",
    "print(\"\\n3. BASE LEVEL ANALYSIS (Architektur-Bias)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "base_levels = df.groupby('Model')['Last_Gain'].agg(['mean', 'std', 'min', 'max'])\n",
    "print(base_levels.round(3))\n",
    "\n",
    "# 4. Modulation Range (max - min per model)\n",
    "print(\"\\n4. MODULATION RANGE (Input-Dependency)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for model in df['Model'].unique():\n",
    "    subset = df[df['Model'] == model]\n",
    "    min_gain = subset['Last_Gain'].min()\n",
    "    max_gain = subset['Last_Gain'].max()\n",
    "    modulation = max_gain - min_gain\n",
    "    print(f\"{model:<20} | Min: {min_gain:.3f} | Max: {max_gain:.3f} | Range: {modulation:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: P3 VALIDATION - Cliche vs Novel\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"P3 VALIDATION: PLATITUEDEN-TUNNEL HYPOTHESIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nHypothesis: Clich\\u00e9s have LOWER entropy than Novel prompts\")\n",
    "print(\"(Because LLMs have strong priors for common phrases)\\n\")\n",
    "\n",
    "p3_results = []\n",
    "for model in df['Model'].unique():\n",
    "    subset = df[df['Model'] == model]\n",
    "    \n",
    "    cliche_entropy = subset[subset['Category'] == 'Cliche']['Entropy']\n",
    "    novel_entropy = subset[subset['Category'] == 'Novel']['Entropy']\n",
    "    \n",
    "    # T-test\n",
    "    t_stat, p_val = ttest_ind(cliche_entropy, novel_entropy)\n",
    "    \n",
    "    # Effect size (Cohen's d)\n",
    "    pooled_std = np.sqrt((cliche_entropy.std()**2 + novel_entropy.std()**2) / 2)\n",
    "    cohens_d = (cliche_entropy.mean() - novel_entropy.mean()) / pooled_std if pooled_std > 0 else 0\n",
    "    \n",
    "    sig = \"***\" if p_val < 0.001 else \"**\" if p_val < 0.01 else \"*\" if p_val < 0.05 else \"ns\"\n",
    "    direction = \"CONFIRMED\" if cliche_entropy.mean() < novel_entropy.mean() else \"REJECTED\"\n",
    "    \n",
    "    p3_results.append({\n",
    "        \"Model\": model,\n",
    "        \"Cliche_Entropy_Mean\": cliche_entropy.mean(),\n",
    "        \"Novel_Entropy_Mean\": novel_entropy.mean(),\n",
    "        \"t_stat\": t_stat,\n",
    "        \"p_val\": p_val,\n",
    "        \"cohens_d\": cohens_d,\n",
    "        \"hypothesis\": direction\n",
    "    })\n",
    "    \n",
    "    print(f\"{model}:\")\n",
    "    print(f\"  Clich\\u00e9 entropy:  {cliche_entropy.mean():.3f} \\u00b1 {cliche_entropy.std():.3f}\")\n",
    "    print(f\"  Novel entropy:   {novel_entropy.mean():.3f} \\u00b1 {novel_entropy.std():.3f}\")\n",
    "    print(f\"  t-test: t={t_stat:.3f}, p={p_val:.4f} ({sig})\")\n",
    "    print(f\"  Cohen's d: {cohens_d:.3f}\")\n",
    "    print(f\"  Hypothesis: {direction}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: VISUALIZATION - Main Results\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "\n",
    "# A. Scatter: Entropy vs Gain (all models)\n",
    "ax1 = axes[0, 0]\n",
    "for model in df['Model'].unique():\n",
    "    subset = df[df['Model'] == model]\n",
    "    ax1.scatter(subset['Entropy'], subset['Last_Gain'], label=model, alpha=0.7, s=80)\n",
    "ax1.axhline(1.0, ls='--', c='gray', alpha=0.5, label='Neutral (1.0)')\n",
    "ax1.set_xlabel('Output Entropy (nats)')\n",
    "ax1.set_ylabel('Last Layer Gain')\n",
    "ax1.set_title('A. Entropy vs Gain (All Models)')\n",
    "ax1.legend(loc='best')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# B. Box: Gain by Category (faceted by model)\n",
    "ax2 = axes[0, 1]\n",
    "category_order = ['Factual', 'Cliche', 'Nonsense', 'Novel', 'Syntactic']\n",
    "sns.boxplot(data=df, x='Category', y='Last_Gain', hue='Model', ax=ax2, order=category_order)\n",
    "ax2.axhline(1.0, ls='--', c='gray', alpha=0.5)\n",
    "ax2.set_xlabel('Prompt Category')\n",
    "ax2.set_ylabel('Last Layer Gain')\n",
    "ax2.set_title('B. Gain by Category (Bremspedal-Gesetz)')\n",
    "ax2.legend(loc='upper right', fontsize=8)\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# C. Box: Entropy by Category  \n",
    "ax3 = axes[1, 0]\n",
    "sns.boxplot(data=df, x='Category', y='Entropy', hue='Model', ax=ax3, order=category_order)\n",
    "ax3.set_xlabel('Prompt Category')\n",
    "ax3.set_ylabel('Output Entropy (nats)')\n",
    "ax3.set_title('C. Entropy by Category (Plattit\\u00fcden-Tunnel)')\n",
    "ax3.legend(loc='upper right', fontsize=8)\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# D. Base Level Bar Chart\n",
    "ax4 = axes[1, 1]\n",
    "model_means = df.groupby('Model')['Last_Gain'].mean().sort_values()\n",
    "model_stds = df.groupby('Model')['Last_Gain'].std()\n",
    "colors = ['#d62728' if m < 1.0 else '#2ca02c' if m > 1.0 else '#1f77b4' for m in model_means]\n",
    "bars = ax4.bar(model_means.index, model_means.values, yerr=model_stds[model_means.index].values, \n",
    "               color=colors, alpha=0.7, capsize=5)\n",
    "ax4.axhline(1.0, ls='--', c='gray', alpha=0.5, label='Neutral')\n",
    "ax4.set_xlabel('Model')\n",
    "ax4.set_ylabel('Mean Last Layer Gain')\n",
    "ax4.set_title('D. Architecture Base Level')\n",
    "ax4.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, val in zip(bars, model_means.values):\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "             f'{val:.2f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(PNG_MAIN, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFigure saved: {PNG_MAIN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: VISUALIZATION - Per-Model Detail\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "\n",
    "models_list = list(df['Model'].unique())\n",
    "for idx, model in enumerate(models_list):\n",
    "    if idx >= 4:\n",
    "        break\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    subset = df[df['Model'] == model]\n",
    "    \n",
    "    # Scatter with category colors\n",
    "    for cat in category_order:\n",
    "        cat_data = subset[subset['Category'] == cat]\n",
    "        ax.scatter(cat_data['Entropy'], cat_data['Last_Gain'], label=cat, s=100, alpha=0.8)\n",
    "    \n",
    "    ax.axhline(1.0, ls='--', c='gray', alpha=0.5)\n",
    "    ax.set_xlabel('Output Entropy')\n",
    "    ax.set_ylabel('Last Layer Gain')\n",
    "    \n",
    "    # Add correlation\n",
    "    r, p = spearmanr(subset['Entropy'], subset['Last_Gain'])\n",
    "    sig = \"***\" if p < 0.001 else \"**\" if p < 0.01 else \"*\" if p < 0.05 else \"\"\n",
    "    ax.set_title(f'{model}\\n(Spearman r={r:.3f}{sig}, p={p:.4f})')\n",
    "    ax.legend(loc='best', fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(PNG_DETAIL, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFigure saved: {PNG_DETAIL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: SAVE RESULTS AS JSON\n",
    "\n",
    "results_json = {\n",
    "    \"experiment\": \"Grand Unified Thermodynamic Benchmark\",\n",
    "    \"date\": TIMESTAMP,\n",
    "    \"n_models\": len(MODELS_TO_TEST),\n",
    "    \"n_prompts\": total_prompts,\n",
    "    \"n_measurements\": len(df),\n",
    "    \"models\": list(MODELS_TO_TEST.keys()),\n",
    "    \"categories\": list(PROMPT_DATASET.keys()),\n",
    "    \"base_levels\": {k: float(v) for k, v in df.groupby('Model')['Last_Gain'].mean().to_dict().items()},\n",
    "    \"modulation_ranges\": {\n",
    "        model: {\n",
    "            \"min\": float(df[df['Model'] == model]['Last_Gain'].min()),\n",
    "            \"max\": float(df[df['Model'] == model]['Last_Gain'].max()),\n",
    "            \"range\": float(df[df['Model'] == model]['Last_Gain'].max() - df[df['Model'] == model]['Last_Gain'].min())\n",
    "        }\n",
    "        for model in df['Model'].unique()\n",
    "    },\n",
    "    \"entropy_correlations\": correlation_results,\n",
    "    \"complexity_correlations\": complexity_correlations,\n",
    "    \"p3_platitude_tunnel\": p3_results,\n",
    "    \"all_results\": all_results\n",
    "}\n",
    "\n",
    "with open(JSON_FILE, 'w') as f:\n",
    "    json.dump(results_json, f, indent=2, default=str)\n",
    "\n",
    "print(f\"Results saved to {JSON_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: FINAL VERDICT\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FINAL VERDICT: PAPER #3 CLAIM VALIDATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. Base Level Hypothesis\n",
    "print(\"\\n1. BASE LEVEL HYPOTHESIS (Architektur-Bias)\")\n",
    "print(\"-\" * 60)\n",
    "base_means = df.groupby('Model')['Last_Gain'].mean()\n",
    "for model in ['LLaMA-3.1-8B', 'Mistral-7B', 'Gemma-7B', 'Pythia-6.9B']:\n",
    "    if model in base_means.index:\n",
    "        mean = base_means[model]\n",
    "        expected = MODELS_TO_TEST[model]['expected_base']\n",
    "        if '< 1' in expected and mean < 1.0:\n",
    "            status = 'CONFIRMED'\n",
    "        elif '> 1' in expected and mean > 1.0:\n",
    "            status = 'CONFIRMED'\n",
    "        elif '~ 1' in expected and 0.9 < mean < 1.1:\n",
    "            status = 'CONFIRMED'\n",
    "        else:\n",
    "            status = 'UNEXPECTED'\n",
    "        print(f\"  {model}: Mean={mean:.3f} | Expected: {expected} | {status}\")\n",
    "\n",
    "# 2. Input-Dependency Hypothesis\n",
    "print(\"\\n2. INPUT-DEPENDENCY HYPOTHESIS (Bremspedal-Gesetz)\")\n",
    "print(\"-\" * 60)\n",
    "for model in df['Model'].unique():\n",
    "    subset = df[df['Model'] == model]\n",
    "    r, p = spearmanr(subset['Complexity'], subset['Last_Gain'])\n",
    "    status = 'CONFIRMED' if p < 0.05 and r > 0 else 'NOT SIGNIFICANT' if p >= 0.05 else 'REJECTED'\n",
    "    print(f\"  {model}: r={r:+.3f}, p={p:.4f} | {status}\")\n",
    "\n",
    "# 3. Plattitueden-Tunnel Hypothesis\n",
    "print(\"\\n3. PLATTITUEDEN-TUNNEL HYPOTHESIS\")\n",
    "print(\"-\" * 60)\n",
    "for model in df['Model'].unique():\n",
    "    subset = df[df['Model'] == model]\n",
    "    cliche_ent = subset[subset['Category'] == 'Cliche']['Entropy'].mean()\n",
    "    novel_ent = subset[subset['Category'] == 'Novel']['Entropy'].mean()\n",
    "    status = 'CONFIRMED' if cliche_ent < novel_ent else 'REJECTED'\n",
    "    print(f\"  {model}: Cliche={cliche_ent:.3f} vs Novel={novel_ent:.3f} | {status}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"BENCHMARK COMPLETE\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: DOWNLOAD ALL RESULTS\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DOWNLOADING RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# List all files to download\n",
    "files_to_download = [CSV_FILE, JSON_FILE, PNG_MAIN, PNG_DETAIL]\n",
    "\n",
    "print(\"\\nFiles to download:\")\n",
    "for f in files_to_download:\n",
    "    if os.path.exists(f):\n",
    "        size = os.path.getsize(f) / 1024  # KB\n",
    "        print(f\"  {f} ({size:.1f} KB)\")\n",
    "    else:\n",
    "        print(f\"  {f} (NOT FOUND)\")\n",
    "\n",
    "print(\"\\nStarting downloads...\")\n",
    "\n",
    "# Download each file\n",
    "for f in files_to_download:\n",
    "    if os.path.exists(f):\n",
    "        try:\n",
    "            files.download(f)\n",
    "            print(f\"  Downloaded: {f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  FAILED to download {f}: {e}\")\n",
    "    else:\n",
    "        print(f\"  SKIPPED (not found): {f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ALL DOWNLOADS COMPLETE\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU",
  "gpuClass": "premium"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
