{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# The \"Beautiful Ones\" Analysis: Per-Head Contribution to Dampening\n",
    "\n",
    "**Paper #3 Experiment:** Understanding WHY Pythia dampens\n",
    "\n",
    "**Universe 25 Analogy:**\n",
    "In Calhoun's mouse utopia experiment, some mice (\"Beautiful Ones\") withdrew from social interaction and focused only on self-grooming. They were physically perfect but socially non-functional.\n",
    "\n",
    "**Hypothesis:** In Pythia's crowded feature space (high œÅ), some attention heads may become \"Beautiful Ones\" - contributing NEGATIVELY to the residual stream (anti-correlation), causing overall dampening.\n",
    "\n",
    "**Key Question:** Are there specific heads that SUBTRACT from the residual stream rather than ADD?\n",
    "\n",
    "**Measurement:**\n",
    "For each head h in layer L:\n",
    "- Attn_output_h = head h's contribution to residual\n",
    "- Correlation with residual growth = sign of contribution\n",
    "- Negative correlation = \"Beautiful One\" (withdrawing energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install transformers torch matplotlib numpy seaborn --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import gc\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models to compare\n",
    "# Pythia (dampening) vs GPT-J (expansion) - same family, opposite behavior\n",
    "\n",
    "MODELS = {\n",
    "    'pythia-6.9b': 'EleutherAI/pythia-6.9b',   # œÅ = 0.25, G ‚âà 0.80 (DAMPEN)\n",
    "    'gpt-j-6b': 'EleutherAI/gpt-j-6B',          # œÅ = 0.0625, G ‚âà 1.065 (EXPAND)\n",
    "}\n",
    "\n",
    "# Select based on GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    if mem >= 20:\n",
    "        MODELS_TO_TEST = list(MODELS.keys())\n",
    "    elif mem >= 18:\n",
    "        MODELS_TO_TEST = ['gpt-j-6b']\n",
    "    else:\n",
    "        MODELS_TO_TEST = []  # Use smaller models instead\n",
    "        print(\"GPU too small for 6B models. Testing smaller variants...\")\n",
    "        MODELS = {\n",
    "            'pythia-1.4b': 'EleutherAI/pythia-1.4b',\n",
    "            'pythia-410m': 'EleutherAI/pythia-410m',\n",
    "        }\n",
    "        MODELS_TO_TEST = list(MODELS.keys())\n",
    "else:\n",
    "    MODELS = {\n",
    "        'pythia-160m': 'EleutherAI/pythia-160m',\n",
    "        'pythia-70m': 'EleutherAI/pythia-70m',\n",
    "    }\n",
    "    MODELS_TO_TEST = list(MODELS.keys())\n",
    "\n",
    "print(f\"Models to test: {MODELS_TO_TEST}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prompts\n",
    "TEST_PROMPTS = [\n",
    "    \"The capital of France is\",\n",
    "    \"Water freezes at\",\n",
    "    \"The quick brown fox\",\n",
    "    \"Actions speak louder than\",\n",
    "    \"In mathematics, pi equals approximately\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerHeadAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyze per-head contributions to residual stream dynamics.\n",
    "    \n",
    "    For Pythia (GPT-NeoX architecture):\n",
    "    - Parallel blocks: attn and mlp see same input\n",
    "    - residual = x + attn(x) + mlp(x)\n",
    "    \n",
    "    We measure how each head's output correlates with residual growth.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer, n_layers, n_heads):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.n_layers = n_layers\n",
    "        self.n_heads = n_heads\n",
    "        self.hooks = []\n",
    "        self.head_outputs = {}  # {layer: (batch, seq, n_heads, d_head)}\n",
    "        self.residual_before = {}  # {layer: residual before this layer}\n",
    "        self.residual_after = {}   # {layer: residual after this layer}\n",
    "    \n",
    "    def _hook_attention_output(self, layer_idx):\n",
    "        \"\"\"Capture per-head attention outputs.\"\"\"\n",
    "        def hook(module, input, output):\n",
    "            # output shape: (batch, seq, hidden)\n",
    "            # We need to reshape to (batch, seq, n_heads, d_head)\n",
    "            attn_output = output[0] if isinstance(output, tuple) else output\n",
    "            batch, seq, hidden = attn_output.shape\n",
    "            d_head = hidden // self.n_heads\n",
    "            \n",
    "            # Reshape to per-head\n",
    "            per_head = attn_output.view(batch, seq, self.n_heads, d_head)\n",
    "            self.head_outputs[layer_idx] = per_head.detach().cpu()\n",
    "        \n",
    "        return hook\n",
    "    \n",
    "    def _hook_residual(self, layer_idx, position):\n",
    "        \"\"\"Capture residual stream before/after layer.\"\"\"\n",
    "        def hook(module, input, output):\n",
    "            if position == 'before':\n",
    "                tensor = input[0] if isinstance(input, tuple) else input\n",
    "                self.residual_before[layer_idx] = tensor.detach().cpu()\n",
    "            else:\n",
    "                tensor = output[0] if isinstance(output, tuple) else output\n",
    "                self.residual_after[layer_idx] = tensor.detach().cpu()\n",
    "        \n",
    "        return hook\n",
    "    \n",
    "    def register_hooks(self):\n",
    "        \"\"\"Register hooks for GPT-NeoX/Pythia architecture.\"\"\"\n",
    "        self.remove_hooks()\n",
    "        \n",
    "        for layer_idx in range(self.n_layers):\n",
    "            # Try different attribute names for different architectures\n",
    "            if hasattr(self.model, 'gpt_neox'):\n",
    "                layer = self.model.gpt_neox.layers[layer_idx]\n",
    "                attn = layer.attention\n",
    "            elif hasattr(self.model, 'transformer'):\n",
    "                layer = self.model.transformer.h[layer_idx]\n",
    "                attn = layer.attn\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown model architecture\")\n",
    "            \n",
    "            # Hook attention output\n",
    "            self.hooks.append(\n",
    "                attn.register_forward_hook(self._hook_attention_output(layer_idx))\n",
    "            )\n",
    "            \n",
    "            # Hook residual (layer input/output)\n",
    "            self.hooks.append(\n",
    "                layer.register_forward_hook(self._hook_residual(layer_idx, 'before'))\n",
    "            )\n",
    "            self.hooks.append(\n",
    "                layer.register_forward_hook(self._hook_residual(layer_idx, 'after'))\n",
    "            )\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks = []\n",
    "    \n",
    "    def clear(self):\n",
    "        self.head_outputs = {}\n",
    "        self.residual_before = {}\n",
    "        self.residual_after = {}\n",
    "    \n",
    "    def analyze_prompt(self, prompt):\n",
    "        \"\"\"Analyze a single prompt.\"\"\"\n",
    "        self.clear()\n",
    "        \n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            _ = self.model(**inputs, output_hidden_states=True)\n",
    "        \n",
    "        return self._compute_head_contributions()\n",
    "    \n",
    "    def _compute_head_contributions(self):\n",
    "        \"\"\"Compute per-head contribution to residual stream.\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for layer_idx in range(self.n_layers):\n",
    "            if layer_idx not in self.head_outputs:\n",
    "                continue\n",
    "            \n",
    "            head_out = self.head_outputs[layer_idx]  # (batch, seq, n_heads, d_head)\n",
    "            \n",
    "            # Compute per-head norms (contribution magnitude)\n",
    "            head_norms = torch.norm(head_out.float(), dim=-1)  # (batch, seq, n_heads)\n",
    "            \n",
    "            # Average over batch and sequence\n",
    "            mean_norms = head_norms.mean(dim=(0, 1))  # (n_heads,)\n",
    "            \n",
    "            # Residual growth\n",
    "            if layer_idx in self.residual_before and layer_idx in self.residual_after:\n",
    "                res_before = self.residual_before[layer_idx]\n",
    "                res_after = self.residual_after[layer_idx]\n",
    "                \n",
    "                # Compute residual growth at last token\n",
    "                norm_before = torch.norm(res_before[:, -1, :].float(), dim=-1).item()\n",
    "                norm_after = torch.norm(res_after[:, -1, :].float(), dim=-1).item()\n",
    "                residual_growth = norm_after - norm_before\n",
    "            else:\n",
    "                residual_growth = 0\n",
    "            \n",
    "            results.append({\n",
    "                'layer': layer_idx,\n",
    "                'head_norms': mean_norms.numpy(),\n",
    "                'residual_growth': residual_growth,\n",
    "                'residual_growth_ratio': norm_after / (norm_before + 1e-10) if layer_idx in self.residual_before else 1.0\n",
    "            })\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_model(model_name, model_path):\n",
    "    \"\"\"Full per-head analysis for a model.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Analyzing: {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Load model\n",
    "    config = AutoConfig.from_pretrained(model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "        device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "    model.eval()\n",
    "    \n",
    "    n_layers = config.num_hidden_layers\n",
    "    n_heads = config.num_attention_heads\n",
    "    d_head = config.hidden_size // n_heads\n",
    "    rho = n_heads / d_head\n",
    "    \n",
    "    print(f\"Layers: {n_layers}, Heads: {n_heads}, d_head: {d_head}\")\n",
    "    print(f\"œÅ = {rho:.4f}\")\n",
    "    \n",
    "    # Create analyzer\n",
    "    analyzer = PerHeadAnalyzer(model, tokenizer, n_layers, n_heads)\n",
    "    analyzer.register_hooks()\n",
    "    \n",
    "    # Analyze all prompts\n",
    "    all_results = []\n",
    "    for prompt in TEST_PROMPTS:\n",
    "        results = analyzer.analyze_prompt(prompt)\n",
    "        all_results.append(results)\n",
    "    \n",
    "    analyzer.remove_hooks()\n",
    "    \n",
    "    # Aggregate across prompts\n",
    "    aggregated = []\n",
    "    for layer_idx in range(n_layers):\n",
    "        layer_data = [r[layer_idx] for r in all_results if layer_idx < len(r)]\n",
    "        if layer_data:\n",
    "            head_norms = np.mean([d['head_norms'] for d in layer_data], axis=0)\n",
    "            residual_growth = np.mean([d['residual_growth'] for d in layer_data])\n",
    "            residual_ratio = np.mean([d['residual_growth_ratio'] for d in layer_data])\n",
    "            \n",
    "            aggregated.append({\n",
    "                'layer': layer_idx,\n",
    "                'head_norms': head_norms,\n",
    "                'residual_growth': residual_growth,\n",
    "                'residual_ratio': residual_ratio\n",
    "            })\n",
    "    \n",
    "    # Identify \"Beautiful Ones\"\n",
    "    # These are heads with unusually low contribution in layers with negative residual growth\n",
    "    last_layer = aggregated[-1] if aggregated else None\n",
    "    \n",
    "    if last_layer:\n",
    "        mean_norm = np.mean(last_layer['head_norms'])\n",
    "        std_norm = np.std(last_layer['head_norms'])\n",
    "        \n",
    "        # Beautiful Ones: heads with norms < mean - 1*std\n",
    "        beautiful_ones = np.where(last_layer['head_norms'] < mean_norm - std_norm)[0]\n",
    "        \n",
    "        print(f\"\\nüìä Last Layer Analysis:\")\n",
    "        print(f\"   Residual Ratio: {last_layer['residual_ratio']:.4f}\")\n",
    "        print(f\"   Head Norm Mean: {mean_norm:.4f}\")\n",
    "        print(f\"   Head Norm Std: {std_norm:.4f}\")\n",
    "        print(f\"   'Beautiful Ones' (low contrib): {len(beautiful_ones)} heads\")\n",
    "        if len(beautiful_ones) > 0:\n",
    "            print(f\"   Head indices: {beautiful_ones.tolist()}\")\n",
    "    \n",
    "    output = {\n",
    "        'model': model_name,\n",
    "        'n_layers': n_layers,\n",
    "        'n_heads': n_heads,\n",
    "        'd_head': d_head,\n",
    "        'rho': rho,\n",
    "        'layers': aggregated,\n",
    "        'beautiful_ones_last_layer': beautiful_ones.tolist() if last_layer else [],\n",
    "        'last_layer_residual_ratio': float(last_layer['residual_ratio']) if last_layer else None\n",
    "    }\n",
    "    \n",
    "    # Cleanup\n",
    "    del model, tokenizer, analyzer\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run analysis\n",
    "all_model_results = {}\n",
    "\n",
    "for name in MODELS_TO_TEST:\n",
    "    path = MODELS[name]\n",
    "    try:\n",
    "        results = analyze_model(name, path)\n",
    "        all_model_results[name] = results\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing {name}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(f\"\\n\\nAnalyzed: {len(all_model_results)} models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Head Norm Heatmaps\n",
    "n_models = len(all_model_results)\n",
    "\n",
    "if n_models > 0:\n",
    "    fig, axes = plt.subplots(1, n_models, figsize=(8 * n_models, 8))\n",
    "    if n_models == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, (name, results) in enumerate(all_model_results.items()):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Build heatmap matrix: layers x heads\n",
    "        n_layers = results['n_layers']\n",
    "        n_heads = results['n_heads']\n",
    "        \n",
    "        heatmap = np.zeros((n_layers, n_heads))\n",
    "        for layer_data in results['layers']:\n",
    "            layer_idx = layer_data['layer']\n",
    "            heatmap[layer_idx, :len(layer_data['head_norms'])] = layer_data['head_norms']\n",
    "        \n",
    "        # Normalize per layer for visibility\n",
    "        heatmap_norm = heatmap / (heatmap.max(axis=1, keepdims=True) + 1e-10)\n",
    "        \n",
    "        sns.heatmap(heatmap_norm, ax=ax, cmap='viridis', cbar_kws={'label': 'Relative Contribution'})\n",
    "        ax.set_xlabel('Head Index')\n",
    "        ax.set_ylabel('Layer')\n",
    "        ax.set_title(f'{name}\\nœÅ = {results[\"rho\"]:.4f}, Last Ratio = {results[\"last_layer_residual_ratio\"]:.4f}')\n",
    "        \n",
    "        # Mark \"Beautiful Ones\" in last layer\n",
    "        for head_idx in results['beautiful_ones_last_layer']:\n",
    "            ax.add_patch(plt.Rectangle((head_idx, n_layers - 1), 1, 1, fill=False, \n",
    "                                        edgecolor='red', linewidth=2))\n",
    "    \n",
    "    plt.suptitle('Per-Head Contribution Heatmaps (Red boxes = \"Beautiful Ones\")', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('beautiful_ones_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"\\nSaved: beautiful_ones_heatmap.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual Growth Profile\n",
    "if n_models > 0:\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    for name, results in all_model_results.items():\n",
    "        layers = [d['layer'] for d in results['layers']]\n",
    "        ratios = [d['residual_ratio'] for d in results['layers']]\n",
    "        \n",
    "        color = 'blue' if results['last_layer_residual_ratio'] < 1.0 else 'red'\n",
    "        ax.plot(layers, ratios, '-o', label=f\"{name} (œÅ={results['rho']:.3f})\", color=color, markersize=3)\n",
    "    \n",
    "    ax.axhline(y=1.0, color='black', linestyle='--', alpha=0.5, label='G=1.0 (Bentov Point)')\n",
    "    ax.set_xlabel('Layer')\n",
    "    ax.set_ylabel('Residual Ratio (||h_l|| / ||h_{l-1}||)')\n",
    "    ax.set_title('Residual Stream Dynamics by Layer')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('residual_growth_profile.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"\\nSaved: residual_growth_profile.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beautiful Ones Summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"'BEAUTIFUL ONES' ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for name, results in all_model_results.items():\n",
    "    print(f\"\\nüìä {name}:\")\n",
    "    print(f\"   œÅ = {results['rho']:.4f}\")\n",
    "    print(f\"   Layers: {results['n_layers']}, Heads: {results['n_heads']}\")\n",
    "    print(f\"   Last Layer Ratio: {results['last_layer_residual_ratio']:.4f}\")\n",
    "    \n",
    "    status = \"DAMPENING\" if results['last_layer_residual_ratio'] < 1.0 else \"EXPANSION\"\n",
    "    print(f\"   Status: {status}\")\n",
    "    \n",
    "    n_beautiful = len(results['beautiful_ones_last_layer'])\n",
    "    pct_beautiful = 100 * n_beautiful / results['n_heads']\n",
    "    print(f\"   'Beautiful Ones' in Last Layer: {n_beautiful}/{results['n_heads']} ({pct_beautiful:.1f}%)\")\n",
    "    \n",
    "    if n_beautiful > 0:\n",
    "        print(f\"   Heads: {results['beautiful_ones_last_layer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Analysis: Beautiful Ones % vs Dampening\n",
    "if len(all_model_results) >= 2:\n",
    "    rhos = [r['rho'] for r in all_model_results.values()]\n",
    "    beautiful_pcts = [100 * len(r['beautiful_ones_last_layer']) / r['n_heads'] for r in all_model_results.values()]\n",
    "    residual_ratios = [r['last_layer_residual_ratio'] for r in all_model_results.values()]\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"CORRELATION ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # œÅ vs Beautiful Ones %\n",
    "    if len(rhos) >= 2:\n",
    "        from scipy import stats\n",
    "        corr1, p1 = stats.pearsonr(rhos, beautiful_pcts)\n",
    "        print(f\"\\nœÅ vs Beautiful Ones %: r = {corr1:.4f} (p = {p1:.4e})\")\n",
    "        \n",
    "        # Beautiful Ones % vs Dampening\n",
    "        corr2, p2 = stats.pearsonr(beautiful_pcts, residual_ratios)\n",
    "        print(f\"Beautiful Ones % vs Residual Ratio: r = {corr2:.4f} (p = {p2:.4e})\")\n",
    "        \n",
    "        if corr1 > 0 and corr2 < 0:\n",
    "            print(\"\\n‚úÖ Pattern Confirmed: Higher œÅ ‚Üí More 'Beautiful Ones' ‚Üí More Dampening\")\n",
    "        else:\n",
    "            print(\"\\n‚ö†Ô∏è Pattern not clearly confirmed (need more data points)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# Convert numpy arrays to lists for JSON serialization\n",
    "serializable_results = {}\n",
    "for name, results in all_model_results.items():\n",
    "    r = results.copy()\n",
    "    r['layers'] = [\n",
    "        {\n",
    "            'layer': d['layer'],\n",
    "            'head_norms': d['head_norms'].tolist() if isinstance(d['head_norms'], np.ndarray) else d['head_norms'],\n",
    "            'residual_growth': float(d['residual_growth']),\n",
    "            'residual_ratio': float(d['residual_ratio'])\n",
    "        }\n",
    "        for d in results['layers']\n",
    "    ]\n",
    "    serializable_results[name] = r\n",
    "\n",
    "output_data = {\n",
    "    'experiment': 'Beautiful Ones Per-Head Analysis',\n",
    "    'hypothesis': 'High œÅ ‚Üí More \"Beautiful Ones\" (low-contrib heads) ‚Üí Dampening',\n",
    "    'date': datetime.now().isoformat(),\n",
    "    'n_prompts': len(TEST_PROMPTS),\n",
    "    'models': serializable_results\n",
    "}\n",
    "\n",
    "filename = f'beautiful_ones_analysis_{timestamp}.json'\n",
    "with open(filename, 'w') as f:\n",
    "    json.dump(output_data, f, indent=2)\n",
    "\n",
    "print(f\"\\nSaved: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-download\n",
    "import zipfile\n",
    "\n",
    "archive_name = f'beautiful_ones_analysis_{timestamp}.zip'\n",
    "\n",
    "with zipfile.ZipFile(archive_name, 'w') as zf:\n",
    "    zf.write(filename)\n",
    "    zf.write('beautiful_ones_heatmap.png')\n",
    "    zf.write('residual_growth_profile.png')\n",
    "\n",
    "print(f\"Created archive: {archive_name}\")\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(filename)\n",
    "    files.download('beautiful_ones_heatmap.png')\n",
    "    files.download('residual_growth_profile.png')\n",
    "    files.download(archive_name)\n",
    "except ImportError:\n",
    "    print(\"Not in Colab - manual download required.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FINAL SUMMARY: Beautiful Ones Analysis\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nüìä Models Analyzed: {len(all_model_results)}\")\n",
    "\n",
    "for name, results in all_model_results.items():\n",
    "    status = \"üîµ DAMPEN\" if results['last_layer_residual_ratio'] < 1.0 else \"üî¥ EXPAND\"\n",
    "    n_beautiful = len(results['beautiful_ones_last_layer'])\n",
    "    print(f\"\\n  {name}: {status}\")\n",
    "    print(f\"    œÅ = {results['rho']:.4f}\")\n",
    "    print(f\"    Last Layer Ratio = {results['last_layer_residual_ratio']:.4f}\")\n",
    "    print(f\"    Beautiful Ones = {n_beautiful}/{results['n_heads']}\")\n",
    "\n",
    "print(f\"\\nüìÅ Output Files:\")\n",
    "print(f\"   ‚Ä¢ {filename}\")\n",
    "print(f\"   ‚Ä¢ beautiful_ones_heatmap.png\")\n",
    "print(f\"   ‚Ä¢ residual_growth_profile.png\")\n",
    "print(f\"   ‚Ä¢ {archive_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
