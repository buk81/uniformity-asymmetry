{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FFN Expansion Analysis: Pythia-6.9B (Cross-Model Validation)\n",
    "\n",
    "**Paper #3 Experiment:** Funnel Model Universality Test\n",
    "\n",
    "**Hypothesis:** The Funnel Model discovered in Pythia-1.4B is universal:\n",
    "- Layers 0-30: Both Attention and MLP contract\n",
    "- Layer 31: MLP EXPLODES (expansion > 1)\n",
    "\n",
    "**Model:** EleutherAI/pythia-6.9b (32 layers, 4096 hidden dim)\n",
    "\n",
    "**Reference (Pythia-1.4B):**\n",
    "- Attention: 24/24 contracting (100%)\n",
    "- MLP: 22/24 contracting (92%), only L3 and L23 expand\n",
    "- Combined: 23/24 contracting, only L23 net expansion (gain=1.34)\n",
    "- Max MLP gain: 3.60 (Layer 23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install transformers torch matplotlib numpy --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load Pythia-6.9B with automatic GPU detection\nMODEL_NAME = \"EleutherAI/pythia-6.9b\"\n\n# Detect GPU type\nif torch.cuda.is_available():\n    gpu_name = torch.cuda.get_device_name(0)\n    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n    print(f\"GPU detected: {gpu_name}\")\n    print(f\"GPU memory: {gpu_mem:.1f} GB\")\n    \n    # A100 has 40GB or 80GB, T4 has 16GB\n    if \"A100\" in gpu_name or gpu_mem > 30:\n        print(\"A100 detected - using float16 with full model\")\n        dtype = torch.float16\n    elif gpu_mem > 14:\n        print(\"High-memory GPU - using float16\")\n        dtype = torch.float16\n    else:\n        print(\"Limited GPU memory - using float16 with offloading\")\n        dtype = torch.float16\nelse:\n    print(\"No GPU detected - this will be slow!\")\n    dtype = torch.float32\n\nprint(f\"\\nLoading {MODEL_NAME}...\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    torch_dtype=dtype,\n    device_map=\"auto\",  # Automatic device placement\n    low_cpu_mem_usage=True\n)\nmodel.eval()\n\nprint(f\"\\nModel loaded!\")\nprint(f\"Layers: {model.config.num_hidden_layers}\")\nprint(f\"Hidden dim: {model.config.hidden_size}\")\nprint(f\"Attention heads: {model.config.num_attention_heads}\")\nprint(f\"Device: {next(model.parameters()).device}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation capture class with hooks\n",
    "class ActivationCapture:\n",
    "    \"\"\"Capture activations at attention and MLP boundaries.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.activations = defaultdict(dict)\n",
    "        self.hooks = []\n",
    "    \n",
    "    def clear(self):\n",
    "        self.activations = defaultdict(dict)\n",
    "    \n",
    "    def _make_hook(self, layer_idx, component, position):\n",
    "        \"\"\"Create a hook function for a specific layer/component.\"\"\"\n",
    "        def hook(module, input, output):\n",
    "            if position == 'input':\n",
    "                # Input is a tuple, take first element\n",
    "                tensor = input[0] if isinstance(input, tuple) else input\n",
    "            else:\n",
    "                # Output handling\n",
    "                if isinstance(output, tuple):\n",
    "                    tensor = output[0]\n",
    "                else:\n",
    "                    tensor = output\n",
    "            \n",
    "            # Store the norm (mean over batch and sequence)\n",
    "            with torch.no_grad():\n",
    "                # tensor shape: (batch, seq_len, hidden_dim)\n",
    "                norms = torch.norm(tensor.float(), dim=-1)  # (batch, seq_len)\n",
    "                mean_norm = norms.mean().item()\n",
    "                self.activations[layer_idx][f\"{component}_{position}\"] = mean_norm\n",
    "        \n",
    "        return hook\n",
    "    \n",
    "    def register_hooks(self, model):\n",
    "        \"\"\"Register hooks on all attention and MLP modules.\"\"\"\n",
    "        self.remove_hooks()  # Clear any existing hooks\n",
    "        \n",
    "        for layer_idx in range(model.config.num_hidden_layers):\n",
    "            layer = model.gpt_neox.layers[layer_idx]\n",
    "            \n",
    "            # Attention hooks\n",
    "            self.hooks.append(\n",
    "                layer.attention.register_forward_hook(\n",
    "                    self._make_hook(layer_idx, 'attn', 'input')\n",
    "                )\n",
    "            )\n",
    "            self.hooks.append(\n",
    "                layer.attention.register_forward_hook(\n",
    "                    self._make_hook(layer_idx, 'attn', 'output')\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # MLP hooks\n",
    "            self.hooks.append(\n",
    "                layer.mlp.register_forward_hook(\n",
    "                    self._make_hook(layer_idx, 'mlp', 'input')\n",
    "                )\n",
    "            )\n",
    "            self.hooks.append(\n",
    "                layer.mlp.register_forward_hook(\n",
    "                    self._make_hook(layer_idx, 'mlp', 'output')\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        print(f\"Registered {len(self.hooks)} hooks on {model.config.num_hidden_layers} layers\")\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        \"\"\"Remove all hooks.\"\"\"\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks = []\n",
    "    \n",
    "    def compute_gains(self, n_layers):\n",
    "        \"\"\"Compute gain ratios from captured activations.\"\"\"\n",
    "        attn_gains = []\n",
    "        mlp_gains = []\n",
    "        \n",
    "        for layer_idx in range(n_layers):\n",
    "            acts = self.activations[layer_idx]\n",
    "            \n",
    "            # Attention gain = ||output|| / ||input||\n",
    "            if 'attn_input' in acts and 'attn_output' in acts:\n",
    "                attn_gain = acts['attn_output'] / (acts['attn_input'] + 1e-10)\n",
    "                attn_gains.append(attn_gain)\n",
    "            else:\n",
    "                attn_gains.append(np.nan)\n",
    "            \n",
    "            # MLP gain = ||output|| / ||input||\n",
    "            if 'mlp_input' in acts and 'mlp_output' in acts:\n",
    "                mlp_gain = acts['mlp_output'] / (acts['mlp_input'] + 1e-10)\n",
    "                mlp_gains.append(mlp_gain)\n",
    "            else:\n",
    "                mlp_gains.append(np.nan)\n",
    "        \n",
    "        return np.array(attn_gains), np.array(mlp_gains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prompts (same as Pythia-1.4B for comparability)\n",
    "TEST_PROMPTS = [\n",
    "    \"The capital of France is Paris, which is known for the Eiffel Tower.\",\n",
    "    \"In mathematics, the Pythagorean theorem states that in a right triangle\",\n",
    "    \"The quick brown fox jumps over the lazy dog near the river bank.\",\n",
    "    \"Artificial intelligence has made significant progress in recent years\",\n",
    "    \"The chemical formula for water is H2O, consisting of two hydrogen atoms\",\n",
    "    \"Shakespeare wrote many famous plays including Hamlet and Macbeth\",\n",
    "    \"The speed of light in vacuum is approximately 299,792 kilometers per second\",\n",
    "    \"Mount Everest is the highest mountain on Earth, located in the Himalayas\"\n",
    "]\n",
    "\n",
    "print(f\"Using {len(TEST_PROMPTS)} test prompts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run forward passes and collect activations\n",
    "capture = ActivationCapture()\n",
    "capture.register_hooks(model)\n",
    "\n",
    "all_attn_gains = []\n",
    "all_mlp_gains = []\n",
    "\n",
    "n_layers = model.config.num_hidden_layers\n",
    "\n",
    "print(\"Running forward passes...\")\n",
    "for i, prompt in enumerate(TEST_PROMPTS):\n",
    "    capture.clear()\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        _ = model(**inputs)\n",
    "    \n",
    "    # Compute gains\n",
    "    attn_gains, mlp_gains = capture.compute_gains(n_layers)\n",
    "    all_attn_gains.append(attn_gains)\n",
    "    all_mlp_gains.append(mlp_gains)\n",
    "    \n",
    "    print(f\"  Prompt {i+1}/{len(TEST_PROMPTS)}: {prompt[:40]}...\")\n",
    "\n",
    "capture.remove_hooks()\n",
    "\n",
    "# Average across prompts\n",
    "mean_attn_gains = np.nanmean(all_attn_gains, axis=0)\n",
    "mean_mlp_gains = np.nanmean(all_mlp_gains, axis=0)\n",
    "combined_gains = mean_attn_gains * mean_mlp_gains\n",
    "\n",
    "print(f\"\\nDone! Computed gains for {n_layers} layers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze results\n",
    "print(\"=\" * 60)\n",
    "print(\"PYTHIA-6.9B FUNNEL MODEL ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Attention analysis\n",
    "attn_contracting = np.sum(mean_attn_gains < 1)\n",
    "attn_expanding = np.sum(mean_attn_gains >= 1)\n",
    "attn_min_idx = np.nanargmin(mean_attn_gains)\n",
    "attn_max_idx = np.nanargmax(mean_attn_gains)\n",
    "\n",
    "print(f\"\\n--- ATTENTION ---\")\n",
    "print(f\"Contracting layers: {attn_contracting}/{n_layers} ({100*attn_contracting/n_layers:.1f}%)\")\n",
    "print(f\"Expanding layers:   {attn_expanding}/{n_layers} ({100*attn_expanding/n_layers:.1f}%)\")\n",
    "print(f\"Min gain: {mean_attn_gains[attn_min_idx]:.4f} (Layer {attn_min_idx})\")\n",
    "print(f\"Max gain: {mean_attn_gains[attn_max_idx]:.4f} (Layer {attn_max_idx})\")\n",
    "\n",
    "# MLP analysis\n",
    "mlp_contracting = np.sum(mean_mlp_gains < 1)\n",
    "mlp_expanding = np.sum(mean_mlp_gains >= 1)\n",
    "mlp_min_idx = np.nanargmin(mean_mlp_gains)\n",
    "mlp_max_idx = np.nanargmax(mean_mlp_gains)\n",
    "\n",
    "print(f\"\\n--- MLP/FFN ---\")\n",
    "print(f\"Contracting layers: {mlp_contracting}/{n_layers} ({100*mlp_contracting/n_layers:.1f}%)\")\n",
    "print(f\"Expanding layers:   {mlp_expanding}/{n_layers} ({100*mlp_expanding/n_layers:.1f}%)\")\n",
    "print(f\"Min gain: {mean_mlp_gains[mlp_min_idx]:.4f} (Layer {mlp_min_idx})\")\n",
    "print(f\"Max gain: {mean_mlp_gains[mlp_max_idx]:.4f} (Layer {mlp_max_idx})\")\n",
    "\n",
    "# Combined analysis\n",
    "combined_contracting = np.sum(combined_gains < 1)\n",
    "combined_expanding = np.sum(combined_gains >= 1)\n",
    "combined_max_idx = np.nanargmax(combined_gains)\n",
    "\n",
    "print(f\"\\n--- COMBINED (Attn x MLP) ---\")\n",
    "print(f\"Net contracting: {combined_contracting}/{n_layers} ({100*combined_contracting/n_layers:.1f}%)\")\n",
    "print(f\"Net expanding:   {combined_expanding}/{n_layers} ({100*combined_expanding/n_layers:.1f}%)\")\n",
    "print(f\"Max combined gain: {combined_gains[combined_max_idx]:.4f} (Layer {combined_max_idx})\")\n",
    "\n",
    "# Hypothesis test\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"HYPOTHESIS TEST: Funnel Model Universality\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "attention_contracts = attn_contracting == n_layers\n",
    "mlp_mostly_contracts = mlp_contracting >= n_layers - 3  # Allow up to 3 expanding\n",
    "last_layer_expands = mean_mlp_gains[-1] > 1.0\n",
    "last_layer_max = mlp_max_idx == n_layers - 1\n",
    "\n",
    "print(f\"\\n1. Attention ALWAYS contracts: {attention_contracts} ({attn_contracting}/{n_layers})\")\n",
    "print(f\"2. MLP mostly contracts: {mlp_mostly_contracts} ({mlp_contracting}/{n_layers})\")\n",
    "print(f\"3. Last layer MLP expands: {last_layer_expands} (gain={mean_mlp_gains[-1]:.3f})\")\n",
    "print(f\"4. Last layer has MAX MLP gain: {last_layer_max} (L{mlp_max_idx})\")\n",
    "\n",
    "funnel_confirmed = attention_contracts and mlp_mostly_contracts and last_layer_expands\n",
    "print(f\"\\n>>> FUNNEL MODEL CONFIRMED: {funnel_confirmed} <<<\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference values from Pythia-1.4B\n",
    "PYTHIA_1_4B_REFERENCE = {\n",
    "    'n_layers': 24,\n",
    "    'attn_min_gain': 0.083,\n",
    "    'attn_max_gain': 0.527,\n",
    "    'attn_contracting': 24,\n",
    "    'mlp_min_gain': 0.261,\n",
    "    'mlp_max_gain': 3.604,\n",
    "    'mlp_contracting': 22,\n",
    "    'mlp_expanding': 2,\n",
    "    'last_layer_mlp_gain': 3.604,\n",
    "    'combined_max_gain': 1.338\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CROSS-MODEL COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n{'Metric':<30} {'Pythia-1.4B':>15} {'Pythia-6.9B':>15}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Layers':<30} {PYTHIA_1_4B_REFERENCE['n_layers']:>15} {n_layers:>15}\")\n",
    "print(f\"{'Attn contracting %':<30} {100*PYTHIA_1_4B_REFERENCE['attn_contracting']/24:>14.1f}% {100*attn_contracting/n_layers:>14.1f}%\")\n",
    "print(f\"{'Attn min gain':<30} {PYTHIA_1_4B_REFERENCE['attn_min_gain']:>15.3f} {mean_attn_gains[attn_min_idx]:>15.3f}\")\n",
    "print(f\"{'Attn max gain':<30} {PYTHIA_1_4B_REFERENCE['attn_max_gain']:>15.3f} {mean_attn_gains[attn_max_idx]:>15.3f}\")\n",
    "print(f\"{'MLP contracting %':<30} {100*PYTHIA_1_4B_REFERENCE['mlp_contracting']/24:>14.1f}% {100*mlp_contracting/n_layers:>14.1f}%\")\n",
    "print(f\"{'MLP min gain':<30} {PYTHIA_1_4B_REFERENCE['mlp_min_gain']:>15.3f} {mean_mlp_gains[mlp_min_idx]:>15.3f}\")\n",
    "print(f\"{'MLP max gain (last layer)':<30} {PYTHIA_1_4B_REFERENCE['mlp_max_gain']:>15.3f} {mean_mlp_gains[-1]:>15.3f}\")\n",
    "print(f\"{'Combined max gain':<30} {PYTHIA_1_4B_REFERENCE['combined_max_gain']:>15.3f} {combined_gains[combined_max_idx]:>15.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle(f'EleutherAI/pythia-6.9b: FFN vs Attention Expansion Analysis\\n(Funnel Model Cross-Validation)', fontsize=14, fontweight='bold')\n",
    "\n",
    "layers = np.arange(n_layers)\n",
    "\n",
    "# Panel 1: Attention vs MLP Gain\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(layers, mean_attn_gains, 'b-o', label='Attention Gain', markersize=4)\n",
    "ax1.plot(layers, mean_mlp_gains, 'r-o', label='MLP/FFN Gain', markersize=4)\n",
    "ax1.axhline(y=1.0, color='gray', linestyle='--', alpha=0.7, label='Neutral (gain=1)')\n",
    "ax1.fill_between(layers, 0, mean_attn_gains, alpha=0.3, color='blue', label='Attn Contraction')\n",
    "ax1.axvline(x=mlp_max_idx, color='red', linestyle=':', alpha=0.7, label=f'Max MLP (L{mlp_max_idx})')\n",
    "ax1.set_xlabel('Layer')\n",
    "ax1.set_ylabel('Gain (||output|| / ||input||)')\n",
    "ax1.set_title('Attention vs MLP Gain per Layer')\n",
    "ax1.legend(loc='upper left', fontsize=8)\n",
    "ax1.set_ylim(0, max(mean_mlp_gains) * 1.1)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Panel 2: Layer-wise comparison with 1.4B\n",
    "ax2 = axes[0, 1]\n",
    "# Normalize layer indices for comparison\n",
    "normalized_layers_69b = layers / (n_layers - 1)\n",
    "normalized_layers_14b = np.arange(24) / 23\n",
    "\n",
    "ax2.plot(normalized_layers_69b, mean_mlp_gains, 'r-o', label='Pythia-6.9B MLP', markersize=4)\n",
    "# Add reference line for 1.4B pattern (approximate)\n",
    "ax2.axhline(y=1.0, color='gray', linestyle='--', alpha=0.7)\n",
    "ax2.axhline(y=PYTHIA_1_4B_REFERENCE['mlp_max_gain'], color='orange', linestyle=':', alpha=0.7, label=f'1.4B max ({PYTHIA_1_4B_REFERENCE[\"mlp_max_gain\"]:.1f})')\n",
    "ax2.set_xlabel('Normalized Layer Position (0=first, 1=last)')\n",
    "ax2.set_ylabel('MLP Gain')\n",
    "ax2.set_title('MLP Gain: Normalized Layer Position')\n",
    "ax2.legend(loc='upper left', fontsize=8)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Panel 3: Combined Gain\n",
    "ax3 = axes[1, 0]\n",
    "colors = ['green' if g >= 1 else 'purple' for g in combined_gains]\n",
    "ax3.bar(layers, combined_gains, color=colors, alpha=0.7, edgecolor='black', linewidth=0.5)\n",
    "ax3.axhline(y=1.0, color='gray', linestyle='--', linewidth=2, label='Neutral (gain=1)')\n",
    "ax3.set_xlabel('Layer')\n",
    "ax3.set_ylabel('Combined Gain (Attn x MLP)')\n",
    "ax3.set_title('Net Effect: Attn x MLP Gain')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Add annotations for expanding layers\n",
    "for i, g in enumerate(combined_gains):\n",
    "    if g >= 1:\n",
    "        ax3.annotate(f'L{i}\\n{g:.2f}', (i, g), textcoords=\"offset points\", \n",
    "                    xytext=(0,10), ha='center', fontsize=8, color='green')\n",
    "\n",
    "# Panel 4: Funnel Visualization\n",
    "ax4 = axes[1, 1]\n",
    "\n",
    "# Create funnel shape based on cumulative contraction\n",
    "cumulative_gain = np.cumprod(combined_gains)\n",
    "ax4.fill_between(layers, 0, cumulative_gain, alpha=0.3, color='blue')\n",
    "ax4.plot(layers, cumulative_gain, 'b-', linewidth=2, label='Cumulative Gain')\n",
    "ax4.axhline(y=1.0, color='gray', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Mark key points\n",
    "min_cumulative_idx = np.argmin(cumulative_gain)\n",
    "ax4.scatter([min_cumulative_idx], [cumulative_gain[min_cumulative_idx]], \n",
    "           color='red', s=100, zorder=5, label=f'Bottleneck (L{min_cumulative_idx})')\n",
    "ax4.scatter([n_layers-1], [cumulative_gain[-1]], \n",
    "           color='green', s=100, zorder=5, label=f'Output (L{n_layers-1})')\n",
    "\n",
    "ax4.set_xlabel('Layer')\n",
    "ax4.set_ylabel('Cumulative Gain (product of all gains)')\n",
    "ax4.set_title('Information Funnel: Cumulative Compression')\n",
    "ax4.legend(loc='upper right', fontsize=8)\n",
    "ax4.set_yscale('log')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ffn_expansion_pythia69b_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSaved: ffn_expansion_pythia69b_analysis.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save results\nresults = {\n    'model': MODEL_NAME,\n    'n_layers': int(n_layers),\n    'hidden_dim': int(model.config.hidden_size),\n    'n_prompts': len(TEST_PROMPTS),\n    'attention': {\n        'gains': [float(x) for x in mean_attn_gains],\n        'min_gain': float(np.nanmin(mean_attn_gains)),\n        'max_gain': float(np.nanmax(mean_attn_gains)),\n        'L_star_min': int(attn_min_idx),\n        'n_contracting': int(attn_contracting),\n        'n_expanding': int(attn_expanding)\n    },\n    'mlp': {\n        'gains': [float(x) for x in mean_mlp_gains],\n        'min_gain': float(np.nanmin(mean_mlp_gains)),\n        'max_gain': float(np.nanmax(mean_mlp_gains)),\n        'L_star_max': int(mlp_max_idx),\n        'n_contracting': int(mlp_contracting),\n        'n_expanding': int(mlp_expanding),\n        'last_layer_gain': float(mean_mlp_gains[-1])\n    },\n    'combined': {\n        'gains': [float(x) for x in combined_gains],\n        'n_net_contracting': int(combined_contracting),\n        'n_net_expanding': int(combined_expanding),\n        'max_gain': float(np.nanmax(combined_gains)),\n        'max_gain_layer': int(combined_max_idx)\n    },\n    'funnel_test': {\n        'attention_always_contracts': True if attention_contracts else False,\n        'mlp_mostly_contracts': True if mlp_mostly_contracts else False,\n        'last_layer_expands': True if last_layer_expands else False,\n        'last_layer_is_max': True if last_layer_max else False,\n        'funnel_confirmed': True if funnel_confirmed else False\n    },\n    'comparison_1_4b': {\n        'attn_contraction_match': True if abs(100*attn_contracting/n_layers - 100) < 5 else False,\n        'mlp_expansion_in_last': True if mean_mlp_gains[-1] > 1.0 else False,\n        'pattern_match': True if funnel_confirmed else False\n    }\n}\n\n# Save JSON\nwith open('ffn_expansion_pythia69b_results.json', 'w') as f:\n    json.dump(results, f, indent=2)\n\nprint(\"Saved: ffn_expansion_pythia69b_results.json\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create timestamped archive and auto-download\nimport zipfile\nfrom datetime import datetime\n\ntimestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\narchive_name = f'ffn_expansion_pythia69b_results_{timestamp}.zip'\n\nwith zipfile.ZipFile(archive_name, 'w') as zf:\n    zf.write('ffn_expansion_pythia69b_results.json')\n    zf.write('ffn_expansion_pythia69b_analysis.png')\n\nprint(f\"Created archive: {archive_name}\")\n\n# Auto-download in Colab\ntry:\n    from google.colab import files\n    print(\"\\nStarting automatic downloads...\")\n    files.download('ffn_expansion_pythia69b_results.json')\n    files.download('ffn_expansion_pythia69b_analysis.png')\n    files.download(archive_name)\n    print(\"Downloads triggered!\")\nexcept ImportError:\n    print(\"\\nNot running in Colab - manual download required.\")\n    print(f\"Files to download:\")\n    print(f\"  - ffn_expansion_pythia69b_results.json\")\n    print(f\"  - ffn_expansion_pythia69b_analysis.png\")\n    print(f\"  - {archive_name}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FINAL SUMMARY: Pythia-6.9B Funnel Model Validation\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n{'Model:':<25} {MODEL_NAME}\")\n",
    "print(f\"{'Layers:':<25} {n_layers}\")\n",
    "print(f\"{'Hidden Dim:':<25} {model.config.hidden_size}\")\n",
    "\n",
    "print(f\"\\n--- FUNNEL MODEL TEST ---\")\n",
    "print(f\"{'Attention contracts:':<35} {'PASS' if attention_contracts else 'FAIL'} ({attn_contracting}/{n_layers})\")\n",
    "print(f\"{'MLP mostly contracts:':<35} {'PASS' if mlp_mostly_contracts else 'FAIL'} ({mlp_contracting}/{n_layers})\")\n",
    "print(f\"{'Last layer (L{n_layers-1}) expands:':<35} {'PASS' if last_layer_expands else 'FAIL'} (gain={mean_mlp_gains[-1]:.3f})\")\n",
    "print(f\"{'Last layer has max MLP gain:':<35} {'PASS' if last_layer_max else 'FAIL'}\")\n",
    "\n",
    "print(f\"\\n>>> FUNNEL MODEL UNIVERSAL: {funnel_confirmed} <<<\")\n",
    "\n",
    "if funnel_confirmed:\n",
    "    print(f\"\\nThe Compression Funnel architecture is CONFIRMED in Pythia-6.9B!\")\n",
    "    print(f\"- 31 layers compress information (Attn + MLP both contract)\")\n",
    "    print(f\"- Only Layer 31 expands for prediction (MLP gain = {mean_mlp_gains[-1]:.2f}x)\")\n",
    "else:\n",
    "    print(f\"\\nFunnel Model NOT fully confirmed. Check individual metrics above.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}