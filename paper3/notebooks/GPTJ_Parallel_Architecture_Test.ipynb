{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# GPT-J-6B Parallel Architecture Test\n\n**Paper #3 - Pythia Anomaly Investigation**\n\n**Date:** 2026-01-05\n\n**Purpose:** Validate the PARALLEL ATTENTION ARCHITECTURE hypothesis\n\n---\n\n## Background\n\nGPT-2 validation REJECTED the \"LayerNorm = Dampening\" hypothesis:\n- Pythia-6.9B (LayerNorm): Gain = 0.80 (DAMPENING)\n- GPT-2 (LayerNorm): Gain = 1.01-1.16 (EXPANSION)\n\n**New Hypothesis:** It's not LayerNorm that causes dampening - it's the **Parallel Attention Architecture**.\n\n---\n\n## Architectural Difference\n\n| Model | Architecture | Attention | Expected Gain |\n|-------|-------------|-----------|---------------|\n| GPT-2 | Sequential | h = x + Attn(x); out = h + MLP(h) | > 1.0 ✓ |\n| Pythia | GPT-NeoX (Parallel) | out = x + Attn(x) + MLP(x) | < 1.0 ✓ |\n| **GPT-J** | **GPT-NeoX (Parallel)** | **out = x + Attn(x) + MLP(x)** | **< 1.0 ?** |\n\n---\n\n## Hypothesis\n\n**If GPT-J-6B shows Gain < 1.0, this CONFIRMS that Parallel Attention causes dampening.**\n\n**If GPT-J-6B shows Gain > 1.0, the hypothesis is REJECTED and we need to investigate further.**",
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "code",
      "source": "# Cell 1: Setup\n!pip install -q transformers accelerate scipy seaborn\n\nimport torch\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, GPTJForCausalLM\nfrom scipy.stats import entropy, spearmanr, pearsonr, ttest_1samp\nimport gc\nimport json\nfrom datetime import datetime\nfrom google.colab import files\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# HF Token handling - GPT-J may need token\ntry:\n    from google.colab import userdata\n    HF_TOKEN = userdata.get('HF_TOKEN')\n    print(\"HF_TOKEN loaded from Colab secrets\")\nexcept:\n    HF_TOKEN = None\n    print(\"No HF_TOKEN found\")\n\n# Configure visualization\nplt.style.use('seaborn-v0_8-paper')\nsns.set_context(\"talk\")\n\n# Global timestamp for all files\nTIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\nprint(f\"\\nSession timestamp: {TIMESTAMP}\")\n\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")",
      "metadata": {
        "id": "setup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Cell 2: PROMPT SET (Same as Grand Unified Benchmark)\n\nPROMPT_DATASET = {\n    \"Factual\": [\n        \"The capital city of France is\",\n        \"The atomic number of oxygen is\",\n        \"Water boils at a temperature of\",\n        \"The largest planet in our solar system is\",\n        \"The currency used in Japan is\"\n    ],\n    \"Syntactic\": [\n        \"The agreement, which, notwithstanding the fact that it was signed only yesterday, effectively binds all parties immediately, stipulates that\",\n        \"Although the weather was extremely cold, and despite the fact that they had no coats, the children decided to\",\n        \"The professor, having reviewed the complex derivation multiple times without finding the error, finally realized that\",\n        \"To imply that such a fundamental shift in policy could occur without significant public debate is to suggest that\",\n        \"Not only did the experiment fail to yield the expected results, but it also demonstrated that the initial hypothesis was\"\n    ],\n    \"Cliche\": [\n        \"The true meaning of happiness is often found in\",\n        \"Actions speak louder than\",\n        \"It is what it is, and we must simply\",\n        \"Time heals all\",\n        \"Life is a journey, not a\"\n    ],\n    \"Novel\": [\n        \"The epistemological implications of quantum decoherence suggest that the observer is\",\n        \"If consciousness creates reality, then the paradox of the unobserved electron implies\",\n        \"The intersection of baroque architecture and cybernetic theory creates a space where\",\n        \"Calculating the trajectory of a hyperspace jump requires factoring in the variability of\",\n        \"The symbiotic relationship between fungal mycelium and digital neural networks results in\"\n    ],\n    \"Nonsense\": [\n        \"Table sky run blue jump quickly under over\",\n        \"Purple idea furiously sleep colorless green\",\n        \"Clock river dance potato seven fast\",\n        \"Window eat loud tomorrow yellow under\",\n        \"Fish bicycle logic cloud mountain swim\"\n    ]\n}\n\nCATEGORY_METADATA = {\n    \"Factual\": {\"expected_entropy\": \"medium\", \"complexity\": 1},\n    \"Syntactic\": {\"expected_entropy\": \"medium\", \"complexity\": 5},\n    \"Cliche\": {\"expected_entropy\": \"low\", \"complexity\": 2},\n    \"Novel\": {\"expected_entropy\": \"high\", \"complexity\": 4},\n    \"Nonsense\": {\"expected_entropy\": \"very_high\", \"complexity\": 3}\n}\n\ntotal_prompts = sum(len(v) for v in PROMPT_DATASET.values())\nprint(f\"Total prompts: {total_prompts} (5 categories x 5 prompts)\")\nfor cat, prompts in PROMPT_DATASET.items():\n    print(f\"  {cat}: {len(prompts)} prompts\")",
      "metadata": {
        "id": "prompts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Cell 3: MODEL CONFIGURATION\n\n# GPT-J-6B - The Test Subject\nMODEL_TO_TEST = {\n    \"GPT-J-6B\": {\n        \"hf_path\": \"EleutherAI/gpt-j-6b\",\n        \"architecture\": \"GPT-NeoX (Parallel)\",\n        \"norm_type\": \"LayerNorm\",\n        \"params\": \"6B\",\n        \"expected_gain\": \"< 1.0 (if hypothesis correct)\",\n        \"role\": \"Parallel Architecture Validation\"\n    }\n}\n\n# Reference values from prior experiments\nREFERENCE_MODELS = {\n    # Parallel Attention (GPT-NeoX)\n    \"Pythia-6.9B\": {\"gain\": 0.80, \"architecture\": \"GPT-NeoX (Parallel)\", \"norm\": \"LayerNorm\"},\n    \n    # Sequential Attention (GPT-2)\n    \"GPT2-XL\": {\"gain\": 1.02, \"architecture\": \"Sequential\", \"norm\": \"LayerNorm\"},\n    \"GPT2-Large\": {\"gain\": 1.01, \"architecture\": \"Sequential\", \"norm\": \"LayerNorm\"},\n    \"GPT2-Medium\": {\"gain\": 1.16, \"architecture\": \"Sequential\", \"norm\": \"LayerNorm\"},\n    \n    # RMSNorm models (various architectures)\n    \"Mistral-7B\": {\"gain\": 1.11, \"architecture\": \"LLaMA-like\", \"norm\": \"RMSNorm\"},\n    \"LLaMA-3.1-8B\": {\"gain\": 1.48, \"architecture\": \"LLaMA\", \"norm\": \"RMSNorm\"},\n    \"Gemma-7B\": {\"gain\": 2.31, \"architecture\": \"Gemma\", \"norm\": \"RMSNorm\"}\n}\n\nprint(\"=\"*70)\nprint(\"PARALLEL ARCHITECTURE HYPOTHESIS TEST\")\nprint(\"=\"*70)\nprint(\"\\nTest Subject:\")\nfor name, info in MODEL_TO_TEST.items():\n    print(f\"  {name}\")\n    print(f\"    Path: {info['hf_path']}\")\n    print(f\"    Architecture: {info['architecture']}\")\n    print(f\"    Norm: {info['norm_type']}\")\n    print(f\"    Expected: {info['expected_gain']}\")\n\nprint(\"\\nReference Models (Parallel Architecture):\")\nfor name, info in REFERENCE_MODELS.items():\n    if \"Parallel\" in info[\"architecture\"]:\n        print(f\"  {name}: Gain={info['gain']:.2f} ({info['architecture']})\")\n\nprint(\"\\nReference Models (Sequential Architecture):\")\nfor name, info in REFERENCE_MODELS.items():\n    if \"Sequential\" in info[\"architecture\"]:\n        print(f\"  {name}: Gain={info['gain']:.2f} ({info['norm']})\")",
      "metadata": {
        "id": "models"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Cell 4: MEASUREMENT ENGINE (GPT-J Compatible)\n\ndef get_layer_list(model):\n    \"\"\"Get the transformer layers from different model architectures.\"\"\"\n    # GPT-J style: model.transformer.h\n    if hasattr(model, 'transformer') and hasattr(model.transformer, 'h'):\n        return model.transformer.h\n    # GPT-2 style: model.transformer.h\n    elif hasattr(model, 'transformer') and hasattr(model.transformer, 'h'):\n        return model.transformer.h\n    # LLaMA/Mistral/Gemma style: model.model.layers\n    elif hasattr(model, 'model') and hasattr(model.model, 'layers'):\n        return model.model.layers\n    # Pythia style: model.gpt_neox.layers\n    elif hasattr(model, 'gpt_neox') and hasattr(model.gpt_neox, 'layers'):\n        return model.gpt_neox.layers\n    else:\n        raise ValueError(f\"Unknown model architecture: {type(model)}\")\n\ndef measure_thermodynamics(model, tokenizer, text, device='cuda'):\n    \"\"\"Measure residual stream gain and output entropy for a given input.\"\"\"\n    \n    # Tokenize - handle padding token\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    \n    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    \n    # Hooks to capture residual stream norms\n    norms = []\n    \n    def get_norm_hook():\n        def hook(module, input, output):\n            # Handle different output formats\n            if isinstance(output, tuple):\n                hidden_state = output[0]\n            else:\n                hidden_state = output\n            \n            # Ensure we're working with a tensor\n            if hasattr(hidden_state, 'last_hidden_state'):\n                hidden_state = hidden_state.last_hidden_state\n            \n            # Norm of last token - convert to float explicitly\n            try:\n                last_token_norm = float(torch.norm(hidden_state[0, -1]).cpu().item())\n                norms.append(last_token_norm)\n            except Exception as e:\n                print(f\"Warning: Could not compute norm: {e}\")\n        return hook\n    \n    # Register hooks on all layers\n    handles = []\n    try:\n        layers = get_layer_list(model)\n        for layer in layers:\n            handles.append(layer.register_forward_hook(get_norm_hook()))\n    except Exception as e:\n        print(f\"Warning: Could not register hooks: {e}\")\n        return None\n    \n    # Forward pass\n    try:\n        with torch.no_grad():\n            outputs = model(**inputs)\n            logits = outputs.logits\n    except Exception as e:\n        print(f\"Error in forward pass: {e}\")\n        for h in handles:\n            h.remove()\n        return None\n    \n    # Cleanup hooks\n    for h in handles:\n        h.remove()\n    \n    # Calculate metrics - explicit float conversion\n    if len(norms) >= 2:\n        last_gain = float(norms[-1] / norms[-2]) if norms[-2] > 0 else 1.0\n        total_amp = float(norms[-1] / norms[0]) if norms[0] > 0 else 1.0\n    else:\n        last_gain = 1.0\n        total_amp = 1.0\n    \n    # Output Entropy (next token distribution)\n    last_token_logits = logits[0, -1, :].float()  # Ensure float32\n    probs = torch.softmax(last_token_logits, dim=0).cpu().numpy().astype(np.float64)\n    \n    # Clip to avoid log(0) issues\n    probs = np.clip(probs, 1e-10, 1.0)\n    probs = probs / probs.sum()  # Renormalize\n    \n    ent = float(entropy(probs))\n    \n    # Top token and probability\n    top_idx = int(torch.argmax(last_token_logits).item())\n    top_prob = float(probs[top_idx])\n    top_token = tokenizer.decode([top_idx])\n    \n    return {\n        \"last_gain\": float(last_gain),\n        \"total_amp\": float(total_amp),\n        \"entropy\": float(ent),\n        \"top_token\": str(top_token),\n        \"top_prob\": float(top_prob),\n        \"n_layers\": int(len(norms)),\n        \"all_norms\": [float(n) for n in norms]\n    }\n\nprint(\"Measurement engine ready (GPT-J compatible).\")",
      "metadata": {
        "id": "engine"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Cell 5: LOAD GPT-J-6B AND RUN MEASUREMENTS\n\nall_results = []\n\nprint(\"=\"*70)\nprint(\"LOADING GPT-J-6B (6 Billion Parameters)\")\nprint(\"This may take a few minutes...\")\nprint(\"=\"*70 + \"\\n\")\n\nmodel_name = \"GPT-J-6B\"\nmodel_info = MODEL_TO_TEST[model_name]\nhf_path = model_info[\"hf_path\"]\n\nprint(f\"Loading {model_name} ({model_info['params']})...\")\nprint(f\"  Path: {hf_path}\")\nprint(f\"  Architecture: {model_info['architecture']}\")\nprint(f\"  Norm: {model_info['norm_type']}\")\n\ntry:\n    # Load tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(\n        hf_path,\n        token=HF_TOKEN if HF_TOKEN else None\n    )\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    \n    # Load model with bfloat16 for memory efficiency\n    model = AutoModelForCausalLM.from_pretrained(\n        hf_path,\n        torch_dtype=torch.bfloat16,\n        device_map=\"auto\",\n        token=HF_TOKEN if HF_TOKEN else None,\n        low_cpu_mem_usage=True\n    )\n    model.eval()\n    \n    print(f\"\\nModel loaded successfully!\")\n    print(f\"Number of layers: {len(get_layer_list(model))}\")\n    \n    print(f\"\\nTesting {len(PROMPT_DATASET)} categories ({total_prompts} prompts)...\\n\")\n    \n    for category, prompts in PROMPT_DATASET.items():\n        print(f\"  {category}: \", end=\"\")\n        for i, prompt in enumerate(prompts):\n            res = measure_thermodynamics(model, tokenizer, prompt)\n            \n            if res is not None:\n                all_results.append({\n                    \"Model\": str(model_name),\n                    \"Architecture\": str(model_info[\"architecture\"]),\n                    \"Norm_Type\": str(model_info[\"norm_type\"]),\n                    \"Params\": str(model_info[\"params\"]),\n                    \"Role\": str(model_info[\"role\"]),\n                    \"Category\": str(category),\n                    \"Complexity\": int(CATEGORY_METADATA[category][\"complexity\"]),\n                    \"Prompt\": str(prompt),\n                    \"Prompt_Short\": str(prompt[:40] + \"...\"),\n                    \"Entropy\": float(res[\"entropy\"]),\n                    \"Last_Gain\": float(res[\"last_gain\"]),\n                    \"Total_Amp\": float(res[\"total_amp\"]),\n                    \"Top_Token\": str(res[\"top_token\"]),\n                    \"Top_Prob\": float(res[\"top_prob\"]),\n                    \"N_Layers\": int(res[\"n_layers\"])\n                })\n                print(\".\", end=\"\", flush=True)\n            else:\n                print(\"X\", end=\"\", flush=True)\n        print(f\" Done (n={len(prompts)})\")\n    \n    # Cleanup\n    del model\n    del tokenizer\n    torch.cuda.empty_cache()\n    gc.collect()\n    print(f\"\\n{model_name} complete. Memory cleared.\")\n    \nexcept Exception as e:\n    print(f\"\\nFAILED: {e}\")\n    import traceback\n    traceback.print_exc()\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"MEASUREMENT COMPLETE\")\nprint(f\"Total measurements: {len(all_results)}\")\nprint(\"=\"*70)",
      "metadata": {
        "id": "execution"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Cell 6: CREATE DATAFRAME & ANALYZE RESULTS\n\ndf = pd.DataFrame(all_results)\n\n# Define filenames with global timestamp\nCSV_FILE = f\"gptj_parallel_test_{TIMESTAMP}.csv\"\nJSON_FILE = f\"gptj_parallel_test_{TIMESTAMP}.json\"\nPNG_MAIN = f\"gptj_parallel_test_{TIMESTAMP}.png\"\nPNG_ARCH = f\"gptj_architecture_comparison_{TIMESTAMP}.png\"\n\n# Save raw results\ndf.to_csv(CSV_FILE, index=False)\nprint(f\"Saved: {CSV_FILE}\")\n\n# Display summary\nprint(\"\\n\" + \"=\"*70)\nprint(\"GPT-J-6B RESULTS SUMMARY\")\nprint(\"=\"*70)\n\nmean_gain = float(df['Last_Gain'].mean())\nstd_gain = float(df['Last_Gain'].std())\nmin_gain = float(df['Last_Gain'].min())\nmax_gain = float(df['Last_Gain'].max())\n\nprint(f\"\\nGPT-J-6B Last Layer Gain:\")\nprint(f\"  Mean:  {mean_gain:.4f}\")\nprint(f\"  Std:   {std_gain:.4f}\")\nprint(f\"  Range: [{min_gain:.4f}, {max_gain:.4f}]\")\n\n# Critical test\nprint(\"\\n\" + \"=\"*70)\nprint(\"CRITICAL HYPOTHESIS TEST\")\nprint(\"=\"*70)\n\nprint(f\"\\nHypothesis: Parallel Attention Architecture causes dampening (Gain < 1.0)\")\nprint(f\"\\nReference:\")\nprint(f\"  Pythia-6.9B (Parallel): 0.80\")\nprint(f\"  GPT-2-XL (Sequential):  1.02\")\n\nprint(f\"\\nResult:\")\nprint(f\"  GPT-J-6B (Parallel):    {mean_gain:.4f}\")\n\nif mean_gain < 1.0:\n    print(f\"\\n  ✅ GPT-J-6B shows DAMPENING (Gain < 1.0)\")\n    print(f\"  ✅ PARALLEL ARCHITECTURE HYPOTHESIS: CONFIRMED\")\n    hypothesis_status = \"CONFIRMED\"\nelse:\n    print(f\"\\n  ❌ GPT-J-6B shows EXPANSION (Gain > 1.0)\")\n    print(f\"  ❌ PARALLEL ARCHITECTURE HYPOTHESIS: REJECTED\")\n    hypothesis_status = \"REJECTED\"\n\nprint(\"\\nMean Gain per Category:\")\nprint(df.groupby('Category')['Last_Gain'].agg(['mean', 'std']).round(4))",
      "metadata": {
        "id": "analysis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Cell 7: STATISTICAL VALIDATION\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"STATISTICAL VALIDATION\")\nprint(\"=\"*70)\n\n# T-test against 1.0 (null hypothesis: gain = 1.0)\nt_stat, p_val_two = ttest_1samp(df['Last_Gain'], 1.0)\n\n# One-sided p-value\nif t_stat < 0:\n    p_one_sided = float(p_val_two / 2)  # Testing for < 1.0\n    direction = \"dampening\"\nelse:\n    p_one_sided = float(1 - p_val_two / 2)  # Testing for > 1.0\n    direction = \"expansion\"\n\nprint(f\"\\nT-test: Is GPT-J-6B mean gain significantly different from 1.0?\")\nprint(f\"  t-statistic: {t_stat:.4f}\")\nprint(f\"  p-value (two-sided): {p_val_two:.6f}\")\nprint(f\"  p-value (one-sided, {direction}): {p_one_sided:.6f}\")\n\nif p_one_sided < 0.001:\n    sig = \"*** (p < 0.001)\"\nelif p_one_sided < 0.01:\n    sig = \"** (p < 0.01)\"\nelif p_one_sided < 0.05:\n    sig = \"* (p < 0.05)\"\nelse:\n    sig = \"(not significant)\"\n\nprint(f\"  Significance: {sig}\")\n\n# Compare with Pythia\npythia_gain = 0.80\ngpt2_gain = 1.02  # GPT2-XL\n\nprint(f\"\\nComparison:\")\nprint(f\"  GPT-J-6B:     {mean_gain:.4f} (This Test)\")\nprint(f\"  Pythia-6.9B:  {pythia_gain:.4f} (Reference - Same Architecture)\")\nprint(f\"  Difference:   {abs(mean_gain - pythia_gain):.4f}\")\n\nif abs(mean_gain - pythia_gain) < 0.15:\n    print(f\"\\n  --> CONSISTENT: GPT-J and Pythia both show dampening\")\nelif mean_gain < 1.0 and pythia_gain < 1.0:\n    print(f\"\\n  --> DIRECTIONALLY CONSISTENT: Both < 1.0 (dampening)\")\nelse:\n    print(f\"\\n  --> DIVERGENT: Different behavior despite same architecture\")",
      "metadata": {
        "id": "statistics"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Cell 8: ARCHITECTURE COMPARISON VISUALIZATION\n\nfig, axes = plt.subplots(2, 2, figsize=(16, 14))\n\n# A. GPT-J Gain Distribution\nax1 = axes[0, 0]\nax1.hist(df['Last_Gain'], bins=20, alpha=0.7, color='#2ca02c', edgecolor='black')\nax1.axvline(1.0, ls='--', c='red', lw=2, label='Neutral (1.0)')\nax1.axvline(0.80, ls=':', c='blue', lw=2, label='Pythia Ref (0.80)')\nax1.axvline(mean_gain, ls='-', c='green', lw=2, label=f'GPT-J Mean ({mean_gain:.3f})')\nax1.set_xlabel('Last Layer Gain')\nax1.set_ylabel('Count')\nax1.set_title('A. GPT-J-6B Gain Distribution')\nax1.legend(loc='best')\nax1.grid(True, alpha=0.3)\n\n# B. Architecture Comparison Bar Chart\nax2 = axes[0, 1]\n\n# Separate by architecture\nparallel_models = [\n    ('Pythia-6.9B', 0.80, 'Reference'),\n    ('GPT-J-6B', mean_gain, 'This Test')\n]\nsequential_models = [\n    ('GPT2-Medium', 1.16, 'LayerNorm'),\n    ('GPT2-XL', 1.02, 'LayerNorm'),\n    ('GPT2-Large', 1.01, 'LayerNorm')\n]\nrmsnorm_models = [\n    ('Mistral-7B', 1.11, 'RMSNorm'),\n    ('LLaMA-3.1-8B', 1.48, 'RMSNorm'),\n    ('Gemma-7B', 2.31, 'RMSNorm')\n]\n\nall_models = parallel_models + sequential_models + rmsnorm_models\nall_models_sorted = sorted(all_models, key=lambda x: x[1])\n\nnames = [m[0] for m in all_models_sorted]\ngains = [m[1] for m in all_models_sorted]\ncolors = []\nfor m in all_models_sorted:\n    if m[0] in ['Pythia-6.9B', 'GPT-J-6B']:\n        colors.append('#2ca02c')  # Green for parallel\n    elif m[0].startswith('GPT2'):\n        colors.append('#1f77b4')  # Blue for sequential LayerNorm\n    else:\n        colors.append('#ff7f0e')  # Orange for RMSNorm\n\nbars = ax2.barh(range(len(names)), gains, color=colors, alpha=0.8)\nax2.axvline(1.0, ls='--', c='red', lw=2, alpha=0.7)\nax2.set_yticks(range(len(names)))\nax2.set_yticklabels(names)\nax2.set_xlabel('Mean Last Layer Gain')\nax2.set_title('B. Architecture Comparison\\n(Green=Parallel, Blue=Sequential, Orange=RMSNorm)')\n\n# Add value labels\nfor bar, val in zip(bars, gains):\n    ax2.text(val + 0.03, bar.get_y() + bar.get_height()/2, \n             f'{val:.2f}', ha='left', va='center', fontsize=10, fontweight='bold')\n\nax2.grid(True, alpha=0.3, axis='x')\nax2.set_xlim(0, max(gains) * 1.15)\n\n# C. Gain by Category\nax3 = axes[1, 0]\ncategory_order = ['Factual', 'Cliche', 'Nonsense', 'Novel', 'Syntactic']\ndf_sorted = df.copy()\ndf_sorted['Category'] = pd.Categorical(df_sorted['Category'], categories=category_order, ordered=True)\ndf_sorted = df_sorted.sort_values('Category')\n\nsns.boxplot(data=df_sorted, x='Category', y='Last_Gain', ax=ax3, color='#2ca02c')\nax3.axhline(1.0, ls='--', c='red', alpha=0.5, label='Neutral')\nax3.axhline(0.80, ls=':', c='blue', alpha=0.5, label='Pythia Ref')\nax3.set_xlabel('Prompt Category')\nax3.set_ylabel('Last Layer Gain')\nax3.set_title('C. GPT-J-6B Gain by Category')\nax3.tick_params(axis='x', rotation=45)\nax3.legend(loc='upper right')\n\n# D. Entropy vs Gain\nax4 = axes[1, 1]\nax4.scatter(df['Entropy'], df['Last_Gain'], c='#2ca02c', alpha=0.7, s=80, edgecolors='black')\nax4.axhline(1.0, ls='--', c='red', alpha=0.5, label='Neutral (1.0)')\nax4.set_xlabel('Output Entropy (nats)')\nax4.set_ylabel('Last Layer Gain')\nax4.set_title('D. Entropy vs Gain (GPT-J-6B)')\n\n# Add correlation\nr, p = pearsonr(df['Entropy'], df['Last_Gain'])\nax4.text(0.05, 0.95, f'r = {r:.3f}\\np = {p:.4f}', transform=ax4.transAxes,\n         fontsize=12, verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\nax4.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig(PNG_MAIN, dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(f\"\\nFigure saved: {PNG_MAIN}\")",
      "metadata": {
        "id": "visualization1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Cell 9: ARCHITECTURE DICHOTOMY VISUALIZATION\n\nfig, ax = plt.subplots(1, 1, figsize=(14, 8))\n\n# Prepare data organized by architecture\nparallel_data = [\n    ('Pythia-6.9B', 0.80, 'GPT-NeoX'),\n    ('GPT-J-6B', mean_gain, 'GPT-NeoX')\n]\n\nsequential_data = [\n    ('GPT2-Large', 1.01, 'GPT-2'),\n    ('GPT2-XL', 1.02, 'GPT-2'),\n    ('GPT2-Medium', 1.16, 'GPT-2')\n]\n\n# Create grouped bar chart\nwidth = 0.35\nx_parallel = np.arange(len(parallel_data))\nx_sequential = np.arange(len(sequential_data)) + len(parallel_data) + 0.5\n\n# Parallel (GPT-NeoX) bars\nbars1 = ax.bar(x_parallel, [d[1] for d in parallel_data], width, \n               label='Parallel (GPT-NeoX)', color='#2ca02c', alpha=0.8)\n\n# Sequential (GPT-2) bars\nbars2 = ax.bar(x_sequential, [d[1] for d in sequential_data], width,\n               label='Sequential (GPT-2)', color='#1f77b4', alpha=0.8)\n\n# Formatting\nax.axhline(1.0, ls='--', c='red', lw=2, label='Neutral (1.0)')\nax.set_ylabel('Mean Last Layer Gain', fontsize=12)\nax.set_title('Parallel vs Sequential Attention Architecture\\n(LayerNorm Models Only)', fontsize=14)\n\n# X-axis labels\nall_x = list(x_parallel) + list(x_sequential)\nall_labels = [d[0] for d in parallel_data] + [d[0] for d in sequential_data]\nax.set_xticks(all_x)\nax.set_xticklabels(all_labels, rotation=45, ha='right')\n\n# Add value labels\nfor bar in bars1:\n    height = bar.get_height()\n    ax.annotate(f'{height:.2f}',\n                xy=(bar.get_x() + bar.get_width() / 2, height),\n                xytext=(0, 3), textcoords=\"offset points\",\n                ha='center', va='bottom', fontsize=11, fontweight='bold')\n\nfor bar in bars2:\n    height = bar.get_height()\n    ax.annotate(f'{height:.2f}',\n                xy=(bar.get_x() + bar.get_width() / 2, height),\n                xytext=(0, 3), textcoords=\"offset points\",\n                ha='center', va='bottom', fontsize=11, fontweight='bold')\n\n# Add annotations\nax.annotate('DAMPENING\\n(Gain < 1.0)', xy=(0.15, 0.25), xycoords='axes fraction',\n            fontsize=14, ha='center', color='#2ca02c', fontweight='bold',\n            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\nax.annotate('EXPANSION\\n(Gain > 1.0)', xy=(0.75, 0.75), xycoords='axes fraction',\n            fontsize=14, ha='center', color='#1f77b4', fontweight='bold',\n            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n\nax.legend(loc='upper left')\nax.grid(True, alpha=0.3, axis='y')\nax.set_ylim(0, 1.4)\n\nplt.tight_layout()\nplt.savefig(PNG_ARCH, dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(f\"\\nFigure saved: {PNG_ARCH}\")",
      "metadata": {
        "id": "visualization2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Cell 10: SAVE COMPREHENSIVE JSON RESULTS\n\ndef make_serializable(obj):\n    if isinstance(obj, (np.integer, np.floating)):\n        return float(obj)\n    elif isinstance(obj, np.ndarray):\n        return obj.tolist()\n    elif isinstance(obj, np.bool_):\n        return bool(obj)\n    elif isinstance(obj, dict):\n        return {k: make_serializable(v) for k, v in obj.items()}\n    elif isinstance(obj, list):\n        return [make_serializable(v) for v in obj]\n    return obj\n\n# Compile all results\nresults_json = {\n    \"experiment\": \"GPT-J-6B Parallel Architecture Test\",\n    \"purpose\": \"Validate hypothesis that Parallel Attention (GPT-NeoX) causes dampening\",\n    \"date\": TIMESTAMP,\n    \"model\": \"GPT-J-6B\",\n    \"architecture\": \"GPT-NeoX (Parallel Attention)\",\n    \"norm_type\": \"LayerNorm\",\n    \"n_prompts\": total_prompts,\n    \"n_measurements\": len(df),\n    \n    \"results\": {\n        \"mean_gain\": float(mean_gain),\n        \"std_gain\": float(std_gain),\n        \"min_gain\": float(min_gain),\n        \"max_gain\": float(max_gain),\n        \"t_statistic\": float(t_stat),\n        \"p_value_one_sided\": float(p_one_sided)\n    },\n    \n    \"category_means\": make_serializable(df.groupby('Category')['Last_Gain'].mean().to_dict()),\n    \n    \"hypothesis_test\": {\n        \"hypothesis\": \"Parallel Attention Architecture causes dampening (Gain < 1.0)\",\n        \"result\": hypothesis_status,\n        \"evidence\": f\"GPT-J-6B shows Gain = {mean_gain:.4f}\",\n        \"comparison\": {\n            \"pythia_6.9b\": {\"gain\": 0.80, \"architecture\": \"Parallel\"},\n            \"gpt2_xl\": {\"gain\": 1.02, \"architecture\": \"Sequential\"},\n            \"gptj_6b\": {\"gain\": float(mean_gain), \"architecture\": \"Parallel\"}\n        }\n    },\n    \n    \"reference_models\": make_serializable(REFERENCE_MODELS),\n    \"all_results\": make_serializable(all_results)\n}\n\nwith open(JSON_FILE, 'w') as f:\n    json.dump(results_json, f, indent=2, default=str)\n\nprint(f\"Results saved to {JSON_FILE}\")",
      "metadata": {
        "id": "save_json"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Cell 11: FINAL VERDICT\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"FINAL VERDICT: PARALLEL ARCHITECTURE HYPOTHESIS\")\nprint(\"=\"*70)\n\nprint(f\"\"\"\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                    GPT-J-6B PARALLEL ARCHITECTURE TEST                       │\n├─────────────────────────────────────────────────────────────────────────────┤\n│                                                                              │\n│   Model: GPT-J-6B (EleutherAI)                                              │\n│   Architecture: GPT-NeoX (Parallel Attention)                               │\n│   Normalization: LayerNorm                                                   │\n│                                                                              │\n│   MEASURED GAIN: {mean_gain:.4f}                                                     │\n│   t-test vs 1.0: t = {t_stat:.4f}, p = {p_one_sided:.6f}                              │\n│                                                                              │\n├─────────────────────────────────────────────────────────────────────────────┤\n│                                                                              │\n│   REFERENCE COMPARISON:                                                      │\n│                                                                              │\n│   PARALLEL ATTENTION (GPT-NeoX):                                            │\n│     Pythia-6.9B:  0.80  (DAMPENING)                                         │\n│     GPT-J-6B:     {mean_gain:.2f}  ({\"DAMPENING\" if mean_gain < 1.0 else \"EXPANSION\"})                                         │\n│                                                                              │\n│   SEQUENTIAL ATTENTION (GPT-2):                                              │\n│     GPT2-XL:      1.02  (EXPANSION)                                         │\n│     GPT2-Large:   1.01  (EXPANSION)                                         │\n│     GPT2-Medium:  1.16  (EXPANSION)                                         │\n│                                                                              │\n├─────────────────────────────────────────────────────────────────────────────┤\n│                                                                              │\n│   HYPOTHESIS: Parallel Attention causes dampening                            │\n│   STATUS: {hypothesis_status}                                                      │\n│                                                                              │\n\"\"\")\n\nif hypothesis_status == \"CONFIRMED\":\n    print(\"│   CONCLUSION:                                                            │\")\n    print(\"│   ✅ Both GPT-NeoX models (Pythia, GPT-J) show Gain < 1.0                │\")\n    print(\"│   ✅ Both GPT-2 models show Gain > 1.0                                   │\")\n    print(\"│   ✅ The PARALLEL ATTENTION architecture causes DAMPENING               │\")\n    print(\"│   ✅ This is NOT a Pythia-specific artifact                             │\")\n    print(\"│                                                                          │\")\n    print(\"│   PAPER #3 CLAIM: VALIDATED                                             │\")\nelse:\n    print(\"│   CONCLUSION:                                                            │\")\n    print(\"│   ❌ GPT-J does NOT show the expected dampening                          │\")\n    print(\"│   ❌ The parallel architecture hypothesis is REJECTED                    │\")\n    print(\"│   ❌ Pythia's dampening remains unexplained                              │\")\n    print(\"│                                                                          │\")\n    print(\"│   FURTHER INVESTIGATION NEEDED                                          │\")\n\nprint(\"│                                                                              │\")\nprint(\"└─────────────────────────────────────────────────────────────────────────────┘\")\nprint(\"\\n\")",
      "metadata": {
        "id": "verdict"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Cell 12: DOWNLOAD ALL RESULTS\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"DOWNLOADING RESULTS\")\nprint(\"=\"*70)\n\n# List all files to download\nfiles_to_download = [CSV_FILE, JSON_FILE, PNG_MAIN, PNG_ARCH]\n\nprint(\"\\nFiles to download:\")\nfor f in files_to_download:\n    if os.path.exists(f):\n        size = os.path.getsize(f) / 1024  # KB\n        print(f\"  {f} ({size:.1f} KB)\")\n    else:\n        print(f\"  {f} (NOT FOUND)\")\n\nprint(\"\\nStarting downloads...\")\n\n# Download each file\nfor f in files_to_download:\n    if os.path.exists(f):\n        try:\n            files.download(f)\n            print(f\"  Downloaded: {f}\")\n        except Exception as e:\n            print(f\"  FAILED to download {f}: {e}\")\n    else:\n        print(f\"  SKIPPED (not found): {f}\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"ALL DOWNLOADS COMPLETE\")\nprint(\"=\"*70)\nprint(\"\\nNext steps:\")\nprint(\"  1. If CONFIRMED: Update PYTHIA_ANOMALY_INVESTIGATION.md\")\nprint(\"  2. If REJECTED: Investigate alternative hypotheses\")\nprint(\"  3. Add results to Paper #3 documentation\")",
      "metadata": {
        "id": "download"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
