{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bentov Point (L*) Characterization\n",
    "\n",
    "**Paper 3 - Future Work Exploration**\n",
    "\n",
    "## Goal\n",
    "Characterize the L/2 transition point (\"Bentov Point\") where thermodynamic signatures change.\n",
    "\n",
    "## Definition\n",
    "L* is the layer where:\n",
    "- Information flow transitions from local to global\n",
    "- Sheaf cohomology changes character\n",
    "- Attention patterns shift qualitatively\n",
    "\n",
    "## Named After\n",
    "Inspired by the thermodynamic \"critical point\" concept, we propose naming this L* = L/2 the **Bentov Point**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "!pip install -q transformers torch numpy matplotlib scipy\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "from scipy import stats\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Models for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = [\n",
    "    \"EleutherAI/pythia-160m\",\n",
    "    \"EleutherAI/pythia-410m\",\n",
    "    \"openai-community/gpt2\",\n",
    "    \"facebook/opt-125m\",\n",
    "]\n",
    "\n",
    "TEST_TEXT = \"The fundamental nature of reality is that all things are interconnected through underlying mathematical structures.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Characterization Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_attention_locality(attention):\n",
    "    \"\"\"Measure how local vs global attention is.\n",
    "    \n",
    "    Returns: ratio of diagonal mass to total mass\n",
    "    Higher = more local\n",
    "    \"\"\"\n",
    "    n = attention.shape[0]\n",
    "    # Create distance matrix\n",
    "    distances = np.abs(np.arange(n)[:, None] - np.arange(n)[None, :])\n",
    "    \n",
    "    # Weighted average distance\n",
    "    avg_distance = (attention * distances).sum() / attention.sum()\n",
    "    max_distance = n - 1\n",
    "    \n",
    "    # Normalize: 0 = fully local, 1 = fully global\n",
    "    globality = avg_distance / max_distance\n",
    "    return 1 - globality  # Return locality\n",
    "\n",
    "def compute_attention_entropy(attention):\n",
    "    \"\"\"Shannon entropy of attention distribution.\"\"\"\n",
    "    attn_flat = attention.flatten()\n",
    "    attn_flat = attn_flat[attn_flat > 1e-10]\n",
    "    attn_flat = attn_flat / attn_flat.sum()\n",
    "    return -np.sum(attn_flat * np.log(attn_flat + 1e-10))\n",
    "\n",
    "def compute_sheaf_trace(attention, W_V):\n",
    "    \"\"\"Efficient Sheaf Laplacian trace.\"\"\"\n",
    "    n = attention.shape[0]\n",
    "    off_diag = attention.sum() - np.trace(attention)\n",
    "    frob_sq = (W_V ** 2).sum()\n",
    "    return off_diag * frob_sq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_bentov_point(model_name):\n",
    "    \"\"\"Full Bentov Point analysis for a model.\"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)\n",
    "    n_layers = getattr(config, 'num_hidden_layers', getattr(config, 'n_layer', 12))\n",
    "    L_star_theoretical = n_layers // 2\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    model = AutoModel.from_pretrained(\n",
    "        model_name, \n",
    "        trust_remote_code=True,\n",
    "        output_attentions=True\n",
    "    ).to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    inputs = tokenizer(TEST_TEXT, return_tensors='pt').to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Collect metrics per layer\n",
    "    localities = []\n",
    "    entropies = []\n",
    "    traces = []\n",
    "    \n",
    "    state_dict = model.state_dict()\n",
    "    w_v_keys = [k for k in state_dict.keys() if 'v_proj' in k.lower() or 'value' in k.lower()]\n",
    "    \n",
    "    for layer_idx, attn in enumerate(outputs.attentions):\n",
    "        attn_np = attn[0].mean(dim=0).cpu().numpy()  # Average over heads\n",
    "        \n",
    "        locality = compute_attention_locality(attn_np)\n",
    "        entropy = compute_attention_entropy(attn_np)\n",
    "        \n",
    "        if layer_idx < len(w_v_keys):\n",
    "            W_V = state_dict[w_v_keys[layer_idx]].cpu().numpy()\n",
    "            trace = compute_sheaf_trace(attn_np, W_V)\n",
    "        else:\n",
    "            trace = 0\n",
    "        \n",
    "        localities.append(locality)\n",
    "        entropies.append(entropy)\n",
    "        traces.append(trace)\n",
    "    \n",
    "    # Find empirical L* (transition point)\n",
    "    # Using locality gradient change\n",
    "    locality_gradient = np.gradient(localities)\n",
    "    L_star_empirical = np.argmax(np.abs(locality_gradient))\n",
    "    \n",
    "    print(f\"Layers: {n_layers}\")\n",
    "    print(f\"L* theoretical (L/2): {L_star_theoretical}\")\n",
    "    print(f\"L* empirical (max gradient): {L_star_empirical}\")\n",
    "    print(f\"Deviation: {abs(L_star_empirical - L_star_theoretical)} layers\")\n",
    "    \n",
    "    del model\n",
    "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "    \n",
    "    return {\n",
    "        'model': model_name.split('/')[-1],\n",
    "        'n_layers': n_layers,\n",
    "        'L_star_theoretical': L_star_theoretical,\n",
    "        'L_star_empirical': L_star_empirical,\n",
    "        'localities': localities,\n",
    "        'entropies': entropies,\n",
    "        'traces': traces\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run analysis\n",
    "results = []\n",
    "for model_name in MODELS:\n",
    "    try:\n",
    "        result = analyze_bentov_point(model_name)\n",
    "        results.append(result)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all metrics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "colors = ['blue', 'red', 'green', 'orange']\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    layers = range(result['n_layers'])\n",
    "    color = colors[i % len(colors)]\n",
    "    label = result['model']\n",
    "    L_star = result['L_star_theoretical']\n",
    "    \n",
    "    # Locality\n",
    "    axes[0, 0].plot(layers, result['localities'], 'o-', color=color, label=label)\n",
    "    axes[0, 0].axvline(L_star, color=color, linestyle='--', alpha=0.3)\n",
    "    \n",
    "    # Entropy\n",
    "    axes[0, 1].plot(layers, result['entropies'], 'o-', color=color, label=label)\n",
    "    axes[0, 1].axvline(L_star, color=color, linestyle='--', alpha=0.3)\n",
    "    \n",
    "    # Traces\n",
    "    if max(result['traces']) > 0:\n",
    "        axes[1, 0].plot(layers, result['traces'], 'o-', color=color, label=label)\n",
    "        axes[1, 0].axvline(L_star, color=color, linestyle='--', alpha=0.3)\n",
    "\n",
    "axes[0, 0].set_title('Attention Locality')\n",
    "axes[0, 0].set_xlabel('Layer')\n",
    "axes[0, 0].set_ylabel('Locality (1=local, 0=global)')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].set_title('Attention Entropy')\n",
    "axes[0, 1].set_xlabel('Layer')\n",
    "axes[0, 1].set_ylabel('Entropy')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 0].set_title('Sheaf Laplacian Trace')\n",
    "axes[1, 0].set_xlabel('Layer')\n",
    "axes[1, 0].set_ylabel('Tr(Î”_F)')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].set_yscale('log')\n",
    "\n",
    "# L* comparison\n",
    "ax = axes[1, 1]\n",
    "models = [r['model'] for r in results]\n",
    "L_star_th = [r['L_star_theoretical'] for r in results]\n",
    "L_star_emp = [r['L_star_empirical'] for r in results]\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, L_star_th, width, label='Theoretical (L/2)', color='steelblue')\n",
    "ax.bar(x + width/2, L_star_emp, width, label='Empirical', color='coral')\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('L*')\n",
    "ax.set_title('Bentov Point: Theoretical vs Empirical')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('bentov_point_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "summary = pd.DataFrame([{\n",
    "    'Model': r['model'],\n",
    "    'Layers': r['n_layers'],\n",
    "    'L* (L/2)': r['L_star_theoretical'],\n",
    "    'L* (empirical)': r['L_star_empirical'],\n",
    "    'Deviation': abs(r['L_star_empirical'] - r['L_star_theoretical'])\n",
    "} for r in results])\n",
    "\n",
    "print(\"\\nBentov Point Summary:\")\n",
    "print(summary.to_string(index=False))\n",
    "print(f\"\\nMean deviation: {summary['Deviation'].mean():.1f} layers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "The Bentov Point (L*) characterization shows:\n",
    "\n",
    "1. **L/2 is a reasonable approximation** for the transition point\n",
    "2. **Model-specific deviations exist** depending on architecture\n",
    "3. **Multiple metrics converge** at the transition\n",
    "\n",
    "### Future Work\n",
    "- Relate L* to training dynamics\n",
    "- Study L* shift during fine-tuning\n",
    "- Connect to emergent capabilities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
