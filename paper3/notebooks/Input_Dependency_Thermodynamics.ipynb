{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input-Dependency Thermodynamics: The Physics of Language\n",
    "\n",
    "**Date:** 2026-01-05\n",
    "**Goal:** Prove that residual stream gain is INPUT-DEPENDENT, not architecture-fixed\n",
    "\n",
    "## The Discovery\n",
    "\n",
    "LLaMA 3.1 showed:\n",
    "- **0.48x contraction** with prompt: \"The capital of France is\"\n",
    "- **1.53x expansion** with prompt: \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "**Difference: 3.19x** - Same model, different prompts!\n",
    "\n",
    "## Hypothesis: Thermodynamics of Difficulty\n",
    "\n",
    "```\n",
    "Factual (Trivial)   → Contraction (Model is \"sure\", low entropy)\n",
    "Syntactic (Grammar) → Contraction? (Grammar constrains, reduces search space)\n",
    "Ambiguous (Open)    → Expansion (Model needs to \"explore\", high entropy)\n",
    "Nonsense (Chaos)    → Maximum Expansion (Confusion/uncertainty)\n",
    "```\n",
    "\n",
    "**Physical Analogy:** Car with automatic transmission\n",
    "- Cruising (easy) → Low RPM, high gear (contraction)\n",
    "- Hill climbing (hard) → Kickdown, high RPM (expansion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 0: HuggingFace Login\n",
    "from huggingface_hub import login\n",
    "login()\n",
    "print(\"HuggingFace login complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup & Imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from scipy.stats import entropy\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Configure plots\n",
    "plt.style.use('seaborn-v0_8-paper')\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "RESULTS_DIR = './Results'\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: The \"Difficulty\" Spectrum\n",
    "# 4 types of prompts with increasing uncertainty\n",
    "\n",
    "PROMPTS = {\n",
    "    \"1_Factual\": {\n",
    "        \"text\": \"The capital of France is\",\n",
    "        \"difficulty\": \"Trivial\",\n",
    "        \"expected\": \"CONTRACTION\",\n",
    "        \"reason\": \"Single correct answer, model is certain\"\n",
    "    },\n",
    "    \"2_Syntactic\": {\n",
    "        \"text\": \"The agreement, which, notwithstanding the fact that it was signed only yesterday, effectively binds all parties immediately, stipulates that\",\n",
    "        \"difficulty\": \"Nested Grammar\",\n",
    "        \"expected\": \"CONTRACTION (Hypothesis!)\",\n",
    "        \"reason\": \"Grammar constrains, search space is LIMITED by syntax\"\n",
    "    },\n",
    "    \"3_Ambiguous\": {\n",
    "        \"text\": \"The true meaning of happiness is often found in\",\n",
    "        \"difficulty\": \"Open-ended\",\n",
    "        \"expected\": \"EXPANSION\",\n",
    "        \"reason\": \"Many valid continuations, high entropy\"\n",
    "    },\n",
    "    \"4_Nonsense\": {\n",
    "        \"text\": \"Table sky run blue jump quickly under over\",\n",
    "        \"difficulty\": \"Chaos\",\n",
    "        \"expected\": \"MAXIMUM EXPANSION\",\n",
    "        \"reason\": \"No grammar, no semantics, pure confusion\"\n",
    "    },\n",
    "    \"5_Original_Test\": {\n",
    "        \"text\": \"The quick brown fox jumps over the lazy dog.\",\n",
    "        \"difficulty\": \"Moderate (reference)\",\n",
    "        \"expected\": \"~1.53x (as measured)\",\n",
    "        \"reason\": \"The original LLaMA 2 vs 3.1 test prompt\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Prompt Spectrum:\")\n",
    "print(\"=\"*70)\n",
    "for name, info in PROMPTS.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Text: '{info['text'][:50]}...' \" if len(info['text']) > 50 else f\"  Text: '{info['text']}'\")\n",
    "    print(f\"  Difficulty: {info['difficulty']}\")\n",
    "    print(f\"  Expected: {info['expected']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Residual Stream + Entropy Analyzer\n",
    "\n",
    "class ThermodynamicsAnalyzer:\n",
    "    \"\"\"Analyze residual stream dynamics AND output entropy.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = next(model.parameters()).device\n",
    "        self.hooks = []\n",
    "        self.residual_norms = []\n",
    "        self.embedding_norm = None\n",
    "    \n",
    "    def _make_embedding_hook(self):\n",
    "        def hook(module, args, output):\n",
    "            with torch.no_grad():\n",
    "                self.embedding_norm = output.float().norm().item()\n",
    "        return hook\n",
    "    \n",
    "    def _make_layer_hook(self, layer_idx):\n",
    "        def hook(module, args, output):\n",
    "            if isinstance(output, tuple):\n",
    "                hidden = output[0]\n",
    "            else:\n",
    "                hidden = output\n",
    "            with torch.no_grad():\n",
    "                norm = hidden.float().norm().item()\n",
    "                self.residual_norms.append((layer_idx, norm))\n",
    "        return hook\n",
    "    \n",
    "    def register_hooks(self):\n",
    "        # Embedding hook\n",
    "        h_emb = self.model.model.embed_tokens.register_forward_hook(self._make_embedding_hook())\n",
    "        self.hooks.append(h_emb)\n",
    "        \n",
    "        # Layer hooks\n",
    "        for i, layer in enumerate(self.model.model.layers):\n",
    "            h = layer.register_forward_hook(self._make_layer_hook(i))\n",
    "            self.hooks.append(h)\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        for h in self.hooks:\n",
    "            h.remove()\n",
    "        self.hooks = []\n",
    "    \n",
    "    def clear(self):\n",
    "        self.residual_norms = []\n",
    "        self.embedding_norm = None\n",
    "    \n",
    "    def analyze(self, prompt):\n",
    "        \"\"\"Full analysis: residual gains + output entropy.\"\"\"\n",
    "        self.clear()\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = self.tokenizer(prompt, return_tensors='pt').to(self.device)\n",
    "        \n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            logits = outputs.logits\n",
    "        \n",
    "        # Compute gains\n",
    "        sorted_norms = sorted(self.residual_norms, key=lambda x: x[0])\n",
    "        all_norms = [('emb', self.embedding_norm)] + sorted_norms\n",
    "        \n",
    "        gains = []\n",
    "        norms = [n for _, n in all_norms]\n",
    "        \n",
    "        for i in range(1, len(all_norms)):\n",
    "            prev_norm = all_norms[i-1][1]\n",
    "            curr_norm = all_norms[i][1]\n",
    "            if prev_norm > 1e-8:\n",
    "                gain = curr_norm / prev_norm\n",
    "            else:\n",
    "                gain = 1.0\n",
    "            gains.append(float(gain))\n",
    "        \n",
    "        # Compute output entropy (Shannon entropy of softmax over last token logits)\n",
    "        last_logits = logits[0, -1, :]\n",
    "        probs = torch.softmax(last_logits, dim=0).cpu().numpy()\n",
    "        token_entropy = entropy(probs)  # Shannon entropy in nats\n",
    "        \n",
    "        # Top 5 most likely tokens\n",
    "        top_k = 5\n",
    "        top_probs, top_indices = torch.topk(torch.tensor(probs), top_k)\n",
    "        top_tokens = [self.tokenizer.decode([idx]) for idx in top_indices.tolist()]\n",
    "        \n",
    "        return {\n",
    "            'prompt': prompt,\n",
    "            'gains': gains,\n",
    "            'norms': norms,\n",
    "            'last_gain': gains[-1] if gains else 0,\n",
    "            'expands': gains[-1] > 1.0 if gains else False,\n",
    "            'entropy': float(token_entropy),\n",
    "            'top_tokens': list(zip(top_tokens, top_probs.tolist())),\n",
    "            'cumulative_energy': float(np.prod(gains)) if gains else 0\n",
    "        }\n",
    "\n",
    "print(\"ThermodynamicsAnalyzer defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Load LLaMA 3.1\n",
    "\n",
    "MODEL_NAME = 'meta-llama/Llama-3.1-8B'\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto',\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(f\"Model loaded: {MODEL_NAME}\")\n",
    "print(f\"Layers: {len(model.model.layers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Run the Thermodynamics Experiment\n",
    "\n",
    "analyzer = ThermodynamicsAnalyzer(model, tokenizer)\n",
    "analyzer.register_hooks()\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"THERMODYNAMICS OF DIFFICULTY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n{'Type':<20} | {'Entropy':>8} | {'Last Gain':>10} | {'Behavior':>12} | Top Token\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for name, info in PROMPTS.items():\n",
    "    text = info['text']\n",
    "    \n",
    "    # Analyze\n",
    "    res = analyzer.analyze(text)\n",
    "    res['prompt_type'] = name\n",
    "    res['difficulty'] = info['difficulty']\n",
    "    res['expected'] = info['expected']\n",
    "    results[name] = res\n",
    "    \n",
    "    # Interpret\n",
    "    gain = res['last_gain']\n",
    "    behavior = \"EXPANDS\" if gain > 1.0 else \"CONTRACTS\"\n",
    "    ent = res['entropy']\n",
    "    top_tok = res['top_tokens'][0][0] if res['top_tokens'] else \"?\"\n",
    "    \n",
    "    print(f\"{name:<20} | {ent:>8.2f} | {gain:>9.3f}x | {behavior:>12} | '{top_tok}'\")\n",
    "\n",
    "analyzer.remove_hooks()\n",
    "print(\"\\nAnalysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Correlation Analysis - Entropy vs Gain\n",
    "\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "entropies = [results[k]['entropy'] for k in PROMPTS.keys()]\n",
    "last_gains = [results[k]['last_gain'] for k in PROMPTS.keys()]\n",
    "\n",
    "# Compute correlations\n",
    "pearson_r, pearson_p = pearsonr(entropies, last_gains)\n",
    "spearman_r, spearman_p = spearmanr(entropies, last_gains)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ENTROPY vs GAIN CORRELATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nPearson r:  {pearson_r:.4f} (p={pearson_p:.4f})\")\n",
    "print(f\"Spearman r: {spearman_r:.4f} (p={spearman_p:.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "if pearson_r > 0.5 and pearson_p < 0.1:\n",
    "    print(\"HYPOTHESIS SUPPORTED!\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nPositive correlation between output entropy and last-layer gain!\")\n",
    "    print(\"Energy Expenditure ∝ Uncertainty\")\n",
    "    print(\"\\nPhysical Law: The model 'works harder' (expands) when uncertain.\")\n",
    "elif pearson_r < -0.5 and pearson_p < 0.1:\n",
    "    print(\"INVERSE CORRELATION!\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nNegative correlation: Low entropy → High gain?\")\n",
    "    print(\"This would suggest the model 'broadcasts' certainty.\")\n",
    "else:\n",
    "    print(\"INCONCLUSIVE\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nCorrelation r={pearson_r:.2f} is not strong enough.\")\n",
    "    print(\"Relationship may be non-linear or other factors matter.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Visualization\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Color coding by difficulty\n",
    "colors = {\n",
    "    '1_Factual': 'green',\n",
    "    '2_Syntactic': 'blue', \n",
    "    '3_Ambiguous': 'orange',\n",
    "    '4_Nonsense': 'red',\n",
    "    '5_Original_Test': 'purple'\n",
    "}\n",
    "\n",
    "# Plot 1: Layer-wise Gains by Prompt Type\n",
    "ax1 = axes[0, 0]\n",
    "for name, res in results.items():\n",
    "    gains = res['gains']\n",
    "    ax1.plot(gains, label=f\"{name} (H={res['entropy']:.1f})\", \n",
    "             color=colors[name], linewidth=2, marker='o', markersize=3, alpha=0.7)\n",
    "ax1.axhline(y=1.0, color='gray', linestyle='--', alpha=0.5, label='Neutral (Gain=1)')\n",
    "ax1.set_xlabel('Layer')\n",
    "ax1.set_ylabel('Gain (||x_{l+1}|| / ||x_l||)')\n",
    "ax1.set_title('Residual Stream Gain by Prompt Type\\n\"Energy Flow\" Through Layers')\n",
    "ax1.legend(fontsize=8)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Last Gain vs Entropy (The Key Relationship!)\n",
    "ax2 = axes[0, 1]\n",
    "for name, res in results.items():\n",
    "    ax2.scatter(res['entropy'], res['last_gain'], s=200, color=colors[name], \n",
    "                label=name, alpha=0.8, edgecolors='black')\n",
    "ax2.axhline(y=1.0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax2.set_xlabel('Output Entropy (nats)')\n",
    "ax2.set_ylabel('Last Layer Gain')\n",
    "ax2.set_title(f'Entropy vs Gain\\n(Pearson r={pearson_r:.2f}, p={pearson_p:.3f})')\n",
    "ax2.legend(fontsize=8)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Add trend line\n",
    "z = np.polyfit(entropies, last_gains, 1)\n",
    "p = np.poly1d(z)\n",
    "x_line = np.linspace(min(entropies), max(entropies), 100)\n",
    "ax2.plot(x_line, p(x_line), 'k--', alpha=0.5, label='Trend')\n",
    "\n",
    "# Plot 3: Bar Chart - Last Gains by Type\n",
    "ax3 = axes[1, 0]\n",
    "names = list(results.keys())\n",
    "gains = [results[k]['last_gain'] for k in names]\n",
    "bar_colors = [colors[k] for k in names]\n",
    "bars = ax3.bar(range(len(names)), gains, color=bar_colors, alpha=0.8, edgecolor='black')\n",
    "ax3.axhline(y=1.0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax3.set_xticks(range(len(names)))\n",
    "ax3.set_xticklabels([n.split('_')[1] for n in names], rotation=30, ha='right')\n",
    "ax3.set_ylabel('Last Layer Gain')\n",
    "ax3.set_title('Last Layer Gain by Prompt Type\\n(>1 = Expansion, <1 = Contraction)')\n",
    "for bar, val in zip(bars, gains):\n",
    "    ax3.annotate(f'{val:.2f}x', xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n",
    "                 ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Plot 4: Entropy Bar Chart\n",
    "ax4 = axes[1, 1]\n",
    "ents = [results[k]['entropy'] for k in names]\n",
    "bars = ax4.bar(range(len(names)), ents, color=bar_colors, alpha=0.8, edgecolor='black')\n",
    "ax4.set_xticks(range(len(names)))\n",
    "ax4.set_xticklabels([n.split('_')[1] for n in names], rotation=30, ha='right')\n",
    "ax4.set_ylabel('Output Entropy (nats)')\n",
    "ax4.set_title('Output Entropy by Prompt Type\\n(Higher = More Uncertain)')\n",
    "for bar, val in zip(bars, ents):\n",
    "    ax4.annotate(f'{val:.1f}', xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n",
    "                 ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_DIR}/input_dependency_thermodynamics.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nVisualization saved to {RESULTS_DIR}/input_dependency_thermodynamics.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: The Schachtelsatz Paradox Test\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"THE SCHACHTELSATZ PARADOX\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nHypothesis:\")\n",
    "print(\"  For HUMANS: Nested sentences are HARD (working memory overload)\")\n",
    "print(\"  For LLMs:   Nested sentences may be EASY (grammar constrains search)\")\n",
    "\n",
    "# Compare Syntactic vs Factual\n",
    "syntactic = results['2_Syntactic']\n",
    "factual = results['1_Factual']\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Factual:   Gain={factual['last_gain']:.3f}x, Entropy={factual['entropy']:.2f}\")\n",
    "print(f\"  Syntactic: Gain={syntactic['last_gain']:.3f}x, Entropy={syntactic['entropy']:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "if syntactic['last_gain'] < 1.0 and factual['last_gain'] < 1.0:\n",
    "    ratio = syntactic['last_gain'] / factual['last_gain']\n",
    "    if abs(ratio - 1.0) < 0.3:\n",
    "        print(\"SCHACHTELSATZ PARADOX CONFIRMED!\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"\\nBoth Syntactic and Factual prompts cause CONTRACTION.\")\n",
    "        print(\"Grammar = Constraint = Low Entropy = Easy for LLM\")\n",
    "        print(\"\\n→ LLM Difficulty ≠ Human Difficulty!\")\n",
    "    else:\n",
    "        print(\"BOTH CONTRACT, but different magnitudes.\")\n",
    "elif syntactic['last_gain'] > 1.0:\n",
    "    print(\"SYNTACTIC EXPANDS - Grammar does NOT constrain!\")\n",
    "else:\n",
    "    print(\"Mixed results - investigate further.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Save Results\n",
    "\n",
    "final_results = {\n",
    "    'experiment': 'Input-Dependency Thermodynamics',\n",
    "    'date': '2026-01-05',\n",
    "    'model': MODEL_NAME,\n",
    "    'hypothesis': 'Residual stream gain is input-dependent, not architecture-fixed',\n",
    "    'discovery': 'LLaMA 3.1 showed 0.48x (factual) vs 1.53x (complex) - 3.19x difference!',\n",
    "    'prompts': {k: {'text': v['text'], 'difficulty': v['difficulty']} for k, v in PROMPTS.items()},\n",
    "    'results': {k: {\n",
    "        'last_gain': v['last_gain'],\n",
    "        'entropy': v['entropy'],\n",
    "        'expands': v['expands'],\n",
    "        'gains': v['gains'],\n",
    "        'top_tokens': v['top_tokens']\n",
    "    } for k, v in results.items()},\n",
    "    'correlation': {\n",
    "        'pearson_r': pearson_r,\n",
    "        'pearson_p': pearson_p,\n",
    "        'spearman_r': spearman_r,\n",
    "        'spearman_p': spearman_p\n",
    "    },\n",
    "    'conclusion': 'PENDING'\n",
    "}\n",
    "\n",
    "# Determine conclusion\n",
    "if pearson_r > 0.5:\n",
    "    final_results['conclusion'] = 'CONFIRMED: Energy ∝ Uncertainty'\n",
    "elif pearson_r < -0.5:\n",
    "    final_results['conclusion'] = 'INVERSE: Energy ∝ Certainty'\n",
    "else:\n",
    "    final_results['conclusion'] = 'COMPLEX: Non-linear relationship'\n",
    "\n",
    "output_path = f'{RESULTS_DIR}/input_dependency_thermodynamics.json'\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(final_results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"Results saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Download Results\n",
    "\n",
    "from google.colab import files\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DOWNLOADING RESULTS...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for filepath in [f'{RESULTS_DIR}/input_dependency_thermodynamics.json',\n",
    "                 f'{RESULTS_DIR}/input_dependency_thermodynamics.png']:\n",
    "    if os.path.exists(filepath):\n",
    "        print(f\"Downloading: {filepath}\")\n",
    "        files.download(filepath)\n",
    "\n",
    "print(\"\\nDownload complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Results\n",
    "\n",
    "### If Hypothesis Confirmed:\n",
    "```\n",
    "Factual (\"The capital of France is\"):    LOW entropy, CONTRACTS\n",
    "Syntactic (nested sentence):             LOW entropy, CONTRACTS (Schachtelsatz Paradox!)\n",
    "Ambiguous (\"meaning of happiness\"):      HIGH entropy, EXPANDS\n",
    "Nonsense (random words):                 HIGHEST entropy, MAXIMUM EXPANSION\n",
    "\n",
    "Correlation: Entropy ↔ Gain (positive)\n",
    "```\n",
    "\n",
    "### Physical Law (if confirmed):\n",
    "```\n",
    "Energy Expenditure = f(Uncertainty)\n",
    "\n",
    "The model \"works harder\" (expands residual stream) when it's unsure.\n",
    "The model \"relaxes\" (contracts residual stream) when it knows the answer.\n",
    "```\n",
    "\n",
    "### Implications for Paper #3:\n",
    "- Residual stream dynamics are NOT just architecture-dependent\n",
    "- They are STATE-dependent (input determines behavior)\n",
    "- The \"0.48x vs 1.53x\" mystery is SOLVED: Different prompts!\n",
    "- New dimension: Static Architecture × Dynamic State"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
