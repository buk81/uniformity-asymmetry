{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# High-œÅ Model Hunt: Finding Other Dampening Models\n",
    "\n",
    "**Paper #3 Experiment:** H25 Cross-Architecture Validation\n",
    "\n",
    "**Hypothesis:** Models with œÅ = n_heads / d_head > 0.2 should show DAMPENING (G < 1.0)\n",
    "\n",
    "**Candidates to Test:**\n",
    "- OPT family (Meta)\n",
    "- BLOOM family (BigScience)\n",
    "- Falcon family (TII)\n",
    "- GPT-Neo family (EleutherAI)\n",
    "- StableLM family (Stability AI)\n",
    "\n",
    "**Goal:** Find at least ONE non-Pythia model that shows dampening due to high œÅ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install transformers torch matplotlib numpy --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import gc\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"GPU memory: {gpu_mem:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Candidate models to analyze\n",
    "# We'll first check their configs to compute œÅ\n",
    "\n",
    "CANDIDATE_MODELS = [\n",
    "    # OPT Family\n",
    "    'facebook/opt-125m',\n",
    "    'facebook/opt-350m',\n",
    "    'facebook/opt-1.3b',\n",
    "    'facebook/opt-2.7b',\n",
    "    'facebook/opt-6.7b',\n",
    "    \n",
    "    # BLOOM Family\n",
    "    'bigscience/bloom-560m',\n",
    "    'bigscience/bloom-1b1',\n",
    "    'bigscience/bloom-1b7',\n",
    "    'bigscience/bloom-3b',\n",
    "    \n",
    "    # Falcon Family\n",
    "    'tiiuae/falcon-7b',\n",
    "    \n",
    "    # GPT-Neo Family\n",
    "    'EleutherAI/gpt-neo-125M',\n",
    "    'EleutherAI/gpt-neo-1.3B',\n",
    "    'EleutherAI/gpt-neo-2.7B',\n",
    "    \n",
    "    # StableLM\n",
    "    'stabilityai/stablelm-base-alpha-3b',\n",
    "    \n",
    "    # Reference models (known values)\n",
    "    'EleutherAI/pythia-6.9b',  # œÅ = 0.25, G ‚âà 0.80\n",
    "    'EleutherAI/gpt-j-6B',      # œÅ = 0.0625, G ‚âà 1.065\n",
    "]\n",
    "\n",
    "print(f\"Candidate models: {len(CANDIDATE_MODELS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_rho(model_name):\n",
    "    \"\"\"Get architecture details and compute œÅ from config.\"\"\"\n",
    "    try:\n",
    "        config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)\n",
    "        \n",
    "        # Get number of heads\n",
    "        n_heads = getattr(config, 'num_attention_heads', None) or \\\n",
    "                  getattr(config, 'n_head', None) or \\\n",
    "                  getattr(config, 'num_heads', None)\n",
    "        \n",
    "        # Get hidden size\n",
    "        d_model = getattr(config, 'hidden_size', None) or \\\n",
    "                  getattr(config, 'n_embd', None) or \\\n",
    "                  getattr(config, 'd_model', None)\n",
    "        \n",
    "        # Get number of layers\n",
    "        n_layers = getattr(config, 'num_hidden_layers', None) or \\\n",
    "                   getattr(config, 'n_layer', None) or \\\n",
    "                   getattr(config, 'num_layers', None)\n",
    "        \n",
    "        # Compute d_head\n",
    "        if n_heads and d_model:\n",
    "            d_head = d_model // n_heads\n",
    "            rho = n_heads / d_head\n",
    "        else:\n",
    "            d_head = None\n",
    "            rho = None\n",
    "        \n",
    "        # Check normalization type\n",
    "        norm_type = 'Unknown'\n",
    "        if hasattr(config, 'layer_norm_eps'):\n",
    "            norm_type = 'LayerNorm'\n",
    "        if hasattr(config, 'rms_norm_eps'):\n",
    "            norm_type = 'RMSNorm'\n",
    "        \n",
    "        return {\n",
    "            'model': model_name,\n",
    "            'n_layers': n_layers,\n",
    "            'n_heads': n_heads,\n",
    "            'd_model': d_model,\n",
    "            'd_head': d_head,\n",
    "            'rho': rho,\n",
    "            'norm_type': norm_type,\n",
    "            'status': 'OK'\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'model': model_name,\n",
    "            'status': f'ERROR: {str(e)[:50]}'\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze all candidate configs\n",
    "all_configs = []\n",
    "\n",
    "print(\"Scanning model configs...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for model_name in CANDIDATE_MODELS:\n",
    "    print(f\"  {model_name}...\", end=\" \")\n",
    "    result = get_model_rho(model_name)\n",
    "    all_configs.append(result)\n",
    "    \n",
    "    if result['status'] == 'OK':\n",
    "        print(f\"œÅ = {result['rho']:.4f}\")\n",
    "    else:\n",
    "        print(result['status'])\n",
    "\n",
    "print(f\"\\nSuccessfully scanned: {sum(1 for c in all_configs if c['status'] == 'OK')} / {len(CANDIDATE_MODELS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter and sort by œÅ\n",
    "valid_configs = [c for c in all_configs if c['status'] == 'OK' and c['rho'] is not None]\n",
    "sorted_configs = sorted(valid_configs, key=lambda x: x['rho'], reverse=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"MODEL RANKING BY HEAD DENSITY (œÅ = n_heads / d_head)\")\n",
    "print(\"=\" * 90)\n",
    "print(f\"\\n{'Model':<45} {'Heads':>6} {'d_head':>8} {'œÅ':>8} {'Norm':>10} {'Prediction':>12}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for c in sorted_configs:\n",
    "    prediction = \"DAMPEN?\" if c['rho'] >= 0.2 else \"EXPAND?\"\n",
    "    marker = \"üîµ\" if c['rho'] >= 0.2 else \"üî¥\"\n",
    "    \n",
    "    # Short name\n",
    "    short_name = c['model'].split('/')[-1]\n",
    "    \n",
    "    print(f\"{short_name:<45} {c['n_heads']:>6} {c['d_head']:>8} {c['rho']:>8.4f} {c['norm_type']:>10} {marker} {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify HIGH-œÅ candidates (œÅ ‚â• 0.2)\n",
    "high_rho_candidates = [c for c in sorted_configs if c['rho'] >= 0.2]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"HIGH-œÅ CANDIDATES (œÅ ‚â• 0.2) - EXPECTED TO DAMPEN\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if high_rho_candidates:\n",
    "    for c in high_rho_candidates:\n",
    "        print(f\"\\n  {c['model']}\")\n",
    "        print(f\"    œÅ = {c['rho']:.4f} (n_heads={c['n_heads']}, d_head={c['d_head']})\")\n",
    "        print(f\"    Layers: {c['n_layers']}, Norm: {c['norm_type']}\")\n",
    "else:\n",
    "    print(\"\\n  ‚ö†Ô∏è No high-œÅ candidates found among scanned models!\")\n",
    "    print(\"  ‚Üí Pythia may be UNIQUE in its high-œÅ configuration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prompts\n",
    "TEST_PROMPTS = [\n",
    "    \"The capital of France is\",\n",
    "    \"Water freezes at a temperature of\",\n",
    "    \"Actions speak louder than\",\n",
    "    \"The quick brown fox jumps over the lazy\",\n",
    "    \"In mathematics, the Pythagorean theorem states that\",\n",
    "]\n",
    "\n",
    "def compute_residual_gain(model, tokenizer, prompts):\n",
    "    \"\"\"Compute Residual Stream Gain = ||h_L|| / ||h_{L-1}||.\"\"\"\n",
    "    gains = []\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, output_hidden_states=True)\n",
    "        \n",
    "        hidden_states = outputs.hidden_states\n",
    "        h_L = hidden_states[-1]\n",
    "        h_Lm1 = hidden_states[-2]\n",
    "        \n",
    "        norm_L = torch.norm(h_L[:, -1, :].float(), dim=-1).item()\n",
    "        norm_Lm1 = torch.norm(h_Lm1[:, -1, :].float(), dim=-1).item()\n",
    "        \n",
    "        gain = norm_L / (norm_Lm1 + 1e-10)\n",
    "        gains.append(gain)\n",
    "    \n",
    "    return np.mean(gains), np.std(gains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select models to test based on GPU memory\n",
    "# Prioritize high-œÅ candidates + reference models\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    available_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"Available GPU memory: {available_mem:.1f} GB\")\n",
    "else:\n",
    "    available_mem = 8  # Conservative CPU estimate\n",
    "\n",
    "# Memory estimates (rough)\n",
    "MEMORY_ESTIMATES = {\n",
    "    'opt-125m': 0.5, 'opt-350m': 1.5, 'opt-1.3b': 5, 'opt-2.7b': 8, 'opt-6.7b': 15,\n",
    "    'bloom-560m': 2, 'bloom-1b1': 4, 'bloom-1b7': 6, 'bloom-3b': 10,\n",
    "    'falcon-7b': 18,\n",
    "    'gpt-neo-125M': 0.5, 'gpt-neo-1.3B': 5, 'gpt-neo-2.7B': 8,\n",
    "    'stablelm-base-alpha-3b': 10,\n",
    "    'pythia-6.9b': 20, 'gpt-j-6B': 18,\n",
    "}\n",
    "\n",
    "# Select models that fit\n",
    "MODELS_TO_TEST = []\n",
    "for c in sorted_configs:\n",
    "    short_name = c['model'].split('/')[-1]\n",
    "    mem_needed = MEMORY_ESTIMATES.get(short_name, 10)\n",
    "    if mem_needed < (available_mem - 2):\n",
    "        MODELS_TO_TEST.append(c)\n",
    "\n",
    "print(f\"\\nModels to test: {len(MODELS_TO_TEST)}\")\n",
    "for c in MODELS_TO_TEST:\n",
    "    print(f\"  - {c['model'].split('/')[-1]} (œÅ = {c['rho']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test each model\n",
    "results = []\n",
    "\n",
    "for config in MODELS_TO_TEST:\n",
    "    model_name = config['model']\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing: {model_name}\")\n",
    "    print(f\"œÅ = {config['rho']:.4f}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "            device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "            trust_remote_code=True,\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "        model.eval()\n",
    "        \n",
    "        mean_gain, std_gain = compute_residual_gain(model, tokenizer, TEST_PROMPTS)\n",
    "        \n",
    "        is_dampening = mean_gain < 1.0\n",
    "        status = \"DAMPENING\" if is_dampening else \"EXPANSION\"\n",
    "        prediction_correct = (config['rho'] >= 0.2 and is_dampening) or (config['rho'] < 0.2 and not is_dampening)\n",
    "        \n",
    "        print(f\"\\nüî¨ RESIDUAL GAIN: {mean_gain:.4f} ¬± {std_gain:.4f}\")\n",
    "        print(f\"   ‚Üí {status}\")\n",
    "        print(f\"   H25 Prediction: {'‚úÖ CORRECT' if prediction_correct else '‚ùå WRONG'}\")\n",
    "        \n",
    "        result = config.copy()\n",
    "        result.update({\n",
    "            'residual_gain_mean': float(mean_gain),\n",
    "            'residual_gain_std': float(std_gain),\n",
    "            'is_dampening': is_dampening,\n",
    "            'h25_prediction_correct': prediction_correct\n",
    "        })\n",
    "        results.append(result)\n",
    "        \n",
    "        del model, tokenizer\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        result = config.copy()\n",
    "        result['status'] = f'ERROR: {str(e)[:50]}'\n",
    "        results.append(result)\n",
    "\n",
    "print(f\"\\n\\nTested: {len(results)} models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Table\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"CROSS-ARCHITECTURE H25 VALIDATION RESULTS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "tested_results = [r for r in results if 'residual_gain_mean' in r]\n",
    "\n",
    "print(f\"\\n{'Model':<40} {'œÅ':>8} {'Gain':>10} {'Status':>12} {'H25':>8}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for r in sorted(tested_results, key=lambda x: x['rho'], reverse=True):\n",
    "    short_name = r['model'].split('/')[-1]\n",
    "    status = \"DAMPEN\" if r['is_dampening'] else \"EXPAND\"\n",
    "    h25 = \"‚úÖ\" if r['h25_prediction_correct'] else \"‚ùå\"\n",
    "    marker = \"üîµ\" if r['is_dampening'] else \"üî¥\"\n",
    "    \n",
    "    print(f\"{short_name:<40} {r['rho']:>8.4f} {r['residual_gain_mean']:>10.4f} {marker} {status:<10} {h25}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "if tested_results:\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    rhos = [r['rho'] for r in tested_results]\n",
    "    gains = [r['residual_gain_mean'] for r in tested_results]\n",
    "    names = [r['model'].split('/')[-1] for r in tested_results]\n",
    "    colors = ['blue' if g < 1.0 else 'red' for g in gains]\n",
    "    \n",
    "    ax.scatter(rhos, gains, c=colors, s=150, zorder=5, edgecolors='black', linewidth=1)\n",
    "    \n",
    "    for i, name in enumerate(names):\n",
    "        ax.annotate(name, (rhos[i], gains[i]), textcoords=\"offset points\", \n",
    "                    xytext=(5, 5), fontsize=8, rotation=15)\n",
    "    \n",
    "    # Critical lines\n",
    "    ax.axhline(y=1.0, color='black', linestyle='--', alpha=0.7, label='G=1.0 (Bentov Point)')\n",
    "    ax.axvline(x=0.2, color='purple', linestyle=':', alpha=0.7, label='œÅ=0.2 (Critical?)')\n",
    "    \n",
    "    ax.set_xlabel('œÅ = n_heads / d_head (Head Density)', fontsize=12)\n",
    "    ax.set_ylabel('Residual Stream Gain', fontsize=12)\n",
    "    ax.set_title('H25 Cross-Architecture Validation: œÅ vs Residual Gain', fontsize=14, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('high_rho_model_hunt.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nSaved: high_rho_model_hunt.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# H25 Verdict\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"H25 CROSS-ARCHITECTURE VERDICT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "correct_predictions = sum(1 for r in tested_results if r['h25_prediction_correct'])\n",
    "accuracy = correct_predictions / len(tested_results) if tested_results else 0\n",
    "\n",
    "print(f\"\\nH25 Prediction Accuracy: {correct_predictions}/{len(tested_results)} = {accuracy*100:.1f}%\")\n",
    "\n",
    "# Find non-Pythia dampening models\n",
    "non_pythia_dampen = [r for r in tested_results \n",
    "                    if r['is_dampening'] and 'pythia' not in r['model'].lower()]\n",
    "\n",
    "if non_pythia_dampen:\n",
    "    print(f\"\\n‚úÖ FOUND {len(non_pythia_dampen)} NON-PYTHIA DAMPENING MODELS:\")\n",
    "    for r in non_pythia_dampen:\n",
    "        print(f\"   - {r['model'].split('/')[-1]}: œÅ = {r['rho']:.4f}, G = {r['residual_gain_mean']:.4f}\")\n",
    "    verdict = \"VALIDATED\"\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è No non-Pythia dampening models found\")\n",
    "    print(f\"   ‚Üí Pythia may have UNIQUE properties beyond just high œÅ\")\n",
    "    verdict = \"PYTHIA_UNIQUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "output_data = {\n",
    "    'experiment': 'High-œÅ Model Hunt',\n",
    "    'hypothesis': 'H25: œÅ ‚â• 0.2 ‚Üí Dampening',\n",
    "    'date': datetime.now().isoformat(),\n",
    "    'models_scanned': len(all_configs),\n",
    "    'models_tested': len(tested_results),\n",
    "    'prediction_accuracy': accuracy,\n",
    "    'non_pythia_dampening': [r['model'] for r in non_pythia_dampen] if non_pythia_dampen else [],\n",
    "    'verdict': verdict,\n",
    "    'all_configs': all_configs,\n",
    "    'tested_results': tested_results\n",
    "}\n",
    "\n",
    "filename = f'high_rho_model_hunt_{timestamp}.json'\n",
    "with open(filename, 'w') as f:\n",
    "    json.dump(output_data, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\nSaved: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-download\n",
    "import zipfile\n",
    "\n",
    "archive_name = f'high_rho_model_hunt_{timestamp}.zip'\n",
    "\n",
    "with zipfile.ZipFile(archive_name, 'w') as zf:\n",
    "    zf.write(filename)\n",
    "    if 'high_rho_model_hunt.png' in globals():\n",
    "        zf.write('high_rho_model_hunt.png')\n",
    "\n",
    "print(f\"Created archive: {archive_name}\")\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(filename)\n",
    "    files.download('high_rho_model_hunt.png')\n",
    "    files.download(archive_name)\n",
    "except ImportError:\n",
    "    print(\"Not in Colab - manual download required.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FINAL SUMMARY: High-œÅ Model Hunt\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nüìä Models Scanned: {len(all_configs)}\")\n",
    "print(f\"üî¨ Models Tested: {len(tested_results)}\")\n",
    "print(f\"üéØ H25 Accuracy: {accuracy*100:.1f}%\")\n",
    "print(f\"\\nüìã Verdict: {verdict}\")\n",
    "\n",
    "if non_pythia_dampen:\n",
    "    print(f\"\\n‚úÖ Found {len(non_pythia_dampen)} non-Pythia dampening models!\")\n",
    "    print(f\"   H25 is ARCHITECTURE-GENERAL, not Pythia-specific.\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Pythia appears to be UNIQUE among tested architectures.\")\n",
    "    print(f\"   Dampening may require BOTH high œÅ AND other Pythia-specific factors.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
