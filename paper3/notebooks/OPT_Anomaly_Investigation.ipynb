{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# OPT Anomaly Investigation\n",
        "\n",
        "## The Anomaly\n",
        "\n",
        "From H4 v2 validation:\n",
        "- **OPT-125m**: Mean Trace (MH) = **2,368** despite G > 1 (EXPAND)\n",
        "- **GPT-2**: Mean Trace (MH) = **62,696** (also EXPAND)\n",
        "\n",
        "**26\u00d7 difference** between two EXPAND models!\n",
        "\n",
        "## Hypotheses\n",
        "\n",
        "1. **Tied Embeddings**: OPT uses tied input/output embeddings\n",
        "2. **W_V Architecture**: Different W_V initialization/structure\n",
        "3. **Attention Pattern**: Different attention distribution\n",
        "4. **Third Thermodynamic Category**: OPT represents a distinct category\n",
        "\n",
        "---\n",
        "*Paper #3: Thermodynamic Constraints in Transformer Architectures*\n",
        "*Author: Davide D'Elia*\n",
        "*Date: 2026-01-06*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install transformers torch numpy scipy matplotlib seaborn pandas -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
        "from tqdm.auto import tqdm\n",
        "import json\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "import gc\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_context(\"paper\", font_scale=1.2)\n",
        "\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA: {torch.cuda.is_available()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. OPT Family Analysis\n",
        "\n",
        "Compare multiple OPT sizes to see if the anomaly persists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "OPT_MODELS = {\n",
        "    'facebook/opt-125m': {'layers': 12, 'heads': 12, 'hidden': 768},\n",
        "    'facebook/opt-350m': {'layers': 24, 'heads': 16, 'hidden': 1024},\n",
        "    'facebook/opt-1.3b': {'layers': 24, 'heads': 32, 'hidden': 2048},\n",
        "}\n",
        "\n",
        "COMPARISON_MODELS = {\n",
        "    'gpt2': {'layers': 12, 'heads': 12, 'hidden': 768, 'lab': 'OpenAI'},\n",
        "    'EleutherAI/pythia-160m': {'layers': 12, 'heads': 12, 'hidden': 768, 'lab': 'EleutherAI'},\n",
        "}\n",
        "\n",
        "TEST_PROMPTS = [\n",
        "    \"The capital of France is Paris.\",\n",
        "    \"Once upon a time in a land far away\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Hypothesis 1: Tied Embeddings Investigation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_tied_embeddings(model_name):\n",
        "    \"\"\"Check if model uses tied embeddings.\"\"\"\n",
        "    config = AutoConfig.from_pretrained(model_name)\n",
        "    \n",
        "    # Different models store this differently\n",
        "    tied = False\n",
        "    if hasattr(config, 'tie_word_embeddings'):\n",
        "        tied = config.tie_word_embeddings\n",
        "    elif hasattr(config, 'tie_encoder_decoder'):\n",
        "        tied = config.tie_encoder_decoder\n",
        "        \n",
        "    return {\n",
        "        'model': model_name,\n",
        "        'tied_embeddings': tied,\n",
        "        'vocab_size': config.vocab_size,\n",
        "        'hidden_size': config.hidden_size if hasattr(config, 'hidden_size') else config.n_embd,\n",
        "    }\n",
        "\n",
        "print(\"Checking tied embeddings...\\n\")\n",
        "for model_name in list(OPT_MODELS.keys()) + list(COMPARISON_MODELS.keys()):\n",
        "    info = check_tied_embeddings(model_name)\n",
        "    print(f\"{model_name.split('/')[-1]:20} | Tied: {info['tied_embeddings']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Hypothesis 2: W_V Architecture Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_W_V_structure(model_name, device='cuda'):\n",
        "    \"\"\"Deep analysis of W_V matrix properties.\"\"\"\n",
        "    print(f\"\\nAnalyzing W_V: {model_name}\")\n",
        "    \n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name, torch_dtype=torch.float16, device_map='auto'\n",
        "    )\n",
        "    model.eval()\n",
        "    \n",
        "    W_V_stats = []\n",
        "    \n",
        "    # Extract W_V based on architecture\n",
        "    if hasattr(model, 'model') and hasattr(model.model, 'decoder'):  # OPT\n",
        "        layers = model.model.decoder.layers\n",
        "        for i, layer in enumerate(layers):\n",
        "            W_V = layer.self_attn.v_proj.weight.data.float().cpu()\n",
        "            W_V_stats.append({\n",
        "                'layer': i,\n",
        "                'frobenius_norm': torch.norm(W_V, 'fro').item(),\n",
        "                'spectral_norm': torch.linalg.norm(W_V, 2).item(),\n",
        "                'mean': W_V.mean().item(),\n",
        "                'std': W_V.std().item(),\n",
        "                'rank': torch.linalg.matrix_rank(W_V).item(),\n",
        "                'condition_number': (torch.linalg.cond(W_V.float())).item(),\n",
        "            })\n",
        "    elif hasattr(model, 'transformer'):  # GPT-2\n",
        "        layers = model.transformer.h\n",
        "        for i, layer in enumerate(layers):\n",
        "            c_attn = layer.attn.c_attn.weight.data.float().cpu()\n",
        "            hidden_size = c_attn.shape[1] // 3\n",
        "            W_V = c_attn[:, 2*hidden_size:].T\n",
        "            W_V_stats.append({\n",
        "                'layer': i,\n",
        "                'frobenius_norm': torch.norm(W_V, 'fro').item(),\n",
        "                'spectral_norm': torch.linalg.norm(W_V, 2).item(),\n",
        "                'mean': W_V.mean().item(),\n",
        "                'std': W_V.std().item(),\n",
        "                'rank': torch.linalg.matrix_rank(W_V).item(),\n",
        "                'condition_number': (torch.linalg.cond(W_V.float())).item(),\n",
        "            })\n",
        "    elif hasattr(model, 'gpt_neox'):  # Pythia\n",
        "        layers = model.gpt_neox.layers\n",
        "        for i, layer in enumerate(layers):\n",
        "            qkv = layer.attention.query_key_value.weight.data.float().cpu()\n",
        "            hidden_size = qkv.shape[0] // 3\n",
        "            W_V = qkv[2*hidden_size:, :]\n",
        "            W_V_stats.append({\n",
        "                'layer': i,\n",
        "                'frobenius_norm': torch.norm(W_V, 'fro').item(),\n",
        "                'spectral_norm': torch.linalg.norm(W_V, 2).item(),\n",
        "                'mean': W_V.mean().item(),\n",
        "                'std': W_V.std().item(),\n",
        "                'rank': torch.linalg.matrix_rank(W_V).item(),\n",
        "                'condition_number': (torch.linalg.cond(W_V.float())).item(),\n",
        "            })\n",
        "    \n",
        "    del model, tokenizer\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    return {\n",
        "        'model': model_name,\n",
        "        'stats': W_V_stats,\n",
        "        'mean_frobenius': np.mean([s['frobenius_norm'] for s in W_V_stats]),\n",
        "        'mean_spectral': np.mean([s['spectral_norm'] for s in W_V_stats]),\n",
        "        'mean_condition': np.mean([s['condition_number'] for s in W_V_stats]),\n",
        "    }\n",
        "\n",
        "print(\"W_V analysis function defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run W_V analysis on key models\n",
        "W_V_results = {}\n",
        "\n",
        "for model_name in ['facebook/opt-125m', 'gpt2', 'EleutherAI/pythia-160m']:\n",
        "    W_V_results[model_name] = analyze_W_V_structure(model_name)\n",
        "\n",
        "# Summary comparison\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"W_V STRUCTURE COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for name, result in W_V_results.items():\n",
        "    print(f\"\\n{name.split('/')[-1]}:\")\n",
        "    print(f\"  ||W_V||_F (mean): {result['mean_frobenius']:.2f}\")\n",
        "    print(f\"  ||W_V||_2 (mean): {result['mean_spectral']:.2f}\")\n",
        "    print(f\"  Condition # (mean): {result['mean_condition']:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Hypothesis 3: Attention Pattern Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_attention_patterns(model_name, prompts, device='cuda'):\n",
        "    \"\"\"Analyze attention entropy and distribution patterns.\"\"\"\n",
        "    print(f\"\\nAnalyzing attention: {model_name}\")\n",
        "    \n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        \n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name, torch_dtype=torch.float16, device_map='auto',\n",
        "        output_attentions=True\n",
        "    )\n",
        "    model.eval()\n",
        "    \n",
        "    all_entropies = []\n",
        "    all_sparsities = []\n",
        "    \n",
        "    for prompt in prompts:\n",
        "        inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs, output_attentions=True)\n",
        "        \n",
        "        for layer_idx, attn in enumerate(outputs.attentions):\n",
        "            # Average over heads and batch\n",
        "            attn_avg = attn[0].mean(dim=0).float().cpu()  # (seq, seq)\n",
        "            \n",
        "            # Entropy of attention distribution\n",
        "            entropy = -torch.sum(attn_avg * torch.log(attn_avg + 1e-10), dim=-1).mean().item()\n",
        "            \n",
        "            # Sparsity (fraction of weights < 0.01)\n",
        "            sparsity = (attn_avg < 0.01).float().mean().item()\n",
        "            \n",
        "            all_entropies.append({'layer': layer_idx, 'entropy': entropy})\n",
        "            all_sparsities.append({'layer': layer_idx, 'sparsity': sparsity})\n",
        "    \n",
        "    del model, tokenizer\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    # Aggregate by layer\n",
        "    df_entropy = pd.DataFrame(all_entropies).groupby('layer').mean()\n",
        "    df_sparsity = pd.DataFrame(all_sparsities).groupby('layer').mean()\n",
        "    \n",
        "    return {\n",
        "        'model': model_name,\n",
        "        'entropy_by_layer': df_entropy['entropy'].tolist(),\n",
        "        'sparsity_by_layer': df_sparsity['sparsity'].tolist(),\n",
        "        'mean_entropy': df_entropy['entropy'].mean(),\n",
        "        'mean_sparsity': df_sparsity['sparsity'].mean(),\n",
        "    }\n",
        "\n",
        "print(\"Attention pattern analysis function defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run attention analysis\n",
        "attn_results = {}\n",
        "\n",
        "for model_name in ['facebook/opt-125m', 'gpt2', 'EleutherAI/pythia-160m']:\n",
        "    attn_results[model_name] = analyze_attention_patterns(model_name, TEST_PROMPTS)\n",
        "\n",
        "# Summary\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ATTENTION PATTERN COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for name, result in attn_results.items():\n",
        "    print(f\"\\n{name.split('/')[-1]}:\")\n",
        "    print(f\"  Mean Entropy: {result['mean_entropy']:.3f}\")\n",
        "    print(f\"  Mean Sparsity: {result['mean_sparsity']:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
        "\n",
        "colors = {'opt-125m': '#3498DB', 'gpt2': '#8E44AD', 'pythia-160m': '#E74C3C'}\n",
        "\n",
        "# Plot 1: W_V Frobenius norm by layer\n",
        "ax1 = axes[0, 0]\n",
        "for name, result in W_V_results.items():\n",
        "    short_name = name.split('/')[-1]\n",
        "    layers = [s['layer'] for s in result['stats']]\n",
        "    norms = [s['frobenius_norm'] for s in result['stats']]\n",
        "    ax1.plot(layers, norms, label=short_name, color=colors.get(short_name, 'gray'), linewidth=2)\n",
        "ax1.set_xlabel('Layer')\n",
        "ax1.set_ylabel('||W_V||_F')\n",
        "ax1.set_title('W_V Frobenius Norm by Layer')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Attention entropy by layer\n",
        "ax2 = axes[0, 1]\n",
        "for name, result in attn_results.items():\n",
        "    short_name = name.split('/')[-1]\n",
        "    layers = range(len(result['entropy_by_layer']))\n",
        "    ax2.plot(layers, result['entropy_by_layer'], label=short_name, \n",
        "             color=colors.get(short_name, 'gray'), linewidth=2)\n",
        "ax2.set_xlabel('Layer')\n",
        "ax2.set_ylabel('Entropy')\n",
        "ax2.set_title('Attention Entropy by Layer')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: W_V spectral norm comparison\n",
        "ax3 = axes[1, 0]\n",
        "models = list(W_V_results.keys())\n",
        "spectral_norms = [W_V_results[m]['mean_spectral'] for m in models]\n",
        "model_names = [m.split('/')[-1] for m in models]\n",
        "bars = ax3.bar(model_names, spectral_norms, color=[colors.get(n, 'gray') for n in model_names])\n",
        "ax3.set_ylabel('Mean ||W_V||_2')\n",
        "ax3.set_title('W_V Spectral Norm (KEY DIFFERENCE!)')\n",
        "ax3.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Plot 4: Summary comparison\n",
        "ax4 = axes[1, 1]\n",
        "# Create comparison table\n",
        "comparison_data = []\n",
        "for name in ['facebook/opt-125m', 'gpt2', 'EleutherAI/pythia-160m']:\n",
        "    short = name.split('/')[-1]\n",
        "    comparison_data.append({\n",
        "        'Model': short,\n",
        "        '||W_V||_F': W_V_results[name]['mean_frobenius'],\n",
        "        'Entropy': attn_results[name]['mean_entropy'],\n",
        "    })\n",
        "\n",
        "df_comparison = pd.DataFrame(comparison_data)\n",
        "ax4.axis('off')\n",
        "table = ax4.table(cellText=df_comparison.round(2).values,\n",
        "                   colLabels=df_comparison.columns,\n",
        "                   cellLoc='center', loc='center')\n",
        "table.auto_set_font_size(False)\n",
        "table.set_fontsize(12)\n",
        "table.scale(1.2, 1.5)\n",
        "ax4.set_title('Summary Comparison', pad=20)\n",
        "\n",
        "plt.suptitle('OPT Anomaly Investigation', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig('OPT_anomaly_investigation.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Conclusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"OPT ANOMALY INVESTIGATION - CONCLUSIONS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Analyze findings\n",
        "opt_spectral = W_V_results['facebook/opt-125m']['mean_spectral']\n",
        "gpt2_spectral = W_V_results['gpt2']['mean_spectral']\n",
        "pythia_spectral = W_V_results['EleutherAI/pythia-160m']['mean_spectral']\n",
        "\n",
        "print(f\"\\n1. W_V SPECTRAL NORM:\")\n",
        "print(f\"   OPT-125m:    {opt_spectral:.2f}\")\n",
        "print(f\"   GPT-2:       {gpt2_spectral:.2f}\")\n",
        "print(f\"   Pythia-160m: {pythia_spectral:.2f}\")\n",
        "print(f\"   Ratio GPT-2/OPT: {gpt2_spectral/opt_spectral:.1f}x\")\n",
        "\n",
        "print(f\"\\n2. ATTENTION ENTROPY:\")\n",
        "print(f\"   OPT-125m:    {attn_results['facebook/opt-125m']['mean_entropy']:.3f}\")\n",
        "print(f\"   GPT-2:       {attn_results['gpt2']['mean_entropy']:.3f}\")\n",
        "print(f\"   Pythia-160m: {attn_results['EleutherAI/pythia-160m']['mean_entropy']:.3f}\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"\\nHYPOTHESIS VERDICT:\")\n",
        "\n",
        "if gpt2_spectral > 3 * opt_spectral:\n",
        "    print(\"\\n\u2705 CONFIRMED: W_V spectral norm explains the trace difference!\")\n",
        "    print(\"   OPT has much smaller ||W_V||, leading to smaller trace.\")\n",
        "    print(\"   Trace \u221d ||W_V||_F\u00b2, so small W_V \u2192 small trace.\")\n",
        "else:\n",
        "    print(\"\\n\u26a0\ufe0f PARTIAL: W_V difference exists but may not fully explain 26x.\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"\\nIMPLICATION:\")\n",
        "print(\"OPT may represent a THIRD thermodynamic category:\")\n",
        "print(\"  - DAMPEN (EleutherAI): High W_V, low trace growth\")\n",
        "print(\"  - EXPAND (OpenAI): High W_V, high trace growth\")\n",
        "print(\"  - COMPACT (Meta?): Low W_V, low trace, but still G > 1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save results\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "output = {\n",
        "    'experiment': 'OPT Anomaly Investigation',\n",
        "    'date': datetime.now().isoformat(),\n",
        "    'W_V_analysis': {k: {kk: vv for kk, vv in v.items() if kk != 'stats'} \n",
        "                     for k, v in W_V_results.items()},\n",
        "    'attention_analysis': attn_results,\n",
        "}\n",
        "\n",
        "filename = f'OPT_anomaly_{timestamp}.json'\n",
        "with open(filename, 'w') as f:\n",
        "    json.dump(output, f, indent=2)\n",
        "print(f\"\\nResults saved: {filename}\")\n",
        "\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download(filename)\n",
        "    files.download('OPT_anomaly_investigation.png')\n",
        "except:\n",
        "    print(\"Files saved locally.\")"
      ]
    }
  ]
}
