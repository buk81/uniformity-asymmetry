{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RLHF Safety Brake Test - Base vs Instruct\n",
    "\n",
    "**Purpose:** Test Gemini's hypothesis that RLHF causes last-layer contraction\n",
    "\n",
    "**Hypothesis:**\n",
    "- LLaMA-3.1-8B **Base** (NO RLHF) → EXPANDS like Mistral (~1.37x)\n",
    "- LLaMA-3.1-8B **Instruct** (WITH RLHF) → CONTRACTS (~0.48x)\n",
    "\n",
    "**If confirmed:** RLHF is thermodynamically a \"Safety Brake\" (energy dampening)\n",
    "\n",
    "**Evidence from Paper #1 Archive:**\n",
    "```\n",
    "META (Llama):\n",
    "├── Normalisiert ALLES\n",
    "├── Alle Statements ~0.58-0.68 (enger Range)\n",
    "└── \"Normalisierung\" aller Positionen\n",
    "```\n",
    "\n",
    "**Connection across 3 Papers:**\n",
    "- Paper #1: LLaMA shows highest uniformity (flattening)\n",
    "- Paper #2: Chat templates cause uniformly negative correlations\n",
    "- Paper #3: RLHF causes last-layer contraction (this test!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup and Imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "import gc\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Colab setup\n",
    "IN_COLAB = False\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "    print(\"Running in Google Colab\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# HuggingFace Login\n",
    "try:\n",
    "    from huggingface_hub import login\n",
    "    from google.colab import userdata\n",
    "    hf_token = userdata.get('HF_TOKEN')\n",
    "    if hf_token:\n",
    "        login(token=hf_token)\n",
    "        print(\"✅ HuggingFace login successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ HF login: {e}\")\n",
    "\n",
    "# GPU Check\n",
    "print(f\"\\nPyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: RLHF Test Models Definition\n",
    "\n",
    "RLHF_TEST_MODELS = {\n",
    "    'llama3.1-8b-BASE': {\n",
    "        'hf_name': 'meta-llama/Llama-3.1-8B',\n",
    "        'layers': 32,\n",
    "        'family': 'llama',\n",
    "        'norm': 'RMSNorm',\n",
    "        'mlp': 'SwiGLU',\n",
    "        'rlhf': False,\n",
    "        'expected': 'EXPANSION (~1.37x like Mistral)'\n",
    "    },\n",
    "    'llama3.1-8b-INSTRUCT': {\n",
    "        'hf_name': 'meta-llama/Llama-3.1-8B-Instruct',\n",
    "        'layers': 32,\n",
    "        'family': 'llama',\n",
    "        'norm': 'RMSNorm',\n",
    "        'mlp': 'SwiGLU',\n",
    "        'rlhf': True,\n",
    "        'expected': 'CONTRACTION (~0.48x)'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Reference values\n",
    "MISTRAL_REFERENCE = {'last_layer_gain': 1.37, 'initial_explosion': 43.86}\n",
    "PREVIOUS_LLAMA_RESULT = {'last_layer_gain': 0.48, 'note': 'From 4-model validation'}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"RLHF SAFETY BRAKE TEST\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nModels to test:\")\n",
    "for name, info in RLHF_TEST_MODELS.items():\n",
    "    rlhf = \"WITH RLHF\" if info['rlhf'] else \"NO RLHF\"\n",
    "    print(f\"  {name}:\")\n",
    "    print(f\"    → {info['hf_name']}\")\n",
    "    print(f\"    → {rlhf}\")\n",
    "    print(f\"    → Expected: {info['expected']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Model Loading\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def load_model(model_key, model_info):\n",
    "    \"\"\"Load model with error handling.\"\"\"\n",
    "    hf_name = model_info['hf_name']\n",
    "    print(f\"\\nLoading {model_key} ({hf_name})...\")\n",
    "    \n",
    "    dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "    \n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(hf_name, trust_remote_code=True)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            hf_name,\n",
    "            torch_dtype=dtype,\n",
    "            device_map='auto',\n",
    "            trust_remote_code=True,\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "        model.eval()\n",
    "        \n",
    "        n_layers = len(model.model.layers)\n",
    "        print(f\"  ✅ Loaded! {n_layers} layers, dtype={dtype}\")\n",
    "        return model, tokenizer, dtype\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ FAILED: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "def cleanup_model(model):\n",
    "    if model is not None:\n",
    "        del model\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    print(\"  Memory cleaned.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Residual Stream Analyzer\n",
    "\n",
    "class ResidualStreamAnalyzer:\n",
    "    def __init__(self, model, model_info):\n",
    "        self.model = model\n",
    "        self.model_info = model_info\n",
    "        self.hooks = []\n",
    "        self.residual_norms = []\n",
    "        self.embedding_norm = None\n",
    "        \n",
    "    def _make_embedding_hook(self):\n",
    "        def hook(module, args, output):\n",
    "            with torch.no_grad():\n",
    "                self.embedding_norm = output.float().norm().item()\n",
    "        return hook\n",
    "    \n",
    "    def _make_layer_hook(self, layer_idx):\n",
    "        def hook(module, args, output):\n",
    "            hidden = output[0] if isinstance(output, tuple) else output\n",
    "            with torch.no_grad():\n",
    "                norm = hidden.float().norm().item()\n",
    "                self.residual_norms.append((layer_idx, norm))\n",
    "        return hook\n",
    "    \n",
    "    def register_hooks(self):\n",
    "        # Embedding\n",
    "        h = self.model.model.embed_tokens.register_forward_hook(self._make_embedding_hook())\n",
    "        self.hooks.append(h)\n",
    "        # Layers\n",
    "        for i, layer in enumerate(self.model.model.layers):\n",
    "            h = layer.register_forward_hook(self._make_layer_hook(i))\n",
    "            self.hooks.append(h)\n",
    "        print(f\"  Registered {len(self.hooks)} hooks\")\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        for h in self.hooks:\n",
    "            h.remove()\n",
    "        self.hooks = []\n",
    "    \n",
    "    def get_layer_gains(self):\n",
    "        sorted_norms = sorted(self.residual_norms, key=lambda x: x[0])\n",
    "        all_norms = [(\"emb\", self.embedding_norm)] + sorted_norms\n",
    "        \n",
    "        gains = []\n",
    "        for i in range(1, len(all_norms)):\n",
    "            prev = all_norms[i-1][1]\n",
    "            curr = all_norms[i][1]\n",
    "            gains.append(curr / prev if prev > 1e-8 else 0.0)\n",
    "        \n",
    "        norms = [n for _, n in all_norms]\n",
    "        return gains, norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: RUN THE RLHF TEST\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"RUNNING RLHF SAFETY BRAKE TEST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "rlhf_results = {}\n",
    "\n",
    "for model_key, model_info in RLHF_TEST_MODELS.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing: {model_key}\")\n",
    "    print(f\"RLHF: {'YES' if model_info['rlhf'] else 'NO'}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    model, tokenizer, dtype = load_model(model_key, model_info)\n",
    "    if model is None:\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        analyzer = ResidualStreamAnalyzer(model, model_info)\n",
    "        analyzer.register_hooks()\n",
    "        \n",
    "        # Test prompt\n",
    "        prompt = \"The capital of France is\"\n",
    "        inputs = tokenizer(prompt, return_tensors='pt')\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "        \n",
    "        print(f\"  Running forward pass...\")\n",
    "        with torch.no_grad():\n",
    "            with torch.amp.autocast(device_type='cuda', dtype=dtype):\n",
    "                outputs = model(**inputs)\n",
    "        \n",
    "        gains, norms = analyzer.get_layer_gains()\n",
    "        \n",
    "        # Results\n",
    "        last_gain = gains[-1]\n",
    "        initial_gain = gains[0]\n",
    "        n_layers = len(gains)\n",
    "        \n",
    "        print(f\"\\n  RESULTS:\")\n",
    "        print(f\"  Initial Gain (Emb→L0): {initial_gain:.2f}x\")\n",
    "        print(f\"  Last Layer Gain: {last_gain:.4f}x\")\n",
    "        print(f\"  {'>>> EXPANDS <<<' if last_gain > 1.0 else '>>> CONTRACTS <<<'}\")\n",
    "        \n",
    "        # Compute cumulative energy\n",
    "        cumulative = np.exp(np.sum(np.log(np.array(gains) + 1e-10)))\n",
    "        print(f\"  Cumulative Energy: {cumulative:.2e}\")\n",
    "        \n",
    "        rlhf_results[model_key] = {\n",
    "            'rlhf': model_info['rlhf'],\n",
    "            'hf_name': model_info['hf_name'],\n",
    "            'initial_gain': float(initial_gain),\n",
    "            'last_gain': float(last_gain),\n",
    "            'last_expands': bool(last_gain > 1.0),\n",
    "            'cumulative_energy': float(cumulative),\n",
    "            'gains': [float(g) for g in gains],\n",
    "            'norms': [float(n) for n in norms]\n",
    "        }\n",
    "        \n",
    "        analyzer.remove_hooks()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    finally:\n",
    "        cleanup_model(model)\n",
    "\n",
    "print(f\"\\n\\n{'='*60}\")\n",
    "print(f\"RLHF TEST COMPLETE - {len(rlhf_results)} models tested\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: RLHF Hypothesis Evaluation\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"RLHF SAFETY BRAKE HYPOTHESIS EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if len(rlhf_results) == 2:\n",
    "    base = rlhf_results.get('llama3.1-8b-BASE')\n",
    "    instruct = rlhf_results.get('llama3.1-8b-INSTRUCT')\n",
    "    \n",
    "    if base and instruct:\n",
    "        print(\"\\n| Model | RLHF | Last Gain | Expands? |\")\n",
    "        print(\"|-------|------|-----------|----------|\")\n",
    "        print(f\"| BASE | NO | {base['last_gain']:.4f}x | {'YES' if base['last_expands'] else 'NO'} |\")\n",
    "        print(f\"| INSTRUCT | YES | {instruct['last_gain']:.4f}x | {'YES' if instruct['last_expands'] else 'NO'} |\")\n",
    "        print(f\"| Mistral (ref) | NO | 1.37x | YES |\")\n",
    "        \n",
    "        # Hypothesis test\n",
    "        ratio = instruct['last_gain'] / base['last_gain'] if base['last_gain'] != 0 else 0\n",
    "        \n",
    "        print(f\"\\n\" + \"=\"*60)\n",
    "        print(\"HYPOTHESIS TEST RESULTS\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"\\nBase Last Gain:     {base['last_gain']:.4f}x\")\n",
    "        print(f\"Instruct Last Gain: {instruct['last_gain']:.4f}x\")\n",
    "        print(f\"Ratio (Inst/Base):  {ratio:.2f}\")\n",
    "        \n",
    "        # Verdict\n",
    "        if base['last_expands'] and not instruct['last_expands']:\n",
    "            print(f\"\\n\" + \"*\"*60)\n",
    "            print(\"*** HYPOTHESIS CONFIRMED! ***\")\n",
    "            print(\"*\"*60)\n",
    "            print(\"\\nRLHF is proven to be a 'SAFETY BRAKE':\")\n",
    "            print(\"  - Base model EXPANDS (natural behavior)\")\n",
    "            print(\"  - Instruct model CONTRACTS (RLHF dampening)\")\n",
    "            print(\"\\nThis connects to:\")\n",
    "            print(\"  - Paper #1: LLaMA shows highest uniformity\")\n",
    "            print(\"  - Paper #2: Chat templates cause negative correlations\")\n",
    "            print(\"  - Paper #3: RLHF causes energy dissipation at output\")\n",
    "            verdict = \"CONFIRMED\"\n",
    "            \n",
    "        elif not base['last_expands'] and not instruct['last_expands']:\n",
    "            print(f\"\\n\" + \"!\"*60)\n",
    "            print(\"*** HYPOTHESIS REJECTED ***\")\n",
    "            print(\"!\"*60)\n",
    "            print(\"\\nBOTH models contract!\")\n",
    "            print(\"  → Contraction is architectural, NOT RLHF-induced\")\n",
    "            print(\"  → LLaMA 3.1 differs from Mistral at architecture level\")\n",
    "            verdict = \"REJECTED - Architectural\"\n",
    "            \n",
    "        elif base['last_expands'] and instruct['last_expands']:\n",
    "            print(f\"\\n\" + \"?\"*60)\n",
    "            print(\"*** UNEXPECTED: BOTH EXPAND ***\")\n",
    "            print(\"?\"*60)\n",
    "            print(\"\\nRLHF does NOT cause contraction in this case\")\n",
    "            verdict = \"UNEXPECTED - Both expand\"\n",
    "            \n",
    "        else:\n",
    "            print(f\"\\n\" + \"?\"*60)\n",
    "            print(\"*** UNEXPECTED: Base contracts, Instruct expands ***\")\n",
    "            print(\"?\"*60)\n",
    "            verdict = \"UNEXPECTED - Reversed\"\n",
    "        \n",
    "        rlhf_results['_hypothesis_test'] = {\n",
    "            'base_expands': base['last_expands'],\n",
    "            'instruct_expands': instruct['last_expands'],\n",
    "            'ratio': ratio,\n",
    "            'verdict': verdict\n",
    "        }\n",
    "else:\n",
    "    print(\"\\n⚠️ Need both models to evaluate hypothesis!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 7: Visualization\nimport os\n\n# Create Results directory (works in Colab!)\nRESULTS_DIR = './Results'\nos.makedirs(RESULTS_DIR, exist_ok=True)\n\nif len(rlhf_results) >= 2:\n    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n    \n    colors = {'llama3.1-8b-BASE': 'blue', 'llama3.1-8b-INSTRUCT': 'red'}\n    \n    # Panel 1: Layer-wise Gains\n    ax1 = axes[0]\n    for name, res in rlhf_results.items():\n        if name.startswith('_'):\n            continue\n        gains = res['gains']\n        ax1.plot(range(len(gains)), gains, 'o-', label=name, \n                 color=colors.get(name, 'gray'), alpha=0.7)\n    ax1.axhline(y=1.0, color='black', linestyle='--', alpha=0.5)\n    ax1.set_xlabel('Layer')\n    ax1.set_ylabel('Gain')\n    ax1.set_title('Layer-wise Gains\\n(Base vs Instruct)')\n    ax1.legend()\n    ax1.set_ylim(0.4, 1.6)\n    \n    # Panel 2: Last Layer Comparison\n    ax2 = axes[1]\n    names = [n for n in rlhf_results.keys() if not n.startswith('_')]\n    last_gains = [rlhf_results[n]['last_gain'] for n in names]\n    bar_colors = [colors.get(n, 'gray') for n in names]\n    \n    # Add Mistral reference\n    names.append('Mistral (ref)')\n    last_gains.append(1.37)\n    bar_colors.append('purple')\n    \n    bars = ax2.bar(range(len(names)), last_gains, color=bar_colors, alpha=0.7)\n    ax2.axhline(y=1.0, color='black', linestyle='--', alpha=0.5)\n    ax2.set_xticks(range(len(names)))\n    ax2.set_xticklabels(names, rotation=15, ha='right')\n    ax2.set_ylabel('Last Layer Gain')\n    ax2.set_title('Last Layer Gain Comparison\\n(RLHF Effect)')\n    \n    for bar, val in zip(bars, last_gains):\n        ax2.annotate(f'{val:.2f}x', xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n                     ha='center', va='bottom', fontsize=11, fontweight='bold')\n    \n    # Panel 3: Cumulative Energy\n    ax3 = axes[2]\n    for name, res in rlhf_results.items():\n        if name.startswith('_'):\n            continue\n        gains = np.array(res['gains'])\n        cumulative = np.exp(np.cumsum(np.log(gains + 1e-10)))\n        ax3.semilogy(range(len(cumulative)), cumulative, 'o-', \n                     label=name, color=colors.get(name, 'gray'), alpha=0.7)\n    ax3.set_xlabel('Layer')\n    ax3.set_ylabel('Cumulative Energy')\n    ax3.set_title('Cumulative Energy\\n(Product of Gains)')\n    ax3.legend()\n    ax3.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    \n    # Save\n    output_path = f'{RESULTS_DIR}/RLHF_safety_brake_test.png'\n    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n    plt.show()\n    print(f\"Saved: {output_path}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 8: Save Results\nimport os\n\n# Use same directory as visualization\nRESULTS_DIR = './Results'\nos.makedirs(RESULTS_DIR, exist_ok=True)\n\noutput = {\n    'experiment': 'RLHF Safety Brake Test',\n    'date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n    'hypothesis': 'RLHF causes last-layer contraction (Safety Brake)',\n    'models': rlhf_results,\n    'references': {\n        'mistral_7b': MISTRAL_REFERENCE,\n        'previous_llama': PREVIOUS_LLAMA_RESULT\n    }\n}\n\noutput_path = f'{RESULTS_DIR}/RLHF_safety_brake_test_results.json'\nwith open(output_path, 'w') as f:\n    json.dump(output, f, indent=2)\nprint(f\"Saved: {output_path}\")\n\n# Auto-download for Colab\ntry:\n    from google.colab import files\n    files.download(output_path)\n    files.download(f'{RESULTS_DIR}/RLHF_safety_brake_test.png')\n    print(\"\\n✅ Files downloaded!\")\nexcept:\n    pass\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"RLHF SAFETY BRAKE TEST COMPLETE\")\nprint(\"=\"*60)\nprint(f\"\\nOutput files:\")\nprint(f\"  - {output_path}\")\nprint(f\"  - {RESULTS_DIR}/RLHF_safety_brake_test.png\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}