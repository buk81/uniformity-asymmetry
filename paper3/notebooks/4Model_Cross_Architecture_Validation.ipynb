{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Complete Residual Stream Validation v2\n\n**Purpose:** Validate \"Compress-then-Broadcast\" pattern with CORRECT residual stream measurement\n\n**Models:**\n- Pythia-6.9B (LayerNorm, GELU) - baseline\n- Gemma-7B (RMSNorm, GeGLU) - verify expansion\n- Apertus-8B (RMSNorm, xIELU) - Paper #2 connection\n- **LLaMA-3.1-8B (RMSNorm, SwiGLU)** - NEW: Compare to Mistral (same MLP type!)\n\n**Key Questions:**\n1. Does LLaMA-3.1 (SwiGLU) behave like Mistral (SwiGLU)?\n2. Is SwiGLU expansion different from GeGLU/xIELU?\n3. Cumulative Energy: Does \"net inertia\" correlate with norm type?\n\n**Reference:** Mistral-7B = 1.37x expansion (RMSNorm + SwiGLU)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 1: Imports and Setup\nimport torch\nimport numpy as np\nimport json\nimport gc\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# For Colab: upgrade transformers for Apertus support (needs v4.56+)\nIN_COLAB = False\ntry:\n    from google.colab import drive\n    IN_COLAB = True\n    print(\"Running in Google Colab\")\n    print(\"Upgrading transformers for Apertus support...\")\n    import subprocess\n    subprocess.run(['pip', 'install', '-q', 'transformers>=4.56.0'], check=True)\n    print(\"Transformers upgraded!\")\nexcept:\n    pass\n\n# HuggingFace Login via Colab Secrets (same as before)\nHF_LOGGED_IN = False\ntry:\n    from huggingface_hub import login\n    from google.colab import userdata\n    hf_token = userdata.get('HF_TOKEN')\n    if hf_token:\n        login(token=hf_token)\n        HF_LOGGED_IN = True\n        print(\"âœ… HuggingFace login successful via Colab secrets!\")\n    else:\n        print(\"âš ï¸ HF_TOKEN not found in Colab secrets\")\nexcept Exception as e:\n    print(f\"âš ï¸ HF login: {e}\")\n\n# Check GPU\nprint(f\"\\nPyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    gpu_name = torch.cuda.get_device_name(0)\n    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n    print(f\"GPU: {gpu_name}\")\n    print(f\"GPU Memory: {gpu_mem:.1f} GB\")\nelse:\n    gpu_name = \"CPU\"\n    gpu_mem = 0\n    print(\"WARNING: Running on CPU - models will be very slow!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 2: Model Definitions\n\nMODELS_TO_TEST = {\n    # Pythia - LayerNorm baseline\n    'pythia-6.9b': {\n        'hf_name': 'EleutherAI/pythia-6.9b',\n        'params': 6.9e9,\n        'layers': 32,\n        'family': 'pythia',\n        'norm': 'LayerNorm',\n        'mlp': 'GELU',\n        'memory_gb': 18,\n        'paper2_inversion': None,\n        'expected_last_gain': 6.3\n    },\n    # Gemma - RMSNorm + GeGLU\n    'gemma-7b': {\n        'hf_name': 'google/gemma-7b',\n        'params': 7e9,\n        'layers': 28,\n        'family': 'gemma',\n        'norm': 'RMSNorm',\n        'mlp': 'GeGLU',\n        'memory_gb': 20,\n        'paper2_inversion': None,\n        'expected_last_gain': 2.52\n    },\n    # Apertus - RMSNorm + xIELU (Paper #2 model!)\n    'apertus-8b': {\n        'hf_name': 'swiss-ai/Apertus-8B-2509',\n        'params': 8e9,\n        'layers': 32,\n        'family': 'apertus',\n        'norm': 'RMSNorm',\n        'mlp': 'xIELU',\n        'memory_gb': 22,\n        'paper2_inversion': 28,  # L28 is where correlation inverts in Paper #2!\n        'expected_last_gain': None\n    },\n    # LLaMA 3.1 - RMSNorm + SwiGLU (SAME as Mistral! Key comparison!)\n    'llama3.1-8b': {\n        'hf_name': 'meta-llama/Llama-3.1-8B',\n        'params': 8e9,\n        'layers': 32,\n        'family': 'llama',\n        'norm': 'RMSNorm',\n        'mlp': 'SwiGLU',\n        'memory_gb': 20,\n        'paper2_inversion': None,\n        'expected_last_gain': 1.37  # Expecting similar to Mistral (same MLP!)\n    }\n}\n\n# Fallback for lower memory GPUs\nFALLBACK_MODELS = {\n    'pythia-2.8b': {\n        'hf_name': 'EleutherAI/pythia-2.8b',\n        'params': 2.8e9,\n        'layers': 32,\n        'family': 'pythia',\n        'norm': 'LayerNorm',\n        'mlp': 'GELU',\n        'memory_gb': 8,\n        'paper2_inversion': None,\n        'expected_last_gain': 2.1\n    },\n    'gemma-2b': {\n        'hf_name': 'google/gemma-2b',\n        'params': 2e9,\n        'layers': 18,\n        'family': 'gemma',\n        'norm': 'RMSNorm',\n        'mlp': 'GeGLU',\n        'memory_gb': 6,\n        'paper2_inversion': None,\n        'expected_last_gain': 13.72\n    }\n}\n\n# Reference: Mistral results from previous investigation\nMISTRAL_REFERENCE = {\n    'last_layer_gain': 1.37,\n    'initial_explosion': 43.86,\n    'plateau_range': (1, 29),\n    'pattern': 'Compress-Hold-Explode',\n    'family': 'mistral',\n    'norm': 'RMSNorm',\n    'mlp': 'SwiGLU'  # Same as LLaMA 3.1!\n}\n\n# Select models based on GPU memory\ndef select_models(gpu_mem_gb):\n    \"\"\"Select appropriate models based on available GPU memory.\"\"\"\n    selected = {}\n    \n    if gpu_mem_gb >= 40:\n        # Full test - all 4 models (A100 or equivalent)\n        selected = MODELS_TO_TEST.copy()\n    elif gpu_mem_gb >= 22:\n        # Skip LLaMA 3.1 (can run separately)\n        selected['pythia-6.9b'] = MODELS_TO_TEST['pythia-6.9b']\n        selected['gemma-7b'] = MODELS_TO_TEST['gemma-7b']\n        selected['apertus-8b'] = MODELS_TO_TEST['apertus-8b']\n    elif gpu_mem_gb >= 20:\n        # Skip Apertus\n        selected['pythia-6.9b'] = MODELS_TO_TEST['pythia-6.9b']\n        selected['gemma-7b'] = MODELS_TO_TEST['gemma-7b']\n        selected['llama3.1-8b'] = MODELS_TO_TEST['llama3.1-8b']\n    elif gpu_mem_gb >= 18:\n        # Only Pythia and LLaMA\n        selected['pythia-6.9b'] = MODELS_TO_TEST['pythia-6.9b']\n        selected['llama3.1-8b'] = MODELS_TO_TEST['llama3.1-8b']\n    elif gpu_mem_gb >= 8:\n        # Use fallbacks\n        selected['pythia-2.8b'] = FALLBACK_MODELS['pythia-2.8b']\n        selected['gemma-2b'] = FALLBACK_MODELS['gemma-2b']\n    else:\n        # Minimal\n        selected['pythia-2.8b'] = FALLBACK_MODELS['pythia-2.8b']\n    \n    return selected\n\n# Select models\nmodels_to_run = select_models(gpu_mem)\n\nprint(f\"\\nSelected models for {gpu_mem:.1f} GB GPU:\")\nfor name, info in models_to_run.items():\n    inv = f\" [Paper #2 inversion @ L{info['paper2_inversion']}]\" if info.get('paper2_inversion') else \"\"\n    print(f\"  {name}: {info['layers']} layers, {info['norm']}, {info['mlp']}{inv}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 3: Model Loading with Error Handling\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ndef load_model(model_key, model_info):\n    \"\"\"Load model with robust error handling.\"\"\"\n    hf_name = model_info['hf_name']\n    family = model_info['family']\n    \n    print(f\"\\nLoading {model_key} ({hf_name})...\")\n    \n    # Use bfloat16 for better numerical stability (especially for Apertus xIELU)\n    if torch.cuda.is_available():\n        # Check if bfloat16 is supported\n        if torch.cuda.is_bf16_supported():\n            dtype = torch.bfloat16\n            print(f\"  Using bfloat16 (best for xIELU)\")\n        else:\n            dtype = torch.float16\n            print(f\"  Using float16\")\n    else:\n        dtype = torch.float32\n    \n    try:\n        # Load tokenizer\n        tokenizer = AutoTokenizer.from_pretrained(\n            hf_name, \n            trust_remote_code=True\n        )\n        \n        # Ensure pad token exists\n        if tokenizer.pad_token is None:\n            tokenizer.pad_token = tokenizer.eos_token\n        \n        # Load model with explicit dtype\n        model = AutoModelForCausalLM.from_pretrained(\n            hf_name,\n            torch_dtype=dtype,\n            device_map='auto' if torch.cuda.is_available() else None,\n            trust_remote_code=True,\n            low_cpu_mem_usage=True\n        )\n        \n        model.eval()\n        \n        # Verify layer access based on family\n        if family == 'pythia':\n            n_layers = len(model.gpt_neox.layers)\n        elif family in ['mistral', 'llama', 'gemma', 'apertus']:\n            n_layers = len(model.model.layers)\n        else:\n            if hasattr(model, 'model') and hasattr(model.model, 'layers'):\n                n_layers = len(model.model.layers)\n            elif hasattr(model, 'transformer') and hasattr(model.transformer, 'h'):\n                n_layers = len(model.transformer.h)\n            else:\n                n_layers = model_info['layers']\n        \n        print(f\"  Loaded successfully! ({n_layers} layers detected)\")\n        \n        if family == 'apertus':\n            print(f\"  Architecture: {model.config.architectures}\")\n            print(f\"  Hidden size: {model.config.hidden_size}\")\n            print(f\"  Model dtype: {next(model.parameters()).dtype}\")\n        \n        return model, tokenizer, dtype\n        \n    except Exception as e:\n        print(f\"  FAILED to load: {e}\")\n        import traceback\n        traceback.print_exc()\n        return None, None, None\n\ndef cleanup_model(model):\n    \"\"\"Properly cleanup model to free GPU memory.\"\"\"\n    if model is not None:\n        del model\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.synchronize()\n    print(\"  Memory cleaned up.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 4: Residual Stream Analyzer (robust version)\n\nclass ResidualStreamAnalyzer:\n    \"\"\"Analyze the residual stream directly, bypassing RMSNorm measurement issues.\"\"\"\n    \n    def __init__(self, model, model_info):\n        self.model = model\n        self.model_info = model_info\n        self.hooks = []\n        self.residual_norms = []\n        self.embedding_norm = None\n        \n    def _get_layers(self):\n        \"\"\"Get transformer layers for different architectures.\"\"\"\n        family = self.model_info['family']\n        \n        if family == 'pythia':\n            return self.model.gpt_neox.layers\n        elif family in ['mistral', 'llama', 'gemma', 'apertus']:\n            # Apertus uses same structure as LLaMA-style models\n            return self.model.model.layers\n        else:\n            # Try common patterns\n            if hasattr(self.model, 'model') and hasattr(self.model.model, 'layers'):\n                return self.model.model.layers\n            elif hasattr(self.model, 'transformer') and hasattr(self.model.transformer, 'h'):\n                return self.model.transformer.h\n            elif hasattr(self.model, 'gpt_neox') and hasattr(self.model.gpt_neox, 'layers'):\n                return self.model.gpt_neox.layers\n            else:\n                raise ValueError(f\"Unknown architecture for family: {family}\")\n    \n    def _get_embedding_layer(self):\n        \"\"\"Get embedding layer for different architectures.\"\"\"\n        family = self.model_info['family']\n        \n        if family == 'pythia':\n            return self.model.gpt_neox.embed_in\n        elif family in ['mistral', 'llama', 'gemma', 'apertus']:\n            # Apertus uses same structure as LLaMA-style models\n            return self.model.model.embed_tokens\n        else:\n            return self.model.get_input_embeddings()\n    \n    def _make_embedding_hook(self):\n        \"\"\"Hook to capture embedding output norm.\"\"\"\n        def hook(module, args, output):\n            with torch.no_grad():\n                self.embedding_norm = output.float().norm().item()\n        return hook\n    \n    def _make_layer_hook(self, layer_idx):\n        \"\"\"Create hook that captures residual stream norm AFTER each layer.\"\"\"\n        def hook(module, args, output):\n            # Output is (hidden_states, ...) or just hidden_states\n            if isinstance(output, tuple):\n                hidden = output[0]\n            else:\n                hidden = output\n            \n            with torch.no_grad():\n                norm = hidden.float().norm().item()\n                self.residual_norms.append((layer_idx, norm))\n        \n        return hook\n    \n    def register_hooks(self):\n        \"\"\"Register hooks on embedding and each layer output.\"\"\"\n        # Embedding hook\n        embed_layer = self._get_embedding_layer()\n        h_emb = embed_layer.register_forward_hook(self._make_embedding_hook())\n        self.hooks.append(h_emb)\n        \n        # Layer hooks\n        layers = self._get_layers()\n        for i, layer in enumerate(layers):\n            h = layer.register_forward_hook(self._make_layer_hook(i))\n            self.hooks.append(h)\n        \n        print(f\"  Registered {len(self.hooks)} hooks (1 embedding + {len(layers)} layers)\")\n    \n    def remove_hooks(self):\n        for h in self.hooks:\n            h.remove()\n        self.hooks = []\n    \n    def clear(self):\n        self.residual_norms = []\n        self.embedding_norm = None\n    \n    def get_layer_gains(self):\n        \"\"\"Compute gain = norm(layer_i) / norm(layer_{i-1}).\"\"\"\n        gains = []\n        norms = []\n        \n        # Sort by layer index\n        sorted_norms = sorted(self.residual_norms, key=lambda x: x[0])\n        \n        # Add embedding norm as \"layer -1\" equivalent\n        if self.embedding_norm:\n            all_norms = [(\"emb\", self.embedding_norm)] + sorted_norms\n        else:\n            all_norms = sorted_norms\n        \n        # Compute gains\n        for i in range(1, len(all_norms)):\n            prev_norm = all_norms[i-1][1]\n            curr_norm = all_norms[i][1]\n            \n            if prev_norm > 1e-8:\n                gain = curr_norm / prev_norm\n            else:\n                gain = 0.0\n            \n            gains.append(float(gain))\n        \n        norms = [n for _, n in all_norms]\n        \n        return gains, norms"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 5: Run Residual Stream Analysis\n\nall_results = {}\n\nfor model_key, model_info in models_to_run.items():\n    print(f\"\\n{'='*60}\")\n    print(f\"Residual Stream Analysis: {model_key}\")\n    print(f\"{'='*60}\")\n    \n    # Load model (now returns dtype too)\n    model, tokenizer, model_dtype = load_model(model_key, model_info)\n    if model is None:\n        print(f\"  Skipping {model_key} due to load failure.\")\n        continue\n    \n    try:\n        # Setup analyzer\n        analyzer = ResidualStreamAnalyzer(model, model_info)\n        analyzer.register_hooks()\n        \n        # Test prompt\n        prompt = \"The capital of France is\"\n        inputs = tokenizer(prompt, return_tensors='pt')\n        \n        # Move to GPU if available\n        if torch.cuda.is_available():\n            inputs = {k: v.cuda() for k, v in inputs.items()}\n        \n        # Forward pass with autocast for dtype consistency (fixes xIELU fallback issue)\n        print(f\"  Running forward pass (dtype: {model_dtype})...\")\n        with torch.no_grad():\n            if torch.cuda.is_available() and model_dtype in [torch.float16, torch.bfloat16]:\n                # Use autocast to ensure consistent dtypes throughout forward pass\n                with torch.amp.autocast(device_type='cuda', dtype=model_dtype):\n                    outputs = model(**inputs)\n            else:\n                outputs = model(**inputs)\n        \n        # Get gains\n        gains, norms = analyzer.get_layer_gains()\n        \n        if gains and len(gains) > 1:\n            # Statistics\n            n_layers = len(gains)\n            contracting = sum(1 for g in gains if g < 1.0)\n            expanding = sum(1 for g in gains if g > 1.0)\n            last_gain = gains[-1] if gains else 0\n            max_gain = max(gains) if gains else 0\n            max_gain_layer = gains.index(max_gain) if gains else -1\n            initial_gain = gains[0] if gains else 0\n            \n            # Find expansion layers\n            expansion_layers = [(i, g) for i, g in enumerate(gains) if g > 1.0]\n            \n            # Check Paper #2 inversion point\n            paper2_layer = model_info.get('paper2_inversion')\n            paper2_gain = None\n            if paper2_layer is not None and paper2_layer < len(gains):\n                paper2_gain = gains[paper2_layer]\n            \n            print(f\"\\n  Results:\")\n            print(f\"  Layers (gains): {n_layers}\")\n            print(f\"  Embedding Norm: {norms[0]:.2f}\")\n            print(f\"  Initial Gain (Embâ†’L0): {initial_gain:.2f}x\")\n            print(f\"  Contracting: {contracting}/{n_layers} ({100*contracting/n_layers:.1f}%)\")\n            print(f\"  Expanding: {expanding}/{n_layers} ({100*expanding/n_layers:.1f}%)\")\n            print(f\"  Last Layer Gain: {last_gain:.4f}x {'EXPANDS!' if last_gain > 1.0 else 'contracts'}\")\n            print(f\"  Max Gain: {max_gain:.4f}x at position {max_gain_layer}\")\n            print(f\"  Final Norm: {norms[-1]:.2f}\")\n            \n            # Compare with expected\n            expected = model_info.get('expected_last_gain')\n            if expected:\n                ratio = last_gain / expected\n                print(f\"  Expected (FFN): {expected:.2f}x, Ratio: {ratio:.2f}\")\n            \n            if paper2_gain is not None:\n                print(f\"\\n  *** Paper #2 Inversion Point (Layer {paper2_layer}): {paper2_gain:.4f}x ***\")\n            \n            # Top expansion layers\n            if expansion_layers:\n                print(f\"\\n  Top Expansion Layers (gain > 1):\")\n                sorted_exp = sorted(expansion_layers, key=lambda x: x[1], reverse=True)[:5]\n                for i, g in sorted_exp:\n                    marker = \" <- PAPER #2 INVERSION!\" if paper2_layer is not None and i == paper2_layer else \"\"\n                    print(f\"    Position {i}: {g:.4f}x{marker}\")\n            \n            # Store results\n            all_results[model_key] = {\n                'family': model_info['family'],\n                'norm_type': model_info['norm'],\n                'mlp_type': model_info['mlp'],\n                'n_layers': n_layers,\n                'gains': gains,\n                'norms': norms,\n                'embedding_norm': norms[0] if norms else None,\n                'initial_gain': float(initial_gain),\n                'last_gain': float(last_gain),\n                'last_expands': bool(last_gain > 1.0),\n                'max_gain': float(max_gain),\n                'max_gain_layer': int(max_gain_layer),\n                'contracting_pct': float(100 * contracting / n_layers),\n                'expanding_pct': float(100 * expanding / n_layers),\n                'expansion_layers': [(int(i), float(g)) for i, g in expansion_layers],\n                'paper2_inversion_layer': paper2_layer,\n                'paper2_inversion_gain': float(paper2_gain) if paper2_gain is not None else None,\n                'expected_last_gain': expected\n            }\n        else:\n            print(f\"  WARNING: No valid gains computed!\")\n        \n        # Cleanup hooks\n        analyzer.remove_hooks()\n        \n    except Exception as e:\n        print(f\"  ERROR during analysis: {e}\")\n        import traceback\n        traceback.print_exc()\n    \n    finally:\n        # Always cleanup model\n        cleanup_model(model)\n\nprint(f\"\\n\\n{'='*60}\")\nprint(f\"Completed residual stream analysis for {len(all_results)} models.\")\nprint(f\"{'='*60}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 6: Cross-Model Comparison (with SwiGLU Analysis)\n\nprint(\"=\"*60)\nprint(\"CROSS-MODEL COMPARISON (Residual Stream)\")\nprint(\"=\"*60)\n\n# Build comparison data including Mistral reference\ncomparison_data = {\n    'mistral-7b (ref)': {\n        'family': 'mistral',\n        'norm_type': 'RMSNorm',\n        'mlp_type': 'SwiGLU',\n        'initial_gain': MISTRAL_REFERENCE['initial_explosion'],\n        'last_gain': MISTRAL_REFERENCE['last_layer_gain'],\n        'last_expands': True,\n        'source': 'Previous Investigation'\n    }\n}\n\nfor name, res in all_results.items():\n    comparison_data[name] = {\n        'family': res['family'],\n        'norm_type': res['norm_type'],\n        'mlp_type': res['mlp_type'],\n        'initial_gain': res['initial_gain'],\n        'last_gain': res['last_gain'],\n        'last_expands': res['last_expands'],\n        'source': 'This Experiment'\n    }\n\nprint(\"\\n| Model | Family | Norm | MLP | Initial | Last | Expands? |\")\nprint(\"|-------|--------|------|-----|---------|------|----------|\")\n\nfor name, data in comparison_data.items():\n    expands = \"YES\" if data['last_expands'] else \"NO\"\n    print(f\"| {name} | {data['family']} | {data['norm_type']} | {data['mlp_type']} | {data['initial_gain']:.2f}x | {data['last_gain']:.2f}x | {expands} |\")\n\n# Universal principle test\nall_expand = all(d['last_expands'] for d in comparison_data.values())\nprint(f\"\\n{'='*60}\")\nif all_expand:\n    print(\"UNIVERSAL PRINCIPLE CONFIRMED: ALL models expand at last layer!\")\nelse:\n    failing = [n for n, d in comparison_data.items() if not d['last_expands']]\n    print(f\"WARNING: These models do NOT expand: {failing}\")\n\n# SwiGLU COMPARISON (Key for Paper #3!)\nprint(f\"\\n{'='*60}\")\nprint(\"SWIGLU COMPARISON (LLaMA-3.1 vs Mistral)\")\nprint(\"=\"*60)\n\nswiglu_models = {n: d for n, d in comparison_data.items() if d['mlp_type'] == 'SwiGLU'}\ngeglu_models = {n: d for n, d in comparison_data.items() if d['mlp_type'] == 'GeGLU'}\ngelu_models = {n: d for n, d in comparison_data.items() if d['mlp_type'] == 'GELU'}\nxield_models = {n: d for n, d in comparison_data.items() if d['mlp_type'] == 'xIELU'}\n\nprint(\"\\nBy MLP Type:\")\nprint(\"-\" * 50)\n\nif swiglu_models:\n    print(f\"\\nðŸ”µ SwiGLU Models: {list(swiglu_models.keys())}\")\n    for name, data in swiglu_models.items():\n        print(f\"   {name}: Initial={data['initial_gain']:.2f}x, Last={data['last_gain']:.2f}x\")\n    \n    # Direct comparison if both LLaMA-3.1 and Mistral present\n    if 'llama3.1-8b' in swiglu_models and 'mistral-7b (ref)' in swiglu_models:\n        llama_gain = swiglu_models['llama3.1-8b']['last_gain']\n        mistral_gain = swiglu_models['mistral-7b (ref)']['last_gain']\n        diff = abs(llama_gain - mistral_gain)\n        ratio = llama_gain / mistral_gain\n        \n        print(f\"\\n   *** SWIGLU COMPARISON ***\")\n        print(f\"   LLaMA-3.1-8B Last Gain:  {llama_gain:.4f}x\")\n        print(f\"   Mistral-7B Last Gain:    {mistral_gain:.4f}x\")\n        print(f\"   Difference: {diff:.4f}x\")\n        print(f\"   Ratio: {ratio:.2f}\")\n        \n        if 0.8 <= ratio <= 1.2:\n            print(f\"\\n   âœ… CONSISTENT! Same MLP type â†’ Similar expansion behavior\")\n        else:\n            print(f\"\\n   âš ï¸ DIVERGENT! Same MLP but different expansion (investigate!)\")\n\nif geglu_models:\n    print(f\"\\nðŸ”´ GeGLU Models: {list(geglu_models.keys())}\")\n    for name, data in geglu_models.items():\n        print(f\"   {name}: Initial={data['initial_gain']:.2f}x, Last={data['last_gain']:.2f}x\")\n\nif gelu_models:\n    print(f\"\\nðŸŸ¢ GELU Models (LayerNorm): {list(gelu_models.keys())}\")\n    for name, data in gelu_models.items():\n        print(f\"   {name}: Initial={data['initial_gain']:.2f}x, Last={data['last_gain']:.2f}x\")\n\nif xield_models:\n    print(f\"\\nðŸŸ  xIELU Models: {list(xield_models.keys())}\")\n    for name, data in xield_models.items():\n        print(f\"   {name}: Initial={data['initial_gain']:.2f}x, Last={data['last_gain']:.2f}x\")\n\n# Summary by Norm Type\nprint(f\"\\n{'='*60}\")\nprint(\"SUMMARY BY NORMALIZATION\")\nprint(\"=\"*60)\n\nrmsnorm_gains = [d['last_gain'] for n, d in comparison_data.items() if d['norm_type'] == 'RMSNorm']\nlayernorm_gains = [d['last_gain'] for n, d in comparison_data.items() if d['norm_type'] == 'LayerNorm']\n\nif rmsnorm_gains:\n    print(f\"\\nRMSNorm Models (n={len(rmsnorm_gains)}):\")\n    print(f\"  Last Gains: {[f'{g:.2f}x' for g in rmsnorm_gains]}\")\n    print(f\"  Mean: {np.mean(rmsnorm_gains):.2f}x, Range: [{min(rmsnorm_gains):.2f}x, {max(rmsnorm_gains):.2f}x]\")\n    print(f\"  ALL EXPAND: {'YES' if all(g > 1.0 for g in rmsnorm_gains) else 'NO'}\")\n\nif layernorm_gains:\n    print(f\"\\nLayerNorm Models (n={len(layernorm_gains)}):\")\n    print(f\"  Last Gains: {[f'{g:.2f}x' for g in layernorm_gains]}\")\n    print(f\"  Mean: {np.mean(layernorm_gains):.2f}x\")\n    print(f\"  ALL CONTRACT: {'YES' if all(g < 1.0 for g in layernorm_gains) else 'NO'}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Paper #2 Connection Analysis\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PAPER #2 <-> PAPER #3 CONNECTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check Apertus\n",
    "apertus_key = 'apertus-8b'\n",
    "if apertus_key in all_results:\n",
    "    apertus = all_results[apertus_key]\n",
    "    \n",
    "    print(\"\\nPaper #2 Finding (Apertus-8B):\")\n",
    "    print(\"  - Embedding-Output correlation INVERTS at Layer 28\")\n",
    "    print(\"  - Before L28: Positive correlation (consensus building)\")\n",
    "    print(\"  - After L28: Negative correlation (discrimination)\")\n",
    "    \n",
    "    print(\"\\nPaper #3 Question:\")\n",
    "    print(\"  - Does Layer 28 show EXPANSION in residual stream?\")\n",
    "    print(\"  - Is 'inversion' actually 'expansion'?\")\n",
    "    \n",
    "    l28_gain = apertus.get('paper2_inversion_gain')\n",
    "    last_gain = apertus['last_gain']\n",
    "    \n",
    "    print(f\"\\nResults:\")\n",
    "    if l28_gain is not None:\n",
    "        print(f\"  Layer 28 Gain: {l28_gain:.4f}x {'(EXPANDS)' if l28_gain > 1.0 else '(contracts)'}\")\n",
    "    print(f\"  Last Layer Gain: {last_gain:.4f}x {'(EXPANDS)' if last_gain > 1.0 else '(contracts)'}\")\n",
    "    \n",
    "    # Analyze expansion profile\n",
    "    exp_layers = apertus.get('expansion_layers', [])\n",
    "    if exp_layers:\n",
    "        exp_indices = [l for l, g in exp_layers]\n",
    "        print(f\"\\n  Expansion layers: {exp_indices}\")\n",
    "        print(f\"  Strongest expansion: Position {apertus['max_gain_layer']} ({apertus['max_gain']:.2f}x)\")\n",
    "        \n",
    "        # Check if L28 is in expansion layers\n",
    "        if 28 in exp_indices or (l28_gain and l28_gain > 1.0):\n",
    "            print(f\"\\n  *** HYPOTHESIS CONFIRMED! ***\")\n",
    "            print(f\"  The 'inversion point' from Paper #2 IS an expansion point!\")\n",
    "            print(f\"  Semantic Inversion = Residual Expansion = Decision Broadcast\")\n",
    "        else:\n",
    "            print(f\"\\n  Layer 28 is NOT an expansion point.\")\n",
    "            if last_gain > 1.0:\n",
    "                print(f\"  But expansion still occurs at last layer!\")\n",
    "else:\n",
    "    print(\"\\nApertus-8B not tested - cannot verify Paper #2 connection.\")\n",
    "    print(\"(Need GPU with >= 22GB memory)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 8: Visualization\n\nimport os\nresults_dir = '../Results'\nos.makedirs(results_dir, exist_ok=True)\n\nn_models = len(all_results)\nif n_models == 0:\n    print(\"No models tested - skipping visualization.\")\nelse:\n    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n    \n    # Updated colors with LLaMA\n    colors = {\n        'gemma': 'red', 'llama': 'green', 'mistral': 'purple', \n        'pythia': 'blue', 'apertus': 'orange'\n    }\n    \n    # Panel 1: Residual Stream Gains (zoomed to interesting range)\n    ax1 = axes[0, 0]\n    all_gains_flat = []\n    for model_key, res in all_results.items():\n        color = colors.get(res['family'], 'gray')\n        gains = res['gains']\n        layers = range(len(gains))\n        ax1.plot(layers, gains, 'o-', label=model_key, color=color, alpha=0.7, markersize=4)\n        all_gains_flat.extend(gains[1:])  # Skip initial explosion for ylim\n        \n        # Mark Paper #2 inversion point if exists\n        if res.get('paper2_inversion_layer') is not None:\n            l28 = res['paper2_inversion_layer']\n            if l28 < len(gains):\n                ax1.axvline(x=l28, color=color, linestyle='--', alpha=0.5)\n                ax1.annotate(f'L{l28}', xy=(l28, gains[l28]), fontsize=8)\n    \n    ax1.axhline(y=1.0, color='black', linestyle='--', alpha=0.5, label='gain=1')\n    ax1.set_xlabel('Layer')\n    ax1.set_ylabel('Residual Stream Gain')\n    ax1.set_title('Residual Stream Gain per Layer (excl. initial)')\n    ax1.legend(loc='upper left')\n    \n    # Dynamic ylim based on data (excluding extreme initial gains)\n    if all_gains_flat:\n        p5, p95 = np.percentile(all_gains_flat, [5, 95])\n        margin = (p95 - p5) * 0.2\n        ax1.set_ylim(max(0.5, p5 - margin), min(2.0, p95 + margin))\n    \n    # Panel 2: Residual Stream Norms\n    ax2 = axes[0, 1]\n    for model_key, res in all_results.items():\n        color = colors.get(res['family'], 'gray')\n        norms = res['norms']\n        layers = range(len(norms))\n        ax2.plot(layers, norms, 's-', label=model_key, color=color, alpha=0.7, markersize=4)\n    ax2.set_xlabel('Position (0=embedding)')\n    ax2.set_ylabel('Residual Norm')\n    ax2.set_title('Residual Stream Norm per Layer')\n    ax2.legend()\n    \n    # Panel 3: Last Layer Gains Comparison (with Mistral)\n    ax3 = axes[1, 0]\n    models = list(all_results.keys()) + ['mistral-7b (ref)']\n    last_gains = [all_results[m]['last_gain'] for m in all_results.keys()] + [MISTRAL_REFERENCE['last_layer_gain']]\n    bar_colors = [colors.get(all_results[m]['family'], 'gray') for m in all_results.keys()] + ['purple']\n    \n    x = np.arange(len(models))\n    bars = ax3.bar(x, last_gains, color=bar_colors, alpha=0.7)\n    ax3.axhline(y=1.0, color='black', linestyle='--', alpha=0.5, label='gain=1')\n    ax3.set_xlabel('Model')\n    ax3.set_ylabel('Last Layer Gain')\n    ax3.set_title('Last Layer Expansion (Residual Stream)')\n    ax3.set_xticks(x)\n    ax3.set_xticklabels(models, rotation=20, ha='right')\n    \n    # Add value labels\n    for bar, val in zip(bars, last_gains):\n        ax3.annotate(f'{val:.2f}x', xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n                     ha='center', va='bottom', fontsize=10)\n    \n    # Panel 4: Initial vs Last Gain (with MLP type markers)\n    ax4 = axes[1, 1]\n    \n    # Different markers for different MLP types\n    mlp_markers = {'GELU': 'o', 'GeGLU': 's', 'SwiGLU': '^', 'xIELU': 'D'}\n    \n    for model_key, res in all_results.items():\n        color = colors.get(res['family'], 'gray')\n        marker = mlp_markers.get(res['mlp_type'], 'o')\n        ax4.scatter(res['initial_gain'], res['last_gain'], s=200, color=color, \n                   marker=marker, label=f\"{model_key} ({res['mlp_type']})\", \n                   alpha=0.7, edgecolors='black')\n    \n    # Add Mistral reference\n    ax4.scatter(MISTRAL_REFERENCE['initial_explosion'], MISTRAL_REFERENCE['last_layer_gain'], \n               s=200, color='purple', label='mistral-7b (SwiGLU)', alpha=0.7, \n               edgecolors='black', marker='^')\n    \n    ax4.axhline(y=1.0, color='black', linestyle='--', alpha=0.3)\n    ax4.axvline(x=1.0, color='black', linestyle='--', alpha=0.3)\n    ax4.set_xlabel('Initial Gain (Embâ†’L0)')\n    ax4.set_ylabel('Last Layer Gain')\n    ax4.set_title('Initial vs Final Expansion (by MLP Type)')\n    ax4.legend(fontsize=8, loc='upper right')\n    ax4.set_xscale('log')\n    \n    plt.tight_layout()\n    output_path = f'{results_dir}/residual_stream_complete_validation.png'\n    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n    plt.show()\n    print(f\"Saved: {output_path}\")"
  },
  {
   "cell_type": "code",
   "source": "# Cell 8b: Cumulative Energy Visualization\n# Product of gains = \"Net Inertia\" across layers\n# This visualizes the total energy transformation from embedding to output\n\nprint(\"=\"*60)\nprint(\"CUMULATIVE ENERGY ANALYSIS\")\nprint(\"=\"*60)\n\nif len(all_results) > 0:\n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    \n    colors = {\n        'gemma': 'red', 'llama': 'green', 'mistral': 'purple', \n        'pythia': 'blue', 'apertus': 'orange'\n    }\n    \n    # Panel 1: Cumulative Energy (log scale product of gains)\n    ax1 = axes[0]\n    \n    for model_key, res in all_results.items():\n        color = colors.get(res['family'], 'gray')\n        gains = res['gains']\n        \n        # Compute cumulative product (in log space for numerical stability)\n        log_gains = np.log(np.array(gains) + 1e-10)  # Avoid log(0)\n        cumulative_log = np.cumsum(log_gains)\n        cumulative_energy = np.exp(cumulative_log)\n        \n        layers = range(len(cumulative_energy))\n        ax1.semilogy(layers, cumulative_energy, 'o-', label=model_key, \n                     color=color, alpha=0.7, markersize=4)\n        \n        # Store for later\n        res['cumulative_energy'] = cumulative_energy.tolist()\n        res['final_cumulative'] = float(cumulative_energy[-1])\n        \n        # Print final cumulative energy\n        print(f\"{model_key}: Final Cumulative Energy = {cumulative_energy[-1]:.2e}\")\n    \n    ax1.set_xlabel('Layer')\n    ax1.set_ylabel('Cumulative Energy (âˆ gains)')\n    ax1.set_title('Cumulative Energy = Product of All Gains\\n\"Net Inertia\" from Embedding to Output')\n    ax1.legend(loc='upper left')\n    ax1.grid(True, alpha=0.3)\n    ax1.axhline(y=1.0, color='black', linestyle='--', alpha=0.3, label='neutral')\n    \n    # Panel 2: Final Cumulative Energy by Norm Type\n    ax2 = axes[1]\n    \n    # Group by norm type\n    norm_groups = {}\n    for name, res in all_results.items():\n        norm = res['norm_type']\n        if norm not in norm_groups:\n            norm_groups[norm] = []\n        norm_groups[norm].append((name, res['final_cumulative']))\n    \n    # Add Mistral reference\n    # Mistral's cumulative would be ~43.86 * 1.0^29 * 1.37 â‰ˆ 60\n    mistral_cumulative = MISTRAL_REFERENCE['initial_explosion'] * MISTRAL_REFERENCE['last_layer_gain']\n    if 'RMSNorm' in norm_groups:\n        norm_groups['RMSNorm'].append(('mistral-7b (ref)', mistral_cumulative))\n    \n    # Bar chart\n    x_pos = 0\n    width = 0.35\n    for norm, models in norm_groups.items():\n        for name, energy in models:\n            bar_color = colors.get(name.split('-')[0], 'gray')\n            if 'mistral' in name:\n                bar_color = 'purple'\n            ax2.bar(x_pos, energy, width, color=bar_color, alpha=0.7, \n                   edgecolor='black', label=name)\n            ax2.annotate(f'{energy:.1e}', xy=(x_pos, energy), \n                        ha='center', va='bottom', fontsize=8, rotation=45)\n            x_pos += 1\n        x_pos += 0.5  # Gap between norm types\n    \n    ax2.set_ylabel('Final Cumulative Energy')\n    ax2.set_title('Total \"Net Inertia\" by Model\\nHigher = More Amplification')\n    ax2.set_yscale('log')\n    ax2.legend(loc='upper right', fontsize=8)\n    \n    # Key insight annotation\n    ax2.annotate('LayerNorm\\nContracts', xy=(0, 1), fontsize=10, ha='center', \n                 xycoords='axes fraction', va='top', color='blue')\n    ax2.annotate('RMSNorm\\nExpands', xy=(0.7, 1), fontsize=10, ha='center',\n                 xycoords='axes fraction', va='top', color='red')\n    \n    plt.tight_layout()\n    output_path = f'{results_dir}/cumulative_energy_analysis.png'\n    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n    plt.show()\n    print(f\"\\nSaved: {output_path}\")\n    \n    # Summary table\n    print(\"\\n\" + \"=\"*60)\n    print(\"CUMULATIVE ENERGY SUMMARY\")\n    print(\"=\"*60)\n    print(\"\\n| Model | Norm | MLP | Final Cumulative | Net Effect |\")\n    print(\"|-------|------|-----|------------------|------------|\")\n    for name, res in all_results.items():\n        net = \"AMPLIFY\" if res['final_cumulative'] > 1.0 else \"ATTENUATE\"\n        print(f\"| {name} | {res['norm_type']} | {res['mlp_type']} | {res['final_cumulative']:.2e} | {net} |\")\n    \n    print(f\"| mistral-7b (ref) | RMSNorm | SwiGLU | {mistral_cumulative:.2e} | AMPLIFY |\")\n    \n    print(\"\\n>>> KEY INSIGHT <<<\")\n    print(\"Cumulative Energy = âˆ(all layer gains)\")\n    print(\"- If > 1: Net AMPLIFICATION from embedding to output\")\n    print(\"- If < 1: Net ATTENUATION (information loss)\")\n    print(\"- Skip connections PRESERVE signal even when FFN contracts\")\nelse:\n    print(\"No models tested - skipping cumulative energy analysis.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 10: Save Results (v2 with Cumulative Energy)\n\nimport os\nresults_dir = '../Results'\nos.makedirs(results_dir, exist_ok=True)\n\n# Compile output\noutput = {\n    'experiment': 'Complete Residual Stream Validation v2',\n    'date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n    'purpose': 'Validate universal Compress-then-Broadcast pattern with SwiGLU comparison',\n    'models_tested': list(all_results.keys()),\n    'mistral_reference': MISTRAL_REFERENCE,\n    'results': {},\n    'swiglu_comparison': {},\n    'cumulative_energy': {},\n    'universal_principle_test': {},\n    'paper2_connection': {},\n    'conclusions': {}\n}\n\n# Convert results (handle numpy types)\nfor name, res in all_results.items():\n    output['results'][name] = {\n        'family': res['family'],\n        'norm_type': res['norm_type'],\n        'mlp_type': res['mlp_type'],\n        'n_layers': res['n_layers'],\n        'initial_gain': res['initial_gain'],\n        'last_gain': res['last_gain'],\n        'last_expands': res['last_expands'],\n        'max_gain': res['max_gain'],\n        'max_gain_layer': res['max_gain_layer'],\n        'contracting_pct': res['contracting_pct'],\n        'expanding_pct': res['expanding_pct'],\n        'paper2_inversion_layer': res.get('paper2_inversion_layer'),\n        'paper2_inversion_gain': res.get('paper2_inversion_gain'),\n        'cumulative_energy': res.get('cumulative_energy'),\n        'final_cumulative': res.get('final_cumulative'),\n        'gains': res['gains'],\n        'norms': res['norms']\n    }\n\n# SwiGLU comparison (LLaMA 3.1 vs Mistral)\nswiglu_models = {n: r for n, r in all_results.items() if r['mlp_type'] == 'SwiGLU'}\nif 'llama3.1-8b' in swiglu_models:\n    llama_gain = swiglu_models['llama3.1-8b']['last_gain']\n    mistral_gain = MISTRAL_REFERENCE['last_layer_gain']\n    output['swiglu_comparison'] = {\n        'llama31_8b_last_gain': llama_gain,\n        'mistral_7b_last_gain': mistral_gain,\n        'ratio': llama_gain / mistral_gain,\n        'consistent': 0.8 <= (llama_gain / mistral_gain) <= 1.2,\n        'interpretation': 'Same MLP type â†’ Similar expansion' if 0.8 <= (llama_gain / mistral_gain) <= 1.2 else 'Divergent behavior'\n    }\n\n# Cumulative Energy analysis\nfor name, res in all_results.items():\n    if 'final_cumulative' in res and res['final_cumulative'] is not None:\n        output['cumulative_energy'][name] = {\n            'final_cumulative': res['final_cumulative'],\n            'net_effect': 'AMPLIFY' if res['final_cumulative'] > 1.0 else 'ATTENUATE'\n        }\n\n# Universal principle test\nall_expand = all(r['last_expands'] for r in all_results.values()) if all_results else False\noutput['universal_principle_test'] = {\n    'all_expand_at_last_layer': all_expand,\n    'families_tested': list(set(r['family'] for r in all_results.values())),\n    'norms_tested': list(set(r['norm_type'] for r in all_results.values())),\n    'mlps_tested': list(set(r['mlp_type'] for r in all_results.values())),\n    'total_models': len(all_results) + 1  # +1 for Mistral reference\n}\n\n# Paper #2 connection\nif 'apertus-8b' in all_results:\n    apertus = all_results['apertus-8b']\n    l28_gain = apertus.get('paper2_inversion_gain')\n    output['paper2_connection'] = {\n        'apertus_tested': True,\n        'layer28_gain': l28_gain,\n        'layer28_expands': l28_gain > 1.0 if l28_gain else None,\n        'last_layer_gain': apertus['last_gain'],\n        'inversion_is_expansion': l28_gain > 1.0 if l28_gain else None\n    }\nelse:\n    output['paper2_connection'] = {'apertus_tested': False}\n\n# Conclusions\nrmsnorm_expand = all(r['last_expands'] for r in all_results.values() if r['norm_type'] == 'RMSNorm')\nlayernorm_contract = all(r['last_gain'] < 1.0 for r in all_results.values() if r['norm_type'] == 'LayerNorm')\n\noutput['conclusions'] = {\n    'universal_expansion': 'CONFIRMED' if all_expand else 'NORMALIZATION-DEPENDENT',\n    'rmsnorm_all_expand': rmsnorm_expand,\n    'layernorm_all_contract': layernorm_contract,\n    'measurement_insight': 'Residual stream measurement required for accurate gain analysis',\n    'skip_connection_role': 'Universal expansion mechanism across all architectures',\n    'cumulative_energy_insight': 'Product of gains reveals net information amplification/attenuation',\n    'swiglu_finding': output.get('swiglu_comparison', {}).get('interpretation', 'Not tested')\n}\n\noutput_path = f'{results_dir}/residual_stream_complete_validation_v2_results.json'\nwith open(output_path, 'w') as f:\n    json.dump(output, f, indent=2)\nprint(f\"Saved: {output_path}\")\n\n# List all output files\nprint(f\"\\nOutput files:\")\nprint(f\"  {output_path}\")\nprint(f\"  {results_dir}/residual_stream_complete_validation.png\")\nprint(f\"  {results_dir}/cumulative_energy_analysis.png\")\n\n# Auto-download for Colab\ntry:\n    from google.colab import files\n    files.download(output_path)\n    files.download(f'{results_dir}/residual_stream_complete_validation.png')\n    files.download(f'{results_dir}/cumulative_energy_analysis.png')\n    print(\"\\nFiles downloaded!\")\nexcept:\n    pass"
  },
  {
   "cell_type": "code",
   "source": "# Cell 11: THE RLHF SAFETY BRAKE TEST - Base vs Instruct\n# This is the definitive test of Gemini's hypothesis!\n#\n# Hypothesis:\n#   - Base model (no RLHF) â†’ EXPANDS like Mistral (~1.37x)\n#   - Instruct model (RLHF) â†’ CONTRACTS like measured (0.48x)\n#\n# If confirmed: RLHF is thermodynamically a \"dampening\" (energy dissipation) effect!\n\nprint(\"=\"*60)\nprint(\"RLHF SAFETY BRAKE TEST: Base vs Instruct\")\nprint(\"=\"*60)\n\n# Define the comparison models\nRLHF_TEST_MODELS = {\n    'llama3.1-8b-base': {\n        'hf_name': 'meta-llama/Llama-3.1-8B',\n        'params': 8e9,\n        'layers': 32,\n        'family': 'llama',\n        'norm': 'RMSNorm',\n        'mlp': 'SwiGLU',\n        'memory_gb': 20,\n        'rlhf': False,  # NO RLHF!\n        'expected': 'EXPANSION (like Mistral ~1.37x)'\n    },\n    'llama3.1-8b-instruct': {\n        'hf_name': 'meta-llama/Llama-3.1-8B-Instruct',\n        'params': 8e9,\n        'layers': 32,\n        'family': 'llama',\n        'norm': 'RMSNorm',\n        'mlp': 'SwiGLU',\n        'memory_gb': 20,\n        'rlhf': True,  # WITH RLHF!\n        'expected': 'CONTRACTION (~0.48x as measured)'\n    }\n}\n\nprint(\"\\nTest Configuration:\")\nprint(\"-\" * 50)\nfor name, info in RLHF_TEST_MODELS.items():\n    rlhf_status = \"WITH RLHF\" if info['rlhf'] else \"NO RLHF\"\n    print(f\"  {name}:\")\n    print(f\"    HuggingFace: {info['hf_name']}\")\n    print(f\"    RLHF: {rlhf_status}\")\n    print(f\"    Expected: {info['expected']}\")\n    print()\n\nprint(\"Hypothesis to test:\")\nprint(\"  If Base EXPANDS and Instruct CONTRACTS â†’\")\nprint(\"  RLHF is proven to be a 'Safety Brake' (energy dampening)\")\nprint()\nprint(\"Run this cell, then execute the analysis loop with:\")\nprint(\"  models_to_run = RLHF_TEST_MODELS\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Save Results\n",
    "\n",
    "import os\n",
    "results_dir = '../Results'\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Compile output\n",
    "output = {\n",
    "    'experiment': 'Complete Residual Stream Validation',\n",
    "    'date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'purpose': 'Validate universal Compress-then-Broadcast pattern across architectures',\n",
    "    'models_tested': list(all_results.keys()),\n",
    "    'mistral_reference': MISTRAL_REFERENCE,\n",
    "    'results': {},\n",
    "    'universal_principle_test': {},\n",
    "    'paper2_connection': {},\n",
    "    'conclusions': {}\n",
    "}\n",
    "\n",
    "# Convert results (handle numpy types)\n",
    "for name, res in all_results.items():\n",
    "    output['results'][name] = {\n",
    "        'family': res['family'],\n",
    "        'norm_type': res['norm_type'],\n",
    "        'mlp_type': res['mlp_type'],\n",
    "        'n_layers': res['n_layers'],\n",
    "        'initial_gain': res['initial_gain'],\n",
    "        'last_gain': res['last_gain'],\n",
    "        'last_expands': res['last_expands'],\n",
    "        'max_gain': res['max_gain'],\n",
    "        'max_gain_layer': res['max_gain_layer'],\n",
    "        'contracting_pct': res['contracting_pct'],\n",
    "        'expanding_pct': res['expanding_pct'],\n",
    "        'paper2_inversion_layer': res.get('paper2_inversion_layer'),\n",
    "        'paper2_inversion_gain': res.get('paper2_inversion_gain'),\n",
    "        'gains': res['gains'],\n",
    "        'norms': res['norms']\n",
    "    }\n",
    "\n",
    "# Universal principle test\n",
    "all_expand = all(r['last_expands'] for r in all_results.values()) if all_results else False\n",
    "output['universal_principle_test'] = {\n",
    "    'all_expand_at_last_layer': all_expand,\n",
    "    'families_tested': list(set(r['family'] for r in all_results.values())),\n",
    "    'norms_tested': list(set(r['norm_type'] for r in all_results.values())),\n",
    "    'total_models': len(all_results) + 1  # +1 for Mistral reference\n",
    "}\n",
    "\n",
    "# Paper #2 connection\n",
    "if 'apertus-8b' in all_results:\n",
    "    apertus = all_results['apertus-8b']\n",
    "    l28_gain = apertus.get('paper2_inversion_gain')\n",
    "    output['paper2_connection'] = {\n",
    "        'apertus_tested': True,\n",
    "        'layer28_gain': l28_gain,\n",
    "        'layer28_expands': l28_gain > 1.0 if l28_gain else None,\n",
    "        'last_layer_gain': apertus['last_gain'],\n",
    "        'inversion_is_expansion': l28_gain > 1.0 if l28_gain else None\n",
    "    }\n",
    "else:\n",
    "    output['paper2_connection'] = {'apertus_tested': False}\n",
    "\n",
    "# Conclusions\n",
    "output['conclusions'] = {\n",
    "    'universal_expansion': 'CONFIRMED' if all_expand else 'PARTIAL',\n",
    "    'measurement_insight': 'Residual stream measurement required for accurate gain analysis',\n",
    "    'skip_connection_role': 'Universal expansion mechanism across all architectures',\n",
    "    'layernorm_vs_rmsnorm': 'Both show expansion, but RMSNorm hides it in residual stream'\n",
    "}\n",
    "\n",
    "output_path = f'{results_dir}/residual_stream_complete_validation_results.json'\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(output, f, indent=2)\n",
    "print(f\"Saved: {output_path}\")\n",
    "\n",
    "# List all output files\n",
    "print(f\"\\nOutput files:\")\n",
    "print(f\"  {output_path}\")\n",
    "print(f\"  {results_dir}/residual_stream_complete_validation.png\")\n",
    "\n",
    "# Auto-download for Colab\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(output_path)\n",
    "    files.download(f'{results_dir}/residual_stream_complete_validation.png')\n",
    "    print(\"\\nFiles downloaded!\")\n",
    "except:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}