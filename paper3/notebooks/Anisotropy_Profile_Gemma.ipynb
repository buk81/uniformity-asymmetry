{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Anisotropy Profile Measurement: Gemma-2B\n",
        "\n",
        "**Paper #3 Empirical Validation - Gauge Theory Test**\n",
        "\n",
        "## Prediction (Gauge-Theorie via Gemini Deep Research)\n",
        "\n",
        "**Hypothesis:** RMSNorm acts as \"gauge fixing\" that projects onto hypersphere S^{d-1}, restricting diffusion to angular coordinates only.\n",
        "\n",
        "**Expected Difference from Pythia:**\n",
        "- **Pythia (LayerNorm):** Clear Bell Curve, strong inversion\n",
        "- **Gemma (RMSNorm):** Flatter profile, subtle/no inversion\n",
        "\n",
        "**Why?** RMSNorm preserves radial degrees of freedom less than LayerNorm. This results in:\n",
        "- Embeddings constrained to unit sphere\n",
        "- Only angular diffusion possible (Connection Laplacian)\n",
        "- Prevents representation collapse, but makes inversion subtler\n",
        "\n",
        "**Reference:** Gemini Deep Research Report (2026-01-04), Section on Architectural Gauge Theory\n",
        "\n",
        "**Author:** Davide D'Elia  \n",
        "**Date:** 2026-01-04"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup & Authentication\n",
        "\n",
        "**Important:** Gemma models require HuggingFace authentication and license acceptance.\n",
        "\n",
        "1. Go to https://huggingface.co/google/gemma-2b\n",
        "2. Accept the license agreement\n",
        "3. Create a token at https://huggingface.co/settings/tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q transformers accelerate einops scipy matplotlib seaborn huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Authenticate with HuggingFace (required for Gemma)\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Option 1: Interactive login (paste token when prompted)\n",
        "login()\n",
        "\n",
        "# Option 2: Direct token (uncomment and paste your token)\n",
        "# login(token=\"hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from scipy import stats\n",
        "from tqdm.auto import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette('husl')\n",
        "\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Model\n",
        "\n",
        "**Architecture Details:**\n",
        "- **Normalization:** RMSNorm (NOT LayerNorm)\n",
        "- **Position Encoding:** RoPE (Rotary Position Embeddings)\n",
        "- **Activation:** GeGLU\n",
        "\n",
        "All three contribute to \"gauge fixing\" that may flatten the anisotropy profile."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model configuration\n",
        "MODEL_NAME = \"google/gemma-2b\"\n",
        "# Alternatives:\n",
        "# MODEL_NAME = \"google/gemma-2-2b\"  # Gemma 2 variant\n",
        "# MODEL_NAME = \"google/gemma-7b\"    # Larger variant (needs more VRAM)\n",
        "\n",
        "print(f\"Loading {MODEL_NAME}...\")\n",
        "print(\"(This may take a few minutes for first download)\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    output_hidden_states=True\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "n_layers = model.config.num_hidden_layers\n",
        "hidden_dim = model.config.hidden_size\n",
        "\n",
        "print(f\"\\nLoaded: {n_layers} layers, {hidden_dim} hidden dim\")\n",
        "print(f\"\\nArchitecture Features:\")\n",
        "print(f\"  - Normalization: RMSNorm (gauge fixing)\")\n",
        "print(f\"  - Position Encoding: RoPE (flat connection)\")\n",
        "print(f\"  - Activation: GeGLU (cohomological gating)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Define Test Prompts\n",
        "\n",
        "Same prompts as Pythia for fair comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Diverse prompts for robust measurement (SAME AS PYTHIA)\n",
        "TEST_PROMPTS = [\n",
        "    # Factual\n",
        "    \"The capital of France is Paris, which is known for\",\n",
        "    \"Water boils at 100 degrees Celsius under standard\",\n",
        "    \"The speed of light in a vacuum is approximately\",\n",
        "    \n",
        "    # Reasoning\n",
        "    \"If all mammals are warm-blooded and whales are mammals, then\",\n",
        "    \"The probability of rolling a six on a fair die is\",\n",
        "    \n",
        "    # Creative\n",
        "    \"Once upon a time in a faraway kingdom, there lived\",\n",
        "    \"The sunset painted the sky in shades of orange and\",\n",
        "    \n",
        "    # Technical\n",
        "    \"In Python, you can define a function using the def keyword\",\n",
        "    \"Machine learning models learn patterns from data by\",\n",
        "    \"The transformer architecture uses self-attention to\",\n",
        "    \n",
        "    # Abstract\n",
        "    \"The concept of infinity has puzzled philosophers because\",\n",
        "    \"Democracy is often considered the best form of government\",\n",
        "    \n",
        "    # Conversational\n",
        "    \"Hello! How are you doing today? I hope you're having\",\n",
        "    \"Thank you for your help with this project. I really\",\n",
        "    \n",
        "    # From our dataset (Paper #1 examples)\n",
        "    \"Functional programming emphasizes immutability and pure functions\",\n",
        "    \"Object-oriented programming uses classes and inheritance for\",\n",
        "]\n",
        "\n",
        "print(f\"Using {len(TEST_PROMPTS)} test prompts (same as Pythia for comparison)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Extract Layer-wise Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_all_layer_embeddings(model, tokenizer, prompts, device=\"cuda\"):\n",
        "    \"\"\"\n",
        "    Extract embeddings from all layers for all prompts.\n",
        "    \n",
        "    Returns:\n",
        "        Dict[layer_idx -> np.array of shape (n_tokens_total, hidden_dim)]\n",
        "    \"\"\"\n",
        "    n_layers = model.config.num_hidden_layers\n",
        "    layer_embeddings = {i: [] for i in range(n_layers + 1)}  # +1 for embedding layer\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for prompt in tqdm(prompts, desc=\"Processing prompts\"):\n",
        "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "            outputs = model(**inputs, output_hidden_states=True)\n",
        "            \n",
        "            # hidden_states: tuple of (n_layers + 1) tensors\n",
        "            # Each tensor: (batch=1, seq_len, hidden_dim)\n",
        "            hidden_states = outputs.hidden_states\n",
        "            \n",
        "            for layer_idx, hidden in enumerate(hidden_states):\n",
        "                # Take all tokens, squeeze batch dimension\n",
        "                emb = hidden.squeeze(0).cpu().float().numpy()  # (seq_len, hidden_dim)\n",
        "                layer_embeddings[layer_idx].append(emb)\n",
        "    \n",
        "    # Concatenate all embeddings per layer\n",
        "    for layer_idx in layer_embeddings:\n",
        "        layer_embeddings[layer_idx] = np.vstack(layer_embeddings[layer_idx])\n",
        "    \n",
        "    return layer_embeddings\n",
        "\n",
        "print(\"Extracting embeddings from all layers...\")\n",
        "layer_embeddings = extract_all_layer_embeddings(model, tokenizer, TEST_PROMPTS)\n",
        "\n",
        "print(f\"\\nExtracted embeddings:\")\n",
        "for layer_idx in [0, n_layers // 2, n_layers]:\n",
        "    print(f\"  Layer {layer_idx}: {layer_embeddings[layer_idx].shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Compute Anisotropy Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_anisotropy_metrics(embeddings):\n",
        "    \"\"\"\n",
        "    Compute multiple anisotropy metrics for a set of embeddings.\n",
        "    \n",
        "    Args:\n",
        "        embeddings: np.array of shape (n_samples, hidden_dim)\n",
        "    \n",
        "    Returns:\n",
        "        dict with various anisotropy measures\n",
        "    \"\"\"\n",
        "    # Center the data\n",
        "    centered = embeddings - embeddings.mean(axis=0)\n",
        "    \n",
        "    # Compute covariance matrix\n",
        "    n_samples = embeddings.shape[0]\n",
        "    cov = (centered.T @ centered) / (n_samples - 1)\n",
        "    \n",
        "    # Eigenvalue decomposition\n",
        "    eigenvalues = np.linalg.eigvalsh(cov)\n",
        "    eigenvalues = np.sort(eigenvalues)[::-1]  # Descending order\n",
        "    eigenvalues = np.maximum(eigenvalues, 1e-10)  # Numerical stability\n",
        "    \n",
        "    # Metric 1: Eigenvalue Variance\n",
        "    eigenvalue_variance = np.var(eigenvalues)\n",
        "    \n",
        "    # Metric 2: Intrinsic Dimension Ratio (lambda_1 / sum(lambda_i))\n",
        "    total_var = eigenvalues.sum()\n",
        "    intrinsic_dim_ratio = eigenvalues[0] / total_var if total_var > 0 else 0\n",
        "    \n",
        "    # Metric 3: Effective Rank = exp(entropy)\n",
        "    normalized = eigenvalues / total_var\n",
        "    entropy = -np.sum(normalized * np.log(normalized + 1e-10))\n",
        "    effective_rank = np.exp(entropy)\n",
        "    \n",
        "    # Metric 4: Average Cosine Similarity to Mean (isotropy score)\n",
        "    mean_vec = embeddings.mean(axis=0)\n",
        "    mean_norm = np.linalg.norm(mean_vec)\n",
        "    if mean_norm > 1e-10:\n",
        "        cos_sims = []\n",
        "        for emb in embeddings:\n",
        "            cos_sim = np.dot(emb, mean_vec) / (np.linalg.norm(emb) * mean_norm + 1e-10)\n",
        "            cos_sims.append(cos_sim)\n",
        "        avg_cos_sim = np.mean(cos_sims)\n",
        "    else:\n",
        "        avg_cos_sim = 0\n",
        "    \n",
        "    # Metric 5: Explained variance by top-k PCs\n",
        "    cumsum = np.cumsum(eigenvalues) / total_var\n",
        "    var_top1 = eigenvalues[0] / total_var\n",
        "    var_top10 = cumsum[min(9, len(cumsum)-1)]\n",
        "    var_top50 = cumsum[min(49, len(cumsum)-1)]\n",
        "    \n",
        "    # Metric 6: Norms (relevant for RMSNorm analysis)\n",
        "    norms = np.linalg.norm(embeddings, axis=1)\n",
        "    mean_norm_value = np.mean(norms)\n",
        "    std_norm_value = np.std(norms)\n",
        "    \n",
        "    return {\n",
        "        'eigenvalue_variance': eigenvalue_variance,\n",
        "        'intrinsic_dim_ratio': intrinsic_dim_ratio,\n",
        "        'effective_rank': effective_rank,\n",
        "        'avg_cos_sim_to_mean': avg_cos_sim,\n",
        "        'var_top1': var_top1,\n",
        "        'var_top10': var_top10,\n",
        "        'var_top50': var_top50,\n",
        "        'mean_norm': mean_norm_value,\n",
        "        'std_norm': std_norm_value,\n",
        "        'norm_cv': std_norm_value / mean_norm_value if mean_norm_value > 0 else 0,  # Coefficient of variation\n",
        "        'eigenvalues': eigenvalues[:100]  # Store top 100 for analysis\n",
        "    }\n",
        "\n",
        "print(\"Computing anisotropy metrics for each layer...\")\n",
        "layer_metrics = {}\n",
        "for layer_idx in tqdm(range(n_layers + 1), desc=\"Layers\"):\n",
        "    layer_metrics[layer_idx] = compute_anisotropy_metrics(layer_embeddings[layer_idx])\n",
        "\n",
        "print(\"\\nDone!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Plot Anisotropy Profile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract metrics for plotting\n",
        "layers = list(range(n_layers + 1))\n",
        "eigenvalue_variance = [layer_metrics[l]['eigenvalue_variance'] for l in layers]\n",
        "intrinsic_dim_ratio = [layer_metrics[l]['intrinsic_dim_ratio'] for l in layers]\n",
        "effective_rank = [layer_metrics[l]['effective_rank'] for l in layers]\n",
        "avg_cos_sim = [layer_metrics[l]['avg_cos_sim_to_mean'] for l in layers]\n",
        "norm_cv = [layer_metrics[l]['norm_cv'] for l in layers]\n",
        "\n",
        "# Find L* (maximum of intrinsic_dim_ratio = maximum anisotropy)\n",
        "L_star = np.argmax(intrinsic_dim_ratio)\n",
        "print(f\"Detected L* (maximum anisotropy): Layer {L_star}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Main Plot: Anisotropy Profile\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "\n",
        "# Plot 1: Intrinsic Dimension Ratio (Primary Anisotropy Measure)\n",
        "ax1 = axes[0, 0]\n",
        "ax1.plot(layers, intrinsic_dim_ratio, 'b-', linewidth=2, marker='o', markersize=4)\n",
        "ax1.axvline(x=L_star, color='red', linestyle='--', linewidth=2, label=f'L* = {L_star}')\n",
        "ax1.fill_between(layers, intrinsic_dim_ratio, alpha=0.3)\n",
        "ax1.set_xlabel('Layer', fontsize=12)\n",
        "ax1.set_ylabel('lambda_1 / sum(lambda_i)', fontsize=12)\n",
        "ax1.set_title('Primary Anisotropy: Variance Concentration', fontsize=14)\n",
        "ax1.legend(fontsize=11)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Effective Rank (Inverse Anisotropy)\n",
        "ax2 = axes[0, 1]\n",
        "ax2.plot(layers, effective_rank, 'g-', linewidth=2, marker='s', markersize=4)\n",
        "ax2.axvline(x=L_star, color='red', linestyle='--', linewidth=2, label=f'L* = {L_star}')\n",
        "ax2.set_xlabel('Layer', fontsize=12)\n",
        "ax2.set_ylabel('Effective Rank', fontsize=12)\n",
        "ax2.set_title('Effective Rank (lower = more anisotropic)', fontsize=14)\n",
        "ax2.legend(fontsize=11)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: Average Cosine Similarity to Mean\n",
        "ax3 = axes[0, 2]\n",
        "ax3.plot(layers, avg_cos_sim, 'm-', linewidth=2, marker='^', markersize=4)\n",
        "ax3.axvline(x=L_star, color='red', linestyle='--', linewidth=2, label=f'L* = {L_star}')\n",
        "ax3.set_xlabel('Layer', fontsize=12)\n",
        "ax3.set_ylabel('Avg Cosine Sim to Mean', fontsize=12)\n",
        "ax3.set_title('Directional Anisotropy', fontsize=14)\n",
        "ax3.legend(fontsize=11)\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 4: Eigenvalue Variance\n",
        "ax4 = axes[1, 0]\n",
        "ax4.semilogy(layers, eigenvalue_variance, 'r-', linewidth=2, marker='d', markersize=4)\n",
        "ax4.axvline(x=L_star, color='red', linestyle='--', linewidth=2, label=f'L* = {L_star}')\n",
        "ax4.set_xlabel('Layer', fontsize=12)\n",
        "ax4.set_ylabel('Var(lambda_i) [log scale]', fontsize=12)\n",
        "ax4.set_title('Eigenvalue Variance', fontsize=14)\n",
        "ax4.legend(fontsize=11)\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 5: Norm Coefficient of Variation (RMSNorm-specific)\n",
        "ax5 = axes[1, 1]\n",
        "ax5.plot(layers, norm_cv, 'c-', linewidth=2, marker='p', markersize=4)\n",
        "ax5.axvline(x=L_star, color='red', linestyle='--', linewidth=2, label=f'L* = {L_star}')\n",
        "ax5.set_xlabel('Layer', fontsize=12)\n",
        "ax5.set_ylabel('Norm CV (std/mean)', fontsize=12)\n",
        "ax5.set_title('Norm Coefficient of Variation\\n(RMSNorm Effect)', fontsize=14)\n",
        "ax5.legend(fontsize=11)\n",
        "ax5.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 6: Summary Comparison\n",
        "ax6 = axes[1, 2]\n",
        "# Normalize all metrics for overlay\n",
        "def normalize(arr):\n",
        "    arr = np.array(arr)\n",
        "    return (arr - arr.min()) / (arr.max() - arr.min() + 1e-10)\n",
        "\n",
        "ax6.plot(layers, normalize(intrinsic_dim_ratio), 'b-', linewidth=2, label='Intrinsic Dim Ratio', alpha=0.8)\n",
        "ax6.plot(layers, 1 - normalize(effective_rank), 'g-', linewidth=2, label='1 - Effective Rank (norm)', alpha=0.8)\n",
        "ax6.plot(layers, normalize(avg_cos_sim), 'm-', linewidth=2, label='Avg Cos Sim', alpha=0.8)\n",
        "ax6.axvline(x=L_star, color='red', linestyle='--', linewidth=2, label=f'L* = {L_star}')\n",
        "ax6.set_xlabel('Layer', fontsize=12)\n",
        "ax6.set_ylabel('Normalized Value', fontsize=12)\n",
        "ax6.set_title('All Metrics Normalized', fontsize=14)\n",
        "ax6.legend(fontsize=9, loc='best')\n",
        "ax6.grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle(f'{MODEL_NAME}: Anisotropy Profile\\n(Gauge Theory Prediction: FLATTER than Pythia due to RMSNorm)', \n",
        "             fontsize=16, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.savefig('anisotropy_profile_gemma.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n>>> Figure saved as 'anisotropy_profile_gemma.png'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Bell Curve Analysis & Gauge Theory Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if profile matches Bell Curve prediction\n",
        "def analyze_bell_curve(values, L_star):\n",
        "    \"\"\"\n",
        "    Analyze if the values follow a Bell Curve pattern:\n",
        "    - Rising before L*\n",
        "    - Falling after L*\n",
        "    \"\"\"\n",
        "    values = np.array(values)\n",
        "    \n",
        "    # Split into phases\n",
        "    phase1 = values[:L_star]  # Before L*\n",
        "    phase2 = values[L_star:]   # After L*\n",
        "    \n",
        "    # Trend analysis (linear regression slope)\n",
        "    if len(phase1) > 1:\n",
        "        slope1, _, r1, p1, _ = stats.linregress(range(len(phase1)), phase1)\n",
        "    else:\n",
        "        slope1, r1, p1 = 0, 0, 1\n",
        "    \n",
        "    if len(phase2) > 1:\n",
        "        slope2, _, r2, p2, _ = stats.linregress(range(len(phase2)), phase2)\n",
        "    else:\n",
        "        slope2, r2, p2 = 0, 0, 1\n",
        "    \n",
        "    # Check Bell Curve pattern\n",
        "    is_bell_curve = (slope1 > 0) and (slope2 < 0)\n",
        "    \n",
        "    # Measure \"flatness\" (variance of the profile)\n",
        "    profile_variance = np.var(values)\n",
        "    profile_range = np.max(values) - np.min(values)\n",
        "    \n",
        "    return {\n",
        "        'is_bell_curve': is_bell_curve,\n",
        "        'phase1_slope': slope1,\n",
        "        'phase1_r': r1,\n",
        "        'phase1_p': p1,\n",
        "        'phase2_slope': slope2,\n",
        "        'phase2_r': r2,\n",
        "        'phase2_p': p2,\n",
        "        'L_star': L_star,\n",
        "        'max_value': values[L_star],\n",
        "        'profile_variance': profile_variance,\n",
        "        'profile_range': profile_range\n",
        "    }\n",
        "\n",
        "# Analyze main anisotropy metric\n",
        "bell_analysis = analyze_bell_curve(intrinsic_dim_ratio, L_star)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"BELL CURVE ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nDetected L* (maximum anisotropy): Layer {L_star}\")\n",
        "print(f\"\\nPhase 1 (layers 0-{L_star}, before L*):\")\n",
        "print(f\"  Slope: {bell_analysis['phase1_slope']:.6f}\")\n",
        "print(f\"  Direction: {'Rising' if bell_analysis['phase1_slope'] > 0 else 'Falling'}\")\n",
        "print(f\"  R-value: {bell_analysis['phase1_r']:.4f}\")\n",
        "print(f\"\\nPhase 2 (layers {L_star}-{n_layers}, after L*):\")\n",
        "print(f\"  Slope: {bell_analysis['phase2_slope']:.6f}\")\n",
        "print(f\"  Direction: {'Rising' if bell_analysis['phase2_slope'] > 0 else 'Falling'}\")\n",
        "print(f\"  R-value: {bell_analysis['phase2_r']:.4f}\")\n",
        "print(f\"\\n\" + \"=\"*60)\n",
        "if bell_analysis['is_bell_curve']:\n",
        "    print(\"Result: Bell Curve pattern detected\")\n",
        "else:\n",
        "    print(\"Result: Pattern does NOT match Bell Curve\")\n",
        "    print(f\"   Phase 1: {'Rising' if bell_analysis['phase1_slope'] > 0 else 'Falling'}\")\n",
        "    print(f\"   Phase 2: {'Rising' if bell_analysis['phase2_slope'] > 0 else 'Falling'}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gauge Theory Test: Flatness Comparison\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"GAUGE THEORY TEST: Profile Flatness\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\nPrediction (RMSNorm Gauge Fixing):\")\n",
        "print(f\"  Gemma should have FLATTER profile than Pythia\")\n",
        "print(f\"  Because RMSNorm constrains to hypersphere S^{{d-1}}\")\n",
        "\n",
        "print(f\"\\nMeasured Profile Statistics:\")\n",
        "print(f\"  Profile Variance: {bell_analysis['profile_variance']:.6f}\")\n",
        "print(f\"  Profile Range: {bell_analysis['profile_range']:.4f}\")\n",
        "print(f\"  Max Anisotropy: {bell_analysis['max_value']:.4f}\")\n",
        "\n",
        "# Reference values from Pythia (hardcoded from previous experiment)\n",
        "PYTHIA_REF = {\n",
        "    'profile_range': 0.934,  # 0.994 - 0.060\n",
        "    'max_anisotropy': 0.994,\n",
        "    'L_star': 7\n",
        "}\n",
        "\n",
        "print(f\"\\nComparison with Pythia-6.9B (Reference):\")\n",
        "print(f\"  Pythia Profile Range: {PYTHIA_REF['profile_range']:.4f}\")\n",
        "print(f\"  Gemma Profile Range:  {bell_analysis['profile_range']:.4f}\")\n",
        "\n",
        "if bell_analysis['profile_range'] < PYTHIA_REF['profile_range']:\n",
        "    flatness_ratio = (PYTHIA_REF['profile_range'] - bell_analysis['profile_range']) / PYTHIA_REF['profile_range'] * 100\n",
        "    print(f\"\\n>>> GAUGE THEORY CONFIRMED: Gemma is {flatness_ratio:.1f}% flatter than Pythia\")\n",
        "else:\n",
        "    print(f\"\\n>>> Unexpected: Gemma is NOT flatter than Pythia\")\n",
        "    print(f\"    This may indicate RMSNorm effect is more subtle than predicted\")\n",
        "\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Eigenvalue Spectrum Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize eigenvalue spectrum at key layers\n",
        "key_layers = [0, L_star // 2, L_star, (L_star + n_layers) // 2, n_layers]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "colors = plt.cm.viridis(np.linspace(0, 1, len(key_layers)))\n",
        "\n",
        "for idx, layer in enumerate(key_layers):\n",
        "    eigenvalues = layer_metrics[layer]['eigenvalues']\n",
        "    normalized_eig = eigenvalues / eigenvalues.sum()\n",
        "    ax.semilogy(range(len(normalized_eig)), normalized_eig, \n",
        "                label=f'Layer {layer}', color=colors[idx], linewidth=2)\n",
        "\n",
        "ax.set_xlabel('Eigenvalue Index', fontsize=12)\n",
        "ax.set_ylabel('Normalized Eigenvalue (log scale)', fontsize=12)\n",
        "ax.set_title(f'{MODEL_NAME}: Eigenvalue Spectrum at Key Layers\\n(L* = {L_star})', fontsize=14)\n",
        "ax.legend(fontsize=11)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('eigenvalue_spectrum_gemma.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\">>> Figure saved as 'eigenvalue_spectrum_gemma.png'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Norm Distribution (RMSNorm-Specific Analysis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze how RMSNorm affects embedding norms across layers\n",
        "mean_norms = [layer_metrics[l]['mean_norm'] for l in layers]\n",
        "std_norms = [layer_metrics[l]['std_norm'] for l in layers]\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Plot 1: Mean Norm per Layer\n",
        "ax1 = axes[0]\n",
        "ax1.plot(layers, mean_norms, 'b-', linewidth=2, marker='o', markersize=4)\n",
        "ax1.fill_between(layers, \n",
        "                  [m - s for m, s in zip(mean_norms, std_norms)],\n",
        "                  [m + s for m, s in zip(mean_norms, std_norms)],\n",
        "                  alpha=0.3)\n",
        "ax1.axvline(x=L_star, color='red', linestyle='--', linewidth=2, label=f'L* = {L_star}')\n",
        "ax1.set_xlabel('Layer', fontsize=12)\n",
        "ax1.set_ylabel('Embedding Norm', fontsize=12)\n",
        "ax1.set_title('Mean Embedding Norm (+/- 1 std)', fontsize=14)\n",
        "ax1.legend(fontsize=11)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Norm Coefficient of Variation\n",
        "ax2 = axes[1]\n",
        "ax2.plot(layers, norm_cv, 'g-', linewidth=2, marker='s', markersize=4)\n",
        "ax2.axvline(x=L_star, color='red', linestyle='--', linewidth=2, label=f'L* = {L_star}')\n",
        "ax2.axhline(y=0, color='gray', linestyle=':', alpha=0.5)\n",
        "ax2.set_xlabel('Layer', fontsize=12)\n",
        "ax2.set_ylabel('CV = std/mean', fontsize=12)\n",
        "ax2.set_title('Norm Coefficient of Variation\\n(low = more uniform norms, RMSNorm effect)', fontsize=14)\n",
        "ax2.legend(fontsize=11)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle(f'{MODEL_NAME}: RMSNorm Effect on Embedding Norms', fontsize=16, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.savefig('norm_analysis_gemma.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\">>> Figure saved as 'norm_analysis_gemma.png'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Summary and Export"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Prepare summary - ensure all numpy types are converted to Python types\n",
        "summary = {\n",
        "    'model': MODEL_NAME,\n",
        "    'n_layers': int(n_layers),\n",
        "    'hidden_dim': int(hidden_dim),\n",
        "    'n_prompts': len(TEST_PROMPTS),\n",
        "    'normalization': 'RMSNorm',\n",
        "    'L_star_anisotropy': int(L_star),\n",
        "    'is_bell_curve': bool(bell_analysis['is_bell_curve']),\n",
        "    'phase1_slope': float(bell_analysis['phase1_slope']),\n",
        "    'phase2_slope': float(bell_analysis['phase2_slope']),\n",
        "    'phase1_r': float(bell_analysis['phase1_r']),\n",
        "    'phase2_r': float(bell_analysis['phase2_r']),\n",
        "    'profile_variance': float(bell_analysis['profile_variance']),\n",
        "    'profile_range': float(bell_analysis['profile_range']),\n",
        "    'max_anisotropy': float(bell_analysis['max_value']),\n",
        "    'intrinsic_dim_ratio': [float(x) for x in intrinsic_dim_ratio],\n",
        "    'effective_rank': [float(x) for x in effective_rank],\n",
        "    'avg_cos_sim': [float(x) for x in avg_cos_sim],\n",
        "    'norm_cv': [float(x) for x in norm_cv],\n",
        "    'gauge_theory_test': {\n",
        "        'pythia_profile_range': PYTHIA_REF['profile_range'],\n",
        "        'gemma_profile_range': float(bell_analysis['profile_range']),\n",
        "        'is_flatter': bool(bell_analysis['profile_range'] < PYTHIA_REF['profile_range'])\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save to JSON\n",
        "with open('anisotropy_results_gemma.json', 'w') as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nModel: {MODEL_NAME}\")\n",
        "print(f\"Layers: {n_layers}\")\n",
        "print(f\"Hidden dim: {hidden_dim}\")\n",
        "print(f\"Normalization: RMSNorm\")\n",
        "print(f\"\\nResults:\")\n",
        "print(f\"  L* (anisotropy max): Layer {L_star}\")\n",
        "print(f\"  Bell Curve: {'Confirmed' if bell_analysis['is_bell_curve'] else 'Not confirmed'}\")\n",
        "print(f\"  Profile Range: {bell_analysis['profile_range']:.4f}\")\n",
        "print(f\"  Max Anisotropy: {bell_analysis['max_value']:.4f}\")\n",
        "print(f\"\\nGauge Theory Test:\")\n",
        "print(f\"  Flatter than Pythia: {'YES' if summary['gauge_theory_test']['is_flatter'] else 'NO'}\")\n",
        "print(f\"\\nFiles saved:\")\n",
        "print(f\"  - anisotropy_profile_gemma.png\")\n",
        "print(f\"  - eigenvalue_spectrum_gemma.png\")\n",
        "print(f\"  - norm_analysis_gemma.png\")\n",
        "print(f\"  - anisotropy_results_gemma.json\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create ZIP archive with all results\n",
        "import zipfile\n",
        "from datetime import datetime\n",
        "\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "zip_filename = f\"anisotropy_results_gemma_{timestamp}.zip\"\n",
        "\n",
        "with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
        "    zipf.write('anisotropy_profile_gemma.png')\n",
        "    zipf.write('eigenvalue_spectrum_gemma.png')\n",
        "    zipf.write('norm_analysis_gemma.png')\n",
        "    zipf.write('anisotropy_results_gemma.json')\n",
        "\n",
        "print(f\">>> Created: {zip_filename}\")\n",
        "print(f\"  Contents: 3 PNG figures + 1 JSON data file\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Interpretation\n",
        "\n",
        "### Gauge Theory Prediction\n",
        "\n",
        "From Gemini Deep Research:\n",
        "\n",
        "> \"Normalization layers act as **gauge fixing** operations.\"\n",
        ">\n",
        "> | Architecture | Norm | Effect |\n",
        "> |-------------|------|--------|\n",
        "> | Pythia | LayerNorm | Radial freedom preserved -> clear inversion |\n",
        "> | Gemma | RMSNorm | Spherical geometry enforced -> subtle/no inversion |\n",
        "\n",
        "### Why RMSNorm Flattens the Profile\n",
        "\n",
        "1. **RMSNorm Formula:** $\\text{RMSNorm}(x) = \\frac{x}{\\sqrt{\\frac{1}{d}\\sum_i x_i^2}}$\n",
        "\n",
        "2. **Effect:** Projects all embeddings onto hypersphere $S^{d-1}$\n",
        "\n",
        "3. **Consequence:** Only angular diffusion possible (Connection Laplacian $\\Delta_\\nabla$)\n",
        "\n",
        "4. **Result:** The compression/expansion dynamics are constrained, leading to a flatter anisotropy profile\n",
        "\n",
        "### Multi-Model Comparison Summary\n",
        "\n",
        "| Model | Normalization | Profile Shape | L* | Interpretation |\n",
        "|-------|---------------|---------------|----|-----------------|\n",
        "| Pythia-6.9B | LayerNorm | Strong Bell Curve | 7 | Clear phase transitions |\n",
        "| Gemma-2B | RMSNorm | Flatter | TBD | Gauge-fixed dynamics |\n",
        "\n",
        "### Implications for Paper #3\n",
        "\n",
        "If Gemma shows a flatter profile:\n",
        "1. Validates Gauge Theory interpretation\n",
        "2. Explains why Paper #2 saw \"no clear inversion\" in Gemma\n",
        "3. Suggests normalization choice affects sheaf topology"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Download Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download all results\n",
        "from google.colab import files\n",
        "\n",
        "print(\"Downloading result files...\")\n",
        "print()\n",
        "\n",
        "# Download ZIP (easiest - single file with everything)\n",
        "print(f\"1. ZIP Archive: {zip_filename}\")\n",
        "files.download(zip_filename)\n",
        "\n",
        "# Also offer individual files\n",
        "print(\"\\n2. Individual files:\")\n",
        "print(\"   - anisotropy_profile_gemma.png\")\n",
        "files.download('anisotropy_profile_gemma.png')\n",
        "print(\"   - eigenvalue_spectrum_gemma.png\")\n",
        "files.download('eigenvalue_spectrum_gemma.png')\n",
        "print(\"   - norm_analysis_gemma.png\")\n",
        "files.download('norm_analysis_gemma.png')\n",
        "print(\"   - anisotropy_results_gemma.json\")\n",
        "files.download('anisotropy_results_gemma.json')\n",
        "\n",
        "print(\"\\n>>> All files downloaded!\")\n",
        "print(f\"\\nTIP: The ZIP file ({zip_filename}) contains all results in one download.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
