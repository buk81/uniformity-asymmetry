{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLaMA 2 vs LLaMA 3.1: Long-Context Hypothesis Test\n",
    "\n",
    "**Date:** 2026-01-05\n",
    "**Goal:** Prove that long-context causes contraction\n",
    "\n",
    "## Hypothesis\n",
    "\n",
    "```\n",
    "If Long-Context Dampening is correct:\n",
    "\n",
    "LLaMA 2 (4k context, RoPE θ=10,000)   → Should EXPAND (like Mistral)\n",
    "LLaMA 3.1 (128k context, RoPE θ=500,000) → CONTRACTS (confirmed)\n",
    "\n",
    "If LLaMA 2 EXPANDS → SMOKING GUN for Long-Context Hypothesis!\n",
    "If LLaMA 2 CONTRACTS → Other factor (family-specific?)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 0: HuggingFace Login\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Login for gated models\n",
    "login()\n",
    "\n",
    "print(\"HuggingFace login complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Results directory\n",
    "RESULTS_DIR = './Results'\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "print(\"Setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Model Definitions\n",
    "\n",
    "MODELS_TO_TEST = {\n",
    "    'llama2-7b': {\n",
    "        'hf_name': 'meta-llama/Llama-2-7b-hf',\n",
    "        'context_length': 4096,\n",
    "        'expected_rope_theta': 10000,\n",
    "        'prediction': 'EXPAND (like Mistral)',\n",
    "        'family': 'LLaMA 2'\n",
    "    },\n",
    "    'llama3.1-8b': {\n",
    "        'hf_name': 'meta-llama/Llama-3.1-8B',\n",
    "        'context_length': 128000,\n",
    "        'expected_rope_theta': 500000,\n",
    "        'prediction': 'CONTRACT (confirmed: 0.48x)',\n",
    "        'family': 'LLaMA 3.1'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Reference values from previous experiments\n",
    "REFERENCE = {\n",
    "    'llama3.1-8b': {\n",
    "        'last_gain': 0.48,\n",
    "        'w_u_spectral': 94.61,\n",
    "        'rope_theta': 500000,\n",
    "        'behavior': 'CONTRACTS'\n",
    "    },\n",
    "    'mistral-7b': {\n",
    "        'last_gain': 1.37,\n",
    "        'w_u_spectral': 16.14,\n",
    "        'rope_theta': 10000,\n",
    "        'behavior': 'EXPANDS'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Models to test:\")\n",
    "for name, info in MODELS_TO_TEST.items():\n",
    "    print(f\"  {name}: context={info['context_length']:,}, prediction={info['prediction']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Residual Stream Analyzer\n",
    "\n",
    "class ResidualStreamAnalyzer:\n",
    "    \"\"\"Analyze residual stream dynamics layer by layer.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = next(model.parameters()).device\n",
    "        \n",
    "    def get_residual_norms(self, text=\"The quick brown fox jumps over the lazy dog.\"):\n",
    "        \"\"\"Get L2 norm of residual stream at each layer.\"\"\"\n",
    "        inputs = self.tokenizer(text, return_tensors='pt').to(self.device)\n",
    "        \n",
    "        norms = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs, output_hidden_states=True)\n",
    "            hidden_states = outputs.hidden_states\n",
    "            \n",
    "            for i, hs in enumerate(hidden_states):\n",
    "                # Mean norm across sequence positions\n",
    "                norm = hs.float().norm(dim=-1).mean().item()\n",
    "                norms.append(norm)\n",
    "        \n",
    "        return norms\n",
    "    \n",
    "    def compute_gains(self, norms):\n",
    "        \"\"\"Compute layer-wise gains from norms.\"\"\"\n",
    "        gains = []\n",
    "        for i in range(1, len(norms)):\n",
    "            gain = norms[i] / norms[i-1] if norms[i-1] > 0 else 1.0\n",
    "            gains.append(gain)\n",
    "        return gains\n",
    "    \n",
    "    def analyze(self, text=\"The quick brown fox jumps over the lazy dog.\"):\n",
    "        \"\"\"Full analysis of residual stream.\"\"\"\n",
    "        norms = self.get_residual_norms(text)\n",
    "        gains = self.compute_gains(norms)\n",
    "        \n",
    "        # Compute statistics\n",
    "        cumulative = 1.0\n",
    "        for g in gains:\n",
    "            cumulative *= g\n",
    "        \n",
    "        results = {\n",
    "            'norms': norms,\n",
    "            'gains': gains,\n",
    "            'embedding_norm': norms[0],\n",
    "            'final_norm': norms[-1],\n",
    "            'initial_gain': gains[0] if gains else 1.0,\n",
    "            'last_gain': gains[-1] if gains else 1.0,\n",
    "            'last_expands': gains[-1] > 1.0 if gains else False,\n",
    "            'cumulative_energy': cumulative,\n",
    "            'num_layers': len(gains),\n",
    "            'contracting_layers': sum(1 for g in gains if g < 1.0),\n",
    "            'expanding_layers': sum(1 for g in gains if g >= 1.0)\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "\n",
    "print(\"ResidualStreamAnalyzer defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: W_U Analyzer\n",
    "\n",
    "def analyze_unembedding(model):\n",
    "    \"\"\"Analyze the unembedding matrix (lm_head).\"\"\"\n",
    "    lm_head = model.lm_head.weight.data.float()\n",
    "    \n",
    "    results = {\n",
    "        'shape': list(lm_head.shape),\n",
    "        'vocab_size': lm_head.shape[0],\n",
    "        'hidden_dim': lm_head.shape[1],\n",
    "        'frobenius_norm': torch.linalg.norm(lm_head, ord='fro').item(),\n",
    "        'spectral_norm': torch.linalg.norm(lm_head, ord=2).item(),\n",
    "        'mean_row_norm': torch.linalg.norm(lm_head, dim=1).mean().item()\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def get_rope_theta(config):\n",
    "    \"\"\"Extract RoPE theta from config.\"\"\"\n",
    "    rope_theta = getattr(config, 'rope_theta', None)\n",
    "    rope_scaling = getattr(config, 'rope_scaling', None)\n",
    "    max_position = getattr(config, 'max_position_embeddings', None)\n",
    "    \n",
    "    return {\n",
    "        'rope_theta': rope_theta,\n",
    "        'rope_scaling': rope_scaling,\n",
    "        'max_position': max_position\n",
    "    }\n",
    "\n",
    "print(\"Analyzers defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Test LLaMA 2\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TESTING: LLaMA 2 (7B)\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nPrediction: Should EXPAND if Long-Context Hypothesis is correct\")\n",
    "print(\"            (4k context, standard RoPE theta ~10,000)\\n\")\n",
    "\n",
    "model_name = 'llama2-7b'\n",
    "model_info = MODELS_TO_TEST[model_name]\n",
    "\n",
    "# Load model\n",
    "print(\"Loading model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_info['hf_name'],\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto',\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_info['hf_name'])\n",
    "config = AutoConfig.from_pretrained(model_info['hf_name'])\n",
    "\n",
    "print(f\"Model loaded: {model_info['hf_name']}\")\n",
    "print(f\"Layers: {config.num_hidden_layers}\")\n",
    "\n",
    "# Residual stream analysis\n",
    "print(\"\\nAnalyzing residual stream...\")\n",
    "analyzer = ResidualStreamAnalyzer(model, tokenizer)\n",
    "residual_results = analyzer.analyze()\n",
    "\n",
    "# W_U analysis\n",
    "print(\"Analyzing W_U (unembedding)...\")\n",
    "wu_results = analyze_unembedding(model)\n",
    "\n",
    "# RoPE analysis\n",
    "rope_results = get_rope_theta(config)\n",
    "\n",
    "# Combine results\n",
    "llama2_results = {\n",
    "    'model': model_name,\n",
    "    'hf_name': model_info['hf_name'],\n",
    "    'num_layers': config.num_hidden_layers,\n",
    "    'residual': residual_results,\n",
    "    'w_u': wu_results,\n",
    "    'rope': rope_results\n",
    "}\n",
    "\n",
    "# Print key results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LLaMA 2 RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nResidual Stream:\")\n",
    "print(f\"  Embedding Norm: {residual_results['embedding_norm']:.2f}\")\n",
    "print(f\"  Final Norm: {residual_results['final_norm']:.2f}\")\n",
    "print(f\"  Initial Gain: {residual_results['initial_gain']:.2f}x\")\n",
    "print(f\"  Last Gain: {residual_results['last_gain']:.4f}x\")\n",
    "print(f\"  Last Expands: {residual_results['last_expands']}\")\n",
    "print(f\"  Cumulative Energy: {residual_results['cumulative_energy']:.2f}\")\n",
    "\n",
    "print(f\"\\nW_U (Unembedding):\")\n",
    "print(f\"  Shape: {wu_results['shape']}\")\n",
    "print(f\"  Spectral Norm: {wu_results['spectral_norm']:.2f}\")\n",
    "\n",
    "print(f\"\\nRoPE:\")\n",
    "print(f\"  Theta: {rope_results['rope_theta']}\")\n",
    "print(f\"  Scaling: {rope_results['rope_scaling']}\")\n",
    "print(f\"  Max Position: {rope_results['max_position']}\")\n",
    "\n",
    "# Clean up\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\nLLaMA 2 analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Test LLaMA 3.1 (for direct comparison)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TESTING: LLaMA 3.1 (8B)\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nExpected: CONTRACTS (confirmed: 0.48x)\")\n",
    "print(\"          (128k context, RoPE theta 500,000)\\n\")\n",
    "\n",
    "model_name = 'llama3.1-8b'\n",
    "model_info = MODELS_TO_TEST[model_name]\n",
    "\n",
    "# Load model\n",
    "print(\"Loading model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_info['hf_name'],\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto',\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_info['hf_name'])\n",
    "config = AutoConfig.from_pretrained(model_info['hf_name'])\n",
    "\n",
    "print(f\"Model loaded: {model_info['hf_name']}\")\n",
    "print(f\"Layers: {config.num_hidden_layers}\")\n",
    "\n",
    "# Residual stream analysis\n",
    "print(\"\\nAnalyzing residual stream...\")\n",
    "analyzer = ResidualStreamAnalyzer(model, tokenizer)\n",
    "residual_results = analyzer.analyze()\n",
    "\n",
    "# W_U analysis\n",
    "print(\"Analyzing W_U (unembedding)...\")\n",
    "wu_results = analyze_unembedding(model)\n",
    "\n",
    "# RoPE analysis\n",
    "rope_results = get_rope_theta(config)\n",
    "\n",
    "# Combine results\n",
    "llama31_results = {\n",
    "    'model': model_name,\n",
    "    'hf_name': model_info['hf_name'],\n",
    "    'num_layers': config.num_hidden_layers,\n",
    "    'residual': residual_results,\n",
    "    'w_u': wu_results,\n",
    "    'rope': rope_results\n",
    "}\n",
    "\n",
    "# Print key results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LLaMA 3.1 RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nResidual Stream:\")\n",
    "print(f\"  Embedding Norm: {residual_results['embedding_norm']:.2f}\")\n",
    "print(f\"  Final Norm: {residual_results['final_norm']:.2f}\")\n",
    "print(f\"  Initial Gain: {residual_results['initial_gain']:.2f}x\")\n",
    "print(f\"  Last Gain: {residual_results['last_gain']:.4f}x\")\n",
    "print(f\"  Last Expands: {residual_results['last_expands']}\")\n",
    "print(f\"  Cumulative Energy: {residual_results['cumulative_energy']:.2f}\")\n",
    "\n",
    "print(f\"\\nW_U (Unembedding):\")\n",
    "print(f\"  Shape: {wu_results['shape']}\")\n",
    "print(f\"  Spectral Norm: {wu_results['spectral_norm']:.2f}\")\n",
    "\n",
    "print(f\"\\nRoPE:\")\n",
    "print(f\"  Theta: {rope_results['rope_theta']}\")\n",
    "print(f\"  Scaling: {rope_results['rope_scaling']}\")\n",
    "print(f\"  Max Position: {rope_results['max_position']}\")\n",
    "\n",
    "# Clean up\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\nLLaMA 3.1 analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: HYPOTHESIS TEST - The Critical Comparison\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LONG-CONTEXT HYPOTHESIS TEST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"COMPARISON TABLE\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(\"\\n| Model | Context | RoPE Theta | Last Gain | W_U σ_max | Behavior |\")\n",
    "print(\"|-------|---------|------------|-----------|-----------|----------|\")\n",
    "\n",
    "# LLaMA 2\n",
    "l2_ctx = MODELS_TO_TEST['llama2-7b']['context_length']\n",
    "l2_theta = llama2_results['rope']['rope_theta']\n",
    "l2_gain = llama2_results['residual']['last_gain']\n",
    "l2_wu = llama2_results['w_u']['spectral_norm']\n",
    "l2_behavior = \"EXPANDS\" if l2_gain > 1.0 else \"CONTRACTS\"\n",
    "print(f\"| LLaMA 2 | {l2_ctx:,} | {l2_theta:,} | {l2_gain:.4f}x | {l2_wu:.2f} | {l2_behavior} |\")\n",
    "\n",
    "# LLaMA 3.1\n",
    "l3_ctx = MODELS_TO_TEST['llama3.1-8b']['context_length']\n",
    "l3_theta = llama31_results['rope']['rope_theta']\n",
    "l3_gain = llama31_results['residual']['last_gain']\n",
    "l3_wu = llama31_results['w_u']['spectral_norm']\n",
    "l3_behavior = \"EXPANDS\" if l3_gain > 1.0 else \"CONTRACTS\"\n",
    "print(f\"| LLaMA 3.1 | {l3_ctx:,} | {l3_theta:,.0f} | {l3_gain:.4f}x | {l3_wu:.2f} | {l3_behavior} |\")\n",
    "\n",
    "# Mistral reference\n",
    "print(f\"| Mistral (ref) | 8,192 | 10,000 | 1.37x | 16.14 | EXPANDS |\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"HYPOTHESIS EVALUATION\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Compute ratios\n",
    "theta_ratio = l3_theta / l2_theta if l2_theta else 0\n",
    "gain_ratio = l3_gain / l2_gain if l2_gain else 0\n",
    "wu_ratio = l3_wu / l2_wu if l2_wu else 0\n",
    "\n",
    "print(f\"\\nRoPE Theta Ratio (LLaMA 3.1 / LLaMA 2): {theta_ratio:.1f}x\")\n",
    "print(f\"Last Gain Ratio (LLaMA 3.1 / LLaMA 2): {gain_ratio:.2f}\")\n",
    "print(f\"W_U Ratio (LLaMA 3.1 / LLaMA 2): {wu_ratio:.2f}x\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Determine verdict\n",
    "if l2_gain > 1.0 and l3_gain < 1.0:\n",
    "    print(\"VERDICT: HYPOTHESIS CONFIRMED!\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nLLaMA 2 (short context) EXPANDS\")\n",
    "    print(\"LLaMA 3.1 (long context) CONTRACTS\")\n",
    "    print(\"\\n--> Long-context causes contraction!\")\n",
    "    print(\"--> This is DELIBERATE ENGINEERING for 128k stability!\")\n",
    "    verdict = \"CONFIRMED\"\n",
    "elif l2_gain < 1.0 and l3_gain < 1.0:\n",
    "    print(\"VERDICT: HYPOTHESIS REJECTED\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nBOTH LLaMA 2 and LLaMA 3.1 CONTRACT\")\n",
    "    print(\"\\n--> Contraction is LLaMA FAMILY trait, not context-dependent!\")\n",
    "    print(\"--> Need to investigate LLaMA-specific architecture\")\n",
    "    verdict = \"REJECTED - Family Trait\"\n",
    "elif l2_gain > 1.0 and l3_gain > 1.0:\n",
    "    print(\"VERDICT: UNEXPECTED - Both Expand\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nBOTH models EXPAND - contradicts previous LLaMA 3.1 finding\")\n",
    "    print(\"\\n--> Check for measurement error\")\n",
    "    verdict = \"UNEXPECTED\"\n",
    "else:\n",
    "    print(\"VERDICT: INVERTED\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nLLaMA 2 CONTRACTS, LLaMA 3.1 EXPANDS - opposite of hypothesis\")\n",
    "    verdict = \"INVERTED\"\n",
    "\n",
    "# Store verdict\n",
    "hypothesis_test = {\n",
    "    'llama2_expands': l2_gain > 1.0,\n",
    "    'llama31_expands': l3_gain > 1.0,\n",
    "    'theta_ratio': theta_ratio,\n",
    "    'gain_ratio': gain_ratio,\n",
    "    'wu_ratio': wu_ratio,\n",
    "    'verdict': verdict\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Visualization\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Colors\n",
    "colors = {'llama2': '#3498db', 'llama3.1': '#e74c3c', 'mistral': '#2ecc71'}\n",
    "\n",
    "# Plot 1: Last Layer Gain Comparison\n",
    "ax1 = axes[0, 0]\n",
    "models = ['LLaMA 2\\n(4k ctx)', 'LLaMA 3.1\\n(128k ctx)', 'Mistral\\n(8k ctx)']\n",
    "gains = [l2_gain, l3_gain, 1.37]\n",
    "bars = ax1.bar(models, gains, color=[colors['llama2'], colors['llama3.1'], colors['mistral']])\n",
    "ax1.axhline(y=1.0, color='gray', linestyle='--', alpha=0.7, label='Expansion Threshold')\n",
    "ax1.set_ylabel('Last Layer Gain')\n",
    "ax1.set_title('Last Layer Gain Comparison\\n(>1 = Expand, <1 = Contract)')\n",
    "for bar, val in zip(bars, gains):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "             f'{val:.2f}x', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot 2: RoPE Theta Comparison\n",
    "ax2 = axes[0, 1]\n",
    "thetas = [l2_theta, l3_theta, 10000]\n",
    "bars = ax2.bar(models, thetas, color=[colors['llama2'], colors['llama3.1'], colors['mistral']])\n",
    "ax2.set_ylabel('RoPE Theta')\n",
    "ax2.set_title('RoPE Theta Comparison\\n(Higher = Longer Context Support)')\n",
    "ax2.set_yscale('log')\n",
    "for bar, val in zip(bars, thetas):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() * 1.1, \n",
    "             f'{val:,.0f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Plot 3: W_U Spectral Norm\n",
    "ax3 = axes[1, 0]\n",
    "wu_norms = [l2_wu, l3_wu, 16.14]\n",
    "bars = ax3.bar(models, wu_norms, color=[colors['llama2'], colors['llama3.1'], colors['mistral']])\n",
    "ax3.set_ylabel('W_U Spectral Norm (σ_max)')\n",
    "ax3.set_title('Unembedding Matrix Spectral Norm\\n(Static Amplification Factor)')\n",
    "for bar, val in zip(bars, wu_norms):\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "             f'{val:.1f}', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Plot 4: Layer-wise Gains\n",
    "ax4 = axes[1, 1]\n",
    "ax4.plot(llama2_results['residual']['gains'], 'b-', label='LLaMA 2', linewidth=2)\n",
    "ax4.plot(llama31_results['residual']['gains'], 'r-', label='LLaMA 3.1', linewidth=2)\n",
    "ax4.axhline(y=1.0, color='gray', linestyle='--', alpha=0.7)\n",
    "ax4.set_xlabel('Layer')\n",
    "ax4.set_ylabel('Gain')\n",
    "ax4.set_title('Layer-wise Gains\\n(Last layer highlighted)')\n",
    "ax4.legend()\n",
    "# Highlight last layer\n",
    "ax4.axvline(x=len(llama2_results['residual']['gains'])-1, color='blue', linestyle=':', alpha=0.5)\n",
    "ax4.axvline(x=len(llama31_results['residual']['gains'])-1, color='red', linestyle=':', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_DIR}/llama2_vs_llama31_hypothesis_test.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nVisualization saved to {RESULTS_DIR}/llama2_vs_llama31_hypothesis_test.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Save Results\n",
    "\n",
    "final_results = {\n",
    "    'experiment': 'LLaMA 2 vs LLaMA 3.1 - Long-Context Hypothesis Test',\n",
    "    'date': '2026-01-05',\n",
    "    'hypothesis': 'Long-context models require dampening (contraction) for numerical stability',\n",
    "    'models': {\n",
    "        'llama2-7b': llama2_results,\n",
    "        'llama3.1-8b': llama31_results\n",
    "    },\n",
    "    'hypothesis_test': hypothesis_test,\n",
    "    'references': REFERENCE\n",
    "}\n",
    "\n",
    "output_path = f'{RESULTS_DIR}/llama2_vs_llama31_hypothesis_test.json'\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(final_results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"Results saved to {output_path}\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPERIMENT COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Download Results\n",
    "\n",
    "from google.colab import files\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DOWNLOADING RESULTS...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Download JSON\n",
    "json_path = f'{RESULTS_DIR}/llama2_vs_llama31_hypothesis_test.json'\n",
    "if os.path.exists(json_path):\n",
    "    print(f\"\\nDownloading: {json_path}\")\n",
    "    files.download(json_path)\n",
    "\n",
    "# Download PNG\n",
    "png_path = f'{RESULTS_DIR}/llama2_vs_llama31_hypothesis_test.png'\n",
    "if os.path.exists(png_path):\n",
    "    print(f\"Downloading: {png_path}\")\n",
    "    files.download(png_path)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DOWNLOAD COMPLETE!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Hypothesis\n",
    "Long-context models (128k) require dampening (contraction) for numerical stability.\n",
    "\n",
    "### Prediction\n",
    "- LLaMA 2 (4k context) → Should EXPAND\n",
    "- LLaMA 3.1 (128k context) → Should CONTRACT (confirmed)\n",
    "\n",
    "### If CONFIRMED\n",
    "This proves that contraction is a **deliberate engineering choice** for long-context stability, not a model weakness.\n",
    "\n",
    "### If REJECTED\n",
    "Contraction is a **LLaMA family trait**, independent of context length. Would need to investigate LLaMA-specific architecture."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
