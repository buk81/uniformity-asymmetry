{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {"id": "cell-0"},
      "source": [
        "# L* Cross-Heritage Validation (v2 - Fixed)\n",
        "\n",
        "**Paper #3: Thermodynamic Constraints in Transformer Architectures**\n",
        "\n",
        "**Author:** Davide D'Elia\n",
        "\n",
        "**Date:** 2026-01-06\n",
        "\n",
        "---\n",
        "\n",
        "## Purpose\n",
        "\n",
        "Validate the L* transition point formula across different **training heritages** (labs).\n",
        "\n",
        "## The Formula\n",
        "\n",
        "```\n",
        "L* = L × (0.11 + 0.012×L + 4.9/H)\n",
        "```\n",
        "\n",
        "## v2 Fixes\n",
        "\n",
        "- Fixed attention output extraction (use `attn_implementation='eager'`)\n",
        "- Fixed BLOOM architecture path (`self_attention` not `attn`)\n",
        "- Added model config debugging\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {"id": "cell-1"},
      "source": [
        "# Cell 1: Setup\n",
        "!pip install -q transformers accelerate scipy seaborn pandas huggingface_hub\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
        "from tqdm.auto import tqdm\n",
        "import json\n",
        "import gc\n",
        "import os\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configure visualization\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_context(\"paper\", font_scale=1.2)\n",
        "\n",
        "# Global timestamp\n",
        "TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "print(f\"Session timestamp: {TIMESTAMP}\")\n",
        "\n",
        "# HF TOKEN\n",
        "HF_TOKEN = None\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "    if HF_TOKEN:\n",
        "        print(f\"HF_TOKEN loaded\")\n",
        "except:\n",
        "    HF_TOKEN = os.environ.get('HF_TOKEN')\n",
        "\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "cell-2"},
      "source": [
        "## 2. Model Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {"id": "cell-3"},
      "source": [
        "# Models to test\n",
        "MODELS_TO_TEST = {\n",
        "    \"EleutherAI/pythia-160m\": {\"lab\": \"EleutherAI\", \"L\": 12, \"H\": 12, \"expected\": \"DAMPEN\"},\n",
        "    \"EleutherAI/pythia-410m\": {\"lab\": \"EleutherAI\", \"L\": 24, \"H\": 16, \"expected\": \"DAMPEN\"},\n",
        "    \"facebook/opt-125m\": {\"lab\": \"Meta\", \"L\": 12, \"H\": 12, \"expected\": \"EXPAND\"},\n",
        "    \"facebook/opt-350m\": {\"lab\": \"Meta\", \"L\": 24, \"H\": 16, \"expected\": \"EXPAND\"},\n",
        "    \"bigscience/bloom-560m\": {\"lab\": \"BigScience\", \"L\": 24, \"H\": 16, \"expected\": \"EXPAND\"},\n",
        "    \"openai-community/gpt2\": {\"lab\": \"OpenAI\", \"L\": 12, \"H\": 12, \"expected\": \"EXPAND\"},\n",
        "}\n",
        "\n",
        "TEST_PROMPTS = [\n",
        "    \"The capital of France is Paris, which is known for the Eiffel Tower.\",\n",
        "    \"In mathematics, the derivative of x squared equals two times x.\",\n",
        "    \"Climate change affects global temperatures and weather patterns significantly.\",\n",
        "    \"The quick brown fox jumps over the lazy dog near the riverbank.\",\n",
        "    \"Once upon a time in a land far away, there lived a wise old king.\",\n",
        "]\n",
        "\n",
        "def predict_l_star_v3(L, H):\n",
        "    \"\"\"L* = L × (0.11 + 0.012×L + 4.9/H)\"\"\"\n",
        "    return L * (0.11 + 0.012 * L + 4.9 / H)\n",
        "\n",
        "print(f\"Models: {len(MODELS_TO_TEST)}, Prompts: {len(TEST_PROMPTS)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "cell-4"},
      "source": [
        "## 3. Architecture-Aware W_V Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {"id": "cell-5"},
      "source": [
        "def get_architecture_info(model):\n",
        "    \"\"\"Detect model architecture type.\"\"\"\n",
        "    if hasattr(model, 'gpt_neox'):\n",
        "        return 'pythia'\n",
        "    elif hasattr(model, 'model') and hasattr(model.model, 'decoder'):\n",
        "        return 'opt'\n",
        "    elif hasattr(model, 'transformer') and hasattr(model.transformer, 'h'):\n",
        "        # Check if it's BLOOM or GPT-2\n",
        "        layer = model.transformer.h[0]\n",
        "        if hasattr(layer, 'self_attention'):\n",
        "            return 'bloom'\n",
        "        elif hasattr(layer, 'attn'):\n",
        "            return 'gpt2'\n",
        "    elif hasattr(model, 'model') and hasattr(model.model, 'layers'):\n",
        "        return 'llama'\n",
        "    return 'unknown'\n",
        "\n",
        "\n",
        "def get_layers(model, arch):\n",
        "    \"\"\"Get transformer layers based on architecture.\"\"\"\n",
        "    if arch == 'pythia':\n",
        "        return model.gpt_neox.layers\n",
        "    elif arch == 'opt':\n",
        "        return model.model.decoder.layers\n",
        "    elif arch in ['bloom', 'gpt2']:\n",
        "        return model.transformer.h\n",
        "    elif arch == 'llama':\n",
        "        return model.model.layers\n",
        "    return []\n",
        "\n",
        "\n",
        "def get_W_V(model, arch, layer_idx):\n",
        "    \"\"\"Extract W_V matrix for a specific layer.\"\"\"\n",
        "    try:\n",
        "        layers = get_layers(model, arch)\n",
        "        layer = layers[layer_idx]\n",
        "        \n",
        "        if arch == 'pythia':\n",
        "            # Pythia: fused QKV\n",
        "            qkv = layer.attention.query_key_value.weight.data.float()\n",
        "            d = qkv.shape[0] // 3\n",
        "            return qkv[2*d:, :].cpu()\n",
        "            \n",
        "        elif arch == 'opt':\n",
        "            # OPT: separate v_proj\n",
        "            return layer.self_attn.v_proj.weight.data.float().cpu()\n",
        "            \n",
        "        elif arch == 'bloom':\n",
        "            # BLOOM: fused QKV in self_attention\n",
        "            qkv = layer.self_attention.query_key_value.weight.data.float()\n",
        "            d = qkv.shape[0] // 3\n",
        "            return qkv[2*d:, :].cpu()\n",
        "            \n",
        "        elif arch == 'gpt2':\n",
        "            # GPT-2: c_attn contains Q, K, V concatenated\n",
        "            c_attn = layer.attn.c_attn.weight.data.float()\n",
        "            d = c_attn.shape[1] // 3\n",
        "            return c_attn[:, 2*d:].T.cpu()\n",
        "            \n",
        "        elif arch == 'llama':\n",
        "            return layer.self_attn.v_proj.weight.data.float().cpu()\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"    W_V extraction error (layer {layer_idx}): {e}\")\n",
        "    \n",
        "    return None\n",
        "\n",
        "print(\"Architecture-aware functions defined.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "cell-6"},
      "source": [
        "## 4. Trace Computation with Attention Fix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {"id": "cell-7"},
      "source": [
        "def compute_traces_and_l_star(model, tokenizer, prompt, arch, device='cuda'):\n",
        "    \"\"\"\n",
        "    Compute Sheaf Laplacian trace for each layer.\n",
        "    Uses efficient formula: Tr(L_F) = (sum(A) - n) * ||W_V||_F^2\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    # Tokenize\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    n_tokens = inputs[\"input_ids\"].shape[1]\n",
        "    \n",
        "    # Forward pass with attention output\n",
        "    with torch.no_grad():\n",
        "        outputs = model(\n",
        "            **inputs,\n",
        "            output_attentions=True,\n",
        "            output_hidden_states=True\n",
        "        )\n",
        "    \n",
        "    attentions = outputs.attentions\n",
        "    \n",
        "    # Debug: check if attentions are available\n",
        "    if attentions is None:\n",
        "        print(f\"    WARNING: attentions is None!\")\n",
        "        return None, None\n",
        "    \n",
        "    n_layers = len(attentions)\n",
        "    traces = []\n",
        "    \n",
        "    for layer_idx in range(n_layers):\n",
        "        # Get attention matrix (average over heads)\n",
        "        attn = attentions[layer_idx]\n",
        "        if attn is None:\n",
        "            traces.append(0.0)\n",
        "            continue\n",
        "            \n",
        "        A = attn[0].float().mean(dim=0).cpu()  # (seq, seq)\n",
        "        \n",
        "        # Get W_V\n",
        "        W_V = get_W_V(model, arch, layer_idx)\n",
        "        \n",
        "        # Compute trace\n",
        "        if W_V is not None:\n",
        "            A_sum = A.sum().item()\n",
        "            W_V_frob_sq = (W_V ** 2).sum().item()\n",
        "            trace = abs((A_sum - n_tokens) * W_V_frob_sq)\n",
        "        else:\n",
        "            # Fallback: just attention sum\n",
        "            A_sum = A.sum().item()\n",
        "            trace = abs(A_sum - n_tokens)\n",
        "        \n",
        "        traces.append(trace)\n",
        "    \n",
        "    # Find L* = layer of maximum trace gradient\n",
        "    traces_arr = np.array(traces)\n",
        "    if len(traces_arr) > 1:\n",
        "        gradients = np.abs(np.diff(traces_arr))\n",
        "        L_star = int(np.argmax(gradients)) + 1\n",
        "    else:\n",
        "        L_star = 0\n",
        "    \n",
        "    return traces, L_star\n",
        "\n",
        "print(\"Trace computation function defined.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "cell-8"},
      "source": [
        "## 5. Run Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {"id": "cell-9"},
      "source": [
        "results = []\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"L* CROSS-HERITAGE VALIDATION (v2)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for model_name, config in tqdm(MODELS_TO_TEST.items(), desc=\"Models\"):\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Model: {model_name}\")\n",
        "    print(f\"Lab: {config['lab']} | L={config['L']} | H={config['H']}\")\n",
        "    \n",
        "    try:\n",
        "        # Load tokenizer\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\n",
        "            model_name,\n",
        "            token=HF_TOKEN if HF_TOKEN else None\n",
        "        )\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "        \n",
        "        # Load model with attention output enabled\n",
        "        # KEY FIX: Use attn_implementation='eager' for attention output\n",
        "        print(\"  Loading model...\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\",\n",
        "            token=HF_TOKEN if HF_TOKEN else None,\n",
        "            trust_remote_code=True,\n",
        "            attn_implementation=\"eager\",  # KEY FIX!\n",
        "            output_attentions=True,\n",
        "            output_hidden_states=True\n",
        "        )\n",
        "        model.eval()\n",
        "        \n",
        "        # Detect architecture\n",
        "        arch = get_architecture_info(model)\n",
        "        print(f\"  Architecture: {arch}\")\n",
        "        \n",
        "        # Test prompts\n",
        "        all_l_stars = []\n",
        "        all_traces = []\n",
        "        \n",
        "        for i, prompt in enumerate(TEST_PROMPTS):\n",
        "            traces, l_star = compute_traces_and_l_star(model, tokenizer, prompt, arch)\n",
        "            if traces is not None:\n",
        "                all_traces.append(traces)\n",
        "                all_l_stars.append(l_star)\n",
        "                print(f\"    Prompt {i+1}: L* = {l_star}\")\n",
        "            else:\n",
        "                print(f\"    Prompt {i+1}: FAILED\")\n",
        "        \n",
        "        if all_l_stars:\n",
        "            L_star_empirical = np.mean(all_l_stars)\n",
        "            L_star_std = np.std(all_l_stars)\n",
        "            L_star_predicted = predict_l_star_v3(config[\"L\"], config[\"H\"])\n",
        "            error = abs(L_star_predicted - L_star_empirical) / config[\"L\"] * 100\n",
        "            \n",
        "            result = {\n",
        "                \"model\": model_name,\n",
        "                \"lab\": config[\"lab\"],\n",
        "                \"L\": config[\"L\"],\n",
        "                \"H\": config[\"H\"],\n",
        "                \"arch\": arch,\n",
        "                \"L_star_predicted\": float(L_star_predicted),\n",
        "                \"L_star_empirical\": float(L_star_empirical),\n",
        "                \"L_star_std\": float(L_star_std),\n",
        "                \"error_pct\": float(error),\n",
        "            }\n",
        "            results.append(result)\n",
        "            \n",
        "            print(f\"\\n  RESULTS:\")\n",
        "            print(f\"    L* predicted:  {L_star_predicted:.1f}\")\n",
        "            print(f\"    L* empirical:  {L_star_empirical:.1f} +/- {L_star_std:.1f}\")\n",
        "            print(f\"    Error:         {error:.1f}%\")\n",
        "        \n",
        "        # Cleanup\n",
        "        del model, tokenizer\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"  ERROR: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"COMPLETE: {len(results)}/{len(MODELS_TO_TEST)} models\")\n",
        "print(\"=\"*80)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "cell-10"},
      "source": [
        "## 6. Results Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {"id": "cell-11"},
      "source": [
        "if results:\n",
        "    df = pd.DataFrame(results)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"CROSS-HERITAGE L* VALIDATION RESULTS\")\n",
        "    print(\"=\"*80)\n",
        "    print(df.to_string(index=False))\n",
        "    \n",
        "    # Summary by lab\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SUMMARY BY LAB\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    for lab in sorted(df['lab'].unique()):\n",
        "        lab_df = df[df['lab'] == lab]\n",
        "        errors = lab_df['error_pct'].values\n",
        "        print(f\"\\n{lab}: n={len(lab_df)}, MAPE={np.mean(errors):.1f}%\")\n",
        "    \n",
        "    # Overall\n",
        "    overall_mape = df['error_pct'].mean()\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"OVERALL MAPE: {overall_mape:.1f}%\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # In-sample vs out-of-sample\n",
        "    pythia = df[df['lab'] == 'EleutherAI']\n",
        "    others = df[df['lab'] != 'EleutherAI']\n",
        "    \n",
        "    if len(pythia) > 0 and len(others) > 0:\n",
        "        print(f\"\\nPythia (in-sample):      MAPE = {pythia['error_pct'].mean():.1f}%\")\n",
        "        print(f\"Non-Pythia (out-of-sample): MAPE = {others['error_pct'].mean():.1f}%\")\n",
        "else:\n",
        "    print(\"No results!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "cell-12"},
      "source": [
        "## 7. Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {"id": "cell-13"},
      "source": [
        "if results:\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    lab_colors = {\n",
        "        \"EleutherAI\": \"#E74C3C\",\n",
        "        \"Meta\": \"#3498DB\",\n",
        "        \"BigScience\": \"#27AE60\",\n",
        "        \"OpenAI\": \"#9B59B6\"\n",
        "    }\n",
        "    \n",
        "    # Plot 1: Predicted vs Empirical\n",
        "    ax1 = axes[0]\n",
        "    for r in results:\n",
        "        color = lab_colors.get(r['lab'], 'gray')\n",
        "        ax1.scatter(r['L_star_predicted'], r['L_star_empirical'],\n",
        "                   c=color, s=150, alpha=0.8, edgecolors='white', linewidths=2)\n",
        "        ax1.annotate(r['model'].split('/')[-1],\n",
        "                    (r['L_star_predicted'], r['L_star_empirical']),\n",
        "                    fontsize=8, xytext=(5, 5), textcoords='offset points')\n",
        "    \n",
        "    max_val = max(max(r['L_star_predicted'] for r in results),\n",
        "                  max(r['L_star_empirical'] for r in results))\n",
        "    ax1.plot([0, max_val*1.1], [0, max_val*1.1], 'k--', alpha=0.5)\n",
        "    ax1.set_xlabel('L* Predicted')\n",
        "    ax1.set_ylabel('L* Empirical')\n",
        "    ax1.set_title('Cross-Heritage L* Validation')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 2: Error by Lab\n",
        "    ax2 = axes[1]\n",
        "    labs = sorted(set(r['lab'] for r in results))\n",
        "    for i, lab in enumerate(labs):\n",
        "        lab_results = [r for r in results if r['lab'] == lab]\n",
        "        errors = [r['error_pct'] for r in lab_results]\n",
        "        ax2.bar(i, np.mean(errors), color=lab_colors.get(lab, 'gray'), alpha=0.7)\n",
        "        ax2.scatter([i]*len(errors), errors, c='black', s=50, zorder=5)\n",
        "    \n",
        "    ax2.axhline(y=10, color='orange', linestyle='--', label='10% threshold')\n",
        "    ax2.set_xticks(range(len(labs)))\n",
        "    ax2.set_xticklabels(labs, rotation=45, ha='right')\n",
        "    ax2.set_ylabel('Error (%)')\n",
        "    ax2.set_title('L* Formula Error by Lab')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    PNG_FILE = f\"l_star_cross_heritage_{TIMESTAMP}.png\"\n",
        "    plt.savefig(PNG_FILE, dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(f\"Saved: {PNG_FILE}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "cell-14"},
      "source": [
        "## 8. Final Verdict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {"id": "cell-15"},
      "source": [
        "if results:\n",
        "    overall_mape = np.mean([r['error_pct'] for r in results])\n",
        "    \n",
        "    if overall_mape < 10:\n",
        "        verdict = \"FORMULA GENERALIZES\"\n",
        "    elif overall_mape < 15:\n",
        "        verdict = \"PARTIAL GENERALIZATION\"\n",
        "    else:\n",
        "        verdict = \"CALIBRATION NEEDED\"\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"FINAL VERDICT\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"\"\"\n",
        "    Formula: L* = L × (0.11 + 0.012×L + 4.9/H)\n",
        "    \n",
        "    Models tested: {len(results)}\n",
        "    Labs tested:   {len(set(r['lab'] for r in results))}\n",
        "    Overall MAPE:  {overall_mape:.1f}%\n",
        "    \n",
        "    VERDICT: {verdict}\n",
        "    \"\"\")\n",
        "    \n",
        "    # Save\n",
        "    output = {\n",
        "        \"experiment\": \"L* Cross-Heritage Validation v2\",\n",
        "        \"timestamp\": TIMESTAMP,\n",
        "        \"formula\": \"L* = L × (0.11 + 0.012×L + 4.9/H)\",\n",
        "        \"n_models\": len(results),\n",
        "        \"overall_mape\": float(overall_mape),\n",
        "        \"verdict\": verdict,\n",
        "        \"results\": results\n",
        "    }\n",
        "    \n",
        "    JSON_FILE = f\"l_star_cross_heritage_{TIMESTAMP}.json\"\n",
        "    with open(JSON_FILE, 'w') as f:\n",
        "        json.dump(output, f, indent=2)\n",
        "    print(f\"Saved: {JSON_FILE}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {"id": "cell-16"},
      "source": [
        "# Download\n",
        "try:\n",
        "    from google.colab import files\n",
        "    if 'JSON_FILE' in dir():\n",
        "        files.download(JSON_FILE)\n",
        "    if 'PNG_FILE' in dir():\n",
        "        files.download(PNG_FILE)\n",
        "    print(\"Downloads started!\")\n",
        "except:\n",
        "    print(\"Files saved locally.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
