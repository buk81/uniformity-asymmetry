{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Pythia Family Sweep: Residual Stream Gain (H25 Validation)\n\n## CORRECTED VERSION - Without Final LayerNorm Artifact\n\n**Paper #3 Experiment:** Dimensional Crowding Hypothesis Validation\n\n**Critical Fix:** Previous versions measured `hidden_states[-1] / hidden_states[-2]`, which INCLUDES the final LayerNorm. This caused ALL models to appear as \"dampening\" due to the normalization artifact.\n\n**Correct Methodology:**\n```\nWRONG:   G = ||hidden_states[-1]|| / ||hidden_states[-2]||  (includes final LN)\nCORRECT: G = ||hidden_states[-2]|| / ||hidden_states[-3]||  (true last layer)\n```\n\n**Primary Question:** Is the Pythia dampening pattern consistent across ALL 8 Pythia sizes?\n\n**H25 Prediction:**\n- High œÅ (n_heads/d_head ‚â• 0.2) ‚Üí Dampening (G < 1.0)\n- Low œÅ (n_heads/d_head < 0.2) ‚Üí Expansion (G > 1.0)\n\n**Expected Pattern:** Dampening should correlate with œÅ"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install transformers torch matplotlib numpy scipy --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import gc\n",
    "from scipy import stats\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"GPU memory: {gpu_mem:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All 8 Pythia models with architectural details\n",
    "PYTHIA_MODELS = {\n",
    "    'pythia-70m': {'params': 70e6, 'n_layers': 6, 'n_heads': 8, 'd_model': 512, 'd_head': 64, 'memory_gb': 0.5},\n",
    "    'pythia-160m': {'params': 160e6, 'n_layers': 12, 'n_heads': 12, 'd_model': 768, 'd_head': 64, 'memory_gb': 1},\n",
    "    'pythia-410m': {'params': 410e6, 'n_layers': 24, 'n_heads': 16, 'd_model': 1024, 'd_head': 64, 'memory_gb': 2},\n",
    "    'pythia-1b': {'params': 1e9, 'n_layers': 16, 'n_heads': 8, 'd_model': 2048, 'd_head': 256, 'memory_gb': 4},\n",
    "    'pythia-1.4b': {'params': 1.4e9, 'n_layers': 24, 'n_heads': 16, 'd_model': 2048, 'd_head': 128, 'memory_gb': 6},\n",
    "    'pythia-2.8b': {'params': 2.8e9, 'n_layers': 32, 'n_heads': 32, 'd_model': 2560, 'd_head': 80, 'memory_gb': 10},\n",
    "    'pythia-6.9b': {'params': 6.9e9, 'n_layers': 32, 'n_heads': 32, 'd_model': 4096, 'd_head': 128, 'memory_gb': 20},\n",
    "    'pythia-12b': {'params': 12e9, 'n_layers': 36, 'n_heads': 40, 'd_model': 5120, 'd_head': 128, 'memory_gb': 30},\n",
    "}\n",
    "\n",
    "# Compute œÅ for each model\n",
    "for name, config in PYTHIA_MODELS.items():\n",
    "    config['rho'] = config['n_heads'] / config['d_head']\n",
    "\n",
    "# Print rho values\n",
    "print(\"œÅ (Head Density) for Pythia Family:\")\n",
    "print(\"-\" * 40)\n",
    "for name, config in sorted(PYTHIA_MODELS.items(), key=lambda x: x[1]['rho']):\n",
    "    print(f\"{name:15} | œÅ = {config['n_heads']}/{config['d_head']} = {config['rho']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select models based on available GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    available_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"\\nAvailable GPU memory: {available_mem:.1f} GB\")\n",
    "    \n",
    "    MODELS_TO_TEST = []\n",
    "    for name, config in PYTHIA_MODELS.items():\n",
    "        if config['memory_gb'] < (available_mem - 2):\n",
    "            MODELS_TO_TEST.append(name)\n",
    "    \n",
    "    print(f\"Models to test: {MODELS_TO_TEST}\")\n",
    "else:\n",
    "    MODELS_TO_TEST = ['pythia-70m', 'pythia-160m', 'pythia-410m']\n",
    "    print(f\"CPU mode - testing small models only: {MODELS_TO_TEST}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prompts (same as 8-model benchmark)\n",
    "TEST_PROMPTS = [\n",
    "    # Factual\n",
    "    \"The capital of France is\",\n",
    "    \"Water freezes at a temperature of\",\n",
    "    \"The largest planet in our solar system is\",\n",
    "    \"Einstein is famous for the theory of\",\n",
    "    \"The chemical symbol for gold is\",\n",
    "    # Syntactic\n",
    "    \"The man who saw the woman who wore the hat that was red and had feathers left the party early because\",\n",
    "    \"Despite the rain that had been falling for three days straight the team decided to continue their journey through the forest which seemed to stretch on endlessly and the leader said\",\n",
    "    \"After the meeting that was scheduled for Tuesday but moved to Wednesday due to the holiday which fell on Monday the committee announced their decision which was to\",\n",
    "    \"The book which the author who won the prize wrote during the summer after the accident happened tells the story of a young girl who discovers that she has the ability to\",\n",
    "    \"Although the evidence suggested otherwise and the witnesses testified against him and the prosecutor demanded the harshest penalty the jury surprisingly decided to\",\n",
    "    # Cliche\n",
    "    \"Actions speak louder than\",\n",
    "    \"The early bird catches the\",\n",
    "    \"A stitch in time saves\",\n",
    "    \"When in Rome do as the Romans\",\n",
    "    \"Birds of a feather flock\",\n",
    "    # Novel\n",
    "    \"The epistemological implications of quantum decoherence suggest that consciousness might be fundamentally\",\n",
    "    \"The Voynich manuscript's undeciphered text has led some researchers to propose that it represents a constructed language designed to\",\n",
    "    \"The Banach-Tarski paradox demonstrates that in mathematics with the axiom of choice one can decompose a sphere and reassemble it into\",\n",
    "    \"The Mpemba effect remains controversial because it challenges our intuition about thermal dynamics by suggesting that under certain conditions hot water can\",\n",
    "    \"The Riemann hypothesis if proven true would have profound implications for our understanding of the distribution of\",\n",
    "    # Nonsense\n",
    "    \"Table sky run blue jump\",\n",
    "    \"Syntax of purple dreams calculates the\",\n",
    "    \"When squared thoughts evaporate into crystalline networks the\",\n",
    "    \"If democracy could photosynthesize under marginal propensity then\",\n",
    "    \"The hypotenuse of existential dread interpolates between\"\n",
    "]\n",
    "\n",
    "print(f\"Using {len(TEST_PROMPTS)} test prompts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": "def compute_residual_gain(model, tokenizer, prompts):\n    \"\"\"\n    Compute Residual Stream Gain - CORRECTED VERSION (no final LayerNorm)\n    \n    WRONG:   G = ||hidden_states[-1]|| / ||hidden_states[-2]||  (includes final LN)\n    CORRECT: G = ||hidden_states[-2]|| / ||hidden_states[-3]||  (true last layer)\n    \n    Returns: \n        - gain_no_ln_mean: Mean gain WITHOUT final LN (correct)\n        - gain_no_ln_std: Std of gains\n        - gain_with_ln_mean: Mean gain WITH final LN (for comparison)\n        - all_gains_no_ln: All individual gains without LN\n        - all_layer_gains: Full layer-by-layer gains for analysis\n    \"\"\"\n    gains_no_ln = []\n    gains_with_ln = []\n    all_layer_gains_list = []\n    \n    for prompt in prompts:\n        inputs = tokenizer(prompt, return_tensors=\"pt\")\n        if torch.cuda.is_available():\n            inputs = {k: v.to(model.device) for k, v in inputs.items()}\n        \n        with torch.no_grad():\n            outputs = model(**inputs, output_hidden_states=True)\n        \n        hidden_states = outputs.hidden_states  # (n_layers+1,) tuple\n        \n        # Compute norms for all hidden states (last token)\n        norms = []\n        for h in hidden_states:\n            norm = torch.norm(h[:, -1, :].float(), dim=-1).item()\n            norms.append(norm)\n        \n        # Compute all layer-by-layer gains\n        layer_gains = []\n        for i in range(1, len(norms)):\n            gain = norms[i] / (norms[i-1] + 1e-10)\n            layer_gains.append(gain)\n        \n        all_layer_gains_list.append(layer_gains)\n        \n        # WRONG metric (includes final LN): last gain\n        gain_with_ln = layer_gains[-1]\n        gains_with_ln.append(gain_with_ln)\n        \n        # CORRECT metric (no final LN): second-to-last gain\n        gain_no_ln = layer_gains[-2] if len(layer_gains) >= 2 else layer_gains[-1]\n        gains_no_ln.append(gain_no_ln)\n    \n    # Average layer gains across prompts\n    avg_layer_gains = np.mean(all_layer_gains_list, axis=0).tolist()\n    \n    return {\n        'gain_no_ln_mean': float(np.mean(gains_no_ln)),\n        'gain_no_ln_std': float(np.std(gains_no_ln)),\n        'gain_with_ln_mean': float(np.mean(gains_with_ln)),\n        'gain_with_ln_std': float(np.std(gains_with_ln)),\n        'all_gains_no_ln': [float(g) for g in gains_no_ln],\n        'all_gains_with_ln': [float(g) for g in gains_with_ln],\n        'all_layer_gains': avg_layer_gains\n    }"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": "def analyze_model(model_name):\n    \"\"\"Analyze a single Pythia model with CORRECTED methodology.\"\"\"\n    full_name = f\"EleutherAI/{model_name}\"\n    print(f\"\\n{'='*60}\")\n    print(f\"Analyzing: {full_name}\")\n    print(f\"{'='*60}\")\n    \n    # Get config first (for metadata)\n    config = AutoConfig.from_pretrained(full_name)\n    \n    # Load model\n    tokenizer = AutoTokenizer.from_pretrained(full_name)\n    model = AutoModelForCausalLM.from_pretrained(\n        full_name,\n        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n        device_map=\"auto\" if torch.cuda.is_available() else None,\n        low_cpu_mem_usage=True\n    )\n    model.eval()\n    \n    # Get architecture details\n    n_layers = config.num_hidden_layers\n    n_heads = config.num_attention_heads\n    d_model = config.hidden_size\n    d_head = d_model // n_heads\n    rho = n_heads / d_head\n    rotary_pct = getattr(config, 'rotary_pct', None) or getattr(config, 'rotary_percent', 0.25)\n    \n    print(f\"Layers: {n_layers}, Heads: {n_heads}, d_model: {d_model}, d_head: {d_head}\")\n    print(f\"œÅ = {n_heads}/{d_head} = {rho:.4f}\")\n    \n    # Compute residual gain with CORRECTED methodology\n    gain_results = compute_residual_gain(model, tokenizer, TEST_PROMPTS)\n    \n    # Extract metrics\n    gain_no_ln = gain_results['gain_no_ln_mean']\n    gain_with_ln = gain_results['gain_with_ln_mean']\n    \n    print(f\"\\nüî¨ RESIDUAL STREAM GAIN:\")\n    print(f\"   WITH final LN (WRONG):    {gain_with_ln:.4f} ¬± {gain_results['gain_with_ln_std']:.4f}\")\n    print(f\"   WITHOUT final LN (CORRECT): {gain_no_ln:.4f} ¬± {gain_results['gain_no_ln_std']:.4f}\")\n    \n    is_dampening = gain_no_ln < 1.0\n    if is_dampening:\n        print(f\"   ‚Üí DAMPENING (G < 1.0)\")\n    else:\n        print(f\"   ‚Üí EXPANSION (G > 1.0)\")\n    \n    # IMPORTANT: Convert all values to native Python types for JSON serialization\n    results = {\n        'model': str(model_name),\n        'n_layers': int(n_layers),\n        'n_heads': int(n_heads),\n        'd_model': int(d_model),\n        'd_head': int(d_head),\n        'rho': float(rho),\n        'rotary_pct': float(rotary_pct) if rotary_pct is not None else 0.25,\n        # CORRECT metric (without final LN)\n        'residual_gain_mean': float(gain_no_ln),\n        'residual_gain_std': float(gain_results['gain_no_ln_std']),\n        'residual_gain_all': gain_results['all_gains_no_ln'],\n        # For comparison: WITH final LN (wrong)\n        'gain_with_ln_mean': float(gain_with_ln),\n        'gain_with_ln_std': float(gain_results['gain_with_ln_std']),\n        # Layer-by-layer for analysis\n        'all_layer_gains': gain_results['all_layer_gains'],\n        'is_dampening': bool(is_dampening)\n    }\n    \n    # Cleanup\n    del model, tokenizer\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    \n    return results"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run analysis on all selected models\n",
    "all_results = []\n",
    "\n",
    "for model_name in MODELS_TO_TEST:\n",
    "    try:\n",
    "        results = analyze_model(model_name)\n",
    "        all_results.append(results)\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing {model_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n\\nSuccessfully analyzed {len(all_results)} models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": "# Summary Table - CORRECTED VERSION\nprint(\"\\n\" + \"=\" * 100)\nprint(\"PYTHIA FAMILY RESIDUAL STREAM GAIN SUMMARY (CORRECTED - No Final LayerNorm)\")\nprint(\"=\" * 100)\nprint(f\"\\n{'Model':<15} {'œÅ':>8} {'G (no LN)':>12} {'G (with LN)':>12} {'Status':>12}\")\nprint(\"-\" * 60)\n\nfor r in sorted(all_results, key=lambda x: x['rho']):\n    status = \"DAMPENING\" if r['is_dampening'] else \"EXPANSION\"\n    marker = \"üîµ\" if r['is_dampening'] else \"üî¥\"\n    gain_no_ln = r['residual_gain_mean']\n    gain_with_ln = r.get('gain_with_ln_mean', 'N/A')\n    \n    if isinstance(gain_with_ln, float):\n        print(f\"{r['model']:<15} {r['rho']:>8.4f} {gain_no_ln:>12.4f} {gain_with_ln:>12.4f} {marker} {status}\")\n    else:\n        print(f\"{r['model']:<15} {r['rho']:>8.4f} {gain_no_ln:>12.4f} {'N/A':>12} {marker} {status}\")\n\nprint(\"\\n‚ö†Ô∏è  'G (no LN)' is the CORRECT metric (without final LayerNorm)\")\nprint(\"   'G (with LN)' is shown for comparison only (includes LN artifact)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Analysis: œÅ vs Gain\n",
    "if len(all_results) >= 3:\n",
    "    rhos = [r['rho'] for r in all_results]\n",
    "    gains = [r['residual_gain_mean'] for r in all_results]\n",
    "    \n",
    "    # Pearson correlation\n",
    "    corr, p_value = stats.pearsonr(rhos, gains)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"CORRELATION ANALYSIS: œÅ vs Residual Gain\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nPearson r = {corr:.4f}\")\n",
    "    print(f\"p-value = {p_value:.4e}\")\n",
    "    \n",
    "    if corr < 0 and p_value < 0.05:\n",
    "        print(f\"\\n‚úÖ H25 VALIDATED: Higher œÅ ‚Üí Lower Gain (Dampening)\")\n",
    "    elif corr < 0:\n",
    "        print(f\"\\n‚ö†Ô∏è H25 TREND SUPPORTED but p > 0.05 (needs more data points)\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå H25 NOT SUPPORTED: No negative correlation\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Need at least 3 models for correlation analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "fig.suptitle('Pythia Family: Head Density (œÅ) vs Residual Stream Gain', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Sort by rho\n",
    "sorted_results = sorted(all_results, key=lambda x: x['rho'])\n",
    "rhos = [r['rho'] for r in sorted_results]\n",
    "gains = [r['residual_gain_mean'] for r in sorted_results]\n",
    "stds = [r['residual_gain_std'] for r in sorted_results]\n",
    "names = [r['model'].replace('pythia-', '') for r in sorted_results]\n",
    "\n",
    "# Panel 1: œÅ vs Gain scatter\n",
    "ax1 = axes[0]\n",
    "colors = ['blue' if g < 1.0 else 'red' for g in gains]\n",
    "ax1.scatter(rhos, gains, c=colors, s=100, zorder=5)\n",
    "ax1.errorbar(rhos, gains, yerr=stds, fmt='none', ecolor='gray', alpha=0.5)\n",
    "\n",
    "# Annotate\n",
    "for i, name in enumerate(names):\n",
    "    ax1.annotate(name, (rhos[i], gains[i]), textcoords=\"offset points\", xytext=(5, 5), fontsize=8)\n",
    "\n",
    "ax1.axhline(y=1.0, color='black', linestyle='--', alpha=0.5, label='G=1.0 (Bentov Point)')\n",
    "ax1.set_xlabel('œÅ = n_heads / d_head')\n",
    "ax1.set_ylabel('Residual Stream Gain')\n",
    "ax1.set_title('œÅ vs Gain')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Panel 2: Bar chart by model\n",
    "ax2 = axes[1]\n",
    "x = np.arange(len(names))\n",
    "bars = ax2.bar(x, gains, color=colors, alpha=0.7, yerr=stds, capsize=3)\n",
    "ax2.axhline(y=1.0, color='black', linestyle='--', alpha=0.5)\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(names, rotation=45)\n",
    "ax2.set_xlabel('Model (sorted by œÅ)')\n",
    "ax2.set_ylabel('Residual Stream Gain')\n",
    "ax2.set_title('Gain by Model Size')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Panel 3: œÅ correlation with trend line\n",
    "ax3 = axes[2]\n",
    "ax3.scatter(rhos, gains, c=colors, s=100, zorder=5)\n",
    "\n",
    "if len(rhos) >= 2:\n",
    "    # Linear regression\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(rhos, gains)\n",
    "    x_fit = np.linspace(min(rhos) * 0.9, max(rhos) * 1.1, 100)\n",
    "    y_fit = slope * x_fit + intercept\n",
    "    ax3.plot(x_fit, y_fit, 'g--', linewidth=2, label=f'r = {r_value:.3f}, p = {p_value:.3e}')\n",
    "\n",
    "ax3.axhline(y=1.0, color='black', linestyle='--', alpha=0.5)\n",
    "ax3.set_xlabel('œÅ = n_heads / d_head')\n",
    "ax3.set_ylabel('Residual Stream Gain')\n",
    "ax3.set_title('Correlation: œÅ ‚Üí Dampening?')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('pythia_family_residual_gain.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSaved: pythia_family_residual_gain.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# H25 Verdict\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"H25 VALIDATION: DIMENSIONAL CROWDING HYPOTHESIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check if all high-œÅ models dampen\n",
    "high_rho_models = [r for r in all_results if r['rho'] >= 0.2]\n",
    "low_rho_models = [r for r in all_results if r['rho'] < 0.2]\n",
    "\n",
    "print(f\"\\nHigh œÅ models (‚â• 0.2): {len(high_rho_models)}\")\n",
    "for r in high_rho_models:\n",
    "    status = \"DAMPEN\" if r['is_dampening'] else \"EXPAND\"\n",
    "    print(f\"  {r['model']}: œÅ = {r['rho']:.4f}, G = {r['residual_gain_mean']:.4f} ‚Üí {status}\")\n",
    "\n",
    "print(f\"\\nLow œÅ models (< 0.2): {len(low_rho_models)}\")\n",
    "for r in low_rho_models:\n",
    "    status = \"DAMPEN\" if r['is_dampening'] else \"EXPAND\"\n",
    "    print(f\"  {r['model']}: œÅ = {r['rho']:.4f}, G = {r['residual_gain_mean']:.4f} ‚Üí {status}\")\n",
    "\n",
    "# Compute verdict\n",
    "high_rho_dampen_rate = sum(1 for r in high_rho_models if r['is_dampening']) / max(len(high_rho_models), 1)\n",
    "low_rho_expand_rate = sum(1 for r in low_rho_models if not r['is_dampening']) / max(len(low_rho_models), 1)\n",
    "\n",
    "print(f\"\\nHigh œÅ dampening rate: {high_rho_dampen_rate*100:.0f}%\")\n",
    "print(f\"Low œÅ expansion rate: {low_rho_expand_rate*100:.0f}%\")\n",
    "\n",
    "if high_rho_dampen_rate > 0.5 or (len(all_results) >= 3 and corr < -0.3):\n",
    "    print(f\"\\n‚úÖ H25 VALIDATED: Dimensional Crowding ‚Üí Dampening\")\n",
    "    verdict = \"VALIDATED\"\n",
    "elif len(all_results) < 3:\n",
    "    print(f\"\\n‚ö†Ô∏è INCONCLUSIVE: Need more data points (only {len(all_results)} models)\")\n",
    "    verdict = \"INCONCLUSIVE\"\n",
    "else:\n",
    "    print(f\"\\n‚ùå H25 NOT VALIDATED\")\n",
    "    verdict = \"NOT_VALIDATED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": "# Save results\ntimestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n\n# Handle case where correlation wasn't computed (< 3 models)\ncorrelation_value = None\np_value_value = None\nif len(all_results) >= 3:\n    try:\n        correlation_value = float(corr)\n        p_value_value = float(p_value)\n    except NameError:\n        pass\n\nresults_data = {\n    'experiment': 'Pythia Family Residual Stream Gain Sweep - CORRECTED (No Final LN)',\n    'hypothesis': 'H25: Dimensional Crowding ‚Üí Dampening',\n    'methodology': {\n        'correct_metric': 'hidden_states[-2] / hidden_states[-3] (true last layer, no final LN)',\n        'wrong_metric': 'hidden_states[-1] / hidden_states[-2] (includes final LN artifact)'\n    },\n    'date': datetime.now().isoformat(),\n    'n_models': len(all_results),\n    'n_prompts': len(TEST_PROMPTS),\n    'models': all_results,\n    'analysis': {\n        'rho_gain_correlation': correlation_value,\n        'correlation_p_value': p_value_value,\n        'high_rho_dampen_rate': float(high_rho_dampen_rate),\n        'low_rho_expand_rate': float(low_rho_expand_rate)\n    },\n    'verdict': str(verdict)\n}\n\nfilename = f'pythia_family_NO_FINAL_LN_{timestamp}.json'\nwith open(filename, 'w') as f:\n    json.dump(results_data, f, indent=2)\n\nprint(f\"\\nSaved: {filename}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": "# Create archive and auto-download\nimport zipfile\n\narchive_name = f'pythia_family_NO_FINAL_LN_{timestamp}.zip'\n\nwith zipfile.ZipFile(archive_name, 'w') as zf:\n    zf.write(filename)\n    zf.write('pythia_family_residual_gain.png')\n\nprint(f\"Created archive: {archive_name}\")\n\n# Auto-download in Colab\ntry:\n    from google.colab import files\n    print(\"\\nStarting automatic downloads...\")\n    files.download(filename)\n    files.download('pythia_family_residual_gain.png')\n    files.download(archive_name)\n    print(\"Downloads triggered!\")\nexcept ImportError:\n    print(\"\\nNot running in Colab - manual download required.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": "# Final Summary\nprint(\"\\n\" + \"=\" * 70)\nprint(\"FINAL SUMMARY: Pythia Family - CORRECTED (No Final LayerNorm)\")\nprint(\"=\" * 70)\n\nprint(f\"\\nüìä Models Tested: {len(all_results)}\")\nprint(f\"üìù Prompts per Model: {len(TEST_PROMPTS)}\")\n\ndampening_count = sum(1 for r in all_results if r['is_dampening'])\nexpansion_count = len(all_results) - dampening_count\n\nprint(f\"\\nüîµ DAMPENING (G < 1.0): {dampening_count} models\")\nprint(f\"üî¥ EXPANSION (G > 1.0): {expansion_count} models\")\n\nif len(all_results) >= 3:\n    try:\n        print(f\"\\nüìà œÅ vs Gain Correlation: r = {corr:.4f} (p = {p_value:.4e})\")\n    except NameError:\n        print(\"\\nüìà œÅ vs Gain Correlation: not computed\")\n\nprint(f\"\\nüéØ H25 VERDICT: {verdict}\")\n\nprint(f\"\\nüìÅ Output Files:\")\nprint(f\"   ‚Ä¢ {filename}\")\nprint(f\"   ‚Ä¢ pythia_family_residual_gain.png\")\nprint(f\"   ‚Ä¢ {archive_name}\")\n\nprint(f\"\\n‚ö†Ô∏è  METHODOLOGY: Using CORRECT metric (no final LayerNorm)\")\nprint(f\"   This excludes the LayerNorm artifact that caused all models to appear as 'dampening'.\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}