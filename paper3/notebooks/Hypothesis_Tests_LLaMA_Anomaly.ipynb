{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLaMA 3.1 Anomaly - Hypothesis Tests\n",
    "\n",
    "**Date:** 2026-01-05\n",
    "**Goal:** Understand WHY LLaMA 3.1 contracts (0.48x) while Mistral expands (1.37x)\n",
    "\n",
    "## Hypotheses to Test:\n",
    "\n",
    "1. **Titanium Projector:** Vocab size 128k vs 32k → W_U must be massive\n",
    "2. **Long-Context Dampening:** 128k context → systematic dampening needed\n",
    "3. **Grokking:** 15T tokens training → ultra-efficient representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 0: HuggingFace Login (REQUIRED for gated models)\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Option 1: Use your HF token directly\n",
    "# login(token=\"hf_YOUR_TOKEN_HERE\")\n",
    "\n",
    "# Option 2: Interactive login (will prompt for token)\n",
    "login()\n",
    "\n",
    "print(\"HuggingFace login complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Results directory\n",
    "RESULTS_DIR = './Results'\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(\"Setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Model Definitions for Comparison\n",
    "\n",
    "MODELS_TO_COMPARE = {\n",
    "    # LLaMA 3.1 (CONTRACTS - 0.48x)\n",
    "    'llama3.1-8b': {\n",
    "        'hf_name': 'meta-llama/Llama-3.1-8B',\n",
    "        'vocab_size': 128256,\n",
    "        'context_length': 128000,\n",
    "        'training_tokens': '15T',\n",
    "        'last_gain': 0.48,\n",
    "        'behavior': 'CONTRACTS'\n",
    "    },\n",
    "    # Mistral (EXPANDS - 1.37x)\n",
    "    'mistral-7b': {\n",
    "        'hf_name': 'mistralai/Mistral-7B-v0.1',\n",
    "        'vocab_size': 32000,\n",
    "        'context_length': 8192,\n",
    "        'training_tokens': '~7-8T',\n",
    "        'last_gain': 1.37,\n",
    "        'behavior': 'EXPANDS'\n",
    "    },\n",
    "    # Gemma 2 9B (ungated alternative to Gemma 7B)\n",
    "    'gemma2-9b': {\n",
    "        'hf_name': 'google/gemma-2-9b',\n",
    "        'vocab_size': 256000,\n",
    "        'context_length': 8192,\n",
    "        'training_tokens': '8T',\n",
    "        'last_gain': 2.85,  # Using Gemma 7B reference\n",
    "        'behavior': 'EXPANDS'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Models defined:\")\n",
    "for name, info in MODELS_TO_COMPARE.items():\n",
    "    print(f\"  {name}: vocab={info['vocab_size']:,}, ctx={info['context_length']:,}, {info['behavior']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Hypothesis 1 - Titanium Projector Test\n",
    "# Measure W_U (unembedding/lm_head) spectral norm\n",
    "\n",
    "def analyze_unembedding_matrix(model_name, hf_name):\n",
    "    \"\"\"Analyze the unembedding matrix (lm_head) of a model.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Analyzing: {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Load model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        hf_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map='auto',\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # Get lm_head (unembedding matrix)\n",
    "    lm_head = model.lm_head.weight.data.float()  # [vocab_size, hidden_dim]\n",
    "    \n",
    "    results = {\n",
    "        'model': model_name,\n",
    "        'shape': list(lm_head.shape),\n",
    "        'vocab_size': lm_head.shape[0],\n",
    "        'hidden_dim': lm_head.shape[1],\n",
    "    }\n",
    "    \n",
    "    # Compute norms\n",
    "    print(f\"\\nW_U shape: {lm_head.shape}\")\n",
    "    \n",
    "    # 1. Frobenius norm (total energy)\n",
    "    frob_norm = torch.linalg.norm(lm_head, ord='fro').item()\n",
    "    results['frobenius_norm'] = frob_norm\n",
    "    print(f\"Frobenius norm: {frob_norm:.2f}\")\n",
    "    \n",
    "    # 2. Spectral norm (largest singular value) - THE KEY METRIC\n",
    "    # This tells us the maximum amplification factor\n",
    "    spectral_norm = torch.linalg.norm(lm_head, ord=2).item()\n",
    "    results['spectral_norm'] = spectral_norm\n",
    "    print(f\"Spectral norm (σ_max): {spectral_norm:.2f}\")\n",
    "    \n",
    "    # 3. Mean row norm (average token representation magnitude)\n",
    "    row_norms = torch.linalg.norm(lm_head, dim=1)\n",
    "    mean_row_norm = row_norms.mean().item()\n",
    "    std_row_norm = row_norms.std().item()\n",
    "    results['mean_row_norm'] = mean_row_norm\n",
    "    results['std_row_norm'] = std_row_norm\n",
    "    print(f\"Mean row norm: {mean_row_norm:.4f} +/- {std_row_norm:.4f}\")\n",
    "    \n",
    "    # 4. Normalized spectral norm (per vocab token)\n",
    "    norm_per_vocab = spectral_norm / np.sqrt(lm_head.shape[0])\n",
    "    results['spectral_per_sqrt_vocab'] = norm_per_vocab\n",
    "    print(f\"Spectral / sqrt(vocab): {norm_per_vocab:.4f}\")\n",
    "    \n",
    "    # 5. Effective amplification (compensating for residual contraction)\n",
    "    last_gain = MODELS_TO_COMPARE[model_name]['last_gain']\n",
    "    effective_amp = spectral_norm * last_gain\n",
    "    results['effective_amplification'] = effective_amp\n",
    "    results['last_gain'] = last_gain\n",
    "    print(f\"\\nEffective amplification (sigma_max x last_gain):\")\n",
    "    print(f\"  {spectral_norm:.2f} x {last_gain:.2f} = {effective_amp:.2f}\")\n",
    "    \n",
    "    # Clean up\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Analysis function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Run Hypothesis 1 Test - LLaMA 3.1 ONLY\n",
    "# (Run this separately to avoid memory issues)\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "# LLaMA 3.1 first (most important)\n",
    "model_name = 'llama3.1-8b'\n",
    "model_info = MODELS_TO_COMPARE[model_name]\n",
    "\n",
    "try:\n",
    "    results = analyze_unembedding_matrix(model_name, model_info['hf_name'])\n",
    "    all_results[model_name] = results\n",
    "except Exception as e:\n",
    "    print(f\"Error loading {model_name}: {e}\")\n",
    "    all_results[model_name] = {'error': str(e)}\n",
    "\n",
    "print(\"\\nLLaMA 3.1 analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Run Hypothesis 1 Test - Mistral\n",
    "\n",
    "model_name = 'mistral-7b'\n",
    "model_info = MODELS_TO_COMPARE[model_name]\n",
    "\n",
    "try:\n",
    "    results = analyze_unembedding_matrix(model_name, model_info['hf_name'])\n",
    "    all_results[model_name] = results\n",
    "except Exception as e:\n",
    "    print(f\"Error loading {model_name}: {e}\")\n",
    "    all_results[model_name] = {'error': str(e)}\n",
    "\n",
    "print(\"\\nMistral analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Run Hypothesis 1 Test - Gemma 2 (optional)\n",
    "\n",
    "model_name = 'gemma2-9b'\n",
    "model_info = MODELS_TO_COMPARE[model_name]\n",
    "\n",
    "try:\n",
    "    results = analyze_unembedding_matrix(model_name, model_info['hf_name'])\n",
    "    all_results[model_name] = results\n",
    "except Exception as e:\n",
    "    print(f\"Error loading {model_name}: {e}\")\n",
    "    all_results[model_name] = {'error': str(e)}\n",
    "\n",
    "print(\"\\nGemma 2 analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Compare Results\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TITANIUM PROJECTOR HYPOTHESIS - RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n| Model | Vocab | sigma_max (W_U) | Last Gain | Effective Amp |\")\n",
    "print(\"|-------|-------|-----------------|-----------|---------------|\")\n",
    "\n",
    "for name, r in all_results.items():\n",
    "    if 'error' not in r:\n",
    "        vocab = r['vocab_size']\n",
    "        sigma = r['spectral_norm']\n",
    "        gain = r['last_gain']\n",
    "        eff = r['effective_amplification']\n",
    "        print(f\"| {name:13} | {vocab:6,} | {sigma:15.2f} | {gain:9.2f} | {eff:13.2f} |\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"HYPOTHESIS EVALUATION:\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Check if LLaMA's W_U compensates for its contraction\n",
    "llama_data = all_results.get('llama3.1-8b', {})\n",
    "mistral_data = all_results.get('mistral-7b', {})\n",
    "\n",
    "if 'error' not in llama_data and 'error' not in mistral_data:\n",
    "    llama_eff = llama_data.get('effective_amplification', 0)\n",
    "    mistral_eff = mistral_data.get('effective_amplification', 0)\n",
    "    \n",
    "    llama_sigma = llama_data.get('spectral_norm', 0)\n",
    "    mistral_sigma = mistral_data.get('spectral_norm', 0)\n",
    "    \n",
    "    sigma_ratio = llama_sigma / mistral_sigma if mistral_sigma > 0 else 0\n",
    "    eff_ratio = llama_eff / mistral_eff if mistral_eff > 0 else 0\n",
    "    \n",
    "    print(f\"\\nLLaMA sigma_max: {llama_sigma:.2f}\")\n",
    "    print(f\"Mistral sigma_max: {mistral_sigma:.2f}\")\n",
    "    print(f\"Sigma ratio (LLaMA/Mistral): {sigma_ratio:.2f}x\")\n",
    "    \n",
    "    print(f\"\\nLLaMA effective amp: {llama_eff:.2f}\")\n",
    "    print(f\"Mistral effective amp: {mistral_eff:.2f}\")\n",
    "    print(f\"Effective ratio (LLaMA/Mistral): {eff_ratio:.2f}x\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    if 0.7 < eff_ratio < 1.3:\n",
    "        print(\"VERDICT: HYPOTHESIS SUPPORTED!\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"\\nLLaMA's larger W_U compensates for its residual contraction.\")\n",
    "        print(\"The 'broadcast' happens via Static Amplification, not stream expansion.\")\n",
    "    elif eff_ratio > 1.3:\n",
    "        print(\"VERDICT: HYPOTHESIS EXCEEDED!\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"\\nLLaMA actually has {eff_ratio:.1f}x MORE effective amplification.\")\n",
    "        print(\"The W_U OVER-compensates for the residual contraction!\")\n",
    "    else:\n",
    "        print(\"VERDICT: HYPOTHESIS PARTIALLY SUPPORTED\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"\\nLLaMA has {eff_ratio:.1f}x the effective amplification.\")\n",
    "        print(f\"W_U is {sigma_ratio:.1f}x larger but doesn't fully compensate.\")\n",
    "        print(\"Other factors may contribute.\")\n",
    "else:\n",
    "    print(\"Cannot evaluate - missing model data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Visualization\n",
    "\n",
    "# Filter out models with errors\n",
    "valid_models = [m for m in all_results.keys() if 'error' not in all_results[m]]\n",
    "\n",
    "if len(valid_models) >= 2:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    colors = ['#e74c3c' if 'llama' in m else '#3498db' if 'mistral' in m else '#2ecc71' for m in valid_models]\n",
    "\n",
    "    # Plot 1: Spectral Norm\n",
    "    ax1 = axes[0]\n",
    "    spectral_norms = [all_results[m].get('spectral_norm', 0) for m in valid_models]\n",
    "    bars1 = ax1.bar(valid_models, spectral_norms, color=colors)\n",
    "    ax1.set_ylabel('Spectral Norm (sigma_max)')\n",
    "    ax1.set_title('W_U Spectral Norm\\n(Maximum Amplification Factor)')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    for bar, val in zip(bars1, spectral_norms):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "                 f'{val:.1f}', ha='center', va='bottom')\n",
    "\n",
    "    # Plot 2: Last Layer Gain\n",
    "    ax2 = axes[1]\n",
    "    last_gains = [all_results[m].get('last_gain', 0) for m in valid_models]\n",
    "    bars2 = ax2.bar(valid_models, last_gains, color=colors)\n",
    "    ax2.set_ylabel('Last Layer Gain')\n",
    "    ax2.set_title('Residual Stream\\nLast Layer Gain')\n",
    "    ax2.axhline(y=1.0, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    for bar, val in zip(bars2, last_gains):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05, \n",
    "                 f'{val:.2f}x', ha='center', va='bottom')\n",
    "\n",
    "    # Plot 3: Effective Amplification\n",
    "    ax3 = axes[2]\n",
    "    effective_amps = [all_results[m].get('effective_amplification', 0) for m in valid_models]\n",
    "    bars3 = ax3.bar(valid_models, effective_amps, color=colors)\n",
    "    ax3.set_ylabel('Effective Amplification')\n",
    "    ax3.set_title('Effective Amplification\\n(sigma_max x Last Gain)')\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "    for bar, val in zip(bars3, effective_amps):\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "                 f'{val:.1f}', ha='center', va='bottom')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{RESULTS_DIR}/titanium_projector_hypothesis.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\nVisualization saved to {RESULTS_DIR}/titanium_projector_hypothesis.png\")\n",
    "else:\n",
    "    print(\"Not enough valid models for visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Hypothesis 2 - RoPE Theta Analysis\n",
    "# Check RoPE base frequency differences\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"HYPOTHESIS 2: LONG-CONTEXT DAMPENING (RoPE Analysis)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "rope_analysis = {}\n",
    "\n",
    "for model_name, model_info in MODELS_TO_COMPARE.items():\n",
    "    try:\n",
    "        config = AutoConfig.from_pretrained(model_info['hf_name'], trust_remote_code=True)\n",
    "        \n",
    "        # Extract RoPE parameters\n",
    "        rope_theta = getattr(config, 'rope_theta', None)\n",
    "        rope_scaling = getattr(config, 'rope_scaling', None)\n",
    "        max_position = getattr(config, 'max_position_embeddings', None)\n",
    "        \n",
    "        rope_analysis[model_name] = {\n",
    "            'rope_theta': rope_theta,\n",
    "            'rope_scaling': rope_scaling,\n",
    "            'max_position': max_position,\n",
    "            'context_length': model_info['context_length']\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{model_name}:\")\n",
    "        print(f\"  RoPE theta: {rope_theta:,}\" if rope_theta else \"  RoPE theta: None\")\n",
    "        print(f\"  RoPE scaling: {rope_scaling}\")\n",
    "        print(f\"  Max positions: {max_position:,}\" if max_position else \"  Max positions: None\")\n",
    "        print(f\"  Target context: {model_info['context_length']:,}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error for {model_name}: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"HYPOTHESIS 2 EVALUATION:\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "llama_theta = rope_analysis.get('llama3.1-8b', {}).get('rope_theta', 0)\n",
    "mistral_theta = rope_analysis.get('mistral-7b', {}).get('rope_theta', 0)\n",
    "\n",
    "if llama_theta and mistral_theta:\n",
    "    ratio = llama_theta / mistral_theta\n",
    "    print(f\"\\nLLaMA RoPE theta: {llama_theta:,}\")\n",
    "    print(f\"Mistral RoPE theta: {mistral_theta:,}\")\n",
    "    print(f\"Ratio: {ratio:.1f}x\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    if ratio > 10:\n",
    "        print(\"VERDICT: HYPOTHESIS SUPPORTED!\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"\\nLLaMA uses much larger RoPE theta for long-context stability.\")\n",
    "        print(\"This may require dampened residual streams to prevent explosion.\")\n",
    "    else:\n",
    "        print(\"VERDICT: INCONCLUSIVE\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"\\nRoPE theta difference alone may not explain the behavior.\")\n",
    "else:\n",
    "    print(\"Cannot evaluate - missing RoPE theta data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 10: Save All Results\n\nfinal_results = {\n    'experiment': 'LLaMA 3.1 Anomaly - Hypothesis Tests',\n    'date': '2026-01-05',\n    'hypothesis_1_titanium_projector': {\n        'description': 'W_U norm compensates for residual contraction',\n        'results': all_results\n    },\n    'hypothesis_2_rope': {\n        'description': 'Long-context RoPE requires dampening',\n        'results': rope_analysis\n    },\n    'hypothesis_3_grokking': {\n        'description': 'Training scale causes efficient representations',\n        'status': 'THEORETICAL - Requires checkpoint analysis'\n    }\n}\n\noutput_path = f'{RESULTS_DIR}/llama_anomaly_hypothesis_tests.json'\nwith open(output_path, 'w') as f:\n    json.dump(final_results, f, indent=2, default=str)\n\nprint(f\"Results saved to {output_path}\")\nprint(\"\\n\" + \"=\"*70)\nprint(\"EXPERIMENT COMPLETE\")\nprint(\"=\"*70)"
  },
  {
   "cell_type": "code",
   "source": "# Cell 11: DOWNLOAD ALL RESULTS\n# Run this cell to download all files to your computer\n\nfrom google.colab import files\nimport glob\n\nprint(\"=\"*70)\nprint(\"DOWNLOADING RESULTS...\")\nprint(\"=\"*70)\n\n# Download JSON\njson_path = f'{RESULTS_DIR}/llama_anomaly_hypothesis_tests.json'\nif os.path.exists(json_path):\n    print(f\"\\nDownloading: {json_path}\")\n    files.download(json_path)\n\n# Download PNG\npng_path = f'{RESULTS_DIR}/titanium_projector_hypothesis.png'\nif os.path.exists(png_path):\n    print(f\"Downloading: {png_path}\")\n    files.download(png_path)\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"DOWNLOAD COMPLETE!\")\nprint(\"=\"*70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Hypothesis 1: Titanium Projector\n",
    "- **Test:** Compare W_U spectral norms\n",
    "- **Prediction:** LLaMA's sigma_max should be ~2-3x larger to compensate for 0.48x contraction\n",
    "\n",
    "### Hypothesis 2: Long-Context Dampening\n",
    "- **Test:** Compare RoPE theta values\n",
    "- **Prediction:** LLaMA should have much larger RoPE theta for 128k context\n",
    "\n",
    "### Hypothesis 3: Grokking\n",
    "- **Status:** THEORETICAL\n",
    "- **Test would require:** Training checkpoints to measure gain over training time\n",
    "- **Prediction:** Gain should collapse at the \"grokking point\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}