{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Clean Residual Gain Validation - WITHOUT Final LayerNorm\n",
        "\n",
        "## Critical Discovery\n",
        "\n",
        "Previous experiments showed **BOTH** Pythia and GPT-J dampening:\n",
        "- Pythia-6.9B: G = 0.320\n",
        "- GPT-J-6B: G = 0.373\n",
        "\n",
        "**But this was WRONG!** We were measuring:\n",
        "```\n",
        "G = ||hidden_states[-1]|| / ||hidden_states[-2]||\n",
        "```\n",
        "\n",
        "In HuggingFace:\n",
        "- `hidden_states[-1]` = AFTER final LayerNorm (`transformer.ln_f`)\n",
        "- `hidden_states[-2]` = Output of last transformer block\n",
        "\n",
        "The final LayerNorm **normalizes** and **shrinks** the output!\n",
        "\n",
        "## Correct Methodology\n",
        "\n",
        "We need to measure the TRUE residual stream gain:\n",
        "```\n",
        "G_true = ||layer_L_output|| / ||layer_{L-1}_output||\n",
        "       = ||hidden_states[-2]|| / ||hidden_states[-3]||\n",
        "```\n",
        "\n",
        "This excludes the final LayerNorm artifact.\n",
        "\n",
        "## H25 Hypothesis\n",
        "\n",
        "$$\\rho = \\frac{n_{heads}}{d_{head}}$$\n",
        "\n",
        "- Pythia: $\\rho = 32/128 = 0.25$ (HIGH) → Should DAMPEN\n",
        "- GPT-J: $\\rho = 16/256 = 0.0625$ (LOW) → Should EXPAND"
      ],
      "metadata": {
        "id": "intro"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch numpy matplotlib --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
        "import json\n",
        "from datetime import datetime\n",
        "import gc\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Fixed seed\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ],
      "metadata": {
        "id": "setup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Canonical prompts - SAME for all experiments\n",
        "CANONICAL_PROMPTS = [\n",
        "    \"The capital of France is\",\n",
        "    \"Water freezes at\",\n",
        "    \"The quick brown fox\",\n",
        "]\n",
        "\n",
        "# Models to compare\n",
        "MODELS = [\n",
        "    ('pythia-6.9b', 'EleutherAI/pythia-6.9b'),\n",
        "    ('gpt-j-6b', 'EleutherAI/gpt-j-6B'),\n",
        "]\n",
        "\n",
        "# Fallback for smaller GPUs\n",
        "FALLBACK_MODELS = [\n",
        "    ('pythia-1.4b', 'EleutherAI/pythia-1.4b'),\n",
        "    ('pythia-410m', 'EleutherAI/pythia-410m'),\n",
        "]\n",
        "\n",
        "print(f\"Prompts: {CANONICAL_PROMPTS}\")"
      ],
      "metadata": {
        "id": "config"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_residual_gains_detailed(model, tokenizer, prompts, verbose=True):\n",
        "    \"\"\"\n",
        "    Compute residual gains with CORRECT methodology.\n",
        "    \n",
        "    Returns TWO metrics:\n",
        "    1. gain_with_ln: ||hidden_states[-1]|| / ||hidden_states[-2]|| (WRONG - includes final LN)\n",
        "    2. gain_no_ln: ||hidden_states[-2]|| / ||hidden_states[-3]|| (CORRECT - true last layer)\n",
        "    \n",
        "    HuggingFace hidden_states structure:\n",
        "    - hidden_states[0] = embedding output\n",
        "    - hidden_states[1] = layer 0 output\n",
        "    - hidden_states[2] = layer 1 output\n",
        "    - ...\n",
        "    - hidden_states[n_layers] = last transformer block output\n",
        "    - hidden_states[n_layers+1] = AFTER final LayerNorm (if model has one)\n",
        "    \n",
        "    For models WITHOUT separate final LN in hidden_states:\n",
        "    - hidden_states[-1] = last transformer block output\n",
        "    - hidden_states[-2] = second-to-last block output\n",
        "    \"\"\"\n",
        "    gains_with_ln = []\n",
        "    gains_no_ln = []\n",
        "    all_layer_gains = []\n",
        "    \n",
        "    for prompt in prompts:\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "        if torch.cuda.is_available():\n",
        "            inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs, output_hidden_states=True)\n",
        "        \n",
        "        hidden_states = outputs.hidden_states\n",
        "        n_states = len(hidden_states)\n",
        "        \n",
        "        if verbose:\n",
        "            print(f\"\\n  Prompt: '{prompt}'\")\n",
        "            print(f\"  Hidden states: {n_states}\")\n",
        "        \n",
        "        # Compute norms for all hidden states (last token)\n",
        "        norms = []\n",
        "        for i, h in enumerate(hidden_states):\n",
        "            norm = torch.norm(h[:, -1, :].float(), dim=-1).item()\n",
        "            norms.append(norm)\n",
        "        \n",
        "        # Compute layer-by-layer gains\n",
        "        layer_gains = []\n",
        "        for i in range(1, len(norms)):\n",
        "            gain = norms[i] / (norms[i-1] + 1e-10)\n",
        "            layer_gains.append(gain)\n",
        "        \n",
        "        all_layer_gains.append(layer_gains)\n",
        "        \n",
        "        # WRONG metric (includes final LN): last gain\n",
        "        gain_with_ln = layer_gains[-1]\n",
        "        gains_with_ln.append(gain_with_ln)\n",
        "        \n",
        "        # CORRECT metric (no final LN): second-to-last gain\n",
        "        # This is ||layer_L|| / ||layer_{L-1}||\n",
        "        gain_no_ln = layer_gains[-2] if len(layer_gains) >= 2 else layer_gains[-1]\n",
        "        gains_no_ln.append(gain_no_ln)\n",
        "        \n",
        "        if verbose:\n",
        "            print(f\"    Norms: embed={norms[0]:.1f}, ..., L-2={norms[-3]:.1f}, L-1={norms[-2]:.1f}, final={norms[-1]:.1f}\")\n",
        "            print(f\"    Gain (with final LN):    {gain_with_ln:.4f}\")\n",
        "            print(f\"    Gain (WITHOUT final LN): {gain_no_ln:.4f}\")\n",
        "    \n",
        "    # Aggregate\n",
        "    avg_layer_gains = np.mean(all_layer_gains, axis=0)\n",
        "    \n",
        "    return {\n",
        "        'gain_with_ln_mean': float(np.mean(gains_with_ln)),\n",
        "        'gain_with_ln_std': float(np.std(gains_with_ln)),\n",
        "        'gain_no_ln_mean': float(np.mean(gains_no_ln)),\n",
        "        'gain_no_ln_std': float(np.std(gains_no_ln)),\n",
        "        'all_layer_gains': avg_layer_gains.tolist(),\n",
        "        'is_dampening_with_ln': bool(np.mean(gains_with_ln) < 1.0),\n",
        "        'is_dampening_no_ln': bool(np.mean(gains_no_ln) < 1.0),\n",
        "        'n_hidden_states': n_states\n",
        "    }"
      ],
      "metadata": {
        "id": "compute"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_rho(config):\n",
        "    \"\"\"Compute head density rho = n_heads / d_head.\"\"\"\n",
        "    n_heads = getattr(config, 'num_attention_heads', None) or \\\n",
        "              getattr(config, 'n_head', None)\n",
        "    d_model = getattr(config, 'hidden_size', None) or \\\n",
        "              getattr(config, 'n_embd', None)\n",
        "    n_layers = getattr(config, 'num_hidden_layers', None) or \\\n",
        "               getattr(config, 'n_layer', None)\n",
        "    \n",
        "    if n_heads and d_model:\n",
        "        d_head = d_model // n_heads\n",
        "        rho = n_heads / d_head\n",
        "        return {\n",
        "            'n_heads': int(n_heads),\n",
        "            'd_model': int(d_model),\n",
        "            'd_head': int(d_head),\n",
        "            'n_layers': int(n_layers) if n_layers else None,\n",
        "            'rho': float(rho)\n",
        "        }\n",
        "    return None"
      ],
      "metadata": {
        "id": "rho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select models based on GPU\n",
        "if torch.cuda.is_available():\n",
        "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    if gpu_mem >= 20:\n",
        "        models_to_test = MODELS\n",
        "        print(f\"GPU has {gpu_mem:.1f}GB - testing full 6B models\")\n",
        "    else:\n",
        "        models_to_test = FALLBACK_MODELS\n",
        "        print(f\"GPU has {gpu_mem:.1f}GB - using smaller models\")\n",
        "else:\n",
        "    models_to_test = FALLBACK_MODELS\n",
        "    print(\"No GPU - using smaller models\")\n",
        "\n",
        "print(f\"\\nModels: {[m[0] for m in models_to_test]}\")"
      ],
      "metadata": {
        "id": "select"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main experiment\n",
        "results = {}\n",
        "\n",
        "for name, path in models_to_test:\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Model: {name}\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    try:\n",
        "        # Load config\n",
        "        config = AutoConfig.from_pretrained(path)\n",
        "        rho_info = compute_rho(config)\n",
        "        \n",
        "        if rho_info:\n",
        "            print(f\"\\n Architecture:\")\n",
        "            print(f\"   n_heads = {rho_info['n_heads']}\")\n",
        "            print(f\"   d_head = {rho_info['d_head']}\")\n",
        "            print(f\"   n_layers = {rho_info['n_layers']}\")\n",
        "            print(f\"   rho = {rho_info['rho']:.4f}\")\n",
        "        \n",
        "        # Load model with float32 for precision\n",
        "        tokenizer = AutoTokenizer.from_pretrained(path)\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            path,\n",
        "            torch_dtype=torch.float32,\n",
        "            device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        "            low_cpu_mem_usage=True\n",
        "        )\n",
        "        model.eval()\n",
        "        \n",
        "        print(f\"\\n Computing gains...\")\n",
        "        result = compute_residual_gains_detailed(model, tokenizer, CANONICAL_PROMPTS)\n",
        "        \n",
        "        # Add metadata\n",
        "        result['model'] = name\n",
        "        result['path'] = path\n",
        "        if rho_info:\n",
        "            result.update(rho_info)\n",
        "        \n",
        "        # Summary\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"RESULTS for {name}:\")\n",
        "        print(f\"{'='*50}\")\n",
        "        print(f\"\\n  WITH final LayerNorm (WRONG):\")\n",
        "        print(f\"    Gain = {result['gain_with_ln_mean']:.4f} +/- {result['gain_with_ln_std']:.4f}\")\n",
        "        status_with = \"DAMPENING\" if result['is_dampening_with_ln'] else \"EXPANSION\"\n",
        "        print(f\"    Status: {status_with}\")\n",
        "        \n",
        "        print(f\"\\n  WITHOUT final LayerNorm (CORRECT):\")\n",
        "        print(f\"    Gain = {result['gain_no_ln_mean']:.4f} +/- {result['gain_no_ln_std']:.4f}\")\n",
        "        status_no = \"DAMPENING\" if result['is_dampening_no_ln'] else \"EXPANSION\"\n",
        "        print(f\"    Status: {status_no}\")\n",
        "        \n",
        "        results[name] = result\n",
        "        \n",
        "        # Cleanup\n",
        "        del model, tokenizer\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "print(f\"\\n\\nTested {len(results)} models\")"
      ],
      "metadata": {
        "id": "main"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparison Table\n",
        "print(\"\\n\" + \"=\"*90)\n",
        "print(\"COMPARISON: With vs Without Final LayerNorm\")\n",
        "print(\"=\"*90)\n",
        "\n",
        "print(f\"\\n{'Model':<15} {'rho':>8} {'Gain+LN':>12} {'Status+LN':>12} {'Gain-LN':>12} {'Status-LN':>12}\")\n",
        "print(\"-\"*75)\n",
        "\n",
        "for name, r in results.items():\n",
        "    rho = r.get('rho', 0)\n",
        "    \n",
        "    gain_with = r['gain_with_ln_mean']\n",
        "    status_with = \"DAMPEN\" if r['is_dampening_with_ln'] else \"EXPAND\"\n",
        "    \n",
        "    gain_no = r['gain_no_ln_mean']\n",
        "    status_no = \"DAMPEN\" if r['is_dampening_no_ln'] else \"EXPAND\"\n",
        "    \n",
        "    print(f\"{name:<15} {rho:>8.4f} {gain_with:>12.4f} {status_with:>12} {gain_no:>12.4f} {status_no:>12}\")"
      ],
      "metadata": {
        "id": "compare"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# H25 Validation with CORRECT metric\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"H25 VALIDATION (Using CORRECT metric - no final LN)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nH25: rho >= 0.2 -> DAMPENING, rho < 0.2 -> EXPANSION\")\n",
        "\n",
        "correct = 0\n",
        "total = len(results)\n",
        "\n",
        "for name, r in results.items():\n",
        "    rho = r.get('rho', 0)\n",
        "    gain = r['gain_no_ln_mean']  # CORRECT metric\n",
        "    \n",
        "    pred_dampen = rho >= 0.2\n",
        "    actual_dampen = gain < 1.0\n",
        "    \n",
        "    is_correct = pred_dampen == actual_dampen\n",
        "    if is_correct:\n",
        "        correct += 1\n",
        "    \n",
        "    print(f\"\\n  {name}:\")\n",
        "    print(f\"    rho = {rho:.4f}\")\n",
        "    print(f\"    H25 predicts: {'DAMPEN' if pred_dampen else 'EXPAND'}\")\n",
        "    print(f\"    Actual (no LN): G = {gain:.4f} -> {'DAMPEN' if actual_dampen else 'EXPAND'}\")\n",
        "    print(f\"    {'CORRECT' if is_correct else 'WRONG'}\")\n",
        "\n",
        "accuracy = 100 * correct / total if total > 0 else 0\n",
        "print(f\"\\n H25 Accuracy: {correct}/{total} = {accuracy:.0f}%\")\n",
        "\n",
        "if accuracy >= 100:\n",
        "    verdict = \"VALIDATED\"\n",
        "    print(f\"\\n H25 VALIDATED!\")\n",
        "elif accuracy >= 50:\n",
        "    verdict = \"PARTIALLY_VALIDATED\"\n",
        "    print(f\"\\n H25 PARTIALLY VALIDATED\")\n",
        "else:\n",
        "    verdict = \"FALSIFIED\"\n",
        "    print(f\"\\n H25 FALSIFIED\")"
      ],
      "metadata": {
        "id": "h25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization\n",
        "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
        "fig.suptitle('Residual Gain: With vs Without Final LayerNorm', fontsize=14, fontweight='bold')\n",
        "\n",
        "# Panel 1: Bar comparison\n",
        "ax = axes[0]\n",
        "names = list(results.keys())\n",
        "x = np.arange(len(names))\n",
        "width = 0.35\n",
        "\n",
        "gains_with = [r['gain_with_ln_mean'] for r in results.values()]\n",
        "gains_no = [r['gain_no_ln_mean'] for r in results.values()]\n",
        "\n",
        "bars1 = ax.bar(x - width/2, gains_with, width, label='With Final LN (WRONG)', color='red', alpha=0.7)\n",
        "bars2 = ax.bar(x + width/2, gains_no, width, label='Without Final LN (CORRECT)', color='blue', alpha=0.7)\n",
        "\n",
        "ax.axhline(y=1.0, color='black', linestyle='--', alpha=0.5, label='G=1.0')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(names)\n",
        "ax.set_ylabel('Last Layer Gain')\n",
        "ax.set_title('Comparison: Metric Matters!')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Panel 2: rho vs Gain (correct metric)\n",
        "ax = axes[1]\n",
        "rhos = [r.get('rho', 0) for r in results.values()]\n",
        "colors = ['blue' if g < 1.0 else 'red' for g in gains_no]\n",
        "\n",
        "ax.scatter(rhos, gains_no, c=colors, s=200, edgecolors='black', linewidth=2, zorder=5)\n",
        "for i, name in enumerate(names):\n",
        "    ax.annotate(name, (rhos[i], gains_no[i]), xytext=(5, 5), textcoords='offset points', fontsize=10)\n",
        "\n",
        "ax.axhline(y=1.0, color='black', linestyle='--', alpha=0.5, label='G=1.0 (Bentov)')\n",
        "ax.axvline(x=0.2, color='purple', linestyle=':', alpha=0.5, label='rho=0.2')\n",
        "ax.set_xlabel('rho = n_heads / d_head')\n",
        "ax.set_ylabel('Last Layer Gain (no final LN)')\n",
        "ax.set_title('H25: rho vs Gain (CORRECT)')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Panel 3: Layer-by-layer dynamics\n",
        "ax = axes[2]\n",
        "for name, r in results.items():\n",
        "    gains = r['all_layer_gains']\n",
        "    layers = list(range(len(gains)))\n",
        "    color = 'blue' if r['is_dampening_no_ln'] else 'red'\n",
        "    ax.plot(layers, gains, '-o', markersize=2, label=f\"{name} (rho={r.get('rho', 0):.2f})\", color=color)\n",
        "\n",
        "ax.axhline(y=1.0, color='black', linestyle='--', alpha=0.5)\n",
        "\n",
        "# Mark the final LN artifact\n",
        "ax.axvspan(len(results[list(results.keys())[0]]['all_layer_gains']) - 1.5,\n",
        "           len(results[list(results.keys())[0]]['all_layer_gains']) - 0.5,\n",
        "           alpha=0.2, color='red', label='Final LN (artifact)')\n",
        "\n",
        "ax.set_xlabel('Layer Index')\n",
        "ax.set_ylabel('Gain')\n",
        "ax.set_title('Layer-by-Layer (red zone = final LN)')\n",
        "ax.legend(fontsize=8)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('clean_residual_NO_FINAL_LN.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"\\nSaved: clean_residual_NO_FINAL_LN.png\")"
      ],
      "metadata": {
        "id": "viz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save results\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "output = {\n",
        "    'experiment': 'Clean Residual Gain - NO FINAL LAYERNORM',\n",
        "    'date': datetime.now().isoformat(),\n",
        "    'prompts': CANONICAL_PROMPTS,\n",
        "    'precision': 'float32',\n",
        "    'methodology': {\n",
        "        'wrong_metric': 'hidden_states[-1] / hidden_states[-2] (includes final LN)',\n",
        "        'correct_metric': 'hidden_states[-2] / hidden_states[-3] (true last layer gain)'\n",
        "    },\n",
        "    'h25_verdict': verdict,\n",
        "    'h25_accuracy': accuracy,\n",
        "    'results': results\n",
        "}\n",
        "\n",
        "filename = f'clean_residual_NO_FINAL_LN_{timestamp}.json'\n",
        "with open(filename, 'w') as f:\n",
        "    json.dump(output, f, indent=2, default=str)\n",
        "\n",
        "print(f\"Saved: {filename}\")"
      ],
      "metadata": {
        "id": "save"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download(filename)\n",
        "    files.download('clean_residual_NO_FINAL_LN.png')\n",
        "except ImportError:\n",
        "    print(\"Not in Colab\")"
      ],
      "metadata": {
        "id": "download"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final Summary\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"FINAL SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\n CRITICAL FINDING:\")\n",
        "print(\"   Previous measurements included FINAL LAYERNORM artifact!\")\n",
        "print(\"   This caused BOTH models to appear as 'dampening'.\")\n",
        "\n",
        "print(\"\\n CORRECTED RESULTS (without final LN):\")\n",
        "for name, r in results.items():\n",
        "    rho = r.get('rho', 0)\n",
        "    gain_wrong = r['gain_with_ln_mean']\n",
        "    gain_correct = r['gain_no_ln_mean']\n",
        "    status = \"DAMPEN\" if r['is_dampening_no_ln'] else \"EXPAND\"\n",
        "    \n",
        "    print(f\"\\n   {name}:\")\n",
        "    print(f\"     rho = {rho:.4f}\")\n",
        "    print(f\"     WRONG (with LN):    G = {gain_wrong:.4f}\")\n",
        "    print(f\"     CORRECT (no LN):    G = {gain_correct:.4f} -> {status}\")\n",
        "\n",
        "print(f\"\\n H25 VERDICT: {verdict}\")\n",
        "print(f\"   Accuracy: {accuracy:.0f}%\")\n",
        "\n",
        "if accuracy == 100:\n",
        "    print(\"\\n   High rho -> DAMPENING\")\n",
        "    print(\"   Low rho -> EXPANSION\")\n",
        "    print(\"   The 'Dimensional Crowding Hypothesis' is CONFIRMED!\")"
      ],
      "metadata": {
        "id": "final"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
