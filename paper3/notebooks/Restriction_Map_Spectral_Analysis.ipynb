{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restriction Map Spectral Analysis\n",
    "\n",
    "## Purpose\n",
    "Validate the **Sheaf-Thermodynamic Bridge** by analyzing the spectral signatures of restriction maps (W_V, W_O) across different labs.\n",
    "\n",
    "## Hypothesis\n",
    "- **EleutherAI (Dampener)**: Lower spectral energy in W_V/W_O → weaker restriction maps\n",
    "- **Meta/BigScience (Expander)**: Higher spectral energy → stronger restriction maps\n",
    "\n",
    "## Method\n",
    "1. Extract W_V (Value projection) and W_O (Output projection) from attention layers\n",
    "2. Compute SVD (Singular Value Decomposition)\n",
    "3. Compare spectral distributions across labs\n",
    "\n",
    "## Theoretical Background\n",
    "The restriction map in our Sheaf formalization is:\n",
    "$$\\rho_{ij} = \\sqrt{A_{ij}} \\cdot W_V$$\n",
    "\n",
    "The spectral norm of W_V determines the \"strength\" of information flow.\n",
    "\n",
    "---\n",
    "*Paper #3: Thermodynamics of Language Models*\n",
    "*Author: Davide D'Elia*\n",
    "*Date: 2026-01-05*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (Colab)\n",
    "!pip install transformers torch matplotlib seaborn pandas -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoConfig\n",
    "import gc\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_context(\"paper\", font_scale=1.2)\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration\n",
    "\n",
    "We test representatives from each lab/heritage group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models to analyze - Representatives from each lab\n",
    "MODELS_CONFIG = {\n",
    "    # EleutherAI (Dampener Heritage)\n",
    "    \"EleutherAI/pythia-160m\": {\n",
    "        \"lab\": \"EleutherAI\",\n",
    "        \"expected\": \"DAMPENER\",\n",
    "        \"color\": \"#E74C3C\",  # Red\n",
    "        \"marker\": \"o\"\n",
    "    },\n",
    "    \"EleutherAI/pythia-410m\": {\n",
    "        \"lab\": \"EleutherAI\",\n",
    "        \"expected\": \"DAMPENER\",\n",
    "        \"color\": \"#C0392B\",  # Dark Red\n",
    "        \"marker\": \"s\"\n",
    "    },\n",
    "    \"EleutherAI/gpt-neo-125M\": {\n",
    "        \"lab\": \"EleutherAI\",\n",
    "        \"expected\": \"DAMPENER\",\n",
    "        \"color\": \"#F39C12\",  # Orange\n",
    "        \"marker\": \"^\"\n",
    "    },\n",
    "    \n",
    "    # Meta (Expander Heritage)\n",
    "    \"facebook/opt-125m\": {\n",
    "        \"lab\": \"Meta\",\n",
    "        \"expected\": \"EXPANDER\",\n",
    "        \"color\": \"#3498DB\",  # Blue\n",
    "        \"marker\": \"o\"\n",
    "    },\n",
    "    \"facebook/opt-350m\": {\n",
    "        \"lab\": \"Meta\",\n",
    "        \"expected\": \"EXPANDER\",\n",
    "        \"color\": \"#2980B9\",  # Dark Blue\n",
    "        \"marker\": \"s\"\n",
    "    },\n",
    "    \n",
    "    # BigScience (Expander Heritage - ALiBi)\n",
    "    \"bigscience/bloom-560m\": {\n",
    "        \"lab\": \"BigScience\",\n",
    "        \"expected\": \"EXPANDER (ALiBi)\",\n",
    "        \"color\": \"#27AE60\",  # Green\n",
    "        \"marker\": \"o\"\n",
    "    },\n",
    "    \n",
    "    # OpenAI (Baseline Reference)\n",
    "    \"gpt2\": {\n",
    "        \"lab\": \"OpenAI\",\n",
    "        \"expected\": \"NEUTRAL\",\n",
    "        \"color\": \"#9B59B6\",  # Purple\n",
    "        \"marker\": \"o\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Testing {len(MODELS_CONFIG)} models from {len(set(v['lab'] for v in MODELS_CONFIG.values()))} labs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_projection_matrices(model, model_name: str) -> Dict[str, List[torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Extract W_V (Value projection) and W_O (Output projection) from all layers.\n",
    "    \n",
    "    These are the key components of the Restriction Map:\n",
    "    rho_ij = sqrt(A_ij) * W_V\n",
    "    \n",
    "    The output projection W_O then maps back to residual stream.\n",
    "    \"\"\"\n",
    "    W_V_list = []\n",
    "    W_O_list = []\n",
    "    W_Q_list = []\n",
    "    W_K_list = []\n",
    "    \n",
    "    # Identify architecture\n",
    "    if hasattr(model, 'gpt_neox'):  # Pythia\n",
    "        layers = model.gpt_neox.layers\n",
    "        arch = 'neox'\n",
    "    elif hasattr(model, 'transformer') and hasattr(model.transformer, 'h'):  # GPT-2, GPT-J, GPT-Neo\n",
    "        layers = model.transformer.h\n",
    "        if hasattr(layers[0].attn, 'out_proj'):  # GPT-J\n",
    "            arch = 'gptj'\n",
    "        else:  # GPT-2, GPT-Neo\n",
    "            arch = 'gpt2'\n",
    "    elif hasattr(model, 'model') and hasattr(model.model, 'layers'):  # Llama/Mistral\n",
    "        layers = model.model.layers\n",
    "        arch = 'llama'\n",
    "    elif hasattr(model, 'transformer') and hasattr(model.transformer, 'blocks'):  # BLOOM\n",
    "        layers = model.transformer.blocks if hasattr(model.transformer, 'blocks') else model.transformer.h\n",
    "        arch = 'bloom'\n",
    "    else:\n",
    "        print(f\"Unknown architecture for {model_name}\")\n",
    "        return {}\n",
    "    \n",
    "    print(f\"  Architecture: {arch}, Layers: {len(layers)}\")\n",
    "    \n",
    "    for i, layer in enumerate(layers):\n",
    "        try:\n",
    "            if arch == 'neox':  # Pythia\n",
    "                # Pythia uses query_key_value combined, then dense for output\n",
    "                qkv = layer.attention.query_key_value.weight.detach()\n",
    "                # Split into Q, K, V (each d_model x d_model/3 approximately)\n",
    "                hidden_size = qkv.shape[0] // 3\n",
    "                W_Q = qkv[:hidden_size, :]\n",
    "                W_K = qkv[hidden_size:2*hidden_size, :]\n",
    "                W_V = qkv[2*hidden_size:, :]\n",
    "                W_O = layer.attention.dense.weight.detach()\n",
    "                \n",
    "            elif arch == 'gptj':  # GPT-J\n",
    "                W_Q = layer.attn.q_proj.weight.detach()\n",
    "                W_K = layer.attn.k_proj.weight.detach()\n",
    "                W_V = layer.attn.v_proj.weight.detach()\n",
    "                W_O = layer.attn.out_proj.weight.detach()\n",
    "                \n",
    "            elif arch == 'gpt2':  # GPT-2, GPT-Neo\n",
    "                # GPT-2 combines c_attn (Q,K,V) and c_proj (O)\n",
    "                c_attn = layer.attn.c_attn.weight.detach()\n",
    "                hidden_size = c_attn.shape[1] // 3\n",
    "                W_Q = c_attn[:, :hidden_size].T\n",
    "                W_K = c_attn[:, hidden_size:2*hidden_size].T\n",
    "                W_V = c_attn[:, 2*hidden_size:].T\n",
    "                W_O = layer.attn.c_proj.weight.detach().T\n",
    "                \n",
    "            elif arch == 'llama':  # Llama/Mistral\n",
    "                W_Q = layer.self_attn.q_proj.weight.detach()\n",
    "                W_K = layer.self_attn.k_proj.weight.detach()\n",
    "                W_V = layer.self_attn.v_proj.weight.detach()\n",
    "                W_O = layer.self_attn.o_proj.weight.detach()\n",
    "                \n",
    "            elif arch == 'bloom':  # BLOOM\n",
    "                # BLOOM uses query_key_value combined\n",
    "                qkv = layer.self_attention.query_key_value.weight.detach()\n",
    "                hidden_size = qkv.shape[0] // 3\n",
    "                W_Q = qkv[:hidden_size, :]\n",
    "                W_K = qkv[hidden_size:2*hidden_size, :]\n",
    "                W_V = qkv[2*hidden_size:, :]\n",
    "                W_O = layer.self_attention.dense.weight.detach()\n",
    "            \n",
    "            W_Q_list.append(W_Q.float())\n",
    "            W_K_list.append(W_K.float())\n",
    "            W_V_list.append(W_V.float())\n",
    "            W_O_list.append(W_O.float())\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error in layer {i}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return {\n",
    "        'W_Q': W_Q_list,\n",
    "        'W_K': W_K_list,\n",
    "        'W_V': W_V_list,\n",
    "        'W_O': W_O_list,\n",
    "        'arch': arch,\n",
    "        'n_layers': len(layers)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_spectral_signature(matrices: List[torch.Tensor]) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Compute spectral signatures (SVD) for a list of matrices.\n",
    "    \n",
    "    Returns:\n",
    "    - singular_values: All singular values concatenated\n",
    "    - spectral_norm: Max singular value per layer (||W||_2)\n",
    "    - frobenius_norm: Frobenius norm per layer (||W||_F)\n",
    "    - effective_rank: Effective rank per layer\n",
    "    - condition_number: Condition number per layer\n",
    "    \"\"\"\n",
    "    all_singular_values = []\n",
    "    spectral_norms = []\n",
    "    frobenius_norms = []\n",
    "    effective_ranks = []\n",
    "    condition_numbers = []\n",
    "    \n",
    "    for W in matrices:\n",
    "        try:\n",
    "            # Compute SVD\n",
    "            S = torch.linalg.svdvals(W)\n",
    "            s = S.numpy()\n",
    "            \n",
    "            all_singular_values.append(s)\n",
    "            spectral_norms.append(s[0])  # Max singular value\n",
    "            frobenius_norms.append(np.sqrt(np.sum(s**2)))\n",
    "            \n",
    "            # Effective rank: exp(entropy of normalized singular values)\n",
    "            s_norm = s / s.sum()\n",
    "            entropy = -np.sum(s_norm * np.log(s_norm + 1e-10))\n",
    "            effective_ranks.append(np.exp(entropy))\n",
    "            \n",
    "            # Condition number: ratio of max to min singular value\n",
    "            condition_numbers.append(s[0] / (s[-1] + 1e-10))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"SVD error: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return {\n",
    "        'singular_values': all_singular_values,\n",
    "        'spectral_norm': np.array(spectral_norms),\n",
    "        'frobenius_norm': np.array(frobenius_norms),\n",
    "        'effective_rank': np.array(effective_ranks),\n",
    "        'condition_number': np.array(condition_numbers)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_model(model_name: str, config: Dict) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Full spectral analysis for a single model.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Analyzing: {model_name}\")\n",
    "    print(f\"Lab: {config['lab']}, Expected: {config['expected']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        # Load model\n",
    "        print(\"  Loading model...\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float32,\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "        model.eval()\n",
    "        \n",
    "        # Extract projection matrices\n",
    "        print(\"  Extracting projection matrices...\")\n",
    "        matrices = get_projection_matrices(model, model_name)\n",
    "        \n",
    "        if not matrices:\n",
    "            return None\n",
    "        \n",
    "        # Compute spectral signatures\n",
    "        print(\"  Computing spectral signatures...\")\n",
    "        results = {\n",
    "            'model': model_name,\n",
    "            'lab': config['lab'],\n",
    "            'expected': config['expected'],\n",
    "            'color': config['color'],\n",
    "            'marker': config['marker'],\n",
    "            'arch': matrices['arch'],\n",
    "            'n_layers': matrices['n_layers']\n",
    "        }\n",
    "        \n",
    "        for matrix_type in ['W_V', 'W_O', 'W_Q', 'W_K']:\n",
    "            if matrices[matrix_type]:\n",
    "                spectral = compute_spectral_signature(matrices[matrix_type])\n",
    "                results[f'{matrix_type}_spectral_norm'] = spectral['spectral_norm']\n",
    "                results[f'{matrix_type}_frobenius_norm'] = spectral['frobenius_norm']\n",
    "                results[f'{matrix_type}_effective_rank'] = spectral['effective_rank']\n",
    "                results[f'{matrix_type}_mean_spectral'] = float(np.mean(spectral['spectral_norm']))\n",
    "                results[f'{matrix_type}_std_spectral'] = float(np.std(spectral['spectral_norm']))\n",
    "        \n",
    "        # Cleanup\n",
    "        del model\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        print(f\"  W_V mean spectral norm: {results.get('W_V_mean_spectral', 'N/A'):.4f}\")\n",
    "        print(f\"  W_O mean spectral norm: {results.get('W_O_mean_spectral', 'N/A'):.4f}\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  FAILED: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run analysis on all models\n",
    "all_results = []\n",
    "\n",
    "for model_name, config in MODELS_CONFIG.items():\n",
    "    result = analyze_model(model_name, config)\n",
    "    if result:\n",
    "        all_results.append(result)\n",
    "\n",
    "print(f\"\\n\\nSuccessfully analyzed {len(all_results)} models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization: Spectral Signatures by Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# Plot 1: W_V Spectral Norm by Layer\n",
    "ax1 = axes[0, 0]\n",
    "for res in all_results:\n",
    "    if 'W_V_spectral_norm' in res:\n",
    "        ax1.plot(\n",
    "            res['W_V_spectral_norm'],\n",
    "            label=f\"{res['model'].split('/')[-1]} ({res['lab']})\",\n",
    "            color=res['color'],\n",
    "            marker=res['marker'],\n",
    "            markersize=4,\n",
    "            alpha=0.8\n",
    "        )\n",
    "ax1.set_title('W_V Spectral Norm (Value Projection) by Layer')\n",
    "ax1.set_xlabel('Layer')\n",
    "ax1.set_ylabel('Spectral Norm ||W_V||_2')\n",
    "ax1.legend(fontsize=8)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: W_O Spectral Norm by Layer\n",
    "ax2 = axes[0, 1]\n",
    "for res in all_results:\n",
    "    if 'W_O_spectral_norm' in res:\n",
    "        ax2.plot(\n",
    "            res['W_O_spectral_norm'],\n",
    "            label=f\"{res['model'].split('/')[-1]} ({res['lab']})\",\n",
    "            color=res['color'],\n",
    "            marker=res['marker'],\n",
    "            markersize=4,\n",
    "            alpha=0.8\n",
    "        )\n",
    "ax2.set_title('W_O Spectral Norm (Output Projection) by Layer')\n",
    "ax2.set_xlabel('Layer')\n",
    "ax2.set_ylabel('Spectral Norm ||W_O||_2')\n",
    "ax2.legend(fontsize=8)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Lab Comparison (Bar Chart)\n",
    "ax3 = axes[1, 0]\n",
    "lab_data = {}\n",
    "for res in all_results:\n",
    "    lab = res['lab']\n",
    "    if lab not in lab_data:\n",
    "        lab_data[lab] = {'W_V': [], 'W_O': [], 'color': res['color']}\n",
    "    if 'W_V_mean_spectral' in res:\n",
    "        lab_data[lab]['W_V'].append(res['W_V_mean_spectral'])\n",
    "    if 'W_O_mean_spectral' in res:\n",
    "        lab_data[lab]['W_O'].append(res['W_O_mean_spectral'])\n",
    "\n",
    "labs = list(lab_data.keys())\n",
    "x = np.arange(len(labs))\n",
    "width = 0.35\n",
    "\n",
    "wv_means = [np.mean(lab_data[l]['W_V']) if lab_data[l]['W_V'] else 0 for l in labs]\n",
    "wo_means = [np.mean(lab_data[l]['W_O']) if lab_data[l]['W_O'] else 0 for l in labs]\n",
    "colors = [lab_data[l]['color'] for l in labs]\n",
    "\n",
    "bars1 = ax3.bar(x - width/2, wv_means, width, label='W_V (Value)', color=colors, alpha=0.7)\n",
    "bars2 = ax3.bar(x + width/2, wo_means, width, label='W_O (Output)', color=colors, alpha=0.4, hatch='//')\n",
    "\n",
    "ax3.set_title('Mean Spectral Norm by Lab (Restriction Map Strength)')\n",
    "ax3.set_xlabel('Lab')\n",
    "ax3.set_ylabel('Mean Spectral Norm')\n",
    "ax3.set_xticks(x)\n",
    "ax3.set_xticklabels(labs)\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 4: Effective Rank Distribution\n",
    "ax4 = axes[1, 1]\n",
    "for res in all_results:\n",
    "    if 'W_V_effective_rank' in res:\n",
    "        ax4.plot(\n",
    "            res['W_V_effective_rank'],\n",
    "            label=f\"{res['model'].split('/')[-1]}\",\n",
    "            color=res['color'],\n",
    "            marker=res['marker'],\n",
    "            markersize=4,\n",
    "            alpha=0.8\n",
    "        )\n",
    "ax4.set_title('W_V Effective Rank by Layer')\n",
    "ax4.set_xlabel('Layer')\n",
    "ax4.set_ylabel('Effective Rank')\n",
    "ax4.legend(fontsize=8)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('restriction_map_spectral_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFigure saved: restriction_map_spectral_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table\n",
    "summary_data = []\n",
    "for res in all_results:\n",
    "    summary_data.append({\n",
    "        'Model': res['model'].split('/')[-1],\n",
    "        'Lab': res['lab'],\n",
    "        'Expected': res['expected'],\n",
    "        'Layers': res['n_layers'],\n",
    "        'W_V Mean': res.get('W_V_mean_spectral', np.nan),\n",
    "        'W_V Std': res.get('W_V_std_spectral', np.nan),\n",
    "        'W_O Mean': res.get('W_O_mean_spectral', np.nan),\n",
    "        'W_O Std': res.get('W_O_std_spectral', np.nan),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(summary_data)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SPECTRAL SIGNATURE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# Lab averages\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LAB AVERAGES\")\n",
    "print(\"=\"*80)\n",
    "lab_summary = df.groupby('Lab').agg({\n",
    "    'W_V Mean': ['mean', 'std'],\n",
    "    'W_O Mean': ['mean', 'std']\n",
    "}).round(4)\n",
    "print(lab_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Test: EleutherAI vs Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Separate EleutherAI from others\n",
    "eleuther_wv = df[df['Lab'] == 'EleutherAI']['W_V Mean'].dropna().values\n",
    "others_wv = df[df['Lab'] != 'EleutherAI']['W_V Mean'].dropna().values\n",
    "\n",
    "eleuther_wo = df[df['Lab'] == 'EleutherAI']['W_O Mean'].dropna().values\n",
    "others_wo = df[df['Lab'] != 'EleutherAI']['W_O Mean'].dropna().values\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HYPOTHESIS TEST: EleutherAI (Dampener) vs Others (Expander)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if len(eleuther_wv) > 0 and len(others_wv) > 0:\n",
    "    # Mann-Whitney U test (non-parametric)\n",
    "    stat_wv, p_wv = stats.mannwhitneyu(eleuther_wv, others_wv, alternative='less')\n",
    "    stat_wo, p_wo = stats.mannwhitneyu(eleuther_wo, others_wo, alternative='less')\n",
    "    \n",
    "    print(f\"\\nW_V Spectral Norm:\")\n",
    "    print(f\"  EleutherAI mean: {np.mean(eleuther_wv):.4f}\")\n",
    "    print(f\"  Others mean:     {np.mean(others_wv):.4f}\")\n",
    "    print(f\"  Mann-Whitney U:  {stat_wv:.2f}, p = {p_wv:.4f}\")\n",
    "    print(f\"  H1 (EleutherAI < Others): {'SUPPORTED' if p_wv < 0.05 else 'NOT SUPPORTED'}\")\n",
    "    \n",
    "    print(f\"\\nW_O Spectral Norm:\")\n",
    "    print(f\"  EleutherAI mean: {np.mean(eleuther_wo):.4f}\")\n",
    "    print(f\"  Others mean:     {np.mean(others_wo):.4f}\")\n",
    "    print(f\"  Mann-Whitney U:  {stat_wo:.2f}, p = {p_wo:.4f}\")\n",
    "    print(f\"  H1 (EleutherAI < Others): {'SUPPORTED' if p_wo < 0.05 else 'NOT SUPPORTED'}\")\n",
    "else:\n",
    "    print(\"Insufficient data for statistical test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare results for JSON\n",
    "def convert_to_serializable(obj):\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, (np.float32, np.float64)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, (np.int32, np.int64)):\n",
    "        return int(obj)\n",
    "    return obj\n",
    "\n",
    "# Convert all results\n",
    "serializable_results = []\n",
    "for res in all_results:\n",
    "    clean_res = {}\n",
    "    for k, v in res.items():\n",
    "        clean_res[k] = convert_to_serializable(v)\n",
    "    serializable_results.append(clean_res)\n",
    "\n",
    "# Save to JSON\n",
    "output = {\n",
    "    'experiment': 'Restriction Map Spectral Analysis',\n",
    "    'date': datetime.now().isoformat(),\n",
    "    'hypothesis': 'EleutherAI (Dampener) has lower spectral energy than others (Expander)',\n",
    "    'models_tested': len(all_results),\n",
    "    'results': serializable_results,\n",
    "    'summary': df.to_dict('records') if len(df) > 0 else []\n",
    "}\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "filename = f'restriction_map_spectral_{timestamp}.json'\n",
    "\n",
    "with open(filename, 'w') as f:\n",
    "    json.dump(output, f, indent=2)\n",
    "\n",
    "print(f\"\\nResults saved to: {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "### Expected Results:\n",
    "\n",
    "If our **Sheaf-Thermodynamic Bridge** hypothesis is correct:\n",
    "\n",
    "1. **EleutherAI models** (Dampeners) should have **lower** spectral norms in W_V and W_O\n",
    "   - This means \"weaker\" restriction maps\n",
    "   - Less energy transfer through the sheaf\n",
    "   - Leads to DAMPENING of residual stream\n",
    "\n",
    "2. **Meta/BigScience models** (Expanders) should have **higher** spectral norms\n",
    "   - \"Stronger\" restriction maps\n",
    "   - More energy transfer\n",
    "   - Leads to EXPANSION of residual stream\n",
    "\n",
    "### Connection to Sheaf Theory:\n",
    "\n",
    "The spectral norm of W_V directly relates to the Sheaf Laplacian:\n",
    "\n",
    "$$||L_\\mathcal{F}|| \\propto \\sum_{j} A_{ij} \\cdot ||W_V^T W_V||$$\n",
    "\n",
    "Higher spectral norm → Stronger coupling → More diffusion → Expansion\n",
    "Lower spectral norm → Weaker coupling → Less diffusion → Dampening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nFiles generated:\")\n",
    "print(f\"  - restriction_map_spectral_analysis.png\")\n",
    "print(f\"  - {filename}\")\n",
    "print(\"\\nThis data provides empirical evidence for the Sheaf-Thermodynamic Bridge.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
