{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mistral Paradox Investigation\n",
    "\n",
    "**Problem:** Mistral-7B zeigt Last MLP Gain = 0.58x (CONTRACTS statt EXPANDS)\n",
    "\n",
    "**Hypothesen:**\n",
    "1. **Silent Exit:** Explosion versteckt in Unembedding Matrix W_U\n",
    "2. **Architektur vs Training:** Apertus-8B (verwandt) zeigt Explosion bei L28?\n",
    "3. **Entropy Efficiency:** Mistral braucht keine Explosion weil Output bereits scharf\n",
    "\n",
    "**Tests:**\n",
    "1. Unembedding Norm Vergleich (Mistral vs Pythia vs Gemma)\n",
    "2. Apertus-8B FFN Expansion Analysis\n",
    "3. Output Logit Entropy vor Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check GPU\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"GPU Memory: {gpu_mem:.1f} GB\")\n",
    "else:\n",
    "    gpu_name = \"CPU\"\n",
    "    gpu_mem = 0\n",
    "    print(\"Running on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Model Definitions\n",
    "\n",
    "MODELS_TO_TEST = {\n",
    "    'mistral-7b': {\n",
    "        'hf_name': 'mistralai/Mistral-7B-v0.1',\n",
    "        'params': 7e9,\n",
    "        'layers': 32,\n",
    "        'family': 'mistral',\n",
    "        'memory_gb': 20\n",
    "    },\n",
    "    'apertus-8b': {\n",
    "        'hf_name': 'jphme/Apertus-8b',  # Multilingual model\n",
    "        'params': 8e9,\n",
    "        'layers': 32,\n",
    "        'family': 'llama',\n",
    "        'memory_gb': 22\n",
    "    },\n",
    "    'pythia-6.9b': {\n",
    "        'hf_name': 'EleutherAI/pythia-6.9b',\n",
    "        'params': 6.9e9,\n",
    "        'layers': 32,\n",
    "        'family': 'pythia',\n",
    "        'memory_gb': 20\n",
    "    },\n",
    "    'gemma-7b': {\n",
    "        'hf_name': 'google/gemma-7b',\n",
    "        'params': 7e9,\n",
    "        'layers': 28,\n",
    "        'family': 'gemma',\n",
    "        'memory_gb': 20\n",
    "    }\n",
    "}\n",
    "\n",
    "# Select based on GPU memory\n",
    "def select_models(mem_gb):\n",
    "    selected = []\n",
    "    # Priority: Mistral (the problem) and one comparison\n",
    "    if mem_gb >= 20:\n",
    "        selected.append('mistral-7b')\n",
    "    if mem_gb >= 22:\n",
    "        selected.append('apertus-8b')\n",
    "    elif mem_gb >= 20:\n",
    "        selected.append('pythia-6.9b')\n",
    "    return selected\n",
    "\n",
    "selected = select_models(gpu_mem) if torch.cuda.is_available() else ['mistral-7b']\n",
    "print(f\"Models to test: {selected}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load Model Function\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def load_model(model_key):\n",
    "    \"\"\"Load model and return model, tokenizer, and key matrices.\"\"\"\n",
    "    info = MODELS_TO_TEST[model_key]\n",
    "    hf_name = info['hf_name']\n",
    "    \n",
    "    print(f\"\\nLoading {model_key} ({hf_name})...\")\n",
    "    \n",
    "    dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(hf_name, trust_remote_code=True)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            hf_name,\n",
    "            torch_dtype=dtype,\n",
    "            device_map='auto' if torch.cuda.is_available() else None,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        if not torch.cuda.is_available():\n",
    "            model = model.to(device)\n",
    "        \n",
    "        model.eval()\n",
    "        print(f\"  Loaded successfully!\")\n",
    "        return model, tokenizer, info\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Failed to load: {e}\")\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## TEST 1: Unembedding Matrix Norm Analysis\n",
    "\n",
    "**Hypothesis:** Mistral hides the \"explosion\" in a statically large W_U (unembedding matrix) instead of dynamically in the final FFN.\n",
    "\n",
    "**Prediction:** Mistral's W_U has disproportionally higher norm than Pythia/Gemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Extract Unembedding Matrix\n",
    "\n",
    "def get_unembedding_matrix(model, family):\n",
    "    \"\"\"Extract the unembedding (lm_head) matrix from different architectures.\"\"\"\n",
    "    \n",
    "    # Most models: lm_head.weight\n",
    "    if hasattr(model, 'lm_head'):\n",
    "        W_U = model.lm_head.weight.data\n",
    "    elif hasattr(model, 'embed_out'):\n",
    "        W_U = model.embed_out.weight.data\n",
    "    else:\n",
    "        raise ValueError(f\"Cannot find unembedding matrix for {family}\")\n",
    "    \n",
    "    return W_U.float()  # Convert to float32 for analysis\n",
    "\n",
    "def get_embedding_matrix(model, family):\n",
    "    \"\"\"Extract the embedding matrix for comparison.\"\"\"\n",
    "    \n",
    "    if family == 'pythia':\n",
    "        W_E = model.gpt_neox.embed_in.weight.data\n",
    "    elif family in ['mistral', 'llama', 'gemma']:\n",
    "        W_E = model.model.embed_tokens.weight.data\n",
    "    else:\n",
    "        W_E = model.get_input_embeddings().weight.data\n",
    "    \n",
    "    return W_E.float()\n",
    "\n",
    "def analyze_matrix_norms(W, name=\"Matrix\"):\n",
    "    \"\"\"Compute various norms of a matrix.\"\"\"\n",
    "    \n",
    "    # Move to CPU for analysis\n",
    "    W_cpu = W.cpu()\n",
    "    \n",
    "    results = {\n",
    "        'shape': list(W_cpu.shape),\n",
    "        'frobenius_norm': float(torch.norm(W_cpu, p='fro').item()),\n",
    "        'spectral_norm': float(torch.linalg.matrix_norm(W_cpu, ord=2).item()),\n",
    "        'max_singular': float(torch.linalg.svdvals(W_cpu)[0].item()),\n",
    "        'mean_row_norm': float(torch.norm(W_cpu, dim=1).mean().item()),\n",
    "        'std_row_norm': float(torch.norm(W_cpu, dim=1).std().item()),\n",
    "        'max_row_norm': float(torch.norm(W_cpu, dim=1).max().item()),\n",
    "        'min_row_norm': float(torch.norm(W_cpu, dim=1).min().item()),\n",
    "        'mean_abs': float(W_cpu.abs().mean().item()),\n",
    "        'std_abs': float(W_cpu.abs().std().item())\n",
    "    }\n",
    "    \n",
    "    # Normalized by dimensions\n",
    "    vocab_size, hidden_dim = W_cpu.shape\n",
    "    results['frobenius_normalized'] = results['frobenius_norm'] / np.sqrt(vocab_size * hidden_dim)\n",
    "    results['spectral_normalized'] = results['spectral_norm'] / np.sqrt(hidden_dim)\n",
    "    \n",
    "    print(f\"\\n{name} Analysis:\")\n",
    "    print(f\"  Shape: {results['shape']}\")\n",
    "    print(f\"  Frobenius Norm: {results['frobenius_norm']:.2f}\")\n",
    "    print(f\"  Spectral Norm: {results['spectral_norm']:.2f}\")\n",
    "    print(f\"  Max Singular Value: {results['max_singular']:.2f}\")\n",
    "    print(f\"  Mean Row Norm: {results['mean_row_norm']:.4f}\")\n",
    "    print(f\"  Row Norm Std: {results['std_row_norm']:.4f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Run Unembedding Analysis on All Models\n",
    "\n",
    "unembedding_results = {}\n",
    "\n",
    "for model_key in selected:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Analyzing {model_key}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    model, tokenizer, info = load_model(model_key)\n",
    "    if model is None:\n",
    "        continue\n",
    "    \n",
    "    family = info['family']\n",
    "    \n",
    "    # Get matrices\n",
    "    try:\n",
    "        W_U = get_unembedding_matrix(model, family)\n",
    "        W_E = get_embedding_matrix(model, family)\n",
    "        \n",
    "        # Analyze\n",
    "        unembed_analysis = analyze_matrix_norms(W_U, f\"{model_key} Unembedding (W_U)\")\n",
    "        embed_analysis = analyze_matrix_norms(W_E, f\"{model_key} Embedding (W_E)\")\n",
    "        \n",
    "        # Compute ratio\n",
    "        ratio = unembed_analysis['frobenius_norm'] / embed_analysis['frobenius_norm']\n",
    "        print(f\"\\n  W_U / W_E Frobenius Ratio: {ratio:.2f}\")\n",
    "        \n",
    "        # Check if tied embeddings\n",
    "        tied = torch.allclose(W_U, W_E, atol=1e-3)\n",
    "        print(f\"  Tied Embeddings: {tied}\")\n",
    "        \n",
    "        unembedding_results[model_key] = {\n",
    "            'family': family,\n",
    "            'unembedding': unembed_analysis,\n",
    "            'embedding': embed_analysis,\n",
    "            'wu_we_ratio': float(ratio),\n",
    "            'tied_embeddings': tied\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "    \n",
    "    # Cleanup\n",
    "    del model\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\n\\nCompleted unembedding analysis for {len(unembedding_results)} models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## TEST 2: FFN Expansion with Corrected Hooks\n",
    "\n",
    "**Problem:** Previous hooks may have measured post-RMSNorm activations.\n",
    "\n",
    "**Solution:** Hook BEFORE and AFTER the layer, not just the submodules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Corrected Hook System - Measure Residual Stream\n",
    "\n",
    "class ResidualStreamAnalyzer:\n",
    "    \"\"\"Analyze the residual stream directly, bypassing RMSNorm issues.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, model_info):\n",
    "        self.model = model\n",
    "        self.model_info = model_info\n",
    "        self.hooks = []\n",
    "        self.residual_norms = []\n",
    "        \n",
    "    def _get_layers(self):\n",
    "        \"\"\"Get transformer layers.\"\"\"\n",
    "        family = self.model_info['family']\n",
    "        \n",
    "        if family == 'pythia':\n",
    "            return self.model.gpt_neox.layers\n",
    "        elif family in ['mistral', 'llama', 'gemma']:\n",
    "            return self.model.model.layers\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown family: {family}\")\n",
    "    \n",
    "    def _make_hook(self, layer_idx):\n",
    "        \"\"\"Create hook that captures residual stream norm AFTER each layer.\"\"\"\n",
    "        def hook(module, args, output):\n",
    "            # Output is (hidden_states, ...) or just hidden_states\n",
    "            if isinstance(output, tuple):\n",
    "                hidden = output[0]\n",
    "            else:\n",
    "                hidden = output\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                norm = hidden.float().norm().item()\n",
    "                self.residual_norms.append((layer_idx, norm))\n",
    "        \n",
    "        return hook\n",
    "    \n",
    "    def register_hooks(self):\n",
    "        \"\"\"Register hooks on each layer output.\"\"\"\n",
    "        layers = self._get_layers()\n",
    "        \n",
    "        for i, layer in enumerate(layers):\n",
    "            h = layer.register_forward_hook(self._make_hook(i))\n",
    "            self.hooks.append(h)\n",
    "        \n",
    "        print(f\"  Registered {len(self.hooks)} residual stream hooks\")\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        for h in self.hooks:\n",
    "            h.remove()\n",
    "        self.hooks = []\n",
    "    \n",
    "    def clear(self):\n",
    "        self.residual_norms = []\n",
    "    \n",
    "    def get_layer_gains(self):\n",
    "        \"\"\"Compute gain = norm(layer_i) / norm(layer_{i-1}).\"\"\"\n",
    "        gains = []\n",
    "        \n",
    "        # Sort by layer index\n",
    "        sorted_norms = sorted(self.residual_norms, key=lambda x: x[0])\n",
    "        \n",
    "        for i in range(1, len(sorted_norms)):\n",
    "            prev_norm = sorted_norms[i-1][1]\n",
    "            curr_norm = sorted_norms[i][1]\n",
    "            \n",
    "            if prev_norm > 1e-8:\n",
    "                gain = curr_norm / prev_norm\n",
    "            else:\n",
    "                gain = 0.0\n",
    "            \n",
    "            gains.append(float(gain))\n",
    "        \n",
    "        return gains, [n for _, n in sorted_norms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Run Residual Stream Analysis\n",
    "\n",
    "residual_results = {}\n",
    "\n",
    "for model_key in selected:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Residual Stream Analysis: {model_key}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    model, tokenizer, info = load_model(model_key)\n",
    "    if model is None:\n",
    "        continue\n",
    "    \n",
    "    # Setup analyzer\n",
    "    analyzer = ResidualStreamAnalyzer(model, info)\n",
    "    analyzer.register_hooks()\n",
    "    \n",
    "    # Test prompt\n",
    "    prompt = \"The capital of France is\"\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "    \n",
    "    # Forward pass\n",
    "    print(f\"  Running forward pass...\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Get gains\n",
    "    gains, norms = analyzer.get_layer_gains()\n",
    "    \n",
    "    if gains:\n",
    "        # Statistics\n",
    "        n_layers = len(gains)\n",
    "        contracting = sum(1 for g in gains if g < 1.0)\n",
    "        last_gain = gains[-1] if gains else 0\n",
    "        max_gain = max(gains) if gains else 0\n",
    "        max_gain_layer = gains.index(max_gain) if gains else -1\n",
    "        \n",
    "        print(f\"\\n  Results:\")\n",
    "        print(f\"  Layers: {n_layers}\")\n",
    "        print(f\"  Contracting: {contracting}/{n_layers} ({100*contracting/n_layers:.1f}%)\")\n",
    "        print(f\"  Last Layer Gain: {last_gain:.4f}\")\n",
    "        print(f\"  Max Gain: {max_gain:.4f} at Layer {max_gain_layer}\")\n",
    "        print(f\"  Final Norm: {norms[-1]:.2f}\")\n",
    "        \n",
    "        # Find explosion point\n",
    "        expansion_layers = [(i, g) for i, g in enumerate(gains) if g > 1.0]\n",
    "        if expansion_layers:\n",
    "            print(f\"\\n  Expansion Layers (gain > 1):\")\n",
    "            for i, g in expansion_layers[:5]:  # Top 5\n",
    "                print(f\"    Layer {i}: {g:.4f}\")\n",
    "        \n",
    "        residual_results[model_key] = {\n",
    "            'family': info['family'],\n",
    "            'n_layers': n_layers,\n",
    "            'gains': gains,\n",
    "            'norms': norms,\n",
    "            'contracting_pct': float(100 * contracting / n_layers),\n",
    "            'last_gain': float(last_gain),\n",
    "            'max_gain': float(max_gain),\n",
    "            'max_gain_layer': int(max_gain_layer),\n",
    "            'expansion_layers': [(int(i), float(g)) for i, g in expansion_layers]\n",
    "        }\n",
    "    \n",
    "    # Cleanup\n",
    "    analyzer.remove_hooks()\n",
    "    del model\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\n\\nCompleted residual stream analysis for {len(residual_results)} models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## TEST 3: Output Logit Entropy Analysis\n",
    "\n",
    "**Hypothesis:** Mistral produces sharper logits (lower entropy) without needing FFN explosion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Logit Entropy Analysis\n",
    "\n",
    "def analyze_logit_entropy(model, tokenizer, prompt=\"The capital of France is\"):\n",
    "    \"\"\"Analyze the entropy of output logits before softmax.\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits  # [batch, seq, vocab]\n",
    "    \n",
    "    # Take last token's logits\n",
    "    last_logits = logits[0, -1, :].float().cpu()\n",
    "    \n",
    "    # Compute statistics before softmax\n",
    "    logit_mean = last_logits.mean().item()\n",
    "    logit_std = last_logits.std().item()\n",
    "    logit_max = last_logits.max().item()\n",
    "    logit_min = last_logits.min().item()\n",
    "    logit_range = logit_max - logit_min\n",
    "    \n",
    "    # Compute entropy after softmax\n",
    "    probs = torch.softmax(last_logits, dim=0)\n",
    "    entropy = -(probs * torch.log(probs + 1e-10)).sum().item()\n",
    "    \n",
    "    # Effective number of choices (exp(entropy))\n",
    "    effective_vocab = np.exp(entropy)\n",
    "    \n",
    "    # Top-k concentration\n",
    "    top_k_probs = probs.topk(10).values.sum().item()\n",
    "    \n",
    "    results = {\n",
    "        'logit_mean': float(logit_mean),\n",
    "        'logit_std': float(logit_std),\n",
    "        'logit_max': float(logit_max),\n",
    "        'logit_min': float(logit_min),\n",
    "        'logit_range': float(logit_range),\n",
    "        'entropy': float(entropy),\n",
    "        'effective_vocab': float(effective_vocab),\n",
    "        'top10_prob_mass': float(top_k_probs)\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Run Entropy Analysis\n",
    "\n",
    "entropy_results = {}\n",
    "\n",
    "for model_key in selected:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Entropy Analysis: {model_key}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    model, tokenizer, info = load_model(model_key)\n",
    "    if model is None:\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        results = analyze_logit_entropy(model, tokenizer)\n",
    "        \n",
    "        print(f\"\\n  Logit Statistics (before softmax):\")\n",
    "        print(f\"    Mean: {results['logit_mean']:.2f}\")\n",
    "        print(f\"    Std: {results['logit_std']:.2f}\")\n",
    "        print(f\"    Range: {results['logit_range']:.2f}\")\n",
    "        print(f\"\\n  Entropy Statistics:\")\n",
    "        print(f\"    Entropy: {results['entropy']:.2f} nats\")\n",
    "        print(f\"    Effective Vocab: {results['effective_vocab']:.0f} tokens\")\n",
    "        print(f\"    Top-10 Prob Mass: {results['top10_prob_mass']:.2%}\")\n",
    "        \n",
    "        entropy_results[model_key] = {\n",
    "            'family': info['family'],\n",
    "            **results\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "    \n",
    "    # Cleanup\n",
    "    del model\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\n\\nCompleted entropy analysis for {len(entropy_results)} models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Visualization\n",
    "\n",
    "import os\n",
    "results_dir = '../Results'\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "colors = {'mistral': 'purple', 'pythia': 'blue', 'gemma': 'red', 'llama': 'green'}\n",
    "\n",
    "# Panel 1: Residual Stream Gains\n",
    "ax1 = axes[0, 0]\n",
    "for model_key, res in residual_results.items():\n",
    "    color = colors.get(res['family'], 'gray')\n",
    "    layers = range(len(res['gains']))\n",
    "    ax1.plot(layers, res['gains'], 'o-', label=model_key, color=color, alpha=0.7)\n",
    "ax1.axhline(y=1.0, color='black', linestyle='--', alpha=0.5)\n",
    "ax1.set_xlabel('Layer')\n",
    "ax1.set_ylabel('Residual Stream Gain')\n",
    "ax1.set_title('Residual Stream Gain per Layer')\n",
    "ax1.legend()\n",
    "ax1.set_ylim(0.9, 1.1)\n",
    "\n",
    "# Panel 2: Residual Stream Norms\n",
    "ax2 = axes[0, 1]\n",
    "for model_key, res in residual_results.items():\n",
    "    color = colors.get(res['family'], 'gray')\n",
    "    layers = range(len(res['norms']))\n",
    "    ax2.plot(layers, res['norms'], 's-', label=model_key, color=color, alpha=0.7)\n",
    "ax2.set_xlabel('Layer')\n",
    "ax2.set_ylabel('Residual Norm')\n",
    "ax2.set_title('Residual Stream Norm per Layer')\n",
    "ax2.legend()\n",
    "\n",
    "# Panel 3: Unembedding Norms Comparison\n",
    "ax3 = axes[1, 0]\n",
    "if unembedding_results:\n",
    "    models = list(unembedding_results.keys())\n",
    "    wu_norms = [unembedding_results[m]['unembedding']['spectral_norm'] for m in models]\n",
    "    we_norms = [unembedding_results[m]['embedding']['spectral_norm'] for m in models]\n",
    "    \n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax3.bar(x - width/2, wu_norms, width, label='W_U (Unembedding)', color='darkred')\n",
    "    ax3.bar(x + width/2, we_norms, width, label='W_E (Embedding)', color='darkblue')\n",
    "    \n",
    "    ax3.set_xlabel('Model')\n",
    "    ax3.set_ylabel('Spectral Norm')\n",
    "    ax3.set_title('Embedding vs Unembedding Spectral Norm')\n",
    "    ax3.set_xticks(x)\n",
    "    ax3.set_xticklabels(models, rotation=15)\n",
    "    ax3.legend()\n",
    "\n",
    "# Panel 4: Entropy Comparison\n",
    "ax4 = axes[1, 1]\n",
    "if entropy_results:\n",
    "    models = list(entropy_results.keys())\n",
    "    entropies = [entropy_results[m]['entropy'] for m in models]\n",
    "    eff_vocabs = [entropy_results[m]['effective_vocab'] for m in models]\n",
    "    \n",
    "    ax4_twin = ax4.twinx()\n",
    "    \n",
    "    x = np.arange(len(models))\n",
    "    ax4.bar(x - 0.2, entropies, 0.4, label='Entropy (nats)', color='green', alpha=0.7)\n",
    "    ax4_twin.bar(x + 0.2, eff_vocabs, 0.4, label='Effective Vocab', color='orange', alpha=0.7)\n",
    "    \n",
    "    ax4.set_xlabel('Model')\n",
    "    ax4.set_ylabel('Entropy (nats)', color='green')\n",
    "    ax4_twin.set_ylabel('Effective Vocab Size', color='orange')\n",
    "    ax4.set_title('Output Distribution Sharpness')\n",
    "    ax4.set_xticks(x)\n",
    "    ax4.set_xticklabels(models, rotation=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "output_path = f'{results_dir}/mistral_paradox_investigation.png'\n",
    "plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Summary and Conclusions\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MISTRAL PARADOX INVESTIGATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. UNEMBEDDING MATRIX ANALYSIS\")\n",
    "print(\"-\" * 40)\n",
    "if unembedding_results:\n",
    "    for m, res in unembedding_results.items():\n",
    "        print(f\"  {m}:\")\n",
    "        print(f\"    W_U Spectral Norm: {res['unembedding']['spectral_norm']:.2f}\")\n",
    "        print(f\"    W_E Spectral Norm: {res['embedding']['spectral_norm']:.2f}\")\n",
    "        print(f\"    W_U/W_E Ratio: {res['wu_we_ratio']:.2f}\")\n",
    "        print(f\"    Tied: {res['tied_embeddings']}\")\n",
    "\n",
    "print(\"\\n2. RESIDUAL STREAM ANALYSIS\")\n",
    "print(\"-\" * 40)\n",
    "if residual_results:\n",
    "    for m, res in residual_results.items():\n",
    "        print(f\"  {m}:\")\n",
    "        print(f\"    Contracting: {res['contracting_pct']:.1f}%\")\n",
    "        print(f\"    Last Layer Gain: {res['last_gain']:.4f}\")\n",
    "        print(f\"    Max Gain: {res['max_gain']:.4f} at Layer {res['max_gain_layer']}\")\n",
    "        if res['expansion_layers']:\n",
    "            print(f\"    Expansion at: {[l for l, g in res['expansion_layers'][:3]]}\")\n",
    "\n",
    "print(\"\\n3. ENTROPY ANALYSIS\")\n",
    "print(\"-\" * 40)\n",
    "if entropy_results:\n",
    "    for m, res in entropy_results.items():\n",
    "        print(f\"  {m}:\")\n",
    "        print(f\"    Entropy: {res['entropy']:.2f} nats\")\n",
    "        print(f\"    Effective Vocab: {res['effective_vocab']:.0f}\")\n",
    "        print(f\"    Logit Range: {res['logit_range']:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONCLUSIONS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "Based on the results, determine:\n",
    "\n",
    "1. SILENT EXIT HYPOTHESIS:\n",
    "   - If Mistral W_U >> Pythia W_U: Confirmed (static amplifier)\n",
    "   - If similar: Rejected\n",
    "\n",
    "2. RESIDUAL STREAM EXPLOSION:\n",
    "   - Check if gains are near 1.0 throughout (RMSNorm effect)\n",
    "   - Or if explosion happens earlier than expected\n",
    "\n",
    "3. ENTROPY EFFICIENCY:\n",
    "   - If Mistral entropy << Pythia: Confirmed (sharper outputs)\n",
    "   - If similar: Rejected\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Save Results\n",
    "\n",
    "import os\n",
    "results_dir = '../Results'\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Combine all results\n",
    "output = {\n",
    "    'experiment': 'Mistral Paradox Investigation',\n",
    "    'date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'models_tested': list(set(list(unembedding_results.keys()) + \n",
    "                              list(residual_results.keys()) + \n",
    "                              list(entropy_results.keys()))),\n",
    "    'unembedding_analysis': unembedding_results,\n",
    "    'residual_stream_analysis': residual_results,\n",
    "    'entropy_analysis': entropy_results,\n",
    "    'hypotheses': {\n",
    "        'silent_exit': 'Check W_U spectral norm comparison',\n",
    "        'residual_explosion': 'Check residual stream gains',\n",
    "        'entropy_efficiency': 'Check output entropy comparison'\n",
    "    }\n",
    "}\n",
    "\n",
    "output_path = f'{results_dir}/mistral_paradox_investigation_results.json'\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(output, f, indent=2)\n",
    "print(f\"Saved: {output_path}\")\n",
    "\n",
    "# Auto-download for Colab\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(output_path)\n",
    "    files.download(f'{results_dir}/mistral_paradox_investigation.png')\n",
    "    print(\"\\nFiles downloaded!\")\n",
    "except:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
