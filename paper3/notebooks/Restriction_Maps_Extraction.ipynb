{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restriction Maps Extraction: Empirical Validation\n",
    "\n",
    "**Paper #3 Core Experiment**\n",
    "\n",
    "## Theoretical Prediction\n",
    "\n",
    "From the Sheaf-Theoretic framework, Transformer attention defines **Restriction Maps**:\n",
    "\n",
    "$$\\rho_{ij} = \\sqrt{A_{ij}} \\cdot W_V$$\n",
    "\n",
    "where:\n",
    "- $A_{ij}$ = attention weight from token $i$ to token $j$\n",
    "- $W_V$ = value projection matrix\n",
    "- $\\rho_{ij}$ = restriction map (linear transformation for \"message passing\")\n",
    "\n",
    "## Key Predictions to Validate\n",
    "\n",
    "1. **Contraction vs Expansion**: $\\|\\rho_{ij}\\| < 1$ for $l < L^*$, $\\|\\rho_{ij}\\| > 1$ for $l > L^*$\n",
    "2. **Spectral Gap**: Sheaf Laplacian $L_F = \\delta^T \\delta$ shows distinct spectral signature at $L^*$\n",
    "3. **Consistency Measure**: Global section existence correlates with semantic coherence\n",
    "\n",
    "**Author:** Davide D'Elia  \n",
    "**Date:** 2026-01-04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers accelerate einops scipy matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from scipy import linalg\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model with Attention Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "MODEL_NAME = \"EleutherAI/pythia-1.4b\"  # Smaller for faster extraction\n",
    "# MODEL_NAME = \"EleutherAI/pythia-6.9b\"  # Full size (needs more VRAM)\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    output_attentions=True,\n",
    "    output_hidden_states=True\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "n_layers = model.config.num_hidden_layers\n",
    "n_heads = model.config.num_attention_heads\n",
    "hidden_dim = model.config.hidden_size\n",
    "head_dim = hidden_dim // n_heads\n",
    "\n",
    "print(f\"\\nModel Configuration:\")\n",
    "print(f\"  Layers: {n_layers}\")\n",
    "print(f\"  Attention Heads: {n_heads}\")\n",
    "print(f\"  Hidden Dim: {hidden_dim}\")\n",
    "print(f\"  Head Dim: {head_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract Attention and Value Matrices\n",
    "\n",
    "For each layer, we need:\n",
    "- $A_{ij}$ = softmax(QK^T / sqrt(d)) — attention weights\n",
    "- $W_V$ = value projection matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value_projection_matrices(model):\n",
    "    \"\"\"\n",
    "    Extract W_V matrices from all layers.\n",
    "    \n",
    "    Returns:\n",
    "        Dict[layer_idx -> W_V tensor of shape (hidden_dim, hidden_dim)]\n",
    "    \"\"\"\n",
    "    W_V_matrices = {}\n",
    "    \n",
    "    for layer_idx in range(model.config.num_hidden_layers):\n",
    "        # Access the attention layer\n",
    "        # Path depends on model architecture (Pythia uses GPT-NeoX structure)\n",
    "        attn = model.gpt_neox.layers[layer_idx].attention\n",
    "        \n",
    "        # In GPT-NeoX, QKV are concatenated in one projection\n",
    "        # query_key_value has shape (3 * hidden_dim, hidden_dim)\n",
    "        qkv_weight = attn.query_key_value.weight.data.float().cpu()\n",
    "        \n",
    "        # Split into Q, K, V\n",
    "        # Shape: (3 * hidden_dim, hidden_dim) -> 3 x (hidden_dim, hidden_dim)\n",
    "        hidden_dim = model.config.hidden_size\n",
    "        \n",
    "        # The weight is stored as (out_features, in_features)\n",
    "        # Split along output dimension\n",
    "        W_Q = qkv_weight[:hidden_dim, :]\n",
    "        W_K = qkv_weight[hidden_dim:2*hidden_dim, :]\n",
    "        W_V = qkv_weight[2*hidden_dim:, :]\n",
    "        \n",
    "        W_V_matrices[layer_idx] = W_V\n",
    "    \n",
    "    return W_V_matrices\n",
    "\n",
    "print(\"Extracting W_V matrices from all layers...\")\n",
    "W_V_matrices = get_value_projection_matrices(model)\n",
    "\n",
    "print(f\"\\nExtracted W_V matrices:\")\n",
    "print(f\"  Shape: {W_V_matrices[0].shape}\")\n",
    "print(f\"  Layers: {len(W_V_matrices)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_attention_and_hidden_states(model, tokenizer, prompt, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Run forward pass and extract attention weights and hidden states.\n",
    "    \n",
    "    Returns:\n",
    "        attentions: tuple of (n_layers) tensors, each (batch, n_heads, seq_len, seq_len)\n",
    "        hidden_states: tuple of (n_layers + 1) tensors, each (batch, seq_len, hidden_dim)\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            **inputs,\n",
    "            output_attentions=True,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "    \n",
    "    return outputs.attentions, outputs.hidden_states, inputs\n",
    "\n",
    "# Test with a sample prompt\n",
    "TEST_PROMPT = \"The capital of France is Paris, which is known for the Eiffel Tower.\"\n",
    "\n",
    "print(f\"Test prompt: '{TEST_PROMPT}'\")\n",
    "attentions, hidden_states, inputs = extract_attention_and_hidden_states(model, tokenizer, TEST_PROMPT)\n",
    "\n",
    "seq_len = attentions[0].shape[-1]\n",
    "print(f\"\\nExtracted:\")\n",
    "print(f\"  Sequence length: {seq_len}\")\n",
    "print(f\"  Attention shape per layer: {attentions[0].shape}\")\n",
    "print(f\"  Hidden state shape per layer: {hidden_states[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compute Restriction Maps\n",
    "\n",
    "The restriction map from token $j$ to token $i$ is:\n",
    "\n",
    "$$\\rho_{ij} = \\sqrt{A_{ij}} \\cdot W_V$$\n",
    "\n",
    "This is a $d \\times d$ matrix that transforms the value vector at position $j$ before it contributes to position $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def compute_restriction_maps(attention_weights, W_V, head_idx=0, threshold=1e-8):\n    \"\"\"\n    Compute restriction maps rho_ij = sqrt(A_ij) * W_V\n\n    Args:\n        attention_weights: tensor of shape (batch, n_heads, seq_len, seq_len)\n        W_V: tensor of shape (hidden_dim, hidden_dim)\n        head_idx: which attention head to use\n        threshold: minimum attention weight to include (lowered for sparse attention)\n\n    Returns:\n        restriction_maps: dict of (i, j) -> rho_ij matrix\n        sqrt_A: the sqrt of attention weights\n    \"\"\"\n    # Get attention for specific head\n    # Shape: (seq_len, seq_len)\n    A = attention_weights[0, head_idx].float().cpu()\n    seq_len = A.shape[0]\n\n    # Compute sqrt of attention weights\n    sqrt_A = torch.sqrt(A + 1e-10)  # Add epsilon for numerical stability\n\n    # For each pair (i, j), compute rho_ij = sqrt(A_ij) * W_V\n    # This gives a scalar * matrix = matrix\n    restriction_maps = {}\n\n    for i in range(seq_len):\n        for j in range(seq_len):\n            if A[i, j] > threshold:  # Only compute for non-zero attention\n                rho_ij = sqrt_A[i, j] * W_V\n                restriction_maps[(i, j)] = rho_ij\n\n    # If no maps found (very sparse attention), include top-k by attention weight\n    if len(restriction_maps) == 0:\n        # Flatten and get top-k indices\n        A_flat = A.flatten()\n        k = min(10, len(A_flat))  # At least 10 maps\n        top_k_indices = torch.topk(A_flat, k).indices\n        for idx in top_k_indices:\n            i = idx.item() // seq_len\n            j = idx.item() % seq_len\n            rho_ij = sqrt_A[i, j] * W_V\n            restriction_maps[(i, j)] = rho_ij\n\n    return restriction_maps, sqrt_A, A\n\n# Compute for first layer\nlayer_idx = 0\nrestriction_maps_L0, sqrt_A_L0, A_L0 = compute_restriction_maps(\n    attentions[layer_idx], \n    W_V_matrices[layer_idx]\n)\n\nprint(f\"Layer {layer_idx} Restriction Maps:\")\nprint(f\"  Number of non-zero maps: {len(restriction_maps_L0)}\")\nprint(f\"  Map shape: {list(restriction_maps_L0.values())[0].shape}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Restriction Map Properties\n",
    "\n",
    "### Key Metrics:\n",
    "1. **Operator Norm**: $\\|\\rho_{ij}\\|_{op}$ — measures contraction/expansion\n",
    "2. **Frobenius Norm**: $\\|\\rho_{ij}\\|_F$ — overall magnitude\n",
    "3. **Spectral Radius**: $\\max|\\lambda_i|$ — long-term behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def analyze_restriction_maps(restriction_maps, W_V):\n    \"\"\"\n    Compute statistics of restriction maps.\n    \n    Returns:\n        dict with various statistics\n    \"\"\"\n    # W_V baseline statistics (compute first, with error handling)\n    W_V_np = W_V.numpy()\n    try:\n        W_V_op_norm = np.linalg.norm(W_V_np, ord=2)\n    except np.linalg.LinAlgError:\n        # SVD didn't converge, use Frobenius norm as fallback\n        W_V_op_norm = np.linalg.norm(W_V_np, ord='fro') / np.sqrt(min(W_V_np.shape))\n    W_V_frob_norm = np.linalg.norm(W_V_np, ord='fro')\n\n    # Handle empty restriction maps\n    if len(restriction_maps) == 0:\n        return {\n            'operator_norms': np.array([]),\n            'frobenius_norms': np.array([]),\n            'spectral_radii': np.array([]),\n            'mean_op_norm': 0.0,\n            'std_op_norm': 0.0,\n            'max_op_norm': 0.0,\n            'min_op_norm': 0.0,\n            'W_V_op_norm': W_V_op_norm,\n            'W_V_frob_norm': W_V_frob_norm,\n            'n_maps': 0,\n            'contraction_ratio': 0.0\n        }\n\n    operator_norms = []\n    frobenius_norms = []\n    spectral_radii = []\n\n    for (i, j), rho in restriction_maps.items():\n        rho_np = rho.numpy()\n\n        # Operator norm (largest singular value) with fallback\n        try:\n            op_norm = np.linalg.norm(rho_np, ord=2)\n        except np.linalg.LinAlgError:\n            # SVD didn't converge, use Frobenius norm estimate\n            op_norm = np.linalg.norm(rho_np, ord='fro') / np.sqrt(min(rho_np.shape))\n        operator_norms.append(op_norm)\n\n        # Frobenius norm (always works)\n        frob_norm = np.linalg.norm(rho_np, ord='fro')\n        frobenius_norms.append(frob_norm)\n\n        # Spectral radius (skip - too expensive and error-prone for large matrices)\n        spectral_radii.append(np.nan)\n\n    return {\n        'operator_norms': np.array(operator_norms),\n        'frobenius_norms': np.array(frobenius_norms),\n        'spectral_radii': np.array(spectral_radii),\n        'mean_op_norm': float(np.mean(operator_norms)) if operator_norms else 0.0,\n        'std_op_norm': float(np.std(operator_norms)) if operator_norms else 0.0,\n        'max_op_norm': float(np.max(operator_norms)) if operator_norms else 0.0,\n        'min_op_norm': float(np.min(operator_norms)) if operator_norms else 0.0,\n        'W_V_op_norm': float(W_V_op_norm),\n        'W_V_frob_norm': float(W_V_frob_norm),\n        'n_maps': len(restriction_maps),\n        'contraction_ratio': float(np.mean(operator_norms) / W_V_op_norm) if (operator_norms and W_V_op_norm > 0) else 0.0\n    }\n\n# Analyze layer 0\nstats_L0 = analyze_restriction_maps(restriction_maps_L0, W_V_matrices[0])\n\nprint(f\"Layer 0 Restriction Map Statistics:\")\nprint(f\"  Number of maps: {stats_L0['n_maps']}\")\nprint(f\"  Mean Operator Norm: {stats_L0['mean_op_norm']:.4f}\")\nprint(f\"  Std Operator Norm: {stats_L0['std_op_norm']:.4f}\")\nprint(f\"  W_V Operator Norm: {stats_L0['W_V_op_norm']:.4f}\")\nprint(f\"  Contraction Ratio: {stats_L0['contraction_ratio']:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Layer-wise Analysis: Contraction vs Expansion\n",
    "\n",
    "**Prediction:** \n",
    "- Layers < L*: Restriction maps are **contractive** (norm < 1 relative to W_V)\n",
    "- Layers > L*: Restriction maps become **expansive** (norm > 1 relative to W_V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Analyze all layers\nprint(\"Analyzing restriction maps for all layers...\")\nprint(\"(Using top-k fallback for sparse attention layers)\\n\")\n\nlayer_stats = {}\nsparse_layers = []\nfailed_layers = []\n\nfor layer_idx in tqdm(range(n_layers), desc=\"Layers\"):\n    try:\n        restriction_maps, sqrt_A, A = compute_restriction_maps(\n            attentions[layer_idx],\n            W_V_matrices[layer_idx]\n        )\n        stats = analyze_restriction_maps(restriction_maps, W_V_matrices[layer_idx])\n        layer_stats[layer_idx] = stats\n        \n        # Track sparse layers\n        if stats['n_maps'] < 20:\n            sparse_layers.append((layer_idx, stats['n_maps']))\n    \n    except Exception as e:\n        # Complete failure - use dummy values\n        failed_layers.append((layer_idx, str(e)[:50]))\n        W_V_np = W_V_matrices[layer_idx].numpy()\n        layer_stats[layer_idx] = {\n            'operator_norms': np.array([]),\n            'frobenius_norms': np.array([]),\n            'spectral_radii': np.array([]),\n            'mean_op_norm': 0.0,\n            'std_op_norm': 0.0,\n            'max_op_norm': 0.0,\n            'min_op_norm': 0.0,\n            'W_V_op_norm': float(np.linalg.norm(W_V_np, ord='fro')),\n            'W_V_frob_norm': float(np.linalg.norm(W_V_np, ord='fro')),\n            'n_maps': 0,\n            'contraction_ratio': 0.0\n        }\n\nprint(\"\\nDone!\")\n\nif failed_layers:\n    print(f\"\\n⚠️  {len(failed_layers)} layers had numerical errors (using fallback):\")\n    for layer, err in failed_layers:\n        print(f\"  Layer {layer}: {err}\")\n\nif sparse_layers:\n    print(f\"\\nNote: {len(sparse_layers)} layers had sparse attention (<20 maps):\")\n    for layer, n_maps in sparse_layers[:10]:  # Show first 10\n        print(f\"  Layer {layer}: {n_maps} maps\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract metrics for plotting\n",
    "layers = list(range(n_layers))\n",
    "mean_op_norms = [layer_stats[l]['mean_op_norm'] for l in layers]\n",
    "contraction_ratios = [layer_stats[l]['contraction_ratio'] for l in layers]\n",
    "W_V_op_norms = [layer_stats[l]['W_V_op_norm'] for l in layers]\n",
    "\n",
    "# Find transition point (where contraction ratio crosses 1 or changes trend)\n",
    "contraction_ratios_np = np.array(contraction_ratios)\n",
    "\n",
    "# Method 1: Find minimum contraction ratio (most contractive layer)\n",
    "L_star_contraction = np.argmin(contraction_ratios_np)\n",
    "\n",
    "# Method 2: Find inflection point\n",
    "second_derivative = np.diff(np.diff(contraction_ratios_np))\n",
    "L_star_inflection = np.argmax(np.abs(second_derivative)) + 1\n",
    "\n",
    "print(f\"Transition Points:\")\n",
    "print(f\"  L* (min contraction): Layer {L_star_contraction}\")\n",
    "print(f\"  L* (inflection): Layer {L_star_inflection}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Contraction Analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Mean Operator Norm of Restriction Maps\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(layers, mean_op_norms, 'b-', linewidth=2, marker='o', markersize=4, label='Mean ||rho_ij||')\n",
    "ax1.plot(layers, W_V_op_norms, 'r--', linewidth=2, label='||W_V||')\n",
    "ax1.axvline(x=L_star_contraction, color='green', linestyle=':', linewidth=2, label=f'L* = {L_star_contraction}')\n",
    "ax1.set_xlabel('Layer', fontsize=12)\n",
    "ax1.set_ylabel('Operator Norm', fontsize=12)\n",
    "ax1.set_title('Restriction Map Norms vs W_V Norm', fontsize=14)\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Contraction Ratio\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(layers, contraction_ratios, 'g-', linewidth=2, marker='s', markersize=4)\n",
    "ax2.axhline(y=1.0, color='red', linestyle='--', linewidth=2, label='Neutral (ratio=1)')\n",
    "ax2.axvline(x=L_star_contraction, color='purple', linestyle=':', linewidth=2, label=f'L* = {L_star_contraction}')\n",
    "ax2.fill_between(layers, contraction_ratios, 1.0, \n",
    "                  where=[c < 1 for c in contraction_ratios], \n",
    "                  alpha=0.3, color='blue', label='Contractive')\n",
    "ax2.fill_between(layers, contraction_ratios, 1.0, \n",
    "                  where=[c >= 1 for c in contraction_ratios], \n",
    "                  alpha=0.3, color='red', label='Expansive')\n",
    "ax2.set_xlabel('Layer', fontsize=12)\n",
    "ax2.set_ylabel('Contraction Ratio (mean||rho|| / ||W_V||)', fontsize=12)\n",
    "ax2.set_title('Contraction vs Expansion by Layer', fontsize=14)\n",
    "ax2.legend(fontsize=9)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: W_V Operator Norm across layers\n",
    "ax3 = axes[1, 0]\n",
    "ax3.plot(layers, W_V_op_norms, 'm-', linewidth=2, marker='^', markersize=4)\n",
    "ax3.axvline(x=L_star_contraction, color='green', linestyle=':', linewidth=2, label=f'L* = {L_star_contraction}')\n",
    "ax3.set_xlabel('Layer', fontsize=12)\n",
    "ax3.set_ylabel('||W_V|| Operator Norm', fontsize=12)\n",
    "ax3.set_title('Value Projection Matrix Norm', fontsize=14)\n",
    "ax3.legend(fontsize=10)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Attention Entropy (how focused is attention?)\n",
    "# Higher entropy = more distributed attention\n",
    "ax4 = axes[1, 1]\n",
    "\n",
    "# Compute attention entropy per layer\n",
    "attention_entropies = []\n",
    "for layer_idx in range(n_layers):\n",
    "    A = attentions[layer_idx][0, 0].float().cpu().numpy()  # Head 0\n",
    "    # Row-wise entropy (how distributed is attention from each position)\n",
    "    entropies = []\n",
    "    for row in A:\n",
    "        row = row + 1e-10  # Numerical stability\n",
    "        row = row / row.sum()  # Ensure normalization\n",
    "        entropy = -np.sum(row * np.log(row))\n",
    "        entropies.append(entropy)\n",
    "    attention_entropies.append(np.mean(entropies))\n",
    "\n",
    "ax4.plot(layers, attention_entropies, 'c-', linewidth=2, marker='d', markersize=4)\n",
    "ax4.axvline(x=L_star_contraction, color='green', linestyle=':', linewidth=2, label=f'L* = {L_star_contraction}')\n",
    "ax4.set_xlabel('Layer', fontsize=12)\n",
    "ax4.set_ylabel('Mean Attention Entropy', fontsize=12)\n",
    "ax4.set_title('Attention Distribution (High = Distributed)', fontsize=14)\n",
    "ax4.legend(fontsize=10)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'{MODEL_NAME}: Restriction Map Analysis\\n(Prediction: Contractive before L*, Expansive after L*)', \n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('restriction_maps_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n>>> Figure saved as 'restriction_maps_analysis.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Sheaf Laplacian Construction\n",
    "\n",
    "The Sheaf Laplacian is:\n",
    "\n",
    "$$L_F = \\delta^T \\delta$$\n",
    "\n",
    "where $\\delta$ is the coboundary operator. For our complete graph with restriction maps:\n",
    "\n",
    "$$\\delta: C^0(G; F) \\to C^1(G; F)$$\n",
    "$$(\\delta x)_{ij} = \\rho_{ji} x_j - \\rho_{ij} x_i$$\n",
    "\n",
    "The Laplacian measures **inconsistency** between local sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def build_sheaf_laplacian(attention_weights, W_V, head_idx=0, use_subsample=True, max_tokens=8):\n    \"\"\"\n    Construct the Sheaf Laplacian L_F = delta^T delta\n    \n    For computational efficiency, we work with a subsampled graph.\n    \n    Args:\n        attention_weights: (batch, n_heads, seq_len, seq_len)\n        W_V: (hidden_dim, hidden_dim)\n        head_idx: which attention head\n        use_subsample: whether to subsample tokens\n        max_tokens: maximum tokens to use if subsampling\n    \n    Returns:\n        L_F: Sheaf Laplacian matrix\n        spectral_info: eigenvalue information\n    \"\"\"\n    A = attention_weights[0, head_idx].float().cpu()\n    seq_len = A.shape[0]\n    d = W_V.shape[0]  # hidden_dim\n    \n    # Subsample for computational efficiency\n    if use_subsample and seq_len > max_tokens:\n        indices = np.linspace(0, seq_len-1, max_tokens, dtype=int)\n        A = A[np.ix_(indices, indices)]\n        seq_len = max_tokens\n    \n    # For efficiency, use a smaller projection of W_V\n    proj_dim = min(32, d)  # Project to smaller dimension\n    W_V_small = W_V[:proj_dim, :proj_dim].numpy()\n    \n    # Compute sqrt(A)\n    sqrt_A = torch.sqrt(A + 1e-10).numpy()\n    \n    # Build block Laplacian\n    # L_F is (n * d_small) x (n * d_small) where n = seq_len\n    n = seq_len\n    d_s = proj_dim\n    \n    L_F = np.zeros((n * d_s, n * d_s))\n    \n    # Diagonal blocks: sum over neighbors\n    for i in range(n):\n        block_ii = np.zeros((d_s, d_s))\n        for j in range(n):\n            if i != j:\n                rho_ij = sqrt_A[i, j] * W_V_small\n                block_ii += rho_ij.T @ rho_ij\n        L_F[i*d_s:(i+1)*d_s, i*d_s:(i+1)*d_s] = block_ii\n    \n    # Off-diagonal blocks\n    for i in range(n):\n        for j in range(n):\n            if i != j:\n                rho_ij = sqrt_A[i, j] * W_V_small\n                rho_ji = sqrt_A[j, i] * W_V_small\n                block_ij = -rho_ij.T @ rho_ji\n                L_F[i*d_s:(i+1)*d_s, j*d_s:(j+1)*d_s] = block_ij\n    \n    # Compute eigenvalues with error handling\n    try:\n        # Add small regularization for numerical stability\n        L_F_reg = L_F + 1e-10 * np.eye(L_F.shape[0])\n        eigenvalues = np.linalg.eigvalsh(L_F_reg)\n        eigenvalues = np.sort(np.real(eigenvalues))  # Ensure real and sorted\n    except np.linalg.LinAlgError:\n        # Eigenvalues didn't converge - use fallback metrics\n        eigenvalues = np.array([0.0, np.trace(L_F) / L_F.shape[0]])  # Approximate\n    \n    # Spectral information with safety checks\n    spectral_info = {\n        'eigenvalues': eigenvalues,\n        'lambda_1': float(eigenvalues[0]) if len(eigenvalues) > 0 else 0.0,\n        'lambda_2': float(eigenvalues[1]) if len(eigenvalues) > 1 else 0.0,\n        'spectral_gap': float(eigenvalues[1] - eigenvalues[0]) if len(eigenvalues) > 1 else 0.0,\n        'trace': float(np.trace(L_F)),\n        'frobenius_norm': float(np.linalg.norm(L_F, 'fro'))\n    }\n    \n    return L_F, spectral_info\n\n# Build Laplacian for layer 0\nL_F_0, spectral_0 = build_sheaf_laplacian(attentions[0], W_V_matrices[0])\n\nprint(f\"Layer 0 Sheaf Laplacian:\")\nprint(f\"  Shape: {L_F_0.shape}\")\nprint(f\"  Lambda_1 (smallest): {spectral_0['lambda_1']:.6f}\")\nprint(f\"  Lambda_2: {spectral_0['lambda_2']:.6f}\")\nprint(f\"  Spectral Gap: {spectral_0['spectral_gap']:.6f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compute Sheaf Laplacian spectral properties for all layers\nprint(\"Computing Sheaf Laplacian for all layers...\")\nprint(\"(With eigenvalue fallback for ill-conditioned matrices)\\n\")\n\nlaplacian_spectral = {}\nfailed_layers = []\n\nfor layer_idx in tqdm(range(n_layers), desc=\"Layers\"):\n    try:\n        L_F, spectral = build_sheaf_laplacian(\n            attentions[layer_idx], \n            W_V_matrices[layer_idx],\n            max_tokens=8  # Keep small for efficiency\n        )\n        laplacian_spectral[layer_idx] = spectral\n    except Exception as e:\n        # Complete failure - use dummy values\n        failed_layers.append((layer_idx, str(e)))\n        laplacian_spectral[layer_idx] = {\n            'eigenvalues': np.array([0.0, 0.0]),\n            'lambda_1': 0.0,\n            'lambda_2': 0.0,\n            'spectral_gap': 0.0,\n            'trace': 0.0,\n            'frobenius_norm': 0.0\n        }\n\nprint(\"\\nDone!\")\nif failed_layers:\n    print(f\"\\nWarning: {len(failed_layers)} layers had numerical issues:\")\n    for layer, err in failed_layers[:5]:  # Show first 5\n        print(f\"  Layer {layer}: {err[:50]}...\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Sheaf Laplacian Spectral Analysis\n",
    "spectral_gaps = [laplacian_spectral[l]['spectral_gap'] for l in layers]\n",
    "lambda_1s = [laplacian_spectral[l]['lambda_1'] for l in layers]\n",
    "lambda_2s = [laplacian_spectral[l]['lambda_2'] for l in layers]\n",
    "traces = [laplacian_spectral[l]['trace'] for l in layers]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Spectral Gap\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(layers, spectral_gaps, 'b-', linewidth=2, marker='o', markersize=4)\n",
    "ax1.axvline(x=L_star_contraction, color='red', linestyle='--', linewidth=2, label=f'L* = {L_star_contraction}')\n",
    "ax1.set_xlabel('Layer', fontsize=12)\n",
    "ax1.set_ylabel('Spectral Gap (lambda_2 - lambda_1)', fontsize=12)\n",
    "ax1.set_title('Sheaf Laplacian Spectral Gap', fontsize=14)\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: First two eigenvalues\n",
    "ax2 = axes[0, 1]\n",
    "ax2.semilogy(layers, [max(l, 1e-10) for l in lambda_1s], 'g-', linewidth=2, marker='s', markersize=4, label='lambda_1')\n",
    "ax2.semilogy(layers, lambda_2s, 'r-', linewidth=2, marker='^', markersize=4, label='lambda_2')\n",
    "ax2.axvline(x=L_star_contraction, color='purple', linestyle='--', linewidth=2, label=f'L* = {L_star_contraction}')\n",
    "ax2.set_xlabel('Layer', fontsize=12)\n",
    "ax2.set_ylabel('Eigenvalue (log scale)', fontsize=12)\n",
    "ax2.set_title('First Two Eigenvalues of L_F', fontsize=14)\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Trace (total \"energy\")\n",
    "ax3 = axes[1, 0]\n",
    "ax3.plot(layers, traces, 'm-', linewidth=2, marker='d', markersize=4)\n",
    "ax3.axvline(x=L_star_contraction, color='red', linestyle='--', linewidth=2, label=f'L* = {L_star_contraction}')\n",
    "ax3.set_xlabel('Layer', fontsize=12)\n",
    "ax3.set_ylabel('Trace(L_F)', fontsize=12)\n",
    "ax3.set_title('Sheaf Laplacian Trace (Total Inconsistency)', fontsize=14)\n",
    "ax3.legend(fontsize=10)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Eigenvalue spectrum at selected layers\n",
    "ax4 = axes[1, 1]\n",
    "key_layers = [0, n_layers // 4, n_layers // 2, 3 * n_layers // 4, n_layers - 1]\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(key_layers)))\n",
    "\n",
    "for idx, layer in enumerate(key_layers):\n",
    "    eigs = laplacian_spectral[layer]['eigenvalues'][:20]  # First 20 eigenvalues\n",
    "    ax4.semilogy(range(len(eigs)), eigs + 1e-10, \n",
    "                 label=f'Layer {layer}', color=colors[idx], linewidth=2)\n",
    "\n",
    "ax4.set_xlabel('Eigenvalue Index', fontsize=12)\n",
    "ax4.set_ylabel('Eigenvalue (log scale)', fontsize=12)\n",
    "ax4.set_title('Eigenvalue Spectrum at Key Layers', fontsize=14)\n",
    "ax4.legend(fontsize=9)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'{MODEL_NAME}: Sheaf Laplacian Spectral Analysis', \n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('sheaf_laplacian_spectral.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n>>> Figure saved as 'sheaf_laplacian_spectral.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Validation Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Prepare summary\n",
    "summary = {\n",
    "    'model': MODEL_NAME,\n",
    "    'n_layers': int(n_layers),\n",
    "    'n_heads': int(n_heads),\n",
    "    'hidden_dim': int(hidden_dim),\n",
    "    'test_prompt': TEST_PROMPT,\n",
    "    'seq_len': int(seq_len),\n",
    "    \n",
    "    'restriction_maps': {\n",
    "        'L_star_min_contraction': int(L_star_contraction),\n",
    "        'L_star_inflection': int(L_star_inflection),\n",
    "        'contraction_ratios': [float(c) for c in contraction_ratios],\n",
    "        'mean_op_norms': [float(n) for n in mean_op_norms],\n",
    "        'W_V_op_norms': [float(n) for n in W_V_op_norms],\n",
    "        'attention_entropies': [float(e) for e in attention_entropies]\n",
    "    },\n",
    "    \n",
    "    'sheaf_laplacian': {\n",
    "        'spectral_gaps': [float(g) for g in spectral_gaps],\n",
    "        'lambda_1': [float(l) for l in lambda_1s],\n",
    "        'lambda_2': [float(l) for l in lambda_2s],\n",
    "        'traces': [float(t) for t in traces]\n",
    "    },\n",
    "    \n",
    "    'validation': {\n",
    "        'formula_tested': 'rho_ij = sqrt(A_ij) * W_V',\n",
    "        'contraction_before_Lstar': bool(np.mean(contraction_ratios[:L_star_contraction]) < 1),\n",
    "        'expansion_after_Lstar': bool(np.mean(contraction_ratios[L_star_contraction:]) > np.mean(contraction_ratios[:L_star_contraction]))\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to JSON\n",
    "with open('restriction_maps_results.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"RESTRICTION MAPS VALIDATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nModel: {MODEL_NAME}\")\n",
    "print(f\"Test prompt: '{TEST_PROMPT[:50]}...'\")\n",
    "print(f\"\\nFormula tested: rho_ij = sqrt(A_ij) * W_V\")\n",
    "print(f\"\\nKey Findings:\")\n",
    "print(f\"  L* (min contraction): Layer {L_star_contraction}\")\n",
    "print(f\"  Contraction before L*: {summary['validation']['contraction_before_Lstar']}\")\n",
    "print(f\"  Relative expansion after L*: {summary['validation']['expansion_after_Lstar']}\")\n",
    "print(f\"\\nSheaf Laplacian:\")\n",
    "print(f\"  Spectral gap range: {min(spectral_gaps):.4f} - {max(spectral_gaps):.4f}\")\n",
    "print(f\"  Max gap at layer: {np.argmax(spectral_gaps)}\")\n",
    "print(f\"\\nFiles saved:\")\n",
    "print(f\"  - restriction_maps_analysis.png\")\n",
    "print(f\"  - sheaf_laplacian_spectral.png\")\n",
    "print(f\"  - restriction_maps_results.json\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ZIP archive\n",
    "import zipfile\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "zip_filename = f\"restriction_maps_results_{timestamp}.zip\"\n",
    "\n",
    "with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
    "    zipf.write('restriction_maps_analysis.png')\n",
    "    zipf.write('sheaf_laplacian_spectral.png')\n",
    "    zipf.write('restriction_maps_results.json')\n",
    "\n",
    "print(f\">>> Created: {zip_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Interpretation\n",
    "\n",
    "### Theoretical Prediction\n",
    "\n",
    "The restriction map formula $\\rho_{ij} = \\sqrt{A_{ij}} \\cdot W_V$ has specific implications:\n",
    "\n",
    "1. **Attention-weighted transport**: The $\\sqrt{A_{ij}}$ factor scales the transport based on attention\n",
    "2. **Value transformation**: $W_V$ transforms the semantic content during transport\n",
    "3. **Contraction/Expansion**: The norm of $\\rho_{ij}$ determines whether information is compressed or expanded\n",
    "\n",
    "### Expected vs Observed\n",
    "\n",
    "| Prediction | Expected | Status |\n",
    "|------------|----------|--------|\n",
    "| $\\rho_{ij}$ follows formula | $\\sqrt{A_{ij}} \\cdot W_V$ | CONSTRUCTED |\n",
    "| Contraction before L* | Norm ratio < 1 | CHECK RESULTS |\n",
    "| Expansion after L* | Norm ratio increases | CHECK RESULTS |\n",
    "| Spectral gap shift at L* | Change in $\\lambda_2 - \\lambda_1$ | CHECK RESULTS |\n",
    "\n",
    "### Significance\n",
    "\n",
    "If validated, this confirms:\n",
    "1. Transformers implicitly implement sheaf diffusion\n",
    "2. The attention mechanism defines restriction maps\n",
    "3. Layer dynamics follow sheaf-theoretic predictions\n",
    "\n",
    "This provides the **mechanistic basis** for the phase-structured dynamics observed in Papers #1 and #2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download all results\n",
    "from google.colab import files\n",
    "\n",
    "print(\"Downloading result files...\")\n",
    "print()\n",
    "\n",
    "# Download ZIP\n",
    "print(f\"1. ZIP Archive: {zip_filename}\")\n",
    "files.download(zip_filename)\n",
    "\n",
    "# Individual files\n",
    "print(\"\\n2. Individual files:\")\n",
    "print(\"   - restriction_maps_analysis.png\")\n",
    "files.download('restriction_maps_analysis.png')\n",
    "print(\"   - sheaf_laplacian_spectral.png\")\n",
    "files.download('sheaf_laplacian_spectral.png')\n",
    "print(\"   - restriction_maps_results.json\")\n",
    "files.download('restriction_maps_results.json')\n",
    "\n",
    "print(\"\\n>>> All files downloaded!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}