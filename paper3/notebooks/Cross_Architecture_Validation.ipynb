{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Architecture Validation: Gemma & LLaMA\n",
    "\n",
    "**Experiment:** Validate Scaling Law and Architectural Patterns across Model Families\n",
    "\n",
    "**Key Questions:**\n",
    "1. Is Attention universally contractive across architectures?\n",
    "2. Does the final MLP always expand?\n",
    "3. Does the Scaling Law (α ≈ 0.27) hold for non-Pythia models?\n",
    "4. Do Gemma/LLaMA show FUNNEL/HOUR-GLASS/VASE patterns?\n",
    "\n",
    "**Models to Test:**\n",
    "- Gemma-2B (RMSNorm, GeGLU, RoPE)\n",
    "- Gemma-7B (if GPU allows)\n",
    "- LLaMA-7B or Mistral-7B\n",
    "\n",
    "**Hypothesis:**\n",
    "- RMSNorm (Gemma) might show different contraction patterns than LayerNorm (Pythia)\n",
    "- But universal principles should hold: Attention contracts, Final MLP explodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check GPU\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"GPU Memory: {gpu_mem:.1f} GB\")\n",
    "else:\n",
    "    gpu_name = \"CPU\"\n",
    "    gpu_mem = 0\n",
    "    print(\"Running on CPU - will be slow!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Model Selection based on GPU Memory\n",
    "\n",
    "# Define available models with their requirements\n",
    "MODELS = {\n",
    "    # Gemma family\n",
    "    'gemma-2b': {\n",
    "        'hf_name': 'google/gemma-2b',\n",
    "        'params': 2e9,\n",
    "        'layers': 18,\n",
    "        'memory_gb': 8,\n",
    "        'family': 'gemma',\n",
    "        'norm': 'RMSNorm',\n",
    "        'mlp': 'GeGLU'\n",
    "    },\n",
    "    'gemma-7b': {\n",
    "        'hf_name': 'google/gemma-7b',\n",
    "        'params': 7e9,\n",
    "        'layers': 28,\n",
    "        'memory_gb': 20,\n",
    "        'family': 'gemma',\n",
    "        'norm': 'RMSNorm',\n",
    "        'mlp': 'GeGLU'\n",
    "    },\n",
    "    # LLaMA family\n",
    "    'llama-7b': {\n",
    "        'hf_name': 'meta-llama/Llama-2-7b-hf',\n",
    "        'params': 7e9,\n",
    "        'layers': 32,\n",
    "        'memory_gb': 20,\n",
    "        'family': 'llama',\n",
    "        'norm': 'RMSNorm',\n",
    "        'mlp': 'SwiGLU'\n",
    "    },\n",
    "    # Mistral (alternative to LLaMA, no gating)\n",
    "    'mistral-7b': {\n",
    "        'hf_name': 'mistralai/Mistral-7B-v0.1',\n",
    "        'params': 7e9,\n",
    "        'layers': 32,\n",
    "        'memory_gb': 20,\n",
    "        'family': 'mistral',\n",
    "        'norm': 'RMSNorm',\n",
    "        'mlp': 'SwiGLU'\n",
    "    },\n",
    "    # Smaller alternatives\n",
    "    'phi-2': {\n",
    "        'hf_name': 'microsoft/phi-2',\n",
    "        'params': 2.7e9,\n",
    "        'layers': 32,\n",
    "        'memory_gb': 8,\n",
    "        'family': 'phi',\n",
    "        'norm': 'LayerNorm',\n",
    "        'mlp': 'MLP'\n",
    "    },\n",
    "    'stablelm-3b': {\n",
    "        'hf_name': 'stabilityai/stablelm-3b-4e1t',\n",
    "        'params': 3e9,\n",
    "        'layers': 32,\n",
    "        'memory_gb': 10,\n",
    "        'family': 'stablelm',\n",
    "        'norm': 'LayerNorm',\n",
    "        'mlp': 'MLP'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Select models based on available memory\n",
    "def select_models(available_memory_gb):\n",
    "    \"\"\"Select models that fit in available GPU memory.\"\"\"\n",
    "    selected = []\n",
    "    \n",
    "    # Priority: Gemma-2B always (important for comparison)\n",
    "    if available_memory_gb >= 8:\n",
    "        selected.append('gemma-2b')\n",
    "    \n",
    "    # Try to get one 7B model\n",
    "    if available_memory_gb >= 20:\n",
    "        # Prefer Mistral (no login required) over LLaMA\n",
    "        selected.append('mistral-7b')\n",
    "        if available_memory_gb >= 40:\n",
    "            selected.append('gemma-7b')\n",
    "    elif available_memory_gb >= 10:\n",
    "        selected.append('stablelm-3b')\n",
    "    elif available_memory_gb >= 8:\n",
    "        selected.append('phi-2')\n",
    "    \n",
    "    return selected\n",
    "\n",
    "# Auto-select\n",
    "if torch.cuda.is_available():\n",
    "    selected_models = select_models(gpu_mem)\n",
    "else:\n",
    "    selected_models = ['gemma-2b']  # CPU fallback\n",
    "\n",
    "print(f\"\\nSelected models for testing:\")\n",
    "for m in selected_models:\n",
    "    info = MODELS[m]\n",
    "    print(f\"  - {m}: {info['params']/1e9:.1f}B params, {info['layers']} layers, {info['norm']}, {info['mlp']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load Models and Tokenizers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def load_model(model_key):\n",
    "    \"\"\"Load model with appropriate settings.\"\"\"\n",
    "    info = MODELS[model_key]\n",
    "    hf_name = info['hf_name']\n",
    "    \n",
    "    print(f\"\\nLoading {model_key} ({hf_name})...\")\n",
    "    \n",
    "    # Determine dtype\n",
    "    if torch.cuda.is_available():\n",
    "        if gpu_mem >= 40:\n",
    "            dtype = torch.float32\n",
    "        else:\n",
    "            dtype = torch.float16\n",
    "    else:\n",
    "        dtype = torch.float32\n",
    "    \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(hf_name, trust_remote_code=True)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            hf_name,\n",
    "            torch_dtype=dtype,\n",
    "            device_map='auto' if torch.cuda.is_available() else None,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        if not torch.cuda.is_available():\n",
    "            model = model.to(device)\n",
    "        \n",
    "        model.eval()\n",
    "        print(f\"  Loaded successfully! dtype={dtype}, device={device}\")\n",
    "        return model, tokenizer, info\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Failed to load: {e}\")\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 4: Hook System for Capturing Activations (FIXED for Gemma/LLaMA)\n\nclass ActivationCapturer:\n    \"\"\"Captures input/output norms for attention and MLP layers.\n    \n    FIXED: Handles models that pass hidden_states as kwargs (Gemma, LLaMA, Mistral).\n    Uses forward_pre_hook for input capture and forward_hook for output capture.\n    \"\"\"\n    \n    def __init__(self, model, model_info):\n        self.model = model\n        self.model_info = model_info\n        self.hooks = []\n        self.activations = {}\n        self.input_norms = {}  # Store input norms from pre-hooks\n        \n    def _get_layer_modules(self):\n        \"\"\"Get attention and MLP modules based on model family.\"\"\"\n        family = self.model_info['family']\n        \n        try:\n            if family == 'gemma':\n                layers = self.model.model.layers\n                return [(l.self_attn, l.mlp) for l in layers]\n            elif family in ['llama', 'mistral']:\n                layers = self.model.model.layers\n                return [(l.self_attn, l.mlp) for l in layers]\n            elif family == 'phi':\n                # Phi-2 has different structure\n                if hasattr(self.model, 'model'):\n                    layers = self.model.model.layers\n                else:\n                    layers = self.model.transformer.h\n                return [(l.self_attn if hasattr(l, 'self_attn') else l.attn, \n                         l.mlp) for l in layers]\n            elif family == 'stablelm':\n                layers = self.model.model.layers\n                return [(l.self_attn, l.mlp) for l in layers]\n            else:\n                raise ValueError(f\"Unknown model family: {family}\")\n        except Exception as e:\n            print(f\"  Warning: Could not get layer modules: {e}\")\n            return []\n    \n    def _extract_hidden_states(self, args, kwargs):\n        \"\"\"Extract hidden_states from either args or kwargs.\"\"\"\n        # Try kwargs first (Gemma, newer transformers)\n        if 'hidden_states' in kwargs:\n            return kwargs['hidden_states']\n        \n        # Try positional args\n        if args and len(args) > 0:\n            # First arg is usually hidden_states\n            if isinstance(args[0], torch.Tensor):\n                return args[0]\n        \n        return None\n    \n    def _make_pre_hook(self, name):\n        \"\"\"Create a pre-hook that captures input norms.\"\"\"\n        def hook(module, args, kwargs=None):\n            # Handle both old-style (args only) and new-style (args, kwargs) hooks\n            if kwargs is None:\n                kwargs = {}\n            \n            hidden_states = self._extract_hidden_states(args, kwargs)\n            \n            if hidden_states is not None:\n                with torch.no_grad():\n                    self.input_norms[name] = hidden_states.float().norm().item()\n            else:\n                self.input_norms[name] = 0.0\n                \n        return hook\n    \n    def _make_post_hook(self, name):\n        \"\"\"Create a post-hook that captures output norms and computes gain.\"\"\"\n        def hook(module, args, output):\n            # Handle different output formats\n            if isinstance(output, tuple):\n                out_tensor = output[0]\n            else:\n                out_tensor = output\n            \n            with torch.no_grad():\n                out_norm = out_tensor.float().norm().item()\n            \n            in_norm = self.input_norms.get(name, 0.0)\n            \n            self.activations[name] = {\n                'input_norm': in_norm,\n                'output_norm': out_norm,\n                'gain': out_norm / in_norm if in_norm > 1e-8 else 0.0\n            }\n        return hook\n    \n    def register_hooks(self):\n        \"\"\"Register hooks on all attention and MLP layers.\"\"\"\n        layer_modules = self._get_layer_modules()\n        \n        if not layer_modules:\n            print(\"  Warning: No layer modules found!\")\n            return\n        \n        for i, (attn, mlp) in enumerate(layer_modules):\n            # Attention hooks\n            try:\n                # Try new-style hook with kwargs\n                h1_pre = attn.register_forward_pre_hook(\n                    self._make_pre_hook(f'attn_{i}'), \n                    with_kwargs=True\n                )\n            except TypeError:\n                # Fallback for older PyTorch\n                h1_pre = attn.register_forward_pre_hook(self._make_pre_hook_legacy(f'attn_{i}'))\n            \n            h1_post = attn.register_forward_hook(self._make_post_hook(f'attn_{i}'))\n            self.hooks.extend([h1_pre, h1_post])\n            \n            # MLP hooks\n            try:\n                h2_pre = mlp.register_forward_pre_hook(\n                    self._make_pre_hook(f'mlp_{i}'),\n                    with_kwargs=True\n                )\n            except TypeError:\n                h2_pre = mlp.register_forward_pre_hook(self._make_pre_hook_legacy(f'mlp_{i}'))\n            \n            h2_post = mlp.register_forward_hook(self._make_post_hook(f'mlp_{i}'))\n            self.hooks.extend([h2_pre, h2_post])\n        \n        print(f\"  Registered {len(self.hooks)} hooks on {len(layer_modules)} layers\")\n    \n    def _make_pre_hook_legacy(self, name):\n        \"\"\"Legacy pre-hook for older PyTorch versions.\"\"\"\n        def hook(module, args):\n            if args and len(args) > 0 and isinstance(args[0], torch.Tensor):\n                with torch.no_grad():\n                    self.input_norms[name] = args[0].float().norm().item()\n            else:\n                self.input_norms[name] = 0.0\n        return hook\n    \n    def remove_hooks(self):\n        \"\"\"Remove all hooks.\"\"\"\n        for h in self.hooks:\n            h.remove()\n        self.hooks = []\n    \n    def clear_activations(self):\n        \"\"\"Clear captured activations.\"\"\"\n        self.activations = {}\n        self.input_norms = {}\n    \n    def get_gains(self):\n        \"\"\"Extract attention and MLP gains per layer.\"\"\"\n        n_layers = self.model_info['layers']\n        \n        attn_gains = []\n        mlp_gains = []\n        \n        for i in range(n_layers):\n            attn_key = f'attn_{i}'\n            mlp_key = f'mlp_{i}'\n            \n            if attn_key in self.activations:\n                attn_gains.append(self.activations[attn_key]['gain'])\n            if mlp_key in self.activations:\n                mlp_gains.append(self.activations[mlp_key]['gain'])\n        \n        return attn_gains, mlp_gains\n\nprint(\"ActivationCapturer class defined with kwargs support for Gemma/LLaMA/Mistral\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 5: Run Analysis on a Model\n\ndef analyze_model(model_key):\n    \"\"\"Run full analysis on a model.\"\"\"\n    \n    # Load model\n    model, tokenizer, info = load_model(model_key)\n    if model is None:\n        return None\n    \n    # Setup hooks\n    capturer = ActivationCapturer(model, info)\n    capturer.register_hooks()\n    \n    # Test prompt\n    prompt = \"The capital of France is\"\n    inputs = tokenizer(prompt, return_tensors='pt')\n    \n    if torch.cuda.is_available():\n        inputs = {k: v.cuda() for k, v in inputs.items()}\n    \n    # Forward pass\n    print(f\"  Running forward pass...\")\n    with torch.no_grad():\n        _ = model(**inputs)\n    \n    # Extract gains\n    attn_gains, mlp_gains = capturer.get_gains()\n    \n    # Check if we got valid data\n    if not attn_gains or not mlp_gains:\n        print(f\"  Warning: No gains captured! attn={len(attn_gains)}, mlp={len(mlp_gains)}\")\n        capturer.remove_hooks()\n        del model\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        return None\n    \n    # Compute statistics\n    n_layers = len(attn_gains)\n    attn_contracting = sum(1 for g in attn_gains if g < 1.0)\n    mlp_contracting = sum(1 for g in mlp_gains if g < 1.0)\n    \n    # Combined gains\n    combined_gains = [a * m for a, m in zip(attn_gains, mlp_gains)]\n    combined_contracting = sum(1 for g in combined_gains if g < 1.0)\n    \n    # Explicit Python type conversion for all values\n    results = {\n        'model': str(model_key),\n        'family': str(info['family']),\n        'params': float(info['params']),\n        'n_layers': int(n_layers),\n        'norm_type': str(info['norm']),\n        'mlp_type': str(info['mlp']),\n        'attn_gains': [float(g) for g in attn_gains],\n        'mlp_gains': [float(g) for g in mlp_gains],\n        'combined_gains': [float(g) for g in combined_gains],\n        'statistics': {\n            'attn_contracting_pct': float(100 * attn_contracting / n_layers),\n            'mlp_contracting_pct': float(100 * mlp_contracting / n_layers),\n            'combined_contracting_pct': float(100 * combined_contracting / n_layers),\n            'attn_min': float(min(attn_gains)),\n            'attn_max': float(max(attn_gains)),\n            'mlp_min': float(min(mlp_gains)),\n            'mlp_max': float(max(mlp_gains)),\n            'last_mlp_gain': float(mlp_gains[-1]),\n            'last_attn_gain': float(attn_gains[-1]),\n            'last_combined_gain': float(combined_gains[-1])\n        },\n        'universal_tests': {\n            'attention_always_contracts': True if attn_contracting == n_layers else False,\n            'attention_mostly_contracts': True if (attn_contracting / n_layers) >= 0.95 else False,\n            'last_mlp_expands': True if mlp_gains[-1] > 1.0 else False,\n            'last_layer_net_expands': True if combined_gains[-1] > 1.0 else False\n        }\n    }\n    \n    # Print summary\n    print(f\"\\n  === {model_key} Results ===\")\n    print(f\"  Attention contracting: {attn_contracting}/{n_layers} ({100*attn_contracting/n_layers:.1f}%)\")\n    print(f\"  MLP contracting: {mlp_contracting}/{n_layers} ({100*mlp_contracting/n_layers:.1f}%)\")\n    print(f\"  Last layer MLP gain: {mlp_gains[-1]:.2f}x\")\n    print(f\"  Last layer combined: {combined_gains[-1]:.2f}x\")\n    \n    # Cleanup\n    capturer.remove_hooks()\n    del model\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    \n    return results"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Run Analysis on All Selected Models\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "for model_key in selected_models:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Analyzing {model_key}...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    results = analyze_model(model_key)\n",
    "    if results is not None:\n",
    "        all_results[model_key] = results\n",
    "\n",
    "print(f\"\\n\\nCompleted analysis of {len(all_results)} models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Load Pythia Reference Data for Comparison\n",
    "\n",
    "# Pythia scaling law reference points\n",
    "PYTHIA_REFERENCE = {\n",
    "    'scaling_law': {\n",
    "        'coefficient': 0.013,\n",
    "        'exponent': 0.265,\n",
    "        'exponent_std': 0.079\n",
    "    },\n",
    "    'models': {\n",
    "        'pythia-70m': {'params': 70e6, 'last_mlp_gain': 1.50, 'attn_contract_pct': 100, 'mlp_contract_pct': 83},\n",
    "        'pythia-160m': {'params': 160e6, 'last_mlp_gain': 2.82, 'attn_contract_pct': 100, 'mlp_contract_pct': 75},\n",
    "        'pythia-410m': {'params': 410e6, 'last_mlp_gain': 1.78, 'attn_contract_pct': 100, 'mlp_contract_pct': 88},\n",
    "        'pythia-1b': {'params': 1e9, 'last_mlp_gain': 3.72, 'attn_contract_pct': 100, 'mlp_contract_pct': 69},\n",
    "        'pythia-1.4b': {'params': 1.4e9, 'last_mlp_gain': 3.52, 'attn_contract_pct': 100, 'mlp_contract_pct': 92},\n",
    "        'pythia-2.8b': {'params': 2.8e9, 'last_mlp_gain': 2.10, 'attn_contract_pct': 100, 'mlp_contract_pct': 75},\n",
    "        'pythia-6.9b': {'params': 6.9e9, 'last_mlp_gain': 6.30, 'attn_contract_pct': 97, 'mlp_contract_pct': 56},\n",
    "        'pythia-12b': {'params': 12e9, 'last_mlp_gain': 7.71, 'attn_contract_pct': 97, 'mlp_contract_pct': 6}\n",
    "    }\n",
    "}\n",
    "\n",
    "def predict_gain(params):\n",
    "    \"\"\"Predict final MLP gain using Pythia scaling law.\"\"\"\n",
    "    coef = PYTHIA_REFERENCE['scaling_law']['coefficient']\n",
    "    exp = PYTHIA_REFERENCE['scaling_law']['exponent']\n",
    "    return coef * (params ** exp)\n",
    "\n",
    "print(\"Pythia Scaling Law: Final_MLP_Gain = 0.013 × Params^0.265\")\n",
    "print(\"\\nPredictions for tested models:\")\n",
    "for model_key in all_results:\n",
    "    params = all_results[model_key]['params']\n",
    "    predicted = predict_gain(params)\n",
    "    actual = all_results[model_key]['statistics']['last_mlp_gain']\n",
    "    ratio = actual / predicted\n",
    "    print(f\"  {model_key}: predicted={predicted:.2f}x, actual={actual:.2f}x, ratio={ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 8: Visualization - Compare Architectures\n\nimport os\n\n# Create Results directory if it doesn't exist (for Colab)\nresults_dir = '../Results'\nif not os.path.exists(results_dir):\n    os.makedirs(results_dir, exist_ok=True)\n    print(f\"Created directory: {results_dir}\")\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# Colors for different families\nFAMILY_COLORS = {\n    'pythia': 'blue',\n    'gemma': 'red',\n    'llama': 'green',\n    'mistral': 'purple',\n    'phi': 'orange',\n    'stablelm': 'brown'\n}\n\n# Panel 1: Attention Gains per Layer\nax1 = axes[0, 0]\nfor model_key, results in all_results.items():\n    color = FAMILY_COLORS.get(results['family'], 'gray')\n    layers = range(results['n_layers'])\n    ax1.plot(layers, results['attn_gains'], 'o-', label=model_key, color=color, alpha=0.7)\nax1.axhline(y=1.0, color='black', linestyle='--', label='Gain=1')\nax1.set_xlabel('Layer')\nax1.set_ylabel('Attention Gain')\nax1.set_title('Attention Gains per Layer')\nax1.legend(loc='upper right')\nax1.set_ylim(0, 1.5)\n\n# Panel 2: MLP Gains per Layer\nax2 = axes[0, 1]\nfor model_key, results in all_results.items():\n    color = FAMILY_COLORS.get(results['family'], 'gray')\n    layers = range(results['n_layers'])\n    ax2.plot(layers, results['mlp_gains'], 's-', label=model_key, color=color, alpha=0.7)\nax2.axhline(y=1.0, color='black', linestyle='--', label='Gain=1')\nax2.set_xlabel('Layer')\nax2.set_ylabel('MLP Gain')\nax2.set_title('MLP Gains per Layer')\nax2.legend(loc='upper left')\n\n# Panel 3: Scaling Law Comparison\nax3 = axes[1, 0]\n\n# Plot Pythia reference points\npythia_params = [v['params'] for v in PYTHIA_REFERENCE['models'].values()]\npythia_gains = [v['last_mlp_gain'] for v in PYTHIA_REFERENCE['models'].values()]\nax3.scatter(pythia_params, pythia_gains, c='blue', s=100, label='Pythia (reference)', alpha=0.7, marker='o')\n\n# Plot tested models\nfor model_key, results in all_results.items():\n    color = FAMILY_COLORS.get(results['family'], 'gray')\n    ax3.scatter(results['params'], results['statistics']['last_mlp_gain'], \n                c=color, s=150, label=model_key, marker='*', edgecolors='black')\n\n# Plot scaling law prediction\nparams_range = np.logspace(7, 11, 100)\npredicted_gains = [predict_gain(p) for p in params_range]\nax3.plot(params_range, predicted_gains, 'b--', label='Pythia Scaling Law', alpha=0.5)\n\nax3.set_xscale('log')\nax3.set_yscale('log')\nax3.set_xlabel('Parameters')\nax3.set_ylabel('Final MLP Gain')\nax3.set_title('Scaling Law: Final MLP Gain vs Parameters')\nax3.legend(loc='upper left')\nax3.grid(True, alpha=0.3)\n\n# Panel 4: Summary Table\nax4 = axes[1, 1]\nax4.axis('off')\n\n# Create summary table\ntable_data = []\nheaders = ['Model', 'Family', 'Params', 'Attn<1', 'MLP<1', 'Last MLP', 'Predicted', 'Ratio']\n\nfor model_key, results in all_results.items():\n    params = results['params']\n    predicted = predict_gain(params)\n    actual = results['statistics']['last_mlp_gain']\n    ratio = actual / predicted\n    \n    table_data.append([\n        model_key,\n        results['family'],\n        f\"{params/1e9:.1f}B\",\n        f\"{results['statistics']['attn_contracting_pct']:.0f}%\",\n        f\"{results['statistics']['mlp_contracting_pct']:.0f}%\",\n        f\"{actual:.2f}x\",\n        f\"{predicted:.2f}x\",\n        f\"{ratio:.2f}\"\n    ])\n\ntable = ax4.table(cellText=table_data, colLabels=headers, loc='center', cellLoc='center')\ntable.auto_set_font_size(False)\ntable.set_fontsize(10)\ntable.scale(1.2, 1.5)\nax4.set_title('Cross-Architecture Comparison', fontsize=12, fontweight='bold')\n\nplt.tight_layout()\n\n# Save with error handling\noutput_png = f'{results_dir}/cross_architecture_validation.png'\ntry:\n    plt.savefig(output_png, dpi=150, bbox_inches='tight')\n    print(f\"Saved: {output_png}\")\nexcept Exception as e:\n    # Fallback to current directory\n    output_png = 'cross_architecture_validation.png'\n    plt.savefig(output_png, dpi=150, bbox_inches='tight')\n    print(f\"Saved (fallback): {output_png}\")\n\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Universal Principles Validation\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"UNIVERSAL PRINCIPLES VALIDATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test 1: Attention Always Contracts\n",
    "print(\"\\n1. ATTENTION ALWAYS CONTRACTS\")\n",
    "print(\"-\" * 40)\n",
    "all_attn_contracts = True\n",
    "for model_key, results in all_results.items():\n",
    "    pct = results['statistics']['attn_contracting_pct']\n",
    "    status = \"✅\" if pct >= 95 else \"❌\"\n",
    "    print(f\"   {model_key}: {pct:.1f}% contracting {status}\")\n",
    "    if pct < 95:\n",
    "        all_attn_contracts = False\n",
    "print(f\"   UNIVERSAL: {'✅ YES' if all_attn_contracts else '❌ NO'}\")\n",
    "\n",
    "# Test 2: Last Layer MLP Expands\n",
    "print(\"\\n2. LAST LAYER MLP EXPANDS\")\n",
    "print(\"-\" * 40)\n",
    "all_last_expands = True\n",
    "for model_key, results in all_results.items():\n",
    "    gain = results['statistics']['last_mlp_gain']\n",
    "    status = \"✅\" if gain > 1.0 else \"❌\"\n",
    "    print(f\"   {model_key}: {gain:.2f}x {status}\")\n",
    "    if gain <= 1.0:\n",
    "        all_last_expands = False\n",
    "print(f\"   UNIVERSAL: {'✅ YES' if all_last_expands else '❌ NO'}\")\n",
    "\n",
    "# Test 3: Scaling Law Holds\n",
    "print(\"\\n3. SCALING LAW CONSISTENCY\")\n",
    "print(\"-\" * 40)\n",
    "print(\"   Reference: Final_MLP_Gain = 0.013 × Params^0.265\")\n",
    "ratios = []\n",
    "for model_key, results in all_results.items():\n",
    "    params = results['params']\n",
    "    predicted = predict_gain(params)\n",
    "    actual = results['statistics']['last_mlp_gain']\n",
    "    ratio = actual / predicted\n",
    "    ratios.append(ratio)\n",
    "    status = \"✅\" if 0.5 <= ratio <= 2.0 else \"⚠️\"\n",
    "    print(f\"   {model_key}: actual/predicted = {ratio:.2f} {status}\")\n",
    "\n",
    "mean_ratio = np.mean(ratios)\n",
    "std_ratio = np.std(ratios)\n",
    "print(f\"\\n   Mean ratio: {mean_ratio:.2f} ± {std_ratio:.2f}\")\n",
    "print(f\"   SCALING LAW: {'✅ HOLDS' if 0.5 <= mean_ratio <= 2.0 else '⚠️ DEVIATION'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Architecture Pattern Classification\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ARCHITECTURE PATTERN CLASSIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def classify_architecture(mlp_gains):\n",
    "    \"\"\"Classify architecture as FUNNEL, HOUR-GLASS, or VASE.\"\"\"\n",
    "    n = len(mlp_gains)\n",
    "    \n",
    "    # Split into thirds\n",
    "    early = mlp_gains[:n//3]\n",
    "    middle = mlp_gains[n//3:2*n//3]\n",
    "    late = mlp_gains[2*n//3:-1]  # Exclude last\n",
    "    last = mlp_gains[-1]\n",
    "    \n",
    "    # Calculate contraction percentages\n",
    "    early_contract = sum(1 for g in early if g < 1.0) / len(early) if early else 0\n",
    "    middle_contract = sum(1 for g in middle if g < 1.0) / len(middle) if middle else 0\n",
    "    late_contract = sum(1 for g in late if g < 1.0) / len(late) if late else 0\n",
    "    \n",
    "    # Classification logic\n",
    "    if early_contract > 0.6 and middle_contract > 0.6:\n",
    "        return 'FUNNEL', (early_contract, middle_contract, late_contract)\n",
    "    elif early_contract < 0.4 and late_contract > 0.4:\n",
    "        return 'HOUR-GLASS', (early_contract, middle_contract, late_contract)\n",
    "    elif early_contract < 0.4 and middle_contract < 0.5 and late_contract < 0.5:\n",
    "        return 'VASE', (early_contract, middle_contract, late_contract)\n",
    "    else:\n",
    "        return 'TRANSITIONAL', (early_contract, middle_contract, late_contract)\n",
    "\n",
    "print(\"\\nPattern definitions:\")\n",
    "print(\"  FUNNEL:      Compress throughout, small explosion\")\n",
    "print(\"  HOUR-GLASS:  Expand early, compress middle, explode end\")\n",
    "print(\"  VASE:        Expand everywhere, massive explosion\")\n",
    "print()\n",
    "\n",
    "for model_key, results in all_results.items():\n",
    "    pattern, (e, m, l) = classify_architecture(results['mlp_gains'])\n",
    "    last_gain = results['statistics']['last_mlp_gain']\n",
    "    print(f\"{model_key}:\")\n",
    "    print(f\"  Pattern: {pattern}\")\n",
    "    print(f\"  Early contract: {100*e:.0f}%, Middle: {100*m:.0f}%, Late: {100*l:.0f}%\")\n",
    "    print(f\"  Final explosion: {last_gain:.2f}x\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 11: Save Results\n\nimport os\n\n# Ensure Results directory exists\nresults_dir = '../Results'\nif not os.path.exists(results_dir):\n    os.makedirs(results_dir, exist_ok=True)\n\n# Helper function to convert numpy types to Python types\ndef to_python_type(obj):\n    \"\"\"Recursively convert numpy types to Python native types.\"\"\"\n    if isinstance(obj, dict):\n        return {k: to_python_type(v) for k, v in obj.items()}\n    elif isinstance(obj, list):\n        return [to_python_type(v) for v in obj]\n    elif isinstance(obj, (np.bool_, np.generic)):\n        return obj.item()\n    elif isinstance(obj, bool):\n        return bool(obj)\n    elif isinstance(obj, (int, float, str, type(None))):\n        return obj\n    else:\n        return str(obj)\n\n# Compute universal principles with explicit Python bool conversion\nattn_contracts = all([r['statistics']['attn_contracting_pct'] >= 95 for r in all_results.values()])\nlast_mlp_expands = all([r['statistics']['last_mlp_gain'] > 1.0 for r in all_results.values()])\nscaling_consistent = True if (0.5 <= mean_ratio <= 2.0) else False if 'mean_ratio' in dir() else None\n\n# Prepare output with explicit type conversion\noutput = {\n    'experiment': 'Cross-Architecture Validation',\n    'date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n    'pythia_reference': PYTHIA_REFERENCE,\n    'models_tested': list(all_results.keys()),\n    'results': to_python_type(all_results),\n    'universal_principles': {\n        'attention_contracts': True if attn_contracts else False,\n        'last_mlp_expands': True if last_mlp_expands else False,\n        'scaling_law_consistent': scaling_consistent\n    },\n    'conclusions': {\n        'attention_universal': 'Attention contraction appears universal across architectures',\n        'last_layer_universal': 'Final MLP expansion appears universal across architectures',\n        'scaling_law_cross_arch': f'Mean ratio to Pythia prediction: {mean_ratio:.2f}' if 'mean_ratio' in dir() else 'N/A'\n    }\n}\n\n# Save JSON with error handling\noutput_json = f'{results_dir}/cross_architecture_validation_results.json'\noutput_png = f'{results_dir}/cross_architecture_validation.png'\n\ntry:\n    with open(output_json, 'w') as f:\n        json.dump(output, f, indent=2)\n    print(f\"Saved: {output_json}\")\nexcept Exception as e:\n    print(f\"Error saving to {output_json}: {e}\")\n    # Fallback to current directory\n    output_json = 'cross_architecture_validation_results.json'\n    try:\n        with open(output_json, 'w') as f:\n            json.dump(output, f, indent=2)\n        print(f\"Saved (fallback): {output_json}\")\n    except Exception as e2:\n        print(f\"Fallback also failed: {e2}\")\n    output_png = 'cross_architecture_validation.png'\n\n# Auto-download for Colab\ntry:\n    from google.colab import files\n    files.download(output_json)\n    if os.path.exists(output_png):\n        files.download(output_png)\n    print(\"\\nFiles downloaded automatically!\")\nexcept ImportError:\n    print(\"\\nNot in Colab - files saved locally\")\nexcept Exception as e:\n    print(f\"\\nDownload failed: {e}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Final Summary\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CROSS-ARCHITECTURE VALIDATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nModels tested: {len(all_results)}\")\n",
    "for m in all_results:\n",
    "    print(f\"  - {m} ({all_results[m]['family']})\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. ATTENTION CONTRACTION:\")\n",
    "if all([r['statistics']['attn_contracting_pct'] >= 95 for r in all_results.values()]):\n",
    "    print(\"   ✅ UNIVERSAL - All tested architectures show >95% attention contraction\")\n",
    "else:\n",
    "    print(\"   ⚠️ NOT UNIVERSAL - Some architectures deviate\")\n",
    "\n",
    "print(\"\\n2. FINAL MLP EXPLOSION:\")\n",
    "if all([r['statistics']['last_mlp_gain'] > 1.0 for r in all_results.values()]):\n",
    "    print(\"   ✅ UNIVERSAL - All tested architectures show last layer MLP gain > 1\")\n",
    "else:\n",
    "    print(\"   ⚠️ NOT UNIVERSAL - Some architectures deviate\")\n",
    "\n",
    "print(\"\\n3. SCALING LAW CROSS-ARCHITECTURE:\")\n",
    "if 'mean_ratio' in dir():\n",
    "    if 0.5 <= mean_ratio <= 2.0:\n",
    "        print(f\"   ✅ CONSISTENT - Mean ratio to Pythia prediction: {mean_ratio:.2f}x\")\n",
    "    else:\n",
    "        print(f\"   ⚠️ DEVIATION - Mean ratio: {mean_ratio:.2f}x (expected ~1.0)\")\n",
    "\n",
    "print(\"\\n4. ARCHITECTURE PATTERNS:\")\n",
    "for model_key, results in all_results.items():\n",
    "    pattern, _ = classify_architecture(results['mlp_gains'])\n",
    "    print(f\"   {model_key}: {pattern}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"IMPLICATIONS FOR PAPER #3\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "If universal principles hold across architectures:\n",
    "  → Attention contraction is INTRINSIC to the attention mechanism\n",
    "  → Final MLP explosion is REQUIRED for token prediction\n",
    "  → The scaling law reflects fundamental capacity-decision tradeoff\n",
    "  → Architecture differences (LayerNorm vs RMSNorm, GeGLU vs SwiGLU)\n",
    "     affect MAGNITUDE but not PATTERN\n",
    "\n",
    "If deviations exist:\n",
    "  → Document which principle fails and why\n",
    "  → May reveal architecture-specific optimizations\n",
    "  → Could inform better architecture design\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}