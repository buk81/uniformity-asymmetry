{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# H4 v2 Extended: Full-Scale Sheaf Laplacian on Larger Models\n",
        "\n",
        "**Extension of H4 v2** to larger models:\n",
        "- Pythia-1B, 2.8B, 6.9B\n",
        "- Mistral-7B\n",
        "- Llama-3.1-8B\n",
        "- Gemma-7B\n",
        "\n",
        "## Key Findings from H4 v2 (4 small models):\n",
        "- GPT-2 shows **26x higher multi-head trace** than OPT-125m\n",
        "- Block-diagonal structure validated: Tr(L_F^total) = \u03a3_h Tr(L_F^(h))\n",
        "- Layer-wise dynamics differ: GPT-2 monotonic, Pythia two-phase\n",
        "\n",
        "## Goal:\n",
        "Validate these findings on larger, production-scale models.\n",
        "\n",
        "---\n",
        "*Paper #3: Thermodynamic Constraints in Transformer Architectures*\n",
        "*Author: Davide D'Elia*\n",
        "*Date: 2026-01-06*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install transformers accelerate torch numpy scipy matplotlib seaborn pandas bitsandbytes -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from tqdm.auto import tqdm\n",
        "import json\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "import gc\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_context(\"paper\", font_scale=1.2)\n",
        "\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Extended Model Configuration\n",
        "\n",
        "Testing larger models with 4-bit quantization for memory efficiency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extended model set - larger models\n",
        "MODELS_EXTENDED = {\n",
        "    # EleutherAI - DAMPENERS (larger)\n",
        "    'EleutherAI/pythia-1b': {\n",
        "        'lab': 'EleutherAI', 'behavior': 'DAMPEN', 'gain': 1.216, 'layers': 16\n",
        "    },\n",
        "    'EleutherAI/pythia-2.8b': {\n",
        "        'lab': 'EleutherAI', 'behavior': 'DAMPEN', 'gain': 0.927, 'layers': 32\n",
        "    },\n",
        "    'EleutherAI/pythia-6.9b': {\n",
        "        'lab': 'EleutherAI', 'behavior': 'DAMPEN', 'gain': 0.994, 'layers': 32\n",
        "    },\n",
        "    # Mistral - Expected EXPANDER\n",
        "    'mistralai/Mistral-7B-v0.1': {\n",
        "        'lab': 'Mistral', 'behavior': 'EXPAND', 'gain': 1.05, 'layers': 32\n",
        "    },\n",
        "    # Google - RMSNorm model\n",
        "    'google/gemma-2b': {\n",
        "        'lab': 'Google', 'behavior': 'UNKNOWN', 'gain': 1.0, 'layers': 18\n",
        "    },\n",
        "}\n",
        "\n",
        "# Reference results from H4 v2 (small models)\n",
        "REFERENCE_RESULTS = {\n",
        "    'gpt2': {'trace_mh': 62696, 'behavior': 'EXPAND'},\n",
        "    'pythia-160m': {'trace_mh': 18887, 'behavior': 'DAMPEN'},\n",
        "    'pythia-410m': {'trace_mh': 11326, 'behavior': 'DAMPEN'},\n",
        "    'opt-125m': {'trace_mh': 2368, 'behavior': 'EXPAND'},\n",
        "}\n",
        "\n",
        "TEST_PROMPTS = [\n",
        "    \"The capital of France is Paris.\",\n",
        "    \"The sky is made of chocolate.\",\n",
        "    \"Once upon a time in a land far away\",\n",
        "    \"def fibonacci(n): return n if n < 2 else\",\n",
        "]\n",
        "\n",
        "COLORS = {\n",
        "    'EleutherAI': '#E74C3C', 'Meta': '#3498DB', 'OpenAI': '#8E44AD',\n",
        "    'Mistral': '#27AE60', 'Google': '#F39C12', 'BigScience': '#1ABC9C'\n",
        "}\n",
        "\n",
        "print(f\"Testing {len(MODELS_EXTENDED)} extended models\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Efficient Full-Scale Trace Computation\n",
        "\n",
        "O(n\u00b2 + d\u00b2) algorithm - no subsampling needed!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_trace_efficient(attention, W_V):\n",
        "    \"\"\"\n",
        "    Compute Tr(\u0394_F) directly from diagonal blocks - NO SUBSAMPLING.\n",
        "    \n",
        "    Tr(\u0394_F) = (\u03a3 A_ij - n) \u00b7 ||W_V||\u00b2_F\n",
        "    \n",
        "    Complexity: O(n\u00b2 + d\u00b2) instead of O(n\u00b3d\u00b3)\n",
        "    \"\"\"\n",
        "    n = attention.shape[0]\n",
        "    \n",
        "    # Frobenius norm squared of W_V\n",
        "    W_V_frobenius_sq = (W_V ** 2).sum().item()\n",
        "    \n",
        "    # Off-diagonal attention sum\n",
        "    attention_sum = attention.sum().item()\n",
        "    attention_trace = attention.trace().item()\n",
        "    off_diag_sum = attention_sum - attention_trace\n",
        "    \n",
        "    return off_diag_sum * W_V_frobenius_sq\n",
        "\n",
        "\n",
        "def compute_multihead_trace(attentions, W_V_list):\n",
        "    \"\"\"\n",
        "    Compute total trace for multi-head attention.\n",
        "    \n",
        "    Block-diagonal structure: Tr(\u0394_F^total) = \u03a3_h Tr(\u0394_F^(h))\n",
        "    \"\"\"\n",
        "    H = attentions.shape[0]\n",
        "    total_trace = 0.0\n",
        "    \n",
        "    for h in range(H):\n",
        "        A_h = attentions[h]\n",
        "        W_V_h = W_V_list[h] if isinstance(W_V_list, list) else W_V_list\n",
        "        total_trace += compute_trace_efficient(A_h, W_V_h)\n",
        "    \n",
        "    return total_trace\n",
        "\n",
        "print(\"Efficient trace functions defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_multihead_attention_W_V(model, model_name, tokenizer, prompt, device='cuda'):\n",
        "    \"\"\"\n",
        "    Extract attention weights and W_V matrices for ALL heads.\n",
        "    Supports: Pythia, OPT, GPT-2, Mistral, Gemma, LLaMA\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs, output_attentions=True, output_hidden_states=True)\n",
        "    \n",
        "    attentions = outputs.attentions\n",
        "    W_V_per_layer = []\n",
        "    n_heads = None\n",
        "    \n",
        "    # Pythia (GPT-NeoX)\n",
        "    if hasattr(model, 'gpt_neox'):\n",
        "        layers = model.gpt_neox.layers\n",
        "        n_heads = model.config.num_attention_heads\n",
        "        head_dim = model.config.hidden_size // n_heads\n",
        "        \n",
        "        for layer in layers:\n",
        "            qkv = layer.attention.query_key_value.weight.data.float().cpu()\n",
        "            hidden_size = qkv.shape[0] // 3\n",
        "            W_V_full = qkv[2*hidden_size:, :]\n",
        "            W_V_heads = [W_V_full[h*head_dim:(h+1)*head_dim, :] for h in range(n_heads)]\n",
        "            W_V_per_layer.append(W_V_heads)\n",
        "    \n",
        "    # OPT\n",
        "    elif hasattr(model, 'model') and hasattr(model.model, 'decoder'):\n",
        "        layers = model.model.decoder.layers\n",
        "        n_heads = model.config.num_attention_heads\n",
        "        head_dim = model.config.hidden_size // n_heads\n",
        "        \n",
        "        for layer in layers:\n",
        "            W_V_full = layer.self_attn.v_proj.weight.data.float().cpu()\n",
        "            W_V_heads = [W_V_full[h*head_dim:(h+1)*head_dim, :] for h in range(n_heads)]\n",
        "            W_V_per_layer.append(W_V_heads)\n",
        "    \n",
        "    # GPT-2\n",
        "    elif hasattr(model, 'transformer') and hasattr(model.transformer, 'h'):\n",
        "        layers = model.transformer.h\n",
        "        n_heads = model.config.n_head\n",
        "        head_dim = model.config.n_embd // n_heads\n",
        "        \n",
        "        for layer in layers:\n",
        "            c_attn = layer.attn.c_attn.weight.data.float().cpu()\n",
        "            hidden_size = c_attn.shape[1] // 3\n",
        "            W_V_full = c_attn[:, 2*hidden_size:].T\n",
        "            W_V_heads = [W_V_full[h*head_dim:(h+1)*head_dim, :] for h in range(n_heads)]\n",
        "            W_V_per_layer.append(W_V_heads)\n",
        "    \n",
        "    # Mistral / LLaMA / Gemma (similar structure)\n",
        "    elif hasattr(model, 'model') and hasattr(model.model, 'layers'):\n",
        "        layers = model.model.layers\n",
        "        n_heads = model.config.num_attention_heads\n",
        "        head_dim = model.config.hidden_size // n_heads\n",
        "        \n",
        "        for layer in layers:\n",
        "            if hasattr(layer.self_attn, 'v_proj'):\n",
        "                W_V_full = layer.self_attn.v_proj.weight.data.float().cpu()\n",
        "            else:\n",
        "                W_V_full = torch.zeros(model.config.hidden_size, model.config.hidden_size)\n",
        "            W_V_heads = [W_V_full[h*head_dim:(h+1)*head_dim, :] for h in range(n_heads)]\n",
        "            W_V_per_layer.append(W_V_heads)\n",
        "    \n",
        "    return attentions, W_V_per_layer, inputs.input_ids.shape[1], n_heads\n",
        "\n",
        "print(\"Multi-head extraction function defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Run Extended Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_model_extended(model_name, config, test_prompts, device='cuda', use_4bit=True):\n",
        "    \"\"\"Full H4 v2 analysis for extended models with optional 4-bit quantization.\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Analyzing: {model_name}\")\n",
        "    print(f\"Lab: {config['lab']}, Expected: {config['behavior']}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Load model with optional quantization\n",
        "    print(\"  Loading model...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    \n",
        "    if use_4bit:\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.float16\n",
        "        )\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name, quantization_config=bnb_config,\n",
        "            device_map='auto', trust_remote_code=True,\n",
        "            output_attentions=True\n",
        "        )\n",
        "    else:\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name, torch_dtype=torch.float16,\n",
        "            device_map='auto', trust_remote_code=True,\n",
        "            output_attentions=True\n",
        "        )\n",
        "    model.eval()\n",
        "    print(f\"  Model loaded. Layers: {config['layers']}\")\n",
        "    \n",
        "    all_traces_full = []\n",
        "    all_traces_mh = []\n",
        "    \n",
        "    for prompt in tqdm(test_prompts, desc=\"  Prompts\"):\n",
        "        try:\n",
        "            attentions, W_V_per_layer, seq_len, n_heads = extract_multihead_attention_W_V(\n",
        "                model, model_name, tokenizer, prompt, device\n",
        "            )\n",
        "            \n",
        "            layer_traces_full = []\n",
        "            layer_traces_mh = []\n",
        "            \n",
        "            for layer_idx in range(len(attentions)):\n",
        "                attn = attentions[layer_idx]\n",
        "                W_V_heads = W_V_per_layer[layer_idx] if layer_idx < len(W_V_per_layer) else W_V_per_layer[-1]\n",
        "                \n",
        "                # Average attention trace\n",
        "                attn_avg = attn[0].mean(dim=0)\n",
        "                trace_full = compute_trace_efficient(attn_avg, W_V_heads[0])\n",
        "                layer_traces_full.append(trace_full)\n",
        "                \n",
        "                # Multi-head trace\n",
        "                trace_mh = compute_multihead_trace(attn[0], W_V_heads)\n",
        "                layer_traces_mh.append(trace_mh)\n",
        "            \n",
        "            all_traces_full.append(layer_traces_full)\n",
        "            all_traces_mh.append(layer_traces_mh)\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  Error: {e}\")\n",
        "            continue\n",
        "    \n",
        "    # Cleanup\n",
        "    del model, tokenizer\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    if not all_traces_full:\n",
        "        return None\n",
        "    \n",
        "    mean_traces_full = np.mean(all_traces_full, axis=0)\n",
        "    mean_traces_mh = np.mean(all_traces_mh, axis=0)\n",
        "    \n",
        "    # L* from trace gradient\n",
        "    gradient = np.gradient(mean_traces_mh)\n",
        "    L_star = int(np.argmax(np.abs(gradient)))\n",
        "    \n",
        "    print(f\"\\n  Results:\")\n",
        "    print(f\"    Mean trace (MH): {np.mean(mean_traces_mh):.0f}\")\n",
        "    print(f\"    L* (max gradient): {L_star}\")\n",
        "    \n",
        "    return {\n",
        "        'model': model_name,\n",
        "        'lab': config['lab'],\n",
        "        'behavior': config['behavior'],\n",
        "        'known_gain': config['gain'],\n",
        "        'n_layers': len(mean_traces_mh),\n",
        "        'n_heads': n_heads,\n",
        "        'traces_full': mean_traces_full.tolist(),\n",
        "        'traces_mh': mean_traces_mh.tolist(),\n",
        "        'mean_trace_mh': float(np.mean(mean_traces_mh)),\n",
        "        'L_star': L_star,\n",
        "        'L_star_ratio': L_star / len(mean_traces_mh),\n",
        "    }\n",
        "\n",
        "print(\"Extended analysis function defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run on all extended models\n",
        "results_extended = []\n",
        "\n",
        "for model_name, config in MODELS_EXTENDED.items():\n",
        "    try:\n",
        "        result = analyze_model_extended(model_name, config, TEST_PROMPTS, use_4bit=True)\n",
        "        if result:\n",
        "            results_extended.append(result)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed on {model_name}: {e}\")\n",
        "        continue\n",
        "\n",
        "print(f\"\\n\\n{'='*60}\")\n",
        "print(f\"Successfully analyzed {len(results_extended)} / {len(MODELS_EXTENDED)} models\")\n",
        "print(f\"{'='*60}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Analysis & Comparison with Small Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if results_extended:\n",
        "    # Summary table\n",
        "    summary = pd.DataFrame([{\n",
        "        'Model': r['model'].split('/')[-1],\n",
        "        'Lab': r['lab'],\n",
        "        'Behavior': r['behavior'],\n",
        "        'Layers': r['n_layers'],\n",
        "        'Heads': r['n_heads'],\n",
        "        'Mean Trace (MH)': f\"{r['mean_trace_mh']:.0f}\",\n",
        "        'L*': r['L_star'],\n",
        "        'L*/L': f\"{r['L_star_ratio']:.2f}\"\n",
        "    } for r in results_extended])\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"H4 v2 EXTENDED RESULTS\")\n",
        "    print(\"=\"*80)\n",
        "    print(summary.to_string(index=False))\n",
        "    \n",
        "    # Comparison with reference\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"COMPARISON WITH SMALL MODELS (Reference)\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"\\nReference (H4 v2 small models):\")\n",
        "    for name, data in REFERENCE_RESULTS.items():\n",
        "        print(f\"  {name}: {data['trace_mh']:.0f} ({data['behavior']})\")\n",
        "    \n",
        "    print(\"\\nExtended models:\")\n",
        "    for r in results_extended:\n",
        "        print(f\"  {r['model'].split('/')[-1]}: {r['mean_trace_mh']:.0f} ({r['behavior']})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization\n",
        "if results_extended:\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
        "    \n",
        "    # Plot 1: Multi-head trace by layer\n",
        "    ax1 = axes[0, 0]\n",
        "    for r in results_extended:\n",
        "        layers = np.arange(len(r['traces_mh']))\n",
        "        norm_layers = layers / len(r['traces_mh'])\n",
        "        ax1.plot(norm_layers, r['traces_mh'],\n",
        "                 label=f\"{r['model'].split('/')[-1]}\",\n",
        "                 color=COLORS.get(r['lab'], 'gray'), linewidth=2)\n",
        "    ax1.set_xlabel('Normalized Layer (l/L)')\n",
        "    ax1.set_ylabel('\u03a3_h Tr(\u0394_F^(h))')\n",
        "    ax1.set_title('Multi-Head Sheaf Laplacian Trace')\n",
        "    ax1.legend(fontsize=9)\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 2: Mean trace by lab\n",
        "    ax2 = axes[0, 1]\n",
        "    labs = [r['lab'] for r in results_extended]\n",
        "    traces = [r['mean_trace_mh'] for r in results_extended]\n",
        "    colors_list = [COLORS.get(lab, 'gray') for lab in labs]\n",
        "    models = [r['model'].split('/')[-1] for r in results_extended]\n",
        "    \n",
        "    bars = ax2.bar(range(len(models)), traces, color=colors_list)\n",
        "    ax2.set_xticks(range(len(models)))\n",
        "    ax2.set_xticklabels(models, rotation=45, ha='right')\n",
        "    ax2.set_ylabel('Mean Trace (MH)')\n",
        "    ax2.set_title('Mean Multi-Head Trace by Model')\n",
        "    ax2.grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    # Plot 3: L* ratio distribution\n",
        "    ax3 = axes[1, 0]\n",
        "    for r in results_extended:\n",
        "        ax3.scatter(r['n_layers'], r['L_star_ratio'],\n",
        "                    color=COLORS.get(r['lab'], 'gray'), s=150,\n",
        "                    label=r['model'].split('/')[-1])\n",
        "    ax3.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
        "    ax3.set_xlabel('Number of Layers')\n",
        "    ax3.set_ylabel('L* / L')\n",
        "    ax3.set_title('Transition Point vs Model Depth')\n",
        "    ax3.legend(fontsize=9)\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 4: Trace scaling with model size\n",
        "    ax4 = axes[1, 1]\n",
        "    for r in results_extended:\n",
        "        size_proxy = r['n_layers'] * (r['n_heads'] if r['n_heads'] else 12)\n",
        "        ax4.scatter(size_proxy, r['mean_trace_mh'],\n",
        "                    color=COLORS.get(r['lab'], 'gray'), s=150,\n",
        "                    label=r['model'].split('/')[-1])\n",
        "    ax4.set_xlabel('Model Size Proxy (Layers \u00d7 Heads)')\n",
        "    ax4.set_ylabel('Mean Trace (MH)')\n",
        "    ax4.set_title('Trace Scaling with Model Size')\n",
        "    ax4.legend(fontsize=9)\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.suptitle('H4 v2 Extended: Large Model Validation', fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('H4_v2_extended_validation.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\n>>> Figure saved: H4_v2_extended_validation.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Save Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if results_extended:\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    \n",
        "    output = {\n",
        "        'experiment': 'H4 v2 Extended - Large Model Validation',\n",
        "        'date': datetime.now().isoformat(),\n",
        "        'models_tested': len(results_extended),\n",
        "        'reference_results': REFERENCE_RESULTS,\n",
        "        'extended_results': results_extended,\n",
        "    }\n",
        "    \n",
        "    filename = f'H4_v2_extended_{timestamp}.json'\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(output, f, indent=2)\n",
        "    print(f\"\\nResults saved: {filename}\")\n",
        "    \n",
        "    # Download\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        files.download(filename)\n",
        "        files.download('H4_v2_extended_validation.png')\n",
        "    except:\n",
        "        print(\"Files saved locally.\")"
      ]
    }
  ]
}
