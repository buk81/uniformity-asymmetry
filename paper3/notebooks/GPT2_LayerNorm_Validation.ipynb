{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# GPT-2 LayerNorm Validation\n",
        "\n",
        "**Paper #3 - Critical Validation Test**\n",
        "\n",
        "**Date:** 2026-01-05\n",
        "\n",
        "**Purpose:** Confirm that LayerNorm models consistently show Gain < 1.0\n",
        "\n",
        "---\n",
        "\n",
        "## Hypothesis\n",
        "\n",
        "If Pythia-6.9B (LayerNorm) shows Gain = 0.80, then GPT-2 (also LayerNorm) should also show Gain < 1.0.\n",
        "\n",
        "This would confirm that **LayerNorm = Dampening** is a general property, not a Pythia-specific artifact.\n",
        "\n",
        "| Model | Norm Type | Expected Gain | Status |\n",
        "|-------|-----------|---------------|--------|\n",
        "| Pythia-6.9B | LayerNorm | 0.80 | MEASURED |\n",
        "| **GPT-2-XL** | **LayerNorm** | **< 1.0** | **TO TEST** |\n",
        "| Mistral-7B | RMSNorm | 1.11 | MEASURED |\n",
        "| LLaMA-3.1-8B | RMSNorm | 1.48 | MEASURED |\n",
        "| Gemma-7B | RMSNorm | 2.31 | MEASURED |\n",
        "\n",
        "---\n",
        "\n",
        "## Models to Test\n",
        "\n",
        "- **GPT-2 XL (1.5B)** - Large enough for meaningful comparison\n",
        "- **GPT-2 Large (774M)** - Secondary validation\n",
        "- **GPT-2 Medium (355M)** - Tertiary validation (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup"
      },
      "source": [
        "# Cell 1: Setup\n",
        "!pip install -q transformers accelerate scipy seaborn\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GPT2LMHeadModel, GPT2Tokenizer\n",
        "from scipy.stats import entropy, spearmanr, pearsonr, ttest_ind\n",
        "import gc\n",
        "import json\n",
        "from datetime import datetime\n",
        "from google.colab import files\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# HF Token handling - GPT-2 is public, but we set up the pattern for consistency\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "    print(\"HF_TOKEN loaded from Colab secrets\")\n",
        "except:\n",
        "    HF_TOKEN = None\n",
        "    print(\"No HF_TOKEN (not needed for GPT-2)\")\n",
        "\n",
        "# Configure visualization\n",
        "plt.style.use('seaborn-v0_8-paper')\n",
        "sns.set_context(\"talk\")\n",
        "\n",
        "# Global timestamp for all files\n",
        "TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "print(f\"\\nSession timestamp: {TIMESTAMP}\")\n",
        "\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prompts"
      },
      "source": [
        "# Cell 2: PROMPT SET (Same as Grand Unified Benchmark)\n",
        "\n",
        "PROMPT_DATASET = {\n",
        "    \"Factual\": [\n",
        "        \"The capital city of France is\",\n",
        "        \"The atomic number of oxygen is\",\n",
        "        \"Water boils at a temperature of\",\n",
        "        \"The largest planet in our solar system is\",\n",
        "        \"The currency used in Japan is\"\n",
        "    ],\n",
        "    \"Syntactic\": [\n",
        "        \"The agreement, which, notwithstanding the fact that it was signed only yesterday, effectively binds all parties immediately, stipulates that\",\n",
        "        \"Although the weather was extremely cold, and despite the fact that they had no coats, the children decided to\",\n",
        "        \"The professor, having reviewed the complex derivation multiple times without finding the error, finally realized that\",\n",
        "        \"To imply that such a fundamental shift in policy could occur without significant public debate is to suggest that\",\n",
        "        \"Not only did the experiment fail to yield the expected results, but it also demonstrated that the initial hypothesis was\"\n",
        "    ],\n",
        "    \"Cliche\": [\n",
        "        \"The true meaning of happiness is often found in\",\n",
        "        \"Actions speak louder than\",\n",
        "        \"It is what it is, and we must simply\",\n",
        "        \"Time heals all\",\n",
        "        \"Life is a journey, not a\"\n",
        "    ],\n",
        "    \"Novel\": [\n",
        "        \"The epistemological implications of quantum decoherence suggest that the observer is\",\n",
        "        \"If consciousness creates reality, then the paradox of the unobserved electron implies\",\n",
        "        \"The intersection of baroque architecture and cybernetic theory creates a space where\",\n",
        "        \"Calculating the trajectory of a hyperspace jump requires factoring in the variability of\",\n",
        "        \"The symbiotic relationship between fungal mycelium and digital neural networks results in\"\n",
        "    ],\n",
        "    \"Nonsense\": [\n",
        "        \"Table sky run blue jump quickly under over\",\n",
        "        \"Purple idea furiously sleep colorless green\",\n",
        "        \"Clock river dance potato seven fast\",\n",
        "        \"Window eat loud tomorrow yellow under\",\n",
        "        \"Fish bicycle logic cloud mountain swim\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "CATEGORY_METADATA = {\n",
        "    \"Factual\": {\"expected_entropy\": \"medium\", \"complexity\": 1},\n",
        "    \"Syntactic\": {\"expected_entropy\": \"medium\", \"complexity\": 5},\n",
        "    \"Cliche\": {\"expected_entropy\": \"low\", \"complexity\": 2},\n",
        "    \"Novel\": {\"expected_entropy\": \"high\", \"complexity\": 4},\n",
        "    \"Nonsense\": {\"expected_entropy\": \"very_high\", \"complexity\": 3}\n",
        "}\n",
        "\n",
        "total_prompts = sum(len(v) for v in PROMPT_DATASET.values())\n",
        "print(f\"Total prompts: {total_prompts} (5 categories x 5 prompts)\")\n",
        "for cat, prompts in PROMPT_DATASET.items():\n",
        "    print(f\"  {cat}: {len(prompts)} prompts\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "models"
      },
      "source": [
        "# Cell 3: GPT-2 MODEL VARIANTS (All LayerNorm)\n",
        "\n",
        "MODELS_TO_TEST = {\n",
        "    \"GPT2-XL\": {\n",
        "        \"hf_path\": \"gpt2-xl\",\n",
        "        \"norm_type\": \"LayerNorm\",\n",
        "        \"params\": \"1.5B\",\n",
        "        \"expected_base\": \"< 1.0\",\n",
        "        \"role\": \"LayerNorm Validation (Large)\"\n",
        "    },\n",
        "    \"GPT2-Large\": {\n",
        "        \"hf_path\": \"gpt2-large\",\n",
        "        \"norm_type\": \"LayerNorm\",\n",
        "        \"params\": \"774M\",\n",
        "        \"expected_base\": \"< 1.0\",\n",
        "        \"role\": \"LayerNorm Validation (Medium)\"\n",
        "    },\n",
        "    \"GPT2-Medium\": {\n",
        "        \"hf_path\": \"gpt2-medium\",\n",
        "        \"norm_type\": \"LayerNorm\",\n",
        "        \"params\": \"355M\",\n",
        "        \"expected_base\": \"< 1.0\",\n",
        "        \"role\": \"LayerNorm Validation (Small)\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Reference values from Grand Unified Benchmark\n",
        "REFERENCE_MODELS = {\n",
        "    \"Pythia-6.9B\": {\"gain\": 0.80, \"norm\": \"LayerNorm\"},\n",
        "    \"Mistral-7B\": {\"gain\": 1.11, \"norm\": \"RMSNorm\"},\n",
        "    \"LLaMA-3.1-8B\": {\"gain\": 1.48, \"norm\": \"RMSNorm\"},\n",
        "    \"Gemma-7B\": {\"gain\": 2.31, \"norm\": \"RMSNorm\"}\n",
        "}\n",
        "\n",
        "print(\"GPT-2 Models to test (all LayerNorm):\")\n",
        "for name, info in MODELS_TO_TEST.items():\n",
        "    print(f\"  {name} ({info['params']}): {info['hf_path']}\")\n",
        "\n",
        "print(\"\\nReference values from Grand Unified Benchmark:\")\n",
        "for name, info in REFERENCE_MODELS.items():\n",
        "    print(f\"  {name}: Gain={info['gain']:.2f} ({info['norm']})\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "measurement_engine"
      },
      "source": [
        "# Cell 4: MEASUREMENT ENGINE (GPT-2 Compatible)\n",
        "\n",
        "def get_layer_list(model):\n",
        "    \"\"\"Get the transformer layers from different model architectures.\"\"\"\n",
        "    # GPT-2 style: model.transformer.h\n",
        "    if hasattr(model, 'transformer') and hasattr(model.transformer, 'h'):\n",
        "        return model.transformer.h\n",
        "    # LLaMA/Mistral/Gemma style: model.model.layers\n",
        "    elif hasattr(model, 'model') and hasattr(model.model, 'layers'):\n",
        "        return model.model.layers\n",
        "    # Pythia style: model.gpt_neox.layers\n",
        "    elif hasattr(model, 'gpt_neox') and hasattr(model.gpt_neox, 'layers'):\n",
        "        return model.gpt_neox.layers\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model architecture: {type(model)}\")\n",
        "\n",
        "def measure_thermodynamics(model, tokenizer, text, device='cuda'):\n",
        "    \"\"\"Measure residual stream gain and output entropy for a given input.\"\"\"\n",
        "    \n",
        "    # Tokenize - handle padding token for GPT-2\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    \n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    \n",
        "    # Hooks to capture residual stream norms\n",
        "    norms = []\n",
        "    \n",
        "    def get_norm_hook():\n",
        "        def hook(module, input, output):\n",
        "            # Handle different output formats\n",
        "            if isinstance(output, tuple):\n",
        "                hidden_state = output[0]\n",
        "            else:\n",
        "                hidden_state = output\n",
        "            \n",
        "            # Ensure we're working with a tensor\n",
        "            if hasattr(hidden_state, 'last_hidden_state'):\n",
        "                hidden_state = hidden_state.last_hidden_state\n",
        "            \n",
        "            # Norm of last token - convert to float explicitly\n",
        "            try:\n",
        "                last_token_norm = float(torch.norm(hidden_state[0, -1]).cpu().item())\n",
        "                norms.append(last_token_norm)\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Could not compute norm: {e}\")\n",
        "        return hook\n",
        "    \n",
        "    # Register hooks on all layers\n",
        "    handles = []\n",
        "    try:\n",
        "        layers = get_layer_list(model)\n",
        "        for layer in layers:\n",
        "            handles.append(layer.register_forward_hook(get_norm_hook()))\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not register hooks: {e}\")\n",
        "        return None\n",
        "    \n",
        "    # Forward pass\n",
        "    try:\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            logits = outputs.logits\n",
        "    except Exception as e:\n",
        "        print(f\"Error in forward pass: {e}\")\n",
        "        for h in handles:\n",
        "            h.remove()\n",
        "        return None\n",
        "    \n",
        "    # Cleanup hooks\n",
        "    for h in handles:\n",
        "        h.remove()\n",
        "    \n",
        "    # Calculate metrics - explicit float conversion to avoid numpy issues\n",
        "    if len(norms) >= 2:\n",
        "        # Last Layer Gain = Output of Last Layer / Output of Penultimate Layer\n",
        "        last_gain = float(norms[-1] / norms[-2]) if norms[-2] > 0 else 1.0\n",
        "        # Total Amplification = Final / Initial\n",
        "        total_amp = float(norms[-1] / norms[0]) if norms[0] > 0 else 1.0\n",
        "    else:\n",
        "        last_gain = 1.0\n",
        "        total_amp = 1.0\n",
        "    \n",
        "    # Output Entropy (next token distribution)\n",
        "    last_token_logits = logits[0, -1, :].float()  # Ensure float32\n",
        "    probs = torch.softmax(last_token_logits, dim=0).cpu().numpy().astype(np.float64)\n",
        "    \n",
        "    # Clip to avoid log(0) issues\n",
        "    probs = np.clip(probs, 1e-10, 1.0)\n",
        "    probs = probs / probs.sum()  # Renormalize\n",
        "    \n",
        "    ent = float(entropy(probs))\n",
        "    \n",
        "    # Top token and probability\n",
        "    top_idx = int(torch.argmax(last_token_logits).item())\n",
        "    top_prob = float(probs[top_idx])\n",
        "    top_token = tokenizer.decode([top_idx])\n",
        "    \n",
        "    return {\n",
        "        \"last_gain\": float(last_gain),\n",
        "        \"total_amp\": float(total_amp),\n",
        "        \"entropy\": float(ent),\n",
        "        \"top_token\": str(top_token),\n",
        "        \"top_prob\": float(top_prob),\n",
        "        \"n_layers\": int(len(norms)),\n",
        "        \"all_norms\": [float(n) for n in norms]\n",
        "    }\n",
        "\n",
        "print(\"Measurement engine ready (GPT-2 compatible).\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "execution"
      },
      "source": [
        "# Cell 5: EXECUTION LOOP\n",
        "\n",
        "all_results = []\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"STARTING GPT-2 LAYERNORM VALIDATION\")\n",
        "print(f\"Models: {len(MODELS_TO_TEST)} | Prompts: {total_prompts}\")\n",
        "print(f\"Total measurements: {len(MODELS_TO_TEST) * total_prompts}\")\n",
        "print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "for model_name, model_info in MODELS_TO_TEST.items():\n",
        "    hf_path = model_info[\"hf_path\"]\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Loading {model_name} ({model_info['params']})...\")\n",
        "    print(f\"  Path: {hf_path}\")\n",
        "    print(f\"  Norm: {model_info['norm_type']}\")\n",
        "    print(f\"  Expected Base: {model_info['expected_base']}\")\n",
        "    \n",
        "    try:\n",
        "        # Load tokenizer\n",
        "        tokenizer = GPT2Tokenizer.from_pretrained(hf_path)\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        \n",
        "        # Load model with explicit dtype\n",
        "        model = GPT2LMHeadModel.from_pretrained(\n",
        "            hf_path,\n",
        "            torch_dtype=torch.float32,  # GPT-2 works better with float32\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "        model.eval()\n",
        "        \n",
        "        print(f\"\\n  Testing {len(PROMPT_DATASET)} categories ({total_prompts} prompts)...\")\n",
        "        \n",
        "        for category, prompts in PROMPT_DATASET.items():\n",
        "            print(f\"    {category}: \", end=\"\")\n",
        "            for i, prompt in enumerate(prompts):\n",
        "                res = measure_thermodynamics(model, tokenizer, prompt)\n",
        "                \n",
        "                if res is not None:\n",
        "                    all_results.append({\n",
        "                        \"Model\": str(model_name),\n",
        "                        \"Norm_Type\": str(model_info[\"norm_type\"]),\n",
        "                        \"Params\": str(model_info[\"params\"]),\n",
        "                        \"Role\": str(model_info[\"role\"]),\n",
        "                        \"Category\": str(category),\n",
        "                        \"Complexity\": int(CATEGORY_METADATA[category][\"complexity\"]),\n",
        "                        \"Prompt\": str(prompt),\n",
        "                        \"Prompt_Short\": str(prompt[:40] + \"...\"),\n",
        "                        \"Entropy\": float(res[\"entropy\"]),\n",
        "                        \"Last_Gain\": float(res[\"last_gain\"]),\n",
        "                        \"Total_Amp\": float(res[\"total_amp\"]),\n",
        "                        \"Top_Token\": str(res[\"top_token\"]),\n",
        "                        \"Top_Prob\": float(res[\"top_prob\"]),\n",
        "                        \"N_Layers\": int(res[\"n_layers\"])\n",
        "                    })\n",
        "                    print(\".\", end=\"\", flush=True)\n",
        "                else:\n",
        "                    print(\"X\", end=\"\", flush=True)\n",
        "            print(f\" Done (n={len(prompts)})\")\n",
        "        \n",
        "        # Cleanup\n",
        "        del model\n",
        "        del tokenizer\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        print(f\"\\n  {model_name} complete. Memory cleared.\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"\\n  FAILED: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"BENCHMARK COMPLETE\")\n",
        "print(f\"Total measurements: {len(all_results)}\")\n",
        "print(\"=\" * 70)"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dataframe"
      },
      "source": [
        "# Cell 6: CREATE DATAFRAME & SUMMARY\n",
        "\n",
        "df = pd.DataFrame(all_results)\n",
        "\n",
        "# Define filenames with global timestamp\n",
        "CSV_FILE = f\"gpt2_layernorm_validation_{TIMESTAMP}.csv\"\n",
        "JSON_FILE = f\"gpt2_layernorm_validation_{TIMESTAMP}.json\"\n",
        "PNG_MAIN = f\"gpt2_layernorm_validation_{TIMESTAMP}.png\"\n",
        "PNG_COMPARISON = f\"gpt2_vs_reference_{TIMESTAMP}.png\"\n",
        "\n",
        "# Save raw results\n",
        "df.to_csv(CSV_FILE, index=False)\n",
        "print(f\"Saved: {CSV_FILE}\")\n",
        "\n",
        "# Display summary\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"GPT-2 RESULTS SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\nMean Gain per Model:\")\n",
        "print(df.groupby('Model')['Last_Gain'].agg(['mean', 'std', 'min', 'max']).round(3))\n",
        "\n",
        "print(\"\\nMean Gain per Category:\")\n",
        "print(df.groupby(['Model', 'Category'])['Last_Gain'].mean().unstack().round(3))"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "validation"
      },
      "source": [
        "# Cell 7: CRITICAL VALIDATION - LayerNorm Hypothesis\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"CRITICAL VALIDATION: LAYERNORM DAMPENING HYPOTHESIS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\nHypothesis: All LayerNorm models show Gain < 1.0 (dampening)\")\n",
        "print(\"\\nReference: Pythia-6.9B (LayerNorm) = 0.80\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Calculate results\n",
        "validation_results = []\n",
        "\n",
        "for model in df['Model'].unique():\n",
        "    subset = df[df['Model'] == model]\n",
        "    mean_gain = float(subset['Last_Gain'].mean())\n",
        "    std_gain = float(subset['Last_Gain'].std())\n",
        "    min_gain = float(subset['Last_Gain'].min())\n",
        "    max_gain = float(subset['Last_Gain'].max())\n",
        "    \n",
        "    # Hypothesis test: is mean significantly < 1.0?\n",
        "    from scipy.stats import ttest_1samp\n",
        "    t_stat, p_val = ttest_1samp(subset['Last_Gain'], 1.0)\n",
        "    \n",
        "    # One-sided p-value (we expect < 1.0)\n",
        "    p_one_sided = float(p_val / 2) if t_stat < 0 else float(1 - p_val / 2)\n",
        "    \n",
        "    is_dampening = mean_gain < 1.0\n",
        "    is_significant = p_one_sided < 0.05 and is_dampening\n",
        "    \n",
        "    status = \"CONFIRMED\" if is_significant else \"NOT SIGNIFICANT\" if is_dampening else \"REJECTED\"\n",
        "    \n",
        "    validation_results.append({\n",
        "        \"Model\": model,\n",
        "        \"Mean_Gain\": mean_gain,\n",
        "        \"Std\": std_gain,\n",
        "        \"Min\": min_gain,\n",
        "        \"Max\": max_gain,\n",
        "        \"t_stat\": float(t_stat),\n",
        "        \"p_value\": p_one_sided,\n",
        "        \"Is_Dampening\": bool(is_dampening),\n",
        "        \"Status\": status\n",
        "    })\n",
        "    \n",
        "    sig_marker = \"***\" if p_one_sided < 0.001 else \"**\" if p_one_sided < 0.01 else \"*\" if p_one_sided < 0.05 else \"\"\n",
        "    \n",
        "    print(f\"\\n{model}:\")\n",
        "    print(f\"  Mean Gain: {mean_gain:.3f} +/- {std_gain:.3f}\")\n",
        "    print(f\"  Range: [{min_gain:.3f}, {max_gain:.3f}]\")\n",
        "    print(f\"  t-test vs 1.0: t={t_stat:.3f}, p={p_one_sided:.4f} {sig_marker}\")\n",
        "    print(f\"  Status: {status}\")\n",
        "\n",
        "# Overall verdict\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"OVERALL VERDICT\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "all_dampening = all(r[\"Is_Dampening\"] for r in validation_results)\n",
        "all_significant = all(r[\"Status\"] == \"CONFIRMED\" for r in validation_results)\n",
        "\n",
        "if all_significant:\n",
        "    print(\"\\n  LAYERNORM DAMPENING HYPOTHESIS: STRONGLY CONFIRMED\")\n",
        "    print(\"  All GPT-2 variants show statistically significant Gain < 1.0\")\n",
        "elif all_dampening:\n",
        "    print(\"\\n  LAYERNORM DAMPENING HYPOTHESIS: CONFIRMED (directionally)\")\n",
        "    print(\"  All GPT-2 variants show Gain < 1.0 (not all statistically significant)\")\n",
        "else:\n",
        "    print(\"\\n  LAYERNORM DAMPENING HYPOTHESIS: INCONCLUSIVE\")\n",
        "    print(\"  Some GPT-2 variants do not show clear dampening\")\n",
        "\n",
        "# Comparison with Pythia\n",
        "gpt2_xl_gain = df[df['Model'] == 'GPT2-XL']['Last_Gain'].mean() if 'GPT2-XL' in df['Model'].values else None\n",
        "pythia_gain = 0.80  # Reference value\n",
        "\n",
        "if gpt2_xl_gain is not None:\n",
        "    print(f\"\\n  GPT2-XL vs Pythia-6.9B:\")\n",
        "    print(f\"    GPT2-XL:    {gpt2_xl_gain:.3f}\")\n",
        "    print(f\"    Pythia-6.9B: {pythia_gain:.3f}\")\n",
        "    print(f\"    Difference: {abs(gpt2_xl_gain - pythia_gain):.3f}\")\n",
        "    \n",
        "    if abs(gpt2_xl_gain - pythia_gain) < 0.15:\n",
        "        print(f\"    --> CONSISTENT (both in dampening regime)\")\n",
        "    else:\n",
        "        print(f\"    --> DIVERGENT (different dampening magnitude)\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "visualization_main"
      },
      "source": [
        "# Cell 8: VISUALIZATION - Main Results\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
        "\n",
        "# A. Gain Distribution per GPT-2 Model\n",
        "ax1 = axes[0, 0]\n",
        "for model in df['Model'].unique():\n",
        "    subset = df[df['Model'] == model]\n",
        "    ax1.hist(subset['Last_Gain'], bins=15, alpha=0.6, label=model)\n",
        "ax1.axvline(1.0, ls='--', c='red', lw=2, label='Neutral (1.0)')\n",
        "ax1.axvline(0.80, ls=':', c='blue', lw=2, label='Pythia Ref (0.80)')\n",
        "ax1.set_xlabel('Last Layer Gain')\n",
        "ax1.set_ylabel('Count')\n",
        "ax1.set_title('A. Gain Distribution (GPT-2 LayerNorm Models)')\n",
        "ax1.legend(loc='best')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# B. Scatter: Entropy vs Gain\n",
        "ax2 = axes[0, 1]\n",
        "for model in df['Model'].unique():\n",
        "    subset = df[df['Model'] == model]\n",
        "    ax2.scatter(subset['Entropy'], subset['Last_Gain'], label=model, alpha=0.7, s=80)\n",
        "ax2.axhline(1.0, ls='--', c='red', alpha=0.5, label='Neutral (1.0)')\n",
        "ax2.set_xlabel('Output Entropy (nats)')\n",
        "ax2.set_ylabel('Last Layer Gain')\n",
        "ax2.set_title('B. Entropy vs Gain (GPT-2 Models)')\n",
        "ax2.legend(loc='best')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# C. Box: Gain by Category\n",
        "ax3 = axes[1, 0]\n",
        "category_order = ['Factual', 'Cliche', 'Nonsense', 'Novel', 'Syntactic']\n",
        "sns.boxplot(data=df, x='Category', y='Last_Gain', hue='Model', ax=ax3, order=category_order)\n",
        "ax3.axhline(1.0, ls='--', c='red', alpha=0.5)\n",
        "ax3.axhline(0.80, ls=':', c='blue', alpha=0.5)\n",
        "ax3.set_xlabel('Prompt Category')\n",
        "ax3.set_ylabel('Last Layer Gain')\n",
        "ax3.set_title('C. Gain by Category (GPT-2)')\n",
        "ax3.legend(loc='upper right', fontsize=8)\n",
        "ax3.tick_params(axis='x', rotation=45)\n",
        "\n",
        "# D. Base Level Bar Chart with Reference\n",
        "ax4 = axes[1, 1]\n",
        "\n",
        "# GPT-2 models\n",
        "gpt2_means = df.groupby('Model')['Last_Gain'].mean().sort_values()\n",
        "gpt2_stds = df.groupby('Model')['Last_Gain'].std()\n",
        "\n",
        "# Combined data for plotting\n",
        "all_models = list(gpt2_means.index) + list(REFERENCE_MODELS.keys())\n",
        "all_means = list(gpt2_means.values) + [v['gain'] for v in REFERENCE_MODELS.values()]\n",
        "all_norms = ['LayerNorm'] * len(gpt2_means) + [v['norm'] for v in REFERENCE_MODELS.values()]\n",
        "\n",
        "# Colors: Blue for LayerNorm, Orange for RMSNorm\n",
        "colors = ['#1f77b4' if n == 'LayerNorm' else '#ff7f0e' for n in all_norms]\n",
        "\n",
        "bars = ax4.bar(range(len(all_models)), all_means, color=colors, alpha=0.7)\n",
        "ax4.axhline(1.0, ls='--', c='red', alpha=0.5, label='Neutral')\n",
        "ax4.set_xticks(range(len(all_models)))\n",
        "ax4.set_xticklabels(all_models, rotation=45, ha='right')\n",
        "ax4.set_xlabel('Model')\n",
        "ax4.set_ylabel('Mean Last Layer Gain')\n",
        "ax4.set_title('D. GPT-2 vs Reference Models\\n(Blue=LayerNorm, Orange=RMSNorm)')\n",
        "\n",
        "# Add value labels\n",
        "for bar, val in zip(bars, all_means):\n",
        "    ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
        "             f'{val:.2f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(PNG_MAIN, dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nFigure saved: {PNG_MAIN}\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "visualization_comparison"
      },
      "source": [
        "# Cell 9: VISUALIZATION - LayerNorm vs RMSNorm Comparison\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
        "\n",
        "# Prepare data\n",
        "layernorm_models = []\n",
        "rmsnorm_models = []\n",
        "\n",
        "# GPT-2 models (all LayerNorm)\n",
        "for model in df['Model'].unique():\n",
        "    gain = float(df[df['Model'] == model]['Last_Gain'].mean())\n",
        "    layernorm_models.append((model, gain, 'GPT-2'))\n",
        "\n",
        "# Pythia (LayerNorm)\n",
        "layernorm_models.append(('Pythia-6.9B', 0.80, 'Reference'))\n",
        "\n",
        "# RMSNorm reference models\n",
        "rmsnorm_models = [\n",
        "    ('Mistral-7B', 1.11, 'Reference'),\n",
        "    ('LLaMA-3.1-8B', 1.48, 'Reference'),\n",
        "    ('Gemma-7B', 2.31, 'Reference')\n",
        "]\n",
        "\n",
        "# Plot LayerNorm models\n",
        "ln_names = [m[0] for m in layernorm_models]\n",
        "ln_gains = [m[1] for m in layernorm_models]\n",
        "ln_colors = ['#1f77b4' if m[2] == 'GPT-2' else '#17becf' for m in layernorm_models]\n",
        "\n",
        "# Plot RMSNorm models\n",
        "rms_names = [m[0] for m in rmsnorm_models]\n",
        "rms_gains = [m[1] for m in rmsnorm_models]\n",
        "\n",
        "# Combined plot\n",
        "all_names = ln_names + rms_names\n",
        "all_gains = ln_gains + rms_gains\n",
        "all_colors = ln_colors + ['#ff7f0e'] * len(rms_names)\n",
        "\n",
        "# Sort by gain\n",
        "sorted_data = sorted(zip(all_names, all_gains, all_colors), key=lambda x: x[1])\n",
        "all_names = [d[0] for d in sorted_data]\n",
        "all_gains = [d[1] for d in sorted_data]\n",
        "all_colors = [d[2] for d in sorted_data]\n",
        "\n",
        "bars = ax.barh(range(len(all_names)), all_gains, color=all_colors, alpha=0.8)\n",
        "ax.axvline(1.0, ls='--', c='red', lw=2, label='Neutral (1.0)')\n",
        "ax.set_yticks(range(len(all_names)))\n",
        "ax.set_yticklabels(all_names)\n",
        "ax.set_xlabel('Mean Last Layer Gain')\n",
        "ax.set_title('LayerNorm (Blue) vs RMSNorm (Orange)\\nThe Dampening Hypothesis', fontsize=14)\n",
        "\n",
        "# Add value labels\n",
        "for bar, val in zip(bars, all_gains):\n",
        "    ax.text(val + 0.03, bar.get_y() + bar.get_height()/2, \n",
        "            f'{val:.2f}', ha='left', va='center', fontsize=11, fontweight='bold')\n",
        "\n",
        "# Add annotations\n",
        "ax.annotate('DAMPENING\\n(Gain < 1.0)', xy=(0.5, 0.3), xycoords='axes fraction',\n",
        "            fontsize=12, ha='center', color='blue', alpha=0.7)\n",
        "ax.annotate('EXPANSION\\n(Gain > 1.0)', xy=(0.8, 0.7), xycoords='axes fraction',\n",
        "            fontsize=12, ha='center', color='orange', alpha=0.7)\n",
        "\n",
        "# Legend\n",
        "from matplotlib.patches import Patch\n",
        "legend_elements = [\n",
        "    Patch(facecolor='#1f77b4', label='GPT-2 (LayerNorm) - This Test'),\n",
        "    Patch(facecolor='#17becf', label='Pythia (LayerNorm) - Reference'),\n",
        "    Patch(facecolor='#ff7f0e', label='RMSNorm Models - Reference')\n",
        "]\n",
        "ax.legend(handles=legend_elements, loc='lower right')\n",
        "\n",
        "ax.grid(True, alpha=0.3, axis='x')\n",
        "ax.set_xlim(0, max(all_gains) * 1.15)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(PNG_COMPARISON, dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nFigure saved: {PNG_COMPARISON}\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "entropy_correlation"
      },
      "source": [
        "# Cell 10: ENTROPY CORRELATION ANALYSIS (Bentov Law Test)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"BENTOV LAW TEST: |Gain - 1.0| vs Entropy\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\nHypothesis: Pythia showed NEGATIVE correlation (r = -0.20)\")\n",
        "print(\"If GPT-2 also shows negative correlation, LayerNorm inverts the physics.\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "bentov_results = []\n",
        "\n",
        "for model in df['Model'].unique():\n",
        "    subset = df[df['Model'] == model].copy()\n",
        "    \n",
        "    # Calculate Bentov Deviation\n",
        "    subset['Bentov_Deviation'] = abs(subset['Last_Gain'] - 1.0)\n",
        "    \n",
        "    # Correlation\n",
        "    r, p = pearsonr(subset['Entropy'], subset['Bentov_Deviation'])\n",
        "    \n",
        "    sig = \"***\" if p < 0.001 else \"**\" if p < 0.01 else \"*\" if p < 0.05 else \"\"\n",
        "    \n",
        "    bentov_results.append({\n",
        "        \"Model\": model,\n",
        "        \"Bentov_Correlation\": float(r),\n",
        "        \"p_value\": float(p),\n",
        "        \"Direction\": \"Positive\" if r > 0 else \"Negative\"\n",
        "    })\n",
        "    \n",
        "    print(f\"\\n{model}:\")\n",
        "    print(f\"  Correlation (Entropy vs |Gain-1|): r = {r:+.3f} {sig}\")\n",
        "    print(f\"  p-value: {p:.4f}\")\n",
        "    print(f\"  Direction: {'POSITIVE (RMSNorm-like)' if r > 0 else 'NEGATIVE (LayerNorm-like)'}\")\n",
        "\n",
        "# Compare with Pythia reference\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(\"Comparison with Reference:\")\n",
        "print(f\"  Pythia-6.9B (LayerNorm): r = -0.199 (NEGATIVE)\")\n",
        "print(f\"  Gemma-7B (RMSNorm):      r = +0.692 (POSITIVE)\")\n",
        "print(\"\\nGPT-2 Results:\")\n",
        "for br in bentov_results:\n",
        "    print(f\"  {br['Model']}: r = {br['Bentov_Correlation']:+.3f} ({br['Direction']})\")\n",
        "\n",
        "# Verdict\n",
        "gpt2_negative = sum(1 for br in bentov_results if br['Bentov_Correlation'] < 0)\n",
        "if gpt2_negative == len(bentov_results):\n",
        "    print(\"\\n  --> ALL GPT-2 models show NEGATIVE correlation (like Pythia)\")\n",
        "    print(\"  --> LAYERNORM INVERTS THE BENTOV LAW: CONFIRMED\")\n",
        "elif gpt2_negative > 0:\n",
        "    print(f\"\\n  --> {gpt2_negative}/{len(bentov_results)} GPT-2 models show negative correlation\")\n",
        "    print(\"  --> PARTIAL CONFIRMATION of LayerNorm inversion\")\n",
        "else:\n",
        "    print(\"\\n  --> GPT-2 models show POSITIVE correlation (unlike Pythia)\")\n",
        "    print(\"  --> LayerNorm inversion NOT confirmed for GPT-2\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save_json"
      },
      "source": [
        "# Cell 11: SAVE ALL RESULTS AS JSON\n",
        "\n",
        "# Helper function to ensure JSON serializable\n",
        "def make_serializable(obj):\n",
        "    if isinstance(obj, (np.integer, np.floating)):\n",
        "        return float(obj)\n",
        "    elif isinstance(obj, np.ndarray):\n",
        "        return obj.tolist()\n",
        "    elif isinstance(obj, np.bool_):\n",
        "        return bool(obj)\n",
        "    elif isinstance(obj, dict):\n",
        "        return {k: make_serializable(v) for k, v in obj.items()}\n",
        "    elif isinstance(obj, list):\n",
        "        return [make_serializable(v) for v in obj]\n",
        "    return obj\n",
        "\n",
        "results_json = {\n",
        "    \"experiment\": \"GPT-2 LayerNorm Validation\",\n",
        "    \"purpose\": \"Confirm LayerNorm dampening hypothesis (n=2)\",\n",
        "    \"date\": TIMESTAMP,\n",
        "    \"n_models\": len(MODELS_TO_TEST),\n",
        "    \"n_prompts\": total_prompts,\n",
        "    \"n_measurements\": len(df),\n",
        "    \"models_tested\": list(MODELS_TO_TEST.keys()),\n",
        "    \"reference_models\": REFERENCE_MODELS,\n",
        "    \"base_levels\": {k: float(v) for k, v in df.groupby('Model')['Last_Gain'].mean().to_dict().items()},\n",
        "    \"validation_results\": make_serializable(validation_results),\n",
        "    \"bentov_law_results\": make_serializable(bentov_results),\n",
        "    \"hypothesis_status\": {\n",
        "        \"layernorm_dampening\": \"CONFIRMED\" if all(r[\"Is_Dampening\"] for r in validation_results) else \"REJECTED\",\n",
        "        \"bentov_inversion\": \"CONFIRMED\" if all(br['Bentov_Correlation'] < 0 for br in bentov_results) else \"PARTIAL\" if any(br['Bentov_Correlation'] < 0 for br in bentov_results) else \"REJECTED\"\n",
        "    },\n",
        "    \"all_results\": make_serializable(all_results)\n",
        "}\n",
        "\n",
        "with open(JSON_FILE, 'w') as f:\n",
        "    json.dump(results_json, f, indent=2, default=str)\n",
        "\n",
        "print(f\"Results saved to {JSON_FILE}\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "final_verdict"
      },
      "source": [
        "# Cell 12: FINAL VERDICT FOR PAPER #3\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"FINAL VERDICT: LAYERNORM HYPOTHESIS VALIDATION\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Calculate key metrics\n",
        "gpt2_models = list(df['Model'].unique())\n",
        "gpt2_gains = {m: float(df[df['Model'] == m]['Last_Gain'].mean()) for m in gpt2_models}\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"LAYERNORM MODELS (n=4 including Pythia reference)\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\n  Pythia-6.9B:   {0.80:.2f}  (Reference)\")\n",
        "for model, gain in sorted(gpt2_gains.items(), key=lambda x: x[1]):\n",
        "    print(f\"  {model}:   {gain:.2f}  (This Test)\")\n",
        "\n",
        "all_layernorm_gains = [0.80] + list(gpt2_gains.values())\n",
        "mean_layernorm = np.mean(all_layernorm_gains)\n",
        "std_layernorm = np.std(all_layernorm_gains)\n",
        "\n",
        "print(f\"\\n  LayerNorm Mean: {mean_layernorm:.3f} +/- {std_layernorm:.3f}\")\n",
        "print(f\"  All < 1.0: {all(g < 1.0 for g in all_layernorm_gains)}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"RMSNORM MODELS (Reference from Grand Unified Benchmark)\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\n  Mistral-7B:    {1.11:.2f}\")\n",
        "print(f\"  LLaMA-3.1-8B:  {1.48:.2f}\")\n",
        "print(f\"  Gemma-7B:      {2.31:.2f}\")\n",
        "\n",
        "rmsnorm_gains = [1.11, 1.48, 2.31]\n",
        "mean_rmsnorm = np.mean(rmsnorm_gains)\n",
        "print(f\"\\n  RMSNorm Mean: {mean_rmsnorm:.3f}\")\n",
        "print(f\"  All > 1.0: {all(g > 1.0 for g in rmsnorm_gains)}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CONCLUSION FOR PAPER #3\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "if all(g < 1.0 for g in all_layernorm_gains) and all(g > 1.0 for g in rmsnorm_gains):\n",
        "    print(\"\"\"\n",
        "  ┌─────────────────────────────────────────────────────────────┐\n",
        "  │  LAYERNORM VS RMSNORM DICHOTOMY: STRONGLY CONFIRMED        │\n",
        "  ├─────────────────────────────────────────────────────────────┤\n",
        "  │                                                             │\n",
        "  │  LayerNorm (n=4): ALL show Gain < 1.0 (Dampening)          │\n",
        "  │  RMSNorm (n=3):   ALL show Gain > 1.0 (Expansion)          │\n",
        "  │                                                             │\n",
        "  │  This is NOT a Pythia-specific artifact.                   │\n",
        "  │  This is a FUNDAMENTAL PROPERTY of the normalization.      │\n",
        "  │                                                             │\n",
        "  │  Paper #3 Claim: VALIDATED (n=2 for LayerNorm)             │\n",
        "  │                                                             │\n",
        "  └─────────────────────────────────────────────────────────────┘\n",
        "\"\"\")\n",
        "else:\n",
        "    print(\"\\n  Results are mixed. See detailed analysis above.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download"
      },
      "source": [
        "# Cell 13: DOWNLOAD ALL RESULTS\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"DOWNLOADING RESULTS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# List all files to download\n",
        "files_to_download = [CSV_FILE, JSON_FILE, PNG_MAIN, PNG_COMPARISON]\n",
        "\n",
        "print(\"\\nFiles to download:\")\n",
        "for f in files_to_download:\n",
        "    if os.path.exists(f):\n",
        "        size = os.path.getsize(f) / 1024  # KB\n",
        "        print(f\"  {f} ({size:.1f} KB)\")\n",
        "    else:\n",
        "        print(f\"  {f} (NOT FOUND)\")\n",
        "\n",
        "print(\"\\nStarting downloads...\")\n",
        "\n",
        "# Download each file\n",
        "for f in files_to_download:\n",
        "    if os.path.exists(f):\n",
        "        try:\n",
        "            files.download(f)\n",
        "            print(f\"  Downloaded: {f}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  FAILED to download {f}: {e}\")\n",
        "    else:\n",
        "        print(f\"  SKIPPED (not found): {f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"ALL DOWNLOADS COMPLETE\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\nNext step: Add these results to Paper #3 documentation.\")"
      ],
      "outputs": []
    }
  ]
}
