{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FFN Expansion Analysis\n",
        "\n",
        "**Paper #3 Hypothesis Test**\n",
        "\n",
        "## Motivation\n",
        "\n",
        "The Restriction Maps experiment showed:\n",
        "- **Attention contracts**: All contraction ratios < 1\n",
        "- **But W_V explodes**: ||W_V|| grows from 5 to 28 in late layers\n",
        "\n",
        "**New Hypothesis:**\n",
        "> Attention compresses information (sheaf diffusion onto H⁰).\n",
        "> FFN expands information (for prediction/logit separation).\n",
        "\n",
        "## What We Measure\n",
        "\n",
        "1. **FFN Gain**: ||FFN(x)|| / ||x|| per layer\n",
        "2. **Attention Gain**: ||Attn(x)|| / ||x|| per layer\n",
        "3. **Residual Contribution**: How much does each component change the hidden state?\n",
        "\n",
        "**Prediction:**\n",
        "- Attention Gain < 1 (contraction) for l < L*\n",
        "- FFN Gain > 1 (expansion) for l > L*\n",
        "\n",
        "**Author:** Davide D'Elia  \n",
        "**Date:** 2026-01-05"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q transformers accelerate matplotlib seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from tqdm.auto import tqdm\n",
        "from collections import defaultdict\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_palette('husl')\n",
        "\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model configuration\n",
        "MODEL_NAME = \"EleutherAI/pythia-1.4b\"\n",
        "# MODEL_NAME = \"EleutherAI/pythia-6.9b\"  # Larger model\n",
        "\n",
        "print(f\"Loading {MODEL_NAME}...\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "n_layers = model.config.num_hidden_layers\n",
        "hidden_dim = model.config.hidden_size\n",
        "\n",
        "print(f\"\\nModel Configuration:\")\n",
        "print(f\"  Layers: {n_layers}\")\n",
        "print(f\"  Hidden Dim: {hidden_dim}\")\n",
        "print(f\"  Intermediate Dim: {model.config.intermediate_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Setup Hooks for FFN and Attention\n",
        "\n",
        "We'll use PyTorch hooks to capture:\n",
        "- Input to Attention\n",
        "- Output of Attention\n",
        "- Input to FFN (MLP)\n",
        "- Output of FFN (MLP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ActivationCapture:\n",
        "    \"\"\"\n",
        "    Captures activations at specific points in the model.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.activations = defaultdict(dict)\n",
        "        self.hooks = []\n",
        "    \n",
        "    def clear(self):\n",
        "        self.activations = defaultdict(dict)\n",
        "    \n",
        "    def remove_hooks(self):\n",
        "        for hook in self.hooks:\n",
        "            hook.remove()\n",
        "        self.hooks = []\n",
        "    \n",
        "    def register_hooks(self, model):\n",
        "        \"\"\"Register hooks on attention and MLP modules.\"\"\"\n",
        "        self.remove_hooks()\n",
        "        \n",
        "        for layer_idx in range(model.config.num_hidden_layers):\n",
        "            layer = model.gpt_neox.layers[layer_idx]\n",
        "            \n",
        "            # Hook for attention input (post layer norm)\n",
        "            def make_attn_input_hook(idx):\n",
        "                def hook(module, input, output):\n",
        "                    # input_layernorm output = attention input\n",
        "                    self.activations[idx]['attn_input'] = output.detach()\n",
        "                return hook\n",
        "            \n",
        "            # Hook for attention output\n",
        "            def make_attn_output_hook(idx):\n",
        "                def hook(module, input, output):\n",
        "                    # output[0] is the attention output\n",
        "                    self.activations[idx]['attn_output'] = output[0].detach()\n",
        "                return hook\n",
        "            \n",
        "            # Hook for MLP input (post layer norm)\n",
        "            def make_mlp_input_hook(idx):\n",
        "                def hook(module, input, output):\n",
        "                    self.activations[idx]['mlp_input'] = output.detach()\n",
        "                return hook\n",
        "            \n",
        "            # Hook for MLP output\n",
        "            def make_mlp_output_hook(idx):\n",
        "                def hook(module, input, output):\n",
        "                    self.activations[idx]['mlp_output'] = output.detach()\n",
        "                return hook\n",
        "            \n",
        "            # Register hooks\n",
        "            h1 = layer.input_layernorm.register_forward_hook(make_attn_input_hook(layer_idx))\n",
        "            h2 = layer.attention.register_forward_hook(make_attn_output_hook(layer_idx))\n",
        "            h3 = layer.post_attention_layernorm.register_forward_hook(make_mlp_input_hook(layer_idx))\n",
        "            h4 = layer.mlp.register_forward_hook(make_mlp_output_hook(layer_idx))\n",
        "            \n",
        "            self.hooks.extend([h1, h2, h3, h4])\n",
        "        \n",
        "        print(f\"Registered {len(self.hooks)} hooks on {model.config.num_hidden_layers} layers\")\n",
        "\n",
        "# Create capture object and register hooks\n",
        "capture = ActivationCapture()\n",
        "capture.register_hooks(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Run Forward Pass and Capture Activations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test prompts (same as restriction maps for comparison)\n",
        "TEST_PROMPTS = [\n",
        "    \"The capital of France is Paris, which is known for the Eiffel Tower.\",\n",
        "    \"Machine learning models learn patterns from data by optimizing loss functions.\",\n",
        "    \"The transformer architecture uses self-attention to process sequences.\",\n",
        "    \"Once upon a time in a faraway kingdom, there lived a wise old wizard.\",\n",
        "    \"Water boils at 100 degrees Celsius under standard atmospheric pressure.\",\n",
        "    \"If all mammals are warm-blooded and whales are mammals, then whales are warm-blooded.\",\n",
        "    \"Functional programming emphasizes immutability and pure functions without side effects.\",\n",
        "    \"The speed of light in a vacuum is approximately 299,792,458 meters per second.\",\n",
        "]\n",
        "\n",
        "print(f\"Using {len(TEST_PROMPTS)} test prompts\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_gains(capture, n_layers):\n",
        "    \"\"\"\n",
        "    Compute gain metrics from captured activations.\n",
        "    \n",
        "    Gain = ||output|| / ||input||\n",
        "    \"\"\"\n",
        "    gains = {\n",
        "        'attn_gain': [],      # ||attn_out|| / ||attn_in||\n",
        "        'mlp_gain': [],       # ||mlp_out|| / ||mlp_in||\n",
        "        'attn_norm_in': [],   # ||attn_in||\n",
        "        'attn_norm_out': [],  # ||attn_out||\n",
        "        'mlp_norm_in': [],    # ||mlp_in||\n",
        "        'mlp_norm_out': [],   # ||mlp_out||\n",
        "    }\n",
        "    \n",
        "    for layer_idx in range(n_layers):\n",
        "        acts = capture.activations[layer_idx]\n",
        "        \n",
        "        # Attention gain\n",
        "        if 'attn_input' in acts and 'attn_output' in acts:\n",
        "            attn_in = acts['attn_input'].float()\n",
        "            attn_out = acts['attn_output'].float()\n",
        "            \n",
        "            # Compute mean norm over all tokens\n",
        "            norm_in = torch.norm(attn_in, dim=-1).mean().item()\n",
        "            norm_out = torch.norm(attn_out, dim=-1).mean().item()\n",
        "            \n",
        "            gains['attn_norm_in'].append(norm_in)\n",
        "            gains['attn_norm_out'].append(norm_out)\n",
        "            gains['attn_gain'].append(norm_out / (norm_in + 1e-10))\n",
        "        else:\n",
        "            gains['attn_norm_in'].append(0)\n",
        "            gains['attn_norm_out'].append(0)\n",
        "            gains['attn_gain'].append(0)\n",
        "        \n",
        "        # MLP gain\n",
        "        if 'mlp_input' in acts and 'mlp_output' in acts:\n",
        "            mlp_in = acts['mlp_input'].float()\n",
        "            mlp_out = acts['mlp_output'].float()\n",
        "            \n",
        "            norm_in = torch.norm(mlp_in, dim=-1).mean().item()\n",
        "            norm_out = torch.norm(mlp_out, dim=-1).mean().item()\n",
        "            \n",
        "            gains['mlp_norm_in'].append(norm_in)\n",
        "            gains['mlp_norm_out'].append(norm_out)\n",
        "            gains['mlp_gain'].append(norm_out / (norm_in + 1e-10))\n",
        "        else:\n",
        "            gains['mlp_norm_in'].append(0)\n",
        "            gains['mlp_norm_out'].append(0)\n",
        "            gains['mlp_gain'].append(0)\n",
        "    \n",
        "    return gains\n",
        "\n",
        "# Collect gains over all prompts\n",
        "all_gains = []\n",
        "\n",
        "print(\"Processing prompts...\")\n",
        "for prompt in tqdm(TEST_PROMPTS):\n",
        "    capture.clear()\n",
        "    \n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        _ = model(**inputs)\n",
        "    \n",
        "    gains = compute_gains(capture, n_layers)\n",
        "    all_gains.append(gains)\n",
        "\n",
        "print(\"\\nDone!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Average gains across all prompts\n",
        "avg_gains = {\n",
        "    'attn_gain': np.mean([g['attn_gain'] for g in all_gains], axis=0),\n",
        "    'mlp_gain': np.mean([g['mlp_gain'] for g in all_gains], axis=0),\n",
        "    'attn_norm_in': np.mean([g['attn_norm_in'] for g in all_gains], axis=0),\n",
        "    'attn_norm_out': np.mean([g['attn_norm_out'] for g in all_gains], axis=0),\n",
        "    'mlp_norm_in': np.mean([g['mlp_norm_in'] for g in all_gains], axis=0),\n",
        "    'mlp_norm_out': np.mean([g['mlp_norm_out'] for g in all_gains], axis=0),\n",
        "}\n",
        "\n",
        "layers = list(range(n_layers))\n",
        "\n",
        "print(\"Average Gains per Layer:\")\n",
        "print(\"\\nLayer | Attn Gain | MLP Gain | Attn > 1? | MLP > 1?\")\n",
        "print(\"-\" * 55)\n",
        "for l in layers:\n",
        "    attn_g = avg_gains['attn_gain'][l]\n",
        "    mlp_g = avg_gains['mlp_gain'][l]\n",
        "    print(f\"  {l:2d}  |   {attn_g:.4f}  |  {mlp_g:.4f}  |    {'YES' if attn_g > 1 else 'no '}    |   {'YES' if mlp_g > 1 else 'no '}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Visualize Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find transition points\n",
        "attn_gains = avg_gains['attn_gain']\n",
        "mlp_gains = avg_gains['mlp_gain']\n",
        "\n",
        "# L* for attention (minimum gain = maximum contraction)\n",
        "L_star_attn = np.argmin(attn_gains)\n",
        "\n",
        "# L* for MLP (where gain crosses 1, or maximum)\n",
        "L_star_mlp = np.argmax(mlp_gains)\n",
        "\n",
        "print(f\"Transition Points:\")\n",
        "print(f\"  Attention min gain: Layer {L_star_attn} (gain = {attn_gains[L_star_attn]:.4f})\")\n",
        "print(f\"  MLP max gain: Layer {L_star_mlp} (gain = {mlp_gains[L_star_mlp]:.4f})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Main visualization\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Plot 1: Attention vs MLP Gain\n",
        "ax1 = axes[0, 0]\n",
        "ax1.plot(layers, attn_gains, 'b-', linewidth=2, marker='o', markersize=4, label='Attention Gain')\n",
        "ax1.plot(layers, mlp_gains, 'r-', linewidth=2, marker='s', markersize=4, label='MLP/FFN Gain')\n",
        "ax1.axhline(y=1.0, color='gray', linestyle='--', linewidth=2, label='Neutral (gain=1)')\n",
        "ax1.axvline(x=L_star_attn, color='blue', linestyle=':', linewidth=1.5, alpha=0.7)\n",
        "ax1.axvline(x=L_star_mlp, color='red', linestyle=':', linewidth=1.5, alpha=0.7)\n",
        "ax1.fill_between(layers, attn_gains, 1, where=[g < 1 for g in attn_gains], \n",
        "                  alpha=0.2, color='blue', label='Attn Contraction')\n",
        "ax1.fill_between(layers, mlp_gains, 1, where=[g > 1 for g in mlp_gains], \n",
        "                  alpha=0.2, color='red', label='MLP Expansion')\n",
        "ax1.set_xlabel('Layer', fontsize=12)\n",
        "ax1.set_ylabel('Gain (||output|| / ||input||)', fontsize=12)\n",
        "ax1.set_title('Attention vs MLP Gain per Layer', fontsize=14)\n",
        "ax1.legend(fontsize=9, loc='best')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Norms\n",
        "ax2 = axes[0, 1]\n",
        "ax2.plot(layers, avg_gains['attn_norm_out'], 'b-', linewidth=2, marker='o', markersize=4, label='Attn Output Norm')\n",
        "ax2.plot(layers, avg_gains['mlp_norm_out'], 'r-', linewidth=2, marker='s', markersize=4, label='MLP Output Norm')\n",
        "ax2.set_xlabel('Layer', fontsize=12)\n",
        "ax2.set_ylabel('Mean Norm', fontsize=12)\n",
        "ax2.set_title('Output Norms per Layer', fontsize=14)\n",
        "ax2.legend(fontsize=10)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: Ratio of gains (MLP/Attn)\n",
        "ax3 = axes[1, 0]\n",
        "gain_ratio = np.array(mlp_gains) / (np.array(attn_gains) + 1e-10)\n",
        "ax3.plot(layers, gain_ratio, 'g-', linewidth=2, marker='^', markersize=4)\n",
        "ax3.axhline(y=1.0, color='gray', linestyle='--', linewidth=2, label='Equal gains')\n",
        "ax3.fill_between(layers, gain_ratio, 1, where=[r > 1 for r in gain_ratio], \n",
        "                  alpha=0.3, color='red', label='MLP dominates')\n",
        "ax3.fill_between(layers, gain_ratio, 1, where=[r < 1 for r in gain_ratio], \n",
        "                  alpha=0.3, color='blue', label='Attn dominates')\n",
        "ax3.set_xlabel('Layer', fontsize=12)\n",
        "ax3.set_ylabel('MLP Gain / Attn Gain', fontsize=12)\n",
        "ax3.set_title('Relative Contribution: MLP vs Attention', fontsize=14)\n",
        "ax3.legend(fontsize=10)\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 4: Combined view with phases\n",
        "ax4 = axes[1, 1]\n",
        "combined_gain = np.array(attn_gains) * np.array(mlp_gains)\n",
        "ax4.plot(layers, combined_gain, 'm-', linewidth=2, marker='d', markersize=4, label='Combined Gain (Attn × MLP)')\n",
        "ax4.axhline(y=1.0, color='gray', linestyle='--', linewidth=2)\n",
        "ax4.fill_between(layers, combined_gain, 1, where=[g > 1 for g in combined_gain], \n",
        "                  alpha=0.3, color='green', label='Net Expansion')\n",
        "ax4.fill_between(layers, combined_gain, 1, where=[g < 1 for g in combined_gain], \n",
        "                  alpha=0.3, color='purple', label='Net Contraction')\n",
        "ax4.set_xlabel('Layer', fontsize=12)\n",
        "ax4.set_ylabel('Combined Gain', fontsize=12)\n",
        "ax4.set_title('Net Effect: Attn × MLP Gain', fontsize=14)\n",
        "ax4.legend(fontsize=10)\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle(f'{MODEL_NAME}: FFN vs Attention Expansion Analysis\\n(Hypothesis: Attention contracts, FFN expands)', \n",
        "             fontsize=16, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.savefig('ffn_expansion_analysis.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n>>> Figure saved as 'ffn_expansion_analysis.png'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Phase Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify phases based on gains\n",
        "print(\"=\"*60)\n",
        "print(\"PHASE ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Count layers in each regime\n",
        "attn_contracting = sum(1 for g in attn_gains if g < 1)\n",
        "attn_expanding = sum(1 for g in attn_gains if g > 1)\n",
        "mlp_contracting = sum(1 for g in mlp_gains if g < 1)\n",
        "mlp_expanding = sum(1 for g in mlp_gains if g > 1)\n",
        "\n",
        "print(f\"\\nAttention:\")\n",
        "print(f\"  Contracting (gain < 1): {attn_contracting}/{n_layers} layers\")\n",
        "print(f\"  Expanding (gain > 1):   {attn_expanding}/{n_layers} layers\")\n",
        "print(f\"  Min gain: {min(attn_gains):.4f} at layer {np.argmin(attn_gains)}\")\n",
        "print(f\"  Max gain: {max(attn_gains):.4f} at layer {np.argmax(attn_gains)}\")\n",
        "\n",
        "print(f\"\\nMLP/FFN:\")\n",
        "print(f\"  Contracting (gain < 1): {mlp_contracting}/{n_layers} layers\")\n",
        "print(f\"  Expanding (gain > 1):   {mlp_expanding}/{n_layers} layers\")\n",
        "print(f\"  Min gain: {min(mlp_gains):.4f} at layer {np.argmin(mlp_gains)}\")\n",
        "print(f\"  Max gain: {max(mlp_gains):.4f} at layer {np.argmax(mlp_gains)}\")\n",
        "\n",
        "# Net effect\n",
        "combined = np.array(attn_gains) * np.array(mlp_gains)\n",
        "net_contracting = sum(1 for g in combined if g < 1)\n",
        "net_expanding = sum(1 for g in combined if g > 1)\n",
        "\n",
        "print(f\"\\nNet Effect (Attn × MLP):\")\n",
        "print(f\"  Net Contracting: {net_contracting}/{n_layers} layers\")\n",
        "print(f\"  Net Expanding:   {net_expanding}/{n_layers} layers\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed layer-by-layer analysis\n",
        "print(\"\\nDetailed Layer Analysis:\")\n",
        "print(\"=\"*70)\n",
        "print(f\"{'Layer':^6} | {'Attn Gain':^10} | {'MLP Gain':^10} | {'Combined':^10} | {'Phase':^15}\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "for l in layers:\n",
        "    ag = attn_gains[l]\n",
        "    mg = mlp_gains[l]\n",
        "    cg = ag * mg\n",
        "    \n",
        "    # Determine phase\n",
        "    if ag < 1 and mg < 1:\n",
        "        phase = \"COMPRESS\"\n",
        "    elif ag < 1 and mg > 1:\n",
        "        phase = \"ATTN-/MLP+\"\n",
        "    elif ag > 1 and mg < 1:\n",
        "        phase = \"ATTN+/MLP-\"\n",
        "    else:\n",
        "        phase = \"EXPAND\"\n",
        "    \n",
        "    # Add net indicator\n",
        "    if cg < 0.9:\n",
        "        net = \"[NET-]\"\n",
        "    elif cg > 1.1:\n",
        "        net = \"[NET+]\"\n",
        "    else:\n",
        "        net = \"[~1.0]\"\n",
        "    \n",
        "    print(f\"{l:^6} | {ag:^10.4f} | {mg:^10.4f} | {cg:^10.4f} | {phase:^10} {net}\")\n",
        "\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Comparison with Restriction Maps Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reference values from restriction maps experiment (Pythia-1.4B)\n",
        "RESTRICTION_MAPS_REF = {\n",
        "    'L_star_contraction': 10,  # Layer with minimum contraction ratio\n",
        "    'L_star_entropy': 10,       # Layer with minimum attention entropy\n",
        "    'contraction_ratios': [0.29, 0.26, 0.26, 0.30, 0.20, 0.21, 0.18, 0.20, 0.23, 0.26,\n",
        "                           0.12, 0.16, 0.15, 0.18, 0.15, 0.18, 0.17, 0.22, 0.18, 0.17, 0.14]\n",
        "}\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"COMPARISON WITH RESTRICTION MAPS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\nRestriction Maps (Attention-only):\")\n",
        "print(f\"  L* (min contraction): Layer {RESTRICTION_MAPS_REF['L_star_contraction']}\")\n",
        "print(f\"  Contraction ratio at L*: {RESTRICTION_MAPS_REF['contraction_ratios'][10]:.4f}\")\n",
        "\n",
        "print(f\"\\nThis Experiment (Attention + MLP):\")\n",
        "print(f\"  Attention min gain: Layer {L_star_attn} (gain = {attn_gains[L_star_attn]:.4f})\")\n",
        "print(f\"  MLP max gain: Layer {L_star_mlp} (gain = {mlp_gains[L_star_mlp]:.4f})\")\n",
        "\n",
        "# Check correlation\n",
        "if len(RESTRICTION_MAPS_REF['contraction_ratios']) >= len(attn_gains):\n",
        "    ref_ratios = RESTRICTION_MAPS_REF['contraction_ratios'][:len(attn_gains)]\n",
        "    correlation = np.corrcoef(ref_ratios, attn_gains)[0, 1]\n",
        "    print(f\"\\nCorrelation (Restriction Ratio vs Attn Gain): {correlation:.4f}\")\n",
        "\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Summary and Export"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Prepare summary\n",
        "summary = {\n",
        "    'model': MODEL_NAME,\n",
        "    'n_layers': int(n_layers),\n",
        "    'hidden_dim': int(hidden_dim),\n",
        "    'n_prompts': len(TEST_PROMPTS),\n",
        "    \n",
        "    'attention': {\n",
        "        'gains': [float(g) for g in attn_gains],\n",
        "        'min_gain': float(min(attn_gains)),\n",
        "        'max_gain': float(max(attn_gains)),\n",
        "        'L_star_min': int(np.argmin(attn_gains)),\n",
        "        'n_contracting': int(attn_contracting),\n",
        "        'n_expanding': int(attn_expanding)\n",
        "    },\n",
        "    \n",
        "    'mlp': {\n",
        "        'gains': [float(g) for g in mlp_gains],\n",
        "        'min_gain': float(min(mlp_gains)),\n",
        "        'max_gain': float(max(mlp_gains)),\n",
        "        'L_star_max': int(np.argmax(mlp_gains)),\n",
        "        'n_contracting': int(mlp_contracting),\n",
        "        'n_expanding': int(mlp_expanding)\n",
        "    },\n",
        "    \n",
        "    'combined': {\n",
        "        'gains': [float(g) for g in combined],\n",
        "        'n_net_contracting': int(net_contracting),\n",
        "        'n_net_expanding': int(net_expanding)\n",
        "    },\n",
        "    \n",
        "    'hypothesis_test': {\n",
        "        'attention_contracts': bool(attn_contracting > attn_expanding),\n",
        "        'mlp_expands': bool(mlp_expanding > mlp_contracting),\n",
        "        'hypothesis_confirmed': bool(attn_contracting > attn_expanding and mlp_expanding > mlp_contracting)\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save to JSON\n",
        "with open('ffn_expansion_results.json', 'w') as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"FFN EXPANSION ANALYSIS SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nModel: {MODEL_NAME}\")\n",
        "print(f\"Prompts: {len(TEST_PROMPTS)}\")\n",
        "print(f\"\\nHypothesis: 'Attention contracts, FFN expands'\")\n",
        "print(f\"\\nResults:\")\n",
        "print(f\"  Attention contracting: {attn_contracting}/{n_layers} layers\")\n",
        "print(f\"  MLP expanding: {mlp_expanding}/{n_layers} layers\")\n",
        "print(f\"\\nHypothesis Confirmed: {summary['hypothesis_test']['hypothesis_confirmed']}\")\n",
        "print(f\"\\nFiles saved:\")\n",
        "print(f\"  - ffn_expansion_analysis.png\")\n",
        "print(f\"  - ffn_expansion_results.json\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create ZIP archive\n",
        "import zipfile\n",
        "from datetime import datetime\n",
        "\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "zip_filename = f\"ffn_expansion_results_{timestamp}.zip\"\n",
        "\n",
        "with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
        "    zipf.write('ffn_expansion_analysis.png')\n",
        "    zipf.write('ffn_expansion_results.json')\n",
        "\n",
        "print(f\">>> Created: {zip_filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Interpretation\n",
        "\n",
        "### Original Hypothesis\n",
        "\n",
        "From the Restriction Maps experiment:\n",
        "- Contraction ratios were ALL < 1 (attention always contracts)\n",
        "- But ||W_V|| explodes in late layers\n",
        "\n",
        "**Revised Hypothesis:**\n",
        "> Attention compresses (sheaf diffusion onto H⁰).\n",
        "> FFN expands (for prediction/logit separation).\n",
        "\n",
        "### Expected Pattern\n",
        "\n",
        "```\n",
        "Layer:    0 -------- L* -------- N\n",
        "          |          |           |\n",
        "Attn:     Contract   Min         Contract\n",
        "MLP:      Contract   Transition  EXPAND\n",
        "Net:      Contract   ~Neutral    EXPAND\n",
        "```\n",
        "\n",
        "### Theoretical Significance\n",
        "\n",
        "If confirmed:\n",
        "1. **Division of Labor**: Attention and FFN have complementary roles\n",
        "2. **Sheaf Interpretation**: Attention = sheaf diffusion (compression)\n",
        "3. **Prediction Mechanism**: FFN = amplification for logit separation\n",
        "\n",
        "This refines the Sheaf-theoretic framework:\n",
        "- Original: \"Attention does both contraction and expansion\"\n",
        "- Revised: \"Attention contracts (sheaf), FFN expands (prediction)\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Download Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download all results\n",
        "from google.colab import files\n",
        "\n",
        "print(\"Downloading result files...\")\n",
        "print()\n",
        "\n",
        "# Download ZIP\n",
        "print(f\"1. ZIP Archive: {zip_filename}\")\n",
        "files.download(zip_filename)\n",
        "\n",
        "# Individual files\n",
        "print(\"\\n2. Individual files:\")\n",
        "print(\"   - ffn_expansion_analysis.png\")\n",
        "files.download('ffn_expansion_analysis.png')\n",
        "print(\"   - ffn_expansion_results.json\")\n",
        "files.download('ffn_expansion_results.json')\n",
        "\n",
        "print(\"\\n>>> All files downloaded!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cleanup\n",
        "capture.remove_hooks()\n",
        "print(\"Hooks removed.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
