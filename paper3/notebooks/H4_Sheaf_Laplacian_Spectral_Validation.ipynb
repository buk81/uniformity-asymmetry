{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# H4 Validation: Sheaf-Laplacian Spectral Gap Predicts L*\n",
    "\n",
    "**Hypothesis H4:** The spectral gap λ₂ of the Sheaf Laplacian marks semantic domain separation and correlates with the transition point L*.\n",
    "\n",
    "## Theoretical Background\n",
    "\n",
    "The Sheaf Laplacian is:\n",
    "$$L_\\mathcal{F} = \\delta^\\top \\delta$$\n",
    "\n",
    "where δ is the coboundary operator. For transformers with restriction maps ρ_{ij} = √(A_{ij}) · W_V:\n",
    "\n",
    "**Predictions:**\n",
    "1. λ₁ ≈ 0 (global section always exists for connected graph)\n",
    "2. λ₂ (spectral gap) indicates \"diffusion speed\" / information mixing\n",
    "3. **KEY:** The layer where λ₂ peaks or transitions should correlate with L* (thermodynamic transition)\n",
    "\n",
    "---\n",
    "*Paper #3: Thermodynamic Constraints in Transformer Architectures*\n",
    "*Author: Davide D'Elia*\n",
    "*Date: 2026-01-06*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch numpy scipy matplotlib seaborn pandas -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import gc\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_context(\"paper\", font_scale=1.2)\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 1. Model Configuration\n",
    "\n",
    "Test across models with KNOWN thermodynamic signatures from H25-H27 validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models with known thermodynamic properties (from H25-H27)\n",
    "MODELS = {\n",
    "    # EleutherAI - DAMPENERS\n",
    "    'EleutherAI/pythia-160m': {\n",
    "        'lab': 'EleutherAI',\n",
    "        'behavior': 'DAMPEN',\n",
    "        'gain': 1.157,  # From pythia_family_NO_FINAL_LN\n",
    "        'layers': 12\n",
    "    },\n",
    "    'EleutherAI/pythia-410m': {\n",
    "        'lab': 'EleutherAI', \n",
    "        'behavior': 'DAMPEN',\n",
    "        'gain': 0.978,\n",
    "        'layers': 24\n",
    "    },\n",
    "    # Meta - EXPANDERS\n",
    "    'facebook/opt-125m': {\n",
    "        'lab': 'Meta',\n",
    "        'behavior': 'EXPAND',\n",
    "        'gain': 1.263,\n",
    "        'layers': 12\n",
    "    },\n",
    "    # OpenAI - EXPANDER (reference)\n",
    "    'gpt2': {\n",
    "        'lab': 'OpenAI',\n",
    "        'behavior': 'EXPAND',\n",
    "        'gain': 1.05,  # Approximate\n",
    "        'layers': 12\n",
    "    }\n",
    "}\n",
    "\n",
    "# Test prompts - semantically distinct domains\n",
    "TEST_PROMPTS = [\n",
    "    \"The capital of France is Paris.\",  # Factual\n",
    "    \"The sky is made of chocolate.\",     # Counterfactual\n",
    "    \"Once upon a time in a land far away\",  # Narrative\n",
    "    \"def fibonacci(n): return n if n < 2 else\",  # Code\n",
    "]\n",
    "\n",
    "print(f\"Testing {len(MODELS)} models with {len(TEST_PROMPTS)} prompts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 2. Sheaf Laplacian Construction\n",
    "\n",
    "For a transformer layer, the Sheaf Laplacian block structure is:\n",
    "\n",
    "$$L_\\mathcal{F}[i,i] = \\sum_{j \\neq i} \\rho_{ij}^\\top \\rho_{ij}$$\n",
    "$$L_\\mathcal{F}[i,j] = -\\rho_{ij}^\\top \\rho_{ji}$$ for $i \\neq j$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention_and_W_V(model, model_name, tokenizer, prompt, device='cuda'):\n",
    "    \"\"\"\n",
    "    Extract attention weights and W_V matrices for all layers.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            **inputs,\n",
    "            output_attentions=True,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "    \n",
    "    attentions = outputs.attentions  # tuple of (batch, heads, seq, seq)\n",
    "    \n",
    "    # Extract W_V based on architecture\n",
    "    W_V_list = []\n",
    "    \n",
    "    if hasattr(model, 'gpt_neox'):  # Pythia\n",
    "        layers = model.gpt_neox.layers\n",
    "        for layer in layers:\n",
    "            qkv = layer.attention.query_key_value.weight.data.float().cpu()\n",
    "            hidden_size = qkv.shape[0] // 3\n",
    "            W_V = qkv[2*hidden_size:, :]\n",
    "            W_V_list.append(W_V)\n",
    "    elif hasattr(model, 'model') and hasattr(model.model, 'decoder'):  # OPT\n",
    "        layers = model.model.decoder.layers\n",
    "        for layer in layers:\n",
    "            W_V = layer.self_attn.v_proj.weight.data.float().cpu()\n",
    "            W_V_list.append(W_V)\n",
    "    elif hasattr(model, 'transformer'):  # GPT-2\n",
    "        layers = model.transformer.h\n",
    "        for layer in layers:\n",
    "            c_attn = layer.attn.c_attn.weight.data.float().cpu()\n",
    "            hidden_size = c_attn.shape[1] // 3\n",
    "            W_V = c_attn[:, 2*hidden_size:].T\n",
    "            W_V_list.append(W_V)\n",
    "    \n",
    "    return attentions, W_V_list, inputs.input_ids.shape[1]\n",
    "\n",
    "\n",
    "def build_sheaf_laplacian_efficient(attention, W_V, max_tokens=6, proj_dim=16):\n",
    "    \"\"\"\n",
    "    Build Sheaf Laplacian efficiently using subsampling.\n",
    "    \n",
    "    Returns spectral properties: λ₁, λ₂, spectral_gap, trace\n",
    "    \"\"\"\n",
    "    # Use first head, average attention\n",
    "    A = attention[0, 0].float().cpu()  # (seq, seq)\n",
    "    seq_len = A.shape[0]\n",
    "    \n",
    "    # Subsample tokens\n",
    "    if seq_len > max_tokens:\n",
    "        indices = np.linspace(0, seq_len-1, max_tokens, dtype=int)\n",
    "        A = A[np.ix_(indices, indices)]\n",
    "        seq_len = max_tokens\n",
    "    \n",
    "    # Project W_V to smaller dimension\n",
    "    d = min(proj_dim, W_V.shape[0], W_V.shape[1])\n",
    "    W_V_small = W_V[:d, :d].numpy()\n",
    "    \n",
    "    # Compute √A\n",
    "    sqrt_A = torch.sqrt(A + 1e-10).numpy()\n",
    "    \n",
    "    # Build block Laplacian\n",
    "    n = seq_len\n",
    "    L_F = np.zeros((n * d, n * d))\n",
    "    \n",
    "    # Diagonal blocks: L[i,i] = Σ_j ρ_ij^T ρ_ij\n",
    "    for i in range(n):\n",
    "        block_ii = np.zeros((d, d))\n",
    "        for j in range(n):\n",
    "            if i != j:\n",
    "                rho_ij = sqrt_A[i, j] * W_V_small\n",
    "                block_ii += rho_ij.T @ rho_ij\n",
    "        L_F[i*d:(i+1)*d, i*d:(i+1)*d] = block_ii\n",
    "    \n",
    "    # Off-diagonal blocks: L[i,j] = -ρ_ij^T ρ_ji\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i != j:\n",
    "                rho_ij = sqrt_A[i, j] * W_V_small\n",
    "                rho_ji = sqrt_A[j, i] * W_V_small\n",
    "                L_F[i*d:(i+1)*d, j*d:(j+1)*d] = -rho_ij.T @ rho_ji\n",
    "    \n",
    "    # Compute eigenvalues\n",
    "    try:\n",
    "        L_F_reg = L_F + 1e-10 * np.eye(L_F.shape[0])\n",
    "        eigenvalues = np.linalg.eigvalsh(L_F_reg)\n",
    "        eigenvalues = np.sort(np.real(eigenvalues))\n",
    "    except:\n",
    "        eigenvalues = np.array([0.0, np.trace(L_F) / L_F.shape[0]])\n",
    "    \n",
    "    return {\n",
    "        'lambda_1': float(eigenvalues[0]) if len(eigenvalues) > 0 else 0.0,\n",
    "        'lambda_2': float(eigenvalues[1]) if len(eigenvalues) > 1 else 0.0,\n",
    "        'spectral_gap': float(eigenvalues[1] - eigenvalues[0]) if len(eigenvalues) > 1 else 0.0,\n",
    "        'trace': float(np.trace(L_F)),\n",
    "        'eigenvalues': eigenvalues[:10].tolist() if len(eigenvalues) >= 10 else eigenvalues.tolist()\n",
    "    }\n",
    "\n",
    "print(\"Sheaf Laplacian functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 3. Run H4 Validation Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_model_h4(model_name, config, test_prompts, device='cuda'):\n",
    "    \"\"\"\n",
    "    Full H4 analysis for a single model.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Analyzing: {model_name}\")\n",
    "    print(f\"Lab: {config['lab']}, Expected: {config['behavior']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Load model\n",
    "    print(\"  Loading model...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map='auto',\n",
    "        output_attentions=True,\n",
    "        output_hidden_states=True\n",
    "    )\n",
    "    model.eval()\n",
    "    \n",
    "    n_layers = config['layers']\n",
    "    \n",
    "    # Analyze each prompt\n",
    "    all_spectral_gaps = []\n",
    "    all_lambda_2 = []\n",
    "    all_traces = []\n",
    "    \n",
    "    for prompt in tqdm(test_prompts, desc=\"Prompts\"):\n",
    "        try:\n",
    "            attentions, W_V_list, seq_len = get_attention_and_W_V(\n",
    "                model, model_name, tokenizer, prompt, device\n",
    "            )\n",
    "            \n",
    "            layer_spectral_gaps = []\n",
    "            layer_lambda_2 = []\n",
    "            layer_traces = []\n",
    "            \n",
    "            for layer_idx in range(min(len(attentions), len(W_V_list))):\n",
    "                spectral = build_sheaf_laplacian_efficient(\n",
    "                    attentions[layer_idx],\n",
    "                    W_V_list[layer_idx]\n",
    "                )\n",
    "                layer_spectral_gaps.append(spectral['spectral_gap'])\n",
    "                layer_lambda_2.append(spectral['lambda_2'])\n",
    "                layer_traces.append(spectral['trace'])\n",
    "            \n",
    "            all_spectral_gaps.append(layer_spectral_gaps)\n",
    "            all_lambda_2.append(layer_lambda_2)\n",
    "            all_traces.append(layer_traces)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error on prompt: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Cleanup\n",
    "    del model, tokenizer\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Average across prompts\n",
    "    if all_spectral_gaps:\n",
    "        mean_spectral_gaps = np.mean(all_spectral_gaps, axis=0)\n",
    "        mean_lambda_2 = np.mean(all_lambda_2, axis=0)\n",
    "        mean_traces = np.mean(all_traces, axis=0)\n",
    "        \n",
    "        # Find key transition points\n",
    "        L_star_gap_max = int(np.argmax(mean_spectral_gaps))\n",
    "        L_star_lambda2_max = int(np.argmax(mean_lambda_2))\n",
    "        \n",
    "        # Compute second derivative for inflection\n",
    "        if len(mean_spectral_gaps) > 2:\n",
    "            second_deriv = np.diff(np.diff(mean_spectral_gaps))\n",
    "            L_star_inflection = int(np.argmax(np.abs(second_deriv))) + 1\n",
    "        else:\n",
    "            L_star_inflection = 0\n",
    "        \n",
    "        return {\n",
    "            'model': model_name,\n",
    "            'lab': config['lab'],\n",
    "            'behavior': config['behavior'],\n",
    "            'known_gain': config['gain'],\n",
    "            'n_layers': len(mean_spectral_gaps),\n",
    "            'spectral_gaps': mean_spectral_gaps.tolist(),\n",
    "            'lambda_2': mean_lambda_2.tolist(),\n",
    "            'traces': mean_traces.tolist(),\n",
    "            'L_star_gap_max': L_star_gap_max,\n",
    "            'L_star_lambda2_max': L_star_lambda2_max,\n",
    "            'L_star_inflection': L_star_inflection,\n",
    "            'max_spectral_gap': float(np.max(mean_spectral_gaps)),\n",
    "            'mean_spectral_gap': float(np.mean(mean_spectral_gaps)),\n",
    "            'spectral_gap_at_half': float(mean_spectral_gaps[len(mean_spectral_gaps)//2])\n",
    "        }\n",
    "    \n",
    "    return None\n",
    "\n",
    "print(\"Analysis function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run analysis on all models\n",
    "results = []\n",
    "\n",
    "for model_name, config in MODELS.items():\n",
    "    result = analyze_model_h4(model_name, config, TEST_PROMPTS)\n",
    "    if result:\n",
    "        results.append(result)\n",
    "        print(f\"\\n  L* (max gap): Layer {result['L_star_gap_max']}\")\n",
    "        print(f\"  L* (max λ₂): Layer {result['L_star_lambda2_max']}\")\n",
    "        print(f\"  Max spectral gap: {result['max_spectral_gap']:.4f}\")\n",
    "\n",
    "print(f\"\\n\\n{'='*60}\")\n",
    "print(f\"Successfully analyzed {len(results)} / {len(MODELS)} models\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 4. H4 Validation: Does λ₂ Predict Thermodynamic Behavior?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "if results:\n",
    "    # Create summary DataFrame\n",
    "    summary = pd.DataFrame([{\n",
    "        'Model': r['model'].split('/')[-1],\n",
    "        'Lab': r['lab'],\n",
    "        'Behavior': r['behavior'],\n",
    "        'Known Gain': r['known_gain'],\n",
    "        'Layers': r['n_layers'],\n",
    "        'L* (gap max)': r['L_star_gap_max'],\n",
    "        'L* / L': r['L_star_gap_max'] / r['n_layers'],\n",
    "        'Max Gap': r['max_spectral_gap'],\n",
    "        'Mean Gap': r['mean_spectral_gap']\n",
    "    } for r in results])\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"H4 VALIDATION SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(summary.to_string(index=False))\n",
    "    \n",
    "    # Key Test 1: Do DAMPENERS have different spectral gap patterns than EXPANDERS?\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TEST 1: Spectral Gap by Thermodynamic Behavior\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    dampeners = summary[summary['Behavior'] == 'DAMPEN']\n",
    "    expanders = summary[summary['Behavior'] == 'EXPAND']\n",
    "    \n",
    "    if len(dampeners) > 0 and len(expanders) > 0:\n",
    "        print(f\"\\nDAMPENERS (EleutherAI):\")\n",
    "        print(f\"  Mean max gap: {dampeners['Max Gap'].mean():.4f}\")\n",
    "        print(f\"  Mean L*/L: {dampeners['L* / L'].mean():.3f}\")\n",
    "        \n",
    "        print(f\"\\nEXPANDERS (Meta, OpenAI):\")\n",
    "        print(f\"  Mean max gap: {expanders['Max Gap'].mean():.4f}\")\n",
    "        print(f\"  Mean L*/L: {expanders['L* / L'].mean():.3f}\")\n",
    "        \n",
    "        # Statistical test\n",
    "        if len(dampeners) >= 2 and len(expanders) >= 2:\n",
    "            stat, p = stats.mannwhitneyu(\n",
    "                dampeners['Max Gap'].values, \n",
    "                expanders['Max Gap'].values,\n",
    "                alternative='two-sided'\n",
    "            )\n",
    "            print(f\"\\nMann-Whitney U test (Max Gap): p = {p:.4f}\")\n",
    "    \n",
    "    # Key Test 2: Correlation between spectral gap and known gain\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TEST 2: Spectral Gap vs Known Residual Gain\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if len(summary) >= 3:\n",
    "        r, p = stats.spearmanr(summary['Max Gap'], summary['Known Gain'])\n",
    "        print(f\"\\nSpearman correlation (Max Gap vs Gain): r = {r:.3f}, p = {p:.4f}\")\n",
    "        \n",
    "        r2, p2 = stats.spearmanr(summary['L* / L'], summary['Known Gain'])\n",
    "        print(f\"Spearman correlation (L*/L vs Gain): r = {r2:.3f}, p = {p2:.4f}\")\n",
    "else:\n",
    "    print(\"No results to analyze!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "if results:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "    \n",
    "    colors = {'EleutherAI': '#E74C3C', 'Meta': '#3498DB', 'OpenAI': '#8E44AD'}\n",
    "    \n",
    "    # Plot 1: Spectral Gap by Layer for each model\n",
    "    ax1 = axes[0, 0]\n",
    "    for r in results:\n",
    "        layers = np.arange(len(r['spectral_gaps']))\n",
    "        normalized_layers = layers / len(r['spectral_gaps'])  # Normalize to [0, 1]\n",
    "        ax1.plot(normalized_layers, r['spectral_gaps'], \n",
    "                 label=f\"{r['model'].split('/')[-1]} ({r['behavior']})\",\n",
    "                 color=colors.get(r['lab'], 'gray'),\n",
    "                 linewidth=2, marker='o', markersize=4)\n",
    "    ax1.axvline(x=0.5, color='gray', linestyle='--', alpha=0.5, label='L/2')\n",
    "    ax1.set_xlabel('Normalized Layer (l/L)', fontsize=12)\n",
    "    ax1.set_ylabel('Spectral Gap (λ₂ - λ₁)', fontsize=12)\n",
    "    ax1.set_title('Sheaf Laplacian Spectral Gap by Layer', fontsize=14)\n",
    "    ax1.legend(fontsize=9)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: λ₂ by Layer\n",
    "    ax2 = axes[0, 1]\n",
    "    for r in results:\n",
    "        layers = np.arange(len(r['lambda_2']))\n",
    "        normalized_layers = layers / len(r['lambda_2'])\n",
    "        ax2.semilogy(normalized_layers, np.array(r['lambda_2']) + 1e-10,\n",
    "                     label=f\"{r['model'].split('/')[-1]}\",\n",
    "                     color=colors.get(r['lab'], 'gray'),\n",
    "                     linewidth=2, marker='s', markersize=4)\n",
    "    ax2.set_xlabel('Normalized Layer (l/L)', fontsize=12)\n",
    "    ax2.set_ylabel('λ₂ (log scale)', fontsize=12)\n",
    "    ax2.set_title('Second Eigenvalue (Algebraic Connectivity)', fontsize=14)\n",
    "    ax2.legend(fontsize=9)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Max Spectral Gap vs Known Gain\n",
    "    ax3 = axes[1, 0]\n",
    "    for r in results:\n",
    "        ax3.scatter(r['known_gain'], r['max_spectral_gap'],\n",
    "                    color=colors.get(r['lab'], 'gray'),\n",
    "                    s=150, edgecolors='white', linewidths=2,\n",
    "                    label=f\"{r['model'].split('/')[-1]}\")\n",
    "    ax3.axvline(x=1.0, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax3.set_xlabel('Known Residual Gain (G)', fontsize=12)\n",
    "    ax3.set_ylabel('Max Spectral Gap', fontsize=12)\n",
    "    ax3.set_title('Spectral Gap vs Thermodynamic Behavior', fontsize=14)\n",
    "    ax3.legend(fontsize=9)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: L* Position vs Known Gain\n",
    "    ax4 = axes[1, 1]\n",
    "    for r in results:\n",
    "        ax4.scatter(r['known_gain'], r['L_star_gap_max'] / r['n_layers'],\n",
    "                    color=colors.get(r['lab'], 'gray'),\n",
    "                    s=150, edgecolors='white', linewidths=2,\n",
    "                    marker='D',\n",
    "                    label=f\"{r['model'].split('/')[-1]}\")\n",
    "    ax4.axvline(x=1.0, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax4.axhline(y=0.5, color='gray', linestyle=':', alpha=0.5)\n",
    "    ax4.set_xlabel('Known Residual Gain (G)', fontsize=12)\n",
    "    ax4.set_ylabel('L* / L (Transition Point)', fontsize=12)\n",
    "    ax4.set_title('Transition Point vs Thermodynamic Behavior', fontsize=14)\n",
    "    ax4.legend(fontsize=9)\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('H4 Validation: Sheaf-Laplacian Spectral Gap Analysis', \n",
    "                 fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('H4_spectral_gap_validation.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n>>> Figure saved: H4_spectral_gap_validation.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 5. H4 Verdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "if results:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"H4 VALIDATION VERDICT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Determine verdict based on findings\n",
    "    findings = []\n",
    "    \n",
    "    # Check 1: Do all models show spectral gap variation?\n",
    "    all_show_variation = all(r['max_spectral_gap'] > r['mean_spectral_gap'] * 1.1 for r in results)\n",
    "    findings.append(f\"All models show spectral gap variation: {all_show_variation}\")\n",
    "    \n",
    "    # Check 2: Is L* consistent (around L/2)?\n",
    "    l_star_ratios = [r['L_star_gap_max'] / r['n_layers'] for r in results]\n",
    "    l_star_near_half = 0.3 < np.mean(l_star_ratios) < 0.7\n",
    "    findings.append(f\"L* near L/2 (mean={np.mean(l_star_ratios):.2f}): {l_star_near_half}\")\n",
    "    \n",
    "    # Check 3: Do dampeners differ from expanders?\n",
    "    dampener_gaps = [r['max_spectral_gap'] for r in results if r['behavior'] == 'DAMPEN']\n",
    "    expander_gaps = [r['max_spectral_gap'] for r in results if r['behavior'] == 'EXPAND']\n",
    "    if dampener_gaps and expander_gaps:\n",
    "        gap_difference = abs(np.mean(dampener_gaps) - np.mean(expander_gaps))\n",
    "        findings.append(f\"Dampener vs Expander gap difference: {gap_difference:.4f}\")\n",
    "    \n",
    "    for f in findings:\n",
    "        print(f\"  • {f}\")\n",
    "    \n",
    "    # Overall verdict\n",
    "    print(\"\\n\" + \"-\"*40)\n",
    "    if all_show_variation and l_star_near_half:\n",
    "        verdict = \"✅ H4 EMPIRICALLY VALIDATED\"\n",
    "        detail = \"Spectral gap λ₂ shows systematic layer-wise variation with transition near L/2\"\n",
    "    else:\n",
    "        verdict = \"⚠️ H4 PARTIALLY SUPPORTED\"\n",
    "        detail = \"Spectral structure exists but correlation with L* needs more data\"\n",
    "    \n",
    "    print(f\"\\n{verdict}\")\n",
    "    print(f\"{detail}\")\n",
    "    \n",
    "    # Save results\n",
    "    output = {\n",
    "        'experiment': 'H4 Sheaf-Laplacian Spectral Gap Validation',\n",
    "        'date': datetime.now().isoformat(),\n",
    "        'hypothesis': 'λ₂ of Sheaf Laplacian marks semantic domain separation and correlates with L*',\n",
    "        'models_tested': len(results),\n",
    "        'verdict': verdict,\n",
    "        'findings': findings,\n",
    "        'results': results\n",
    "    }\n",
    "    \n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    filename = f'H4_validation_{timestamp}.json'\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(output, f, indent=2)\n",
    "    print(f\"\\nResults saved: {filename}\")\n",
    "else:\n",
    "    print(\"No results - cannot determine verdict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download results\n",
    "import glob\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "    print(\"Downloading results...\")\n",
    "    \n",
    "    if 'H4_spectral_gap_validation.png' in glob.glob('*.png'):\n",
    "        files.download('H4_spectral_gap_validation.png')\n",
    "    \n",
    "    for f in glob.glob('H4_validation_*.json'):\n",
    "        files.download(f)\n",
    "        \n",
    "    print(\"Done!\")\n",
    "except ImportError:\n",
    "    print(\"Files saved locally:\")\n",
    "    print(\"  - H4_spectral_gap_validation.png\")\n",
    "    for f in glob.glob('H4_validation_*.json'):\n",
    "        print(f\"  - {f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
