{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cell-0",
      "metadata": {},
      "source": [
        "# L* Cross-Heritage Validation (v3 - Sign Change Definition)\n",
        "\n",
        "**Paper #3: Thermodynamic Constraints in Transformer Architectures**\n",
        "\n",
        "**Author:** Davide D'Elia\n",
        "\n",
        "**Date:** 2026-01-06\n",
        "\n",
        "---\n",
        "\n",
        "## Purpose\n",
        "\n",
        "Validate the L* transition point formula using the **correct definition**:\n",
        "\n",
        "> **L* = Layer where d/dl[Tr(L_F)] changes sign**\n",
        "\n",
        "This is the inflection point where trace derivative goes from positive to negative.\n",
        "\n",
        "## v3 Fixes (vs v2)\n",
        "\n",
        "- **CRITICAL**: Changed L* definition from `argmax(|gradient|)` to `sign_change(gradient)`\n",
        "- This matches the original calibration methodology\n",
        "\n",
        "## The Formula\n",
        "\n",
        "```\n",
        "L* = L × (0.11 + 0.012×L + 4.9/H)\n",
        "```\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: Setup\n",
        "!pip install -q transformers accelerate scipy seaborn pandas huggingface_hub\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
        "from tqdm.auto import tqdm\n",
        "import json\n",
        "import gc\n",
        "import os\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configure visualization\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "sns.set_context(\"paper\", font_scale=1.2)\n",
        "\n",
        "# Global timestamp\n",
        "TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "print(f\"Session timestamp: {TIMESTAMP}\")\n",
        "\n",
        "# HF TOKEN\n",
        "HF_TOKEN = None\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "    if HF_TOKEN:\n",
        "        print(f\"HF_TOKEN loaded\")\n",
        "except:\n",
        "    HF_TOKEN = os.environ.get('HF_TOKEN')\n",
        "\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-2",
      "metadata": {},
      "source": [
        "## 2. Model Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Models to test - same as calibration set + cross-heritage\n",
        "MODELS_TO_TEST = {\n",
        "    # EleutherAI (calibration heritage)\n",
        "    \"EleutherAI/pythia-160m\": {\"lab\": \"EleutherAI\", \"L\": 12, \"H\": 12, \"expected\": \"DAMPEN\", \"calibration_L_star\": 7},\n",
        "    \"EleutherAI/pythia-410m\": {\"lab\": \"EleutherAI\", \"L\": 24, \"H\": 16, \"expected\": \"DAMPEN\", \"calibration_L_star\": 16},\n",
        "    # Meta (cross-heritage)\n",
        "    \"facebook/opt-125m\": {\"lab\": \"Meta\", \"L\": 12, \"H\": 12, \"expected\": \"EXPAND\", \"calibration_L_star\": 8},\n",
        "    \"facebook/opt-350m\": {\"lab\": \"Meta\", \"L\": 24, \"H\": 16, \"expected\": \"EXPAND\", \"calibration_L_star\": None},\n",
        "    # BigScience (ALiBi - cross-heritage)\n",
        "    \"bigscience/bloom-560m\": {\"lab\": \"BigScience\", \"L\": 24, \"H\": 16, \"expected\": \"EXPAND\", \"calibration_L_star\": None},\n",
        "    # OpenAI (cross-heritage)\n",
        "    \"openai-community/gpt2\": {\"lab\": \"OpenAI\", \"L\": 12, \"H\": 12, \"expected\": \"EXPAND\", \"calibration_L_star\": 9},\n",
        "}\n",
        "\n",
        "TEST_PROMPTS = [\n",
        "    \"The capital of France is Paris, which is known for the Eiffel Tower.\",\n",
        "    \"In mathematics, the derivative of x squared equals two times x.\",\n",
        "    \"Climate change affects global temperatures and weather patterns significantly.\",\n",
        "    \"The quick brown fox jumps over the lazy dog near the riverbank.\",\n",
        "    \"Once upon a time in a land far away, there lived a wise old king.\",\n",
        "]\n",
        "\n",
        "def predict_l_star_v3(L, H):\n",
        "    \"\"\"L* = L × (0.11 + 0.012×L + 4.9/H)\"\"\"\n",
        "    return L * (0.11 + 0.012 * L + 4.9 / H)\n",
        "\n",
        "print(f\"Models: {len(MODELS_TO_TEST)}, Prompts: {len(TEST_PROMPTS)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-4",
      "metadata": {},
      "source": [
        "## 3. Architecture-Aware Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-5",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_architecture_info(model):\n",
        "    \"\"\"Detect model architecture type.\"\"\"\n",
        "    if hasattr(model, 'gpt_neox'):\n",
        "        return 'pythia'\n",
        "    elif hasattr(model, 'model') and hasattr(model.model, 'decoder'):\n",
        "        return 'opt'\n",
        "    elif hasattr(model, 'transformer') and hasattr(model.transformer, 'h'):\n",
        "        layer = model.transformer.h[0]\n",
        "        if hasattr(layer, 'self_attention'):\n",
        "            return 'bloom'\n",
        "        elif hasattr(layer, 'attn'):\n",
        "            return 'gpt2'\n",
        "    elif hasattr(model, 'model') and hasattr(model.model, 'layers'):\n",
        "        return 'llama'\n",
        "    return 'unknown'\n",
        "\n",
        "\n",
        "def get_layers(model, arch):\n",
        "    \"\"\"Get transformer layers based on architecture.\"\"\"\n",
        "    if arch == 'pythia':\n",
        "        return model.gpt_neox.layers\n",
        "    elif arch == 'opt':\n",
        "        return model.model.decoder.layers\n",
        "    elif arch in ['bloom', 'gpt2']:\n",
        "        return model.transformer.h\n",
        "    elif arch == 'llama':\n",
        "        return model.model.layers\n",
        "    return []\n",
        "\n",
        "\n",
        "def get_W_V(model, arch, layer_idx):\n",
        "    \"\"\"Extract W_V matrix for a specific layer.\"\"\"\n",
        "    try:\n",
        "        layers = get_layers(model, arch)\n",
        "        layer = layers[layer_idx]\n",
        "        \n",
        "        if arch == 'pythia':\n",
        "            qkv = layer.attention.query_key_value.weight.data.float()\n",
        "            d = qkv.shape[0] // 3\n",
        "            return qkv[2*d:, :].cpu()\n",
        "            \n",
        "        elif arch == 'opt':\n",
        "            return layer.self_attn.v_proj.weight.data.float().cpu()\n",
        "            \n",
        "        elif arch == 'bloom':\n",
        "            qkv = layer.self_attention.query_key_value.weight.data.float()\n",
        "            d = qkv.shape[0] // 3\n",
        "            return qkv[2*d:, :].cpu()\n",
        "            \n",
        "        elif arch == 'gpt2':\n",
        "            c_attn = layer.attn.c_attn.weight.data.float()\n",
        "            d = c_attn.shape[1] // 3\n",
        "            return c_attn[:, 2*d:].T.cpu()\n",
        "            \n",
        "        elif arch == 'llama':\n",
        "            return layer.self_attn.v_proj.weight.data.float().cpu()\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"    W_V extraction error (layer {layer_idx}): {e}\")\n",
        "    \n",
        "    return None\n",
        "\n",
        "print(\"Architecture-aware functions defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-6",
      "metadata": {},
      "source": [
        "## 4. L* Computation with SIGN CHANGE Definition\n",
        "\n",
        "**CRITICAL FIX**: L* is defined as the layer where the trace derivative **changes sign** (from positive to negative), NOT the layer of maximum gradient magnitude."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-7",
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_l_star_sign_change(traces):\n",
        "    \"\"\"\n",
        "    Find L* using the CORRECT definition: layer where trace derivative changes sign.\n",
        "    \n",
        "    L* = layer where d/dl[Tr(L_F)] goes from positive to negative\n",
        "    \n",
        "    This is the inflection point / peak of the trace curve.\n",
        "    \n",
        "    Returns:\n",
        "        L_star: Layer index where sign change occurs (or peak if monotonic)\n",
        "        method: 'sign_change', 'peak', or 'fallback'\n",
        "    \"\"\"\n",
        "    traces_arr = np.array(traces)\n",
        "    n_layers = len(traces_arr)\n",
        "    \n",
        "    if n_layers < 3:\n",
        "        return n_layers // 2, 'fallback'\n",
        "    \n",
        "    # Compute gradients (first derivative)\n",
        "    gradients = np.diff(traces_arr)\n",
        "    \n",
        "    # Method 1: Find sign change (+ to -)\n",
        "    # This indicates the peak of the trace curve\n",
        "    for i in range(len(gradients) - 1):\n",
        "        if gradients[i] > 0 and gradients[i+1] < 0:\n",
        "            # Sign change at layer i+1 (0-indexed)\n",
        "            return i + 1, 'sign_change'\n",
        "    \n",
        "    # Method 2: If no sign change, find peak of trace\n",
        "    # (trace might be monotonically increasing then plateau)\n",
        "    peak_idx = int(np.argmax(traces_arr))\n",
        "    if peak_idx > 0 and peak_idx < n_layers - 1:\n",
        "        return peak_idx, 'peak'\n",
        "    \n",
        "    # Method 3: Find where gradient magnitude drops significantly\n",
        "    # (indicates transition from steep to flat)\n",
        "    grad_magnitude = np.abs(gradients)\n",
        "    if len(grad_magnitude) > 1:\n",
        "        # Find where gradient drops below 50% of max\n",
        "        max_grad = np.max(grad_magnitude)\n",
        "        for i in range(len(grad_magnitude)):\n",
        "            if grad_magnitude[i] >= max_grad * 0.5:\n",
        "                # Last layer with significant gradient\n",
        "                last_significant = i\n",
        "        return last_significant + 1, 'gradient_drop'\n",
        "    \n",
        "    # Fallback: midpoint\n",
        "    return n_layers // 2, 'fallback'\n",
        "\n",
        "\n",
        "def compute_traces_and_l_star(model, tokenizer, prompt, arch, device='cuda'):\n",
        "    \"\"\"\n",
        "    Compute Sheaf Laplacian trace for each layer and find L* via sign change.\n",
        "    \n",
        "    Trace formula: Tr(L_F) = (sum(A) - n) * ||W_V||_F^2\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    # Tokenize\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    n_tokens = inputs[\"input_ids\"].shape[1]\n",
        "    \n",
        "    # Forward pass with attention output\n",
        "    with torch.no_grad():\n",
        "        outputs = model(\n",
        "            **inputs,\n",
        "            output_attentions=True,\n",
        "            output_hidden_states=True\n",
        "        )\n",
        "    \n",
        "    attentions = outputs.attentions\n",
        "    \n",
        "    if attentions is None:\n",
        "        print(f\"    WARNING: attentions is None!\")\n",
        "        return None, None, None\n",
        "    \n",
        "    n_layers = len(attentions)\n",
        "    traces = []\n",
        "    \n",
        "    for layer_idx in range(n_layers):\n",
        "        attn = attentions[layer_idx]\n",
        "        if attn is None:\n",
        "            traces.append(0.0)\n",
        "            continue\n",
        "            \n",
        "        # Average attention over heads\n",
        "        A = attn[0].float().mean(dim=0).cpu()  # (seq, seq)\n",
        "        \n",
        "        # Get W_V\n",
        "        W_V = get_W_V(model, arch, layer_idx)\n",
        "        \n",
        "        # Compute trace: Tr(L_F) = (sum(A) - n) * ||W_V||_F^2\n",
        "        if W_V is not None:\n",
        "            A_sum = A.sum().item()\n",
        "            W_V_frob_sq = (W_V ** 2).sum().item()\n",
        "            trace = abs((A_sum - n_tokens) * W_V_frob_sq)\n",
        "        else:\n",
        "            A_sum = A.sum().item()\n",
        "            trace = abs(A_sum - n_tokens)\n",
        "        \n",
        "        traces.append(trace)\n",
        "    \n",
        "    # Find L* using SIGN CHANGE definition\n",
        "    L_star, method = find_l_star_sign_change(traces)\n",
        "    \n",
        "    return traces, L_star, method\n",
        "\n",
        "print(\"Trace computation with SIGN CHANGE L* definition ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-8",
      "metadata": {},
      "source": [
        "## 5. Run Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-9",
      "metadata": {},
      "outputs": [],
      "source": [
        "results = []\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"L* CROSS-HERITAGE VALIDATION (v3 - SIGN CHANGE DEFINITION)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for model_name, config in tqdm(MODELS_TO_TEST.items(), desc=\"Models\"):\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Model: {model_name}\")\n",
        "    print(f\"Lab: {config['lab']} | L={config['L']} | H={config['H']}\")\n",
        "    if config.get('calibration_L_star'):\n",
        "        print(f\"Calibration L*: {config['calibration_L_star']}\")\n",
        "    \n",
        "    try:\n",
        "        # Load tokenizer\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\n",
        "            model_name,\n",
        "            token=HF_TOKEN if HF_TOKEN else None\n",
        "        )\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "        \n",
        "        # Load model with attention output\n",
        "        print(\"  Loading model...\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\",\n",
        "            token=HF_TOKEN if HF_TOKEN else None,\n",
        "            trust_remote_code=True,\n",
        "            attn_implementation=\"eager\",\n",
        "            output_attentions=True,\n",
        "            output_hidden_states=True\n",
        "        )\n",
        "        model.eval()\n",
        "        \n",
        "        # Detect architecture\n",
        "        arch = get_architecture_info(model)\n",
        "        print(f\"  Architecture: {arch}\")\n",
        "        \n",
        "        # Test prompts\n",
        "        all_l_stars = []\n",
        "        all_traces = []\n",
        "        all_methods = []\n",
        "        \n",
        "        for i, prompt in enumerate(TEST_PROMPTS):\n",
        "            traces, l_star, method = compute_traces_and_l_star(model, tokenizer, prompt, arch)\n",
        "            if traces is not None:\n",
        "                all_traces.append(traces)\n",
        "                all_l_stars.append(l_star)\n",
        "                all_methods.append(method)\n",
        "                print(f\"    Prompt {i+1}: L* = {l_star} ({method})\")\n",
        "            else:\n",
        "                print(f\"    Prompt {i+1}: FAILED\")\n",
        "        \n",
        "        if all_l_stars:\n",
        "            L_star_empirical = np.mean(all_l_stars)\n",
        "            L_star_std = np.std(all_l_stars)\n",
        "            L_star_predicted = predict_l_star_v3(config[\"L\"], config[\"H\"])\n",
        "            error = abs(L_star_predicted - L_star_empirical) / config[\"L\"] * 100\n",
        "            \n",
        "            # Most common method\n",
        "            from collections import Counter\n",
        "            method_counts = Counter(all_methods)\n",
        "            dominant_method = method_counts.most_common(1)[0][0]\n",
        "            \n",
        "            result = {\n",
        "                \"model\": model_name,\n",
        "                \"lab\": config[\"lab\"],\n",
        "                \"L\": config[\"L\"],\n",
        "                \"H\": config[\"H\"],\n",
        "                \"arch\": arch,\n",
        "                \"L_star_predicted\": float(L_star_predicted),\n",
        "                \"L_star_empirical\": float(L_star_empirical),\n",
        "                \"L_star_std\": float(L_star_std),\n",
        "                \"L_star_calibration\": config.get('calibration_L_star'),\n",
        "                \"error_pct\": float(error),\n",
        "                \"detection_method\": dominant_method,\n",
        "                \"individual_L_stars\": all_l_stars,\n",
        "            }\n",
        "            results.append(result)\n",
        "            \n",
        "            print(f\"\\n  RESULTS (Sign Change Definition):\")\n",
        "            print(f\"    L* predicted:    {L_star_predicted:.1f}\")\n",
        "            print(f\"    L* empirical:    {L_star_empirical:.1f} +/- {L_star_std:.1f}\")\n",
        "            if config.get('calibration_L_star'):\n",
        "                print(f\"    L* calibration:  {config['calibration_L_star']}\")\n",
        "                calib_diff = abs(L_star_empirical - config['calibration_L_star'])\n",
        "                print(f\"    Calib. diff:     {calib_diff:.1f}\")\n",
        "            print(f\"    Error:           {error:.1f}%\")\n",
        "            print(f\"    Method:          {dominant_method}\")\n",
        "        \n",
        "        # Cleanup\n",
        "        del model, tokenizer\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"  ERROR: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"COMPLETE: {len(results)}/{len(MODELS_TO_TEST)} models\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-10",
      "metadata": {},
      "source": [
        "## 6. Results Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-11",
      "metadata": {},
      "outputs": [],
      "source": [
        "if results:\n",
        "    df = pd.DataFrame(results)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"CROSS-HERITAGE L* VALIDATION RESULTS (SIGN CHANGE DEFINITION)\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Display key columns\n",
        "    display_cols = ['model', 'lab', 'L', 'H', 'L_star_predicted', 'L_star_empirical', \n",
        "                    'L_star_calibration', 'error_pct', 'detection_method']\n",
        "    print(df[display_cols].to_string(index=False))\n",
        "    \n",
        "    # Calibration consistency check\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"CALIBRATION CONSISTENCY CHECK\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    for _, row in df.iterrows():\n",
        "        if row['L_star_calibration'] is not None:\n",
        "            diff = abs(row['L_star_empirical'] - row['L_star_calibration'])\n",
        "            status = \"MATCH\" if diff < 2 else \"MISMATCH\"\n",
        "            print(f\"{row['model'].split('/')[-1]:20} Empirical={row['L_star_empirical']:.1f} Calib={row['L_star_calibration']} Diff={diff:.1f} [{status}]\")\n",
        "    \n",
        "    # Summary by lab\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SUMMARY BY LAB\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    for lab in sorted(df['lab'].unique()):\n",
        "        lab_df = df[df['lab'] == lab]\n",
        "        errors = lab_df['error_pct'].values\n",
        "        print(f\"\\n{lab}: n={len(lab_df)}, MAPE={np.mean(errors):.1f}%\")\n",
        "    \n",
        "    # Overall\n",
        "    overall_mape = df['error_pct'].mean()\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"OVERALL MAPE: {overall_mape:.1f}%\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Compare with v2 (argmax) results\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"COMPARISON: Sign Change vs Argmax Gradient\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"v2 (argmax):      15.7% MAPE\")\n",
        "    print(f\"v3 (sign change): {overall_mape:.1f}% MAPE\")\n",
        "    improvement = 15.7 - overall_mape\n",
        "    print(f\"Improvement:      {improvement:+.1f}pp\")\n",
        "else:\n",
        "    print(\"No results!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-12",
      "metadata": {},
      "source": [
        "## 7. Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-13",
      "metadata": {},
      "outputs": [],
      "source": [
        "if results:\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "    \n",
        "    lab_colors = {\n",
        "        \"EleutherAI\": \"#E74C3C\",\n",
        "        \"Meta\": \"#3498DB\",\n",
        "        \"BigScience\": \"#27AE60\",\n",
        "        \"OpenAI\": \"#9B59B6\"\n",
        "    }\n",
        "    \n",
        "    # Plot 1: Predicted vs Empirical\n",
        "    ax1 = axes[0]\n",
        "    for r in results:\n",
        "        color = lab_colors.get(r['lab'], 'gray')\n",
        "        ax1.scatter(r['L_star_predicted'], r['L_star_empirical'],\n",
        "                   c=color, s=150, alpha=0.8, edgecolors='white', linewidths=2)\n",
        "        ax1.annotate(r['model'].split('/')[-1],\n",
        "                    (r['L_star_predicted'], r['L_star_empirical']),\n",
        "                    fontsize=8, xytext=(5, 5), textcoords='offset points')\n",
        "    \n",
        "    max_val = max(max(r['L_star_predicted'] for r in results),\n",
        "                  max(r['L_star_empirical'] for r in results))\n",
        "    ax1.plot([0, max_val*1.1], [0, max_val*1.1], 'k--', alpha=0.5, label='Perfect')\n",
        "    ax1.set_xlabel('L* Predicted', fontsize=12)\n",
        "    ax1.set_ylabel('L* Empirical (Sign Change)', fontsize=12)\n",
        "    ax1.set_title('L* Validation (Sign Change Definition)', fontsize=14)\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    ax1.legend()\n",
        "    \n",
        "    # Plot 2: Error by Lab\n",
        "    ax2 = axes[1]\n",
        "    labs = sorted(set(r['lab'] for r in results))\n",
        "    for i, lab in enumerate(labs):\n",
        "        lab_results = [r for r in results if r['lab'] == lab]\n",
        "        errors = [r['error_pct'] for r in lab_results]\n",
        "        ax2.bar(i, np.mean(errors), color=lab_colors.get(lab, 'gray'), alpha=0.7)\n",
        "        ax2.scatter([i]*len(errors), errors, c='black', s=50, zorder=5)\n",
        "    \n",
        "    ax2.axhline(y=10, color='green', linestyle='--', label='10% threshold')\n",
        "    ax2.axhline(y=15, color='orange', linestyle='--', label='15% threshold')\n",
        "    ax2.set_xticks(range(len(labs)))\n",
        "    ax2.set_xticklabels(labs, rotation=45, ha='right')\n",
        "    ax2.set_ylabel('Error (%)', fontsize=12)\n",
        "    ax2.set_title('L* Formula Error by Lab', fontsize=14)\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    # Plot 3: Empirical vs Calibration (where available)\n",
        "    ax3 = axes[2]\n",
        "    calib_results = [r for r in results if r['L_star_calibration'] is not None]\n",
        "    if calib_results:\n",
        "        for r in calib_results:\n",
        "            color = lab_colors.get(r['lab'], 'gray')\n",
        "            ax3.scatter(r['L_star_calibration'], r['L_star_empirical'],\n",
        "                       c=color, s=150, alpha=0.8, edgecolors='white', linewidths=2)\n",
        "            ax3.annotate(r['model'].split('/')[-1],\n",
        "                        (r['L_star_calibration'], r['L_star_empirical']),\n",
        "                        fontsize=8, xytext=(5, 5), textcoords='offset points')\n",
        "        \n",
        "        max_calib = max(max(r['L_star_calibration'] for r in calib_results),\n",
        "                        max(r['L_star_empirical'] for r in calib_results))\n",
        "        ax3.plot([0, max_calib*1.1], [0, max_calib*1.1], 'k--', alpha=0.5)\n",
        "        ax3.set_xlabel('L* Calibration (Original)', fontsize=12)\n",
        "        ax3.set_ylabel('L* Empirical (This Run)', fontsize=12)\n",
        "        ax3.set_title('Calibration Consistency', fontsize=14)\n",
        "        ax3.grid(True, alpha=0.3)\n",
        "    else:\n",
        "        ax3.text(0.5, 0.5, 'No calibration data', ha='center', va='center', fontsize=14)\n",
        "        ax3.set_title('Calibration Consistency', fontsize=14)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    PNG_FILE = f\"l_star_sign_change_{TIMESTAMP}.png\"\n",
        "    plt.savefig(PNG_FILE, dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(f\"Saved: {PNG_FILE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-14",
      "metadata": {},
      "source": [
        "## 8. Final Verdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-15",
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_serializable(obj):\n",
        "    \"\"\"Convert numpy types to Python native types for JSON serialization.\"\"\"\n",
        "    if isinstance(obj, dict):\n",
        "        return {k: make_serializable(v) for k, v in obj.items()}\n",
        "    elif isinstance(obj, list):\n",
        "        return [make_serializable(v) for v in obj]\n",
        "    elif isinstance(obj, (np.integer, np.floating)):\n",
        "        return float(obj)\n",
        "    elif isinstance(obj, np.ndarray):\n",
        "        return obj.tolist()\n",
        "    return obj\n",
        "\n",
        "\n",
        "if results:\n",
        "    overall_mape = np.mean([r['error_pct'] for r in results])\n",
        "    \n",
        "    # Check calibration consistency\n",
        "    calib_matches = 0\n",
        "    calib_total = 0\n",
        "    for r in results:\n",
        "        if r['L_star_calibration'] is not None:\n",
        "            calib_total += 1\n",
        "            if abs(r['L_star_empirical'] - r['L_star_calibration']) < 2:\n",
        "                calib_matches += 1\n",
        "    \n",
        "    if overall_mape < 10:\n",
        "        verdict = \"FORMULA VALIDATED\"\n",
        "    elif overall_mape < 15:\n",
        "        verdict = \"PARTIAL GENERALIZATION\"\n",
        "    else:\n",
        "        verdict = \"CALIBRATION NEEDED\"\n",
        "    \n",
        "    print(\"\\n\" + \"#\"*80)\n",
        "    print(\"#\" + \" \"*30 + \"FINAL VERDICT\" + \" \"*33 + \"#\")\n",
        "    print(\"#\"*80)\n",
        "    print(f\"\"\"\n",
        "    L* Definition: SIGN CHANGE (d/dl[Tr] changes from + to -)\n",
        "    \n",
        "    Formula: L* = L × (0.11 + 0.012×L + 4.9/H)\n",
        "    \n",
        "    Models tested:       {len(results)}\n",
        "    Labs tested:         {len(set(r['lab'] for r in results))}\n",
        "    Overall MAPE:        {overall_mape:.1f}%\n",
        "    \n",
        "    Calibration check:   {calib_matches}/{calib_total} matches (tolerance ±2)\n",
        "    \n",
        "    v2 (argmax):         15.7% MAPE\n",
        "    v3 (sign change):    {overall_mape:.1f}% MAPE\n",
        "    \n",
        "    VERDICT: {verdict}\n",
        "    \"\"\")\n",
        "    print(\"#\"*80)\n",
        "    \n",
        "    # Save\n",
        "    output = {\n",
        "        \"experiment\": \"L* Cross-Heritage Validation v3 (Sign Change)\",\n",
        "        \"timestamp\": TIMESTAMP,\n",
        "        \"l_star_definition\": \"Layer where d/dl[Tr(L_F)] changes sign (+ to -)\",\n",
        "        \"formula\": \"L* = L × (0.11 + 0.012×L + 4.9/H)\",\n",
        "        \"n_models\": len(results),\n",
        "        \"overall_mape\": float(overall_mape),\n",
        "        \"calibration_consistency\": f\"{calib_matches}/{calib_total}\",\n",
        "        \"comparison_v2_argmax\": 15.7,\n",
        "        \"improvement_pp\": float(15.7 - overall_mape),\n",
        "        \"verdict\": verdict,\n",
        "        \"results\": make_serializable(results)\n",
        "    }\n",
        "    \n",
        "    JSON_FILE = f\"l_star_sign_change_{TIMESTAMP}.json\"\n",
        "    with open(JSON_FILE, 'w') as f:\n",
        "        json.dump(output, f, indent=2)\n",
        "    print(f\"\\nSaved: {JSON_FILE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-16",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download\n",
        "try:\n",
        "    from google.colab import files\n",
        "    if 'JSON_FILE' in dir():\n",
        "        files.download(JSON_FILE)\n",
        "    if 'PNG_FILE' in dir():\n",
        "        files.download(PNG_FILE)\n",
        "    print(\"Downloads started!\")\n",
        "except:\n",
        "    print(\"Files saved locally.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
