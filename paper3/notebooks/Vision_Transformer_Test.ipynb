{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer Sheaf Laplacian Test\n",
    "\n",
    "**Paper 3 - Future Work Exploration**\n",
    "\n",
    "## Goal\n",
    "Test whether the Sheaf Neural Network framework (H4) applies to Vision Transformers.\n",
    "\n",
    "## Hypothesis\n",
    "If the thermodynamic constraints are universal, ViT models should also show:\n",
    "- DAMPEN or EXPAND signatures in Tr(Δ_F)\n",
    "- Layer-wise transition at L*\n",
    "\n",
    "## Key Difference from LLMs\n",
    "- ViT uses image patches as tokens\n",
    "- CLS token aggregates global information\n",
    "- Pre-trained on ImageNet, not text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "!pip install -q transformers torch timm pillow matplotlib numpy\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import ViTModel, ViTImageProcessor, AutoConfig\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Vision Transformer Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ViT models to test\n",
    "VIT_MODELS = [\n",
    "    \"google/vit-base-patch16-224\",     # 12 layers, 768 dim\n",
    "    \"google/vit-large-patch16-224\",    # 24 layers, 1024 dim\n",
    "    \"facebook/deit-base-patch16-224\",  # DeiT variant\n",
    "]\n",
    "\n",
    "print(f\"Testing {len(VIT_MODELS)} ViT models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a test image\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "response = requests.get(url)\n",
    "image = Image.open(BytesIO(response.content))\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(image)\n",
    "plt.title(\"Test Image\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sheaf Laplacian Computation for ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sheaf_trace_vit(model, processor, image, layer_idx):\n",
    "    \"\"\"Compute Sheaf Laplacian trace for a ViT layer.\n",
    "    \n",
    "    Formula: Tr(Δ_F) = (Σ A_ij - n) × ||W_V||_F²\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Process image\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Get attention weights\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_attentions=True)\n",
    "    \n",
    "    # Attention from specified layer, average over heads\n",
    "    attn = outputs.attentions[layer_idx][0].mean(dim=0).cpu().numpy()\n",
    "    n = attn.shape[0]\n",
    "    \n",
    "    # Get W_V from state dict\n",
    "    state_dict = model.state_dict()\n",
    "    w_v_key = f\"encoder.layer.{layer_idx}.attention.attention.value.weight\"\n",
    "    \n",
    "    if w_v_key in state_dict:\n",
    "        W_V = state_dict[w_v_key].cpu().numpy()\n",
    "        W_V_frob_sq = (W_V ** 2).sum()\n",
    "    else:\n",
    "        # Fallback\n",
    "        W_V_frob_sq = 1.0\n",
    "    \n",
    "    # Efficient trace computation\n",
    "    off_diag_sum = attn.sum() - np.trace(attn)\n",
    "    trace = off_diag_sum * W_V_frob_sq\n",
    "    \n",
    "    return trace, attn, n\n",
    "\n",
    "def analyze_vit_model(model_name, image):\n",
    "    \"\"\"Full analysis of a ViT model.\"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "    n_layers = config.num_hidden_layers\n",
    "    \n",
    "    processor = ViTImageProcessor.from_pretrained(model_name)\n",
    "    model = ViTModel.from_pretrained(model_name, output_attentions=True).to(device)\n",
    "    \n",
    "    print(f\"Layers: {n_layers}, Hidden: {config.hidden_size}, Heads: {config.num_attention_heads}\")\n",
    "    \n",
    "    # Compute trace for each layer\n",
    "    traces = []\n",
    "    for layer_idx in range(n_layers):\n",
    "        trace, _, _ = compute_sheaf_trace_vit(model, processor, image, layer_idx)\n",
    "        traces.append(trace)\n",
    "        print(f\"  Layer {layer_idx}: Tr(Δ_F) = {trace:,.0f}\")\n",
    "    \n",
    "    # Classify signature\n",
    "    first_half = np.mean(traces[:n_layers//2])\n",
    "    second_half = np.mean(traces[n_layers//2:])\n",
    "    \n",
    "    if second_half > first_half * 1.2:\n",
    "        signature = \"EXPAND\"\n",
    "    elif first_half > second_half * 1.2:\n",
    "        signature = \"DAMPEN\"\n",
    "    else:\n",
    "        signature = \"STABLE\"\n",
    "    \n",
    "    print(f\"\\nSignature: {signature}\")\n",
    "    print(f\"First half mean: {first_half:,.0f}\")\n",
    "    print(f\"Second half mean: {second_half:,.0f}\")\n",
    "    \n",
    "    # Cleanup\n",
    "    del model\n",
    "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "    \n",
    "    return {\n",
    "        'model': model_name.split('/')[-1],\n",
    "        'n_layers': n_layers,\n",
    "        'traces': traces,\n",
    "        'signature': signature,\n",
    "        'first_half_mean': first_half,\n",
    "        'second_half_mean': second_half\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run analysis on all ViT models\n",
    "results = []\n",
    "\n",
    "for model_name in VIT_MODELS:\n",
    "    try:\n",
    "        result = analyze_vit_model(model_name, image)\n",
    "        results.append(result)\n",
    "    except Exception as e:\n",
    "        print(f\"Error with {model_name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot layer-wise traces\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "colors = ['blue', 'red', 'green', 'purple']\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    layers = range(result['n_layers'])\n",
    "    ax.plot(layers, result['traces'], 'o-', \n",
    "            label=f\"{result['model']} ({result['signature']})\",\n",
    "            color=colors[i % len(colors)], linewidth=2, markersize=6)\n",
    "\n",
    "ax.set_xlabel('Layer', fontsize=12)\n",
    "ax.set_ylabel('Tr(Δ_F)', fontsize=12)\n",
    "ax.set_title('Vision Transformer Sheaf Laplacian Traces', fontsize=14)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('vit_sheaf_traces.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "import pandas as pd\n",
    "\n",
    "summary = pd.DataFrame([{\n",
    "    'Model': r['model'],\n",
    "    'Layers': r['n_layers'],\n",
    "    'Signature': r['signature'],\n",
    "    'First Half': f\"{r['first_half_mean']:,.0f}\",\n",
    "    'Second Half': f\"{r['second_half_mean']:,.0f}\",\n",
    "    'Ratio': f\"{r['second_half_mean']/r['first_half_mean']:.2f}x\"\n",
    "} for r in results])\n",
    "\n",
    "print(\"\\nViT Sheaf Analysis Summary:\")\n",
    "print(summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conclusions\n",
    "\n",
    "### Questions Answered\n",
    "\n",
    "1. **Do ViTs show thermodynamic signatures?**\n",
    "   - Run the notebook to find out!\n",
    "\n",
    "2. **Is the pattern consistent with LLMs?**\n",
    "   - Compare signatures with GPT-2, Pythia, OPT\n",
    "\n",
    "3. **Does training objective matter?**\n",
    "   - ViT: classification vs LLM: next-token prediction\n",
    "\n",
    "### Future Extensions\n",
    "\n",
    "- Test CLIP vision encoder\n",
    "- Test Segment Anything Model (SAM)\n",
    "- Test multimodal models (LLaVA, etc.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
