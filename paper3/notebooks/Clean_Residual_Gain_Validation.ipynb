{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Clean Residual Gain Validation\n",
        "\n",
        "**Problem:** Previous experiments showed INCONSISTENT results for GPT-J:\n",
        "- Beautiful Ones: G = 1.058 (EXPANSION)\n",
        "- High-œÅ Hunt: G = 0.368 (DAMPENING)\n",
        "\n",
        "**Goal:** Run a CLEAN, reproducible test with:\n",
        "- Same prompts\n",
        "- Same methodology\n",
        "- Same models\n",
        "- Clear documentation of what's measured\n",
        "\n",
        "**Key Metric:**\n",
        "$$G = \\frac{||h_L||}{||h_{L-1}||} = \\text{Last Layer Residual Gain}$$\n",
        "\n",
        "Where $h_L$ = hidden state after final transformer block."
      ],
      "metadata": {
        "id": "intro"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch numpy --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
        "import json\n",
        "from datetime import datetime\n",
        "import gc\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# FIXED seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ],
      "metadata": {
        "id": "setup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EXACTLY the same prompts for ALL experiments\n",
        "CANONICAL_PROMPTS = [\n",
        "    \"The capital of France is\",\n",
        "    \"Water freezes at\",\n",
        "    \"The quick brown fox\",\n",
        "]\n",
        "\n",
        "# Models to compare\n",
        "MODELS = [\n",
        "    ('pythia-6.9b', 'EleutherAI/pythia-6.9b'),\n",
        "    ('gpt-j-6b', 'EleutherAI/gpt-j-6B'),\n",
        "]\n",
        "\n",
        "# Fallback for smaller GPUs\n",
        "FALLBACK_MODELS = [\n",
        "    ('pythia-1.4b', 'EleutherAI/pythia-1.4b'),\n",
        "    ('pythia-410m', 'EleutherAI/pythia-410m'),\n",
        "]\n",
        "\n",
        "print(f\"Prompts: {CANONICAL_PROMPTS}\")"
      ],
      "metadata": {
        "id": "prompts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_residual_gain_clean(model, tokenizer, prompts, verbose=True):\n",
        "    \"\"\"\n",
        "    CLEAN implementation of residual gain computation.\n",
        "    \n",
        "    Returns:\n",
        "        - last_layer_gain: ||h_L|| / ||h_{L-1}|| for last layer\n",
        "        - all_layer_gains: list of gains for each layer\n",
        "        - per_prompt_gains: dict of prompt -> last_layer_gain\n",
        "    \"\"\"\n",
        "    all_layer_gains = []\n",
        "    last_layer_gains = []\n",
        "    per_prompt = {}\n",
        "    \n",
        "    for prompt in prompts:\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "        if torch.cuda.is_available():\n",
        "            inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs, output_hidden_states=True)\n",
        "        \n",
        "        hidden_states = outputs.hidden_states\n",
        "        n_layers = len(hidden_states) - 1  # Exclude embedding layer\n",
        "        \n",
        "        if verbose:\n",
        "            print(f\"\\n  Prompt: '{prompt[:30]}...'\")\n",
        "            print(f\"  Hidden states: {len(hidden_states)} (1 embed + {n_layers} layers)\")\n",
        "        \n",
        "        # Compute gain for each layer transition\n",
        "        layer_gains = []\n",
        "        for i in range(1, len(hidden_states)):\n",
        "            h_curr = hidden_states[i][:, -1, :].float()  # Last token\n",
        "            h_prev = hidden_states[i-1][:, -1, :].float()\n",
        "            \n",
        "            norm_curr = torch.norm(h_curr, dim=-1).item()\n",
        "            norm_prev = torch.norm(h_prev, dim=-1).item()\n",
        "            \n",
        "            gain = norm_curr / (norm_prev + 1e-10)\n",
        "            layer_gains.append(gain)\n",
        "            \n",
        "            if verbose and (i == 1 or i == len(hidden_states) - 1):\n",
        "                layer_name = \"Layer 0\" if i == 1 else f\"Layer {i-1} (LAST)\"\n",
        "                print(f\"    {layer_name}: ||h||={norm_curr:.2f}, gain={gain:.4f}\")\n",
        "        \n",
        "        all_layer_gains.append(layer_gains)\n",
        "        last_layer_gain = layer_gains[-1]\n",
        "        last_layer_gains.append(last_layer_gain)\n",
        "        per_prompt[prompt] = last_layer_gain\n",
        "    \n",
        "    # Aggregate\n",
        "    mean_last_gain = np.mean(last_layer_gains)\n",
        "    std_last_gain = np.std(last_layer_gains)\n",
        "    \n",
        "    # Average across prompts, per layer\n",
        "    avg_layer_gains = np.mean(all_layer_gains, axis=0)\n",
        "    \n",
        "    return {\n",
        "        'last_layer_gain_mean': float(mean_last_gain),\n",
        "        'last_layer_gain_std': float(std_last_gain),\n",
        "        'all_layer_gains': avg_layer_gains.tolist(),\n",
        "        'per_prompt_gains': per_prompt,\n",
        "        'is_dampening': mean_last_gain < 1.0,\n",
        "        'n_layers': n_layers\n",
        "    }"
      ],
      "metadata": {
        "id": "compute"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_rho(config):\n",
        "    \"\"\"Compute head density œÅ = n_heads / d_head.\"\"\"\n",
        "    n_heads = getattr(config, 'num_attention_heads', None) or \\\n",
        "              getattr(config, 'n_head', None)\n",
        "    d_model = getattr(config, 'hidden_size', None) or \\\n",
        "              getattr(config, 'n_embd', None)\n",
        "    \n",
        "    if n_heads and d_model:\n",
        "        d_head = d_model // n_heads\n",
        "        rho = n_heads / d_head\n",
        "        return {\n",
        "            'n_heads': n_heads,\n",
        "            'd_model': d_model,\n",
        "            'd_head': d_head,\n",
        "            'rho': rho\n",
        "        }\n",
        "    return None"
      ],
      "metadata": {
        "id": "rho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select models based on GPU memory\n",
        "if torch.cuda.is_available():\n",
        "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    if gpu_mem >= 20:\n",
        "        models_to_test = MODELS\n",
        "        print(f\"‚úÖ GPU has {gpu_mem:.1f}GB - testing full models\")\n",
        "    else:\n",
        "        models_to_test = FALLBACK_MODELS\n",
        "        print(f\"‚ö†Ô∏è GPU has {gpu_mem:.1f}GB - using fallback models\")\n",
        "else:\n",
        "    models_to_test = FALLBACK_MODELS\n",
        "    print(\"‚ö†Ô∏è No GPU - using fallback models\")\n",
        "\n",
        "print(f\"\\nModels to test: {[m[0] for m in models_to_test]}\")"
      ],
      "metadata": {
        "id": "select_models"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run clean validation\n",
        "results = {}\n",
        "\n",
        "for name, path in models_to_test:\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Testing: {name}\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    try:\n",
        "        # Load config first\n",
        "        config = AutoConfig.from_pretrained(path)\n",
        "        rho_info = compute_rho(config)\n",
        "        \n",
        "        if rho_info:\n",
        "            print(f\"\\nüìê Architecture:\")\n",
        "            print(f\"   n_heads = {rho_info['n_heads']}\")\n",
        "            print(f\"   d_head = {rho_info['d_head']}\")\n",
        "            print(f\"   œÅ = {rho_info['rho']:.4f}\")\n",
        "        \n",
        "        # Load model\n",
        "        tokenizer = AutoTokenizer.from_pretrained(path)\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            path,\n",
        "            torch_dtype=torch.float32,  # Use fp32 for precision!\n",
        "            device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        "            low_cpu_mem_usage=True\n",
        "        )\n",
        "        model.eval()\n",
        "        \n",
        "        print(f\"\\nüî¨ Computing residual gains...\")\n",
        "        result = compute_residual_gain_clean(model, tokenizer, CANONICAL_PROMPTS)\n",
        "        \n",
        "        # Add model info\n",
        "        result['model'] = name\n",
        "        result['path'] = path\n",
        "        if rho_info:\n",
        "            result.update(rho_info)\n",
        "        \n",
        "        # Summary\n",
        "        status = \"DAMPENING üîµ\" if result['is_dampening'] else \"EXPANSION üî¥\"\n",
        "        print(f\"\\nüìä RESULT:\")\n",
        "        print(f\"   Last Layer Gain: {result['last_layer_gain_mean']:.4f} ¬± {result['last_layer_gain_std']:.4f}\")\n",
        "        print(f\"   Status: {status}\")\n",
        "        \n",
        "        results[name] = result\n",
        "        \n",
        "        # Cleanup\n",
        "        del model, tokenizer\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "print(f\"\\n\\n‚úÖ Tested {len(results)} models\")"
      ],
      "metadata": {
        "id": "main_test"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary Table\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CLEAN RESIDUAL GAIN VALIDATION - SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\n{'Model':<20} {'œÅ':>10} {'Last Gain':>12} {'Status':>15} {'H25 Pred':>10}\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "for name, r in results.items():\n",
        "    rho = r.get('rho', 0)\n",
        "    gain = r['last_layer_gain_mean']\n",
        "    status = \"DAMPEN\" if r['is_dampening'] else \"EXPAND\"\n",
        "    \n",
        "    # H25 prediction: œÅ ‚â• 0.2 ‚Üí should dampen\n",
        "    h25_pred = (rho >= 0.2 and r['is_dampening']) or (rho < 0.2 and not r['is_dampening'])\n",
        "    h25_marker = \"‚úÖ\" if h25_pred else \"‚ùå\"\n",
        "    \n",
        "    print(f\"{name:<20} {rho:>10.4f} {gain:>12.4f} {status:>15} {h25_marker:>10}\")"
      ],
      "metadata": {
        "id": "summary"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if len(results) >= 1:\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    # Plot 1: œÅ vs Last Layer Gain\n",
        "    ax = axes[0]\n",
        "    rhos = [r.get('rho', 0) for r in results.values()]\n",
        "    gains = [r['last_layer_gain_mean'] for r in results.values()]\n",
        "    names = list(results.keys())\n",
        "    colors = ['blue' if g < 1.0 else 'red' for g in gains]\n",
        "    \n",
        "    ax.scatter(rhos, gains, c=colors, s=200, edgecolors='black', linewidth=2, zorder=5)\n",
        "    for i, name in enumerate(names):\n",
        "        ax.annotate(name, (rhos[i], gains[i]), xytext=(5, 5), textcoords='offset points', fontsize=10)\n",
        "    \n",
        "    ax.axhline(y=1.0, color='black', linestyle='--', alpha=0.7, label='G=1.0 (Bentov)')\n",
        "    ax.axvline(x=0.2, color='purple', linestyle=':', alpha=0.7, label='œÅ=0.2')\n",
        "    ax.set_xlabel('œÅ = n_heads / d_head', fontsize=12)\n",
        "    ax.set_ylabel('Last Layer Residual Gain', fontsize=12)\n",
        "    ax.set_title('H25: Does œÅ predict Gain?', fontsize=14, fontweight='bold')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 2: Layer-by-Layer Dynamics\n",
        "    ax = axes[1]\n",
        "    for name, r in results.items():\n",
        "        layer_gains = r['all_layer_gains']\n",
        "        layers = list(range(len(layer_gains)))\n",
        "        color = 'blue' if r['is_dampening'] else 'red'\n",
        "        ax.plot(layers, layer_gains, '-o', markersize=2, label=f\"{name} (œÅ={r.get('rho', 0):.2f})\", color=color)\n",
        "    \n",
        "    ax.axhline(y=1.0, color='black', linestyle='--', alpha=0.5)\n",
        "    ax.set_xlabel('Layer', fontsize=12)\n",
        "    ax.set_ylabel('Residual Gain', fontsize=12)\n",
        "    ax.set_title('Layer-by-Layer Dynamics', fontsize=14, fontweight='bold')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('clean_residual_validation.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(\"\\nSaved: clean_residual_validation.png\")"
      ],
      "metadata": {
        "id": "visualization"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# H25 Verdict\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"H25 HYPOTHESIS VALIDATION\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nH25: œÅ = n_heads / d_head ‚â• 0.2 ‚Üí DAMPENING (G < 1.0)\")\n",
        "print(\"    œÅ < 0.2 ‚Üí EXPANSION (G ‚â• 1.0)\")\n",
        "\n",
        "correct = 0\n",
        "total = len(results)\n",
        "\n",
        "for name, r in results.items():\n",
        "    rho = r.get('rho', 0)\n",
        "    gain = r['last_layer_gain_mean']\n",
        "    \n",
        "    pred_dampen = rho >= 0.2\n",
        "    actual_dampen = gain < 1.0\n",
        "    \n",
        "    is_correct = pred_dampen == actual_dampen\n",
        "    if is_correct:\n",
        "        correct += 1\n",
        "    \n",
        "    print(f\"\\n  {name}:\")\n",
        "    print(f\"    œÅ = {rho:.4f} ‚Üí Predicted: {'DAMPEN' if pred_dampen else 'EXPAND'}\")\n",
        "    print(f\"    G = {gain:.4f} ‚Üí Actual: {'DAMPEN' if actual_dampen else 'EXPAND'}\")\n",
        "    print(f\"    {'‚úÖ CORRECT' if is_correct else '‚ùå WRONG'}\")\n",
        "\n",
        "accuracy = 100 * correct / total if total > 0 else 0\n",
        "print(f\"\\nüìä H25 Accuracy: {correct}/{total} = {accuracy:.1f}%\")\n",
        "\n",
        "if accuracy >= 75:\n",
        "    print(\"\\n‚úÖ H25 VALIDATED\")\n",
        "elif accuracy >= 50:\n",
        "    print(\"\\n‚ö†Ô∏è H25 PARTIALLY VALIDATED\")\n",
        "else:\n",
        "    print(\"\\n‚ùå H25 FALSIFIED\")"
      ],
      "metadata": {
        "id": "verdict"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save results\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "output = {\n",
        "    'experiment': 'Clean Residual Gain Validation',\n",
        "    'date': datetime.now().isoformat(),\n",
        "    'prompts': CANONICAL_PROMPTS,\n",
        "    'precision': 'float32',\n",
        "    'methodology': 'hidden_states[-1] / hidden_states[-2], last token, L2 norm',\n",
        "    'results': results\n",
        "}\n",
        "\n",
        "filename = f'clean_residual_validation_{timestamp}.json'\n",
        "with open(filename, 'w') as f:\n",
        "    json.dump(output, f, indent=2, default=str)\n",
        "\n",
        "print(f\"\\nSaved: {filename}\")"
      ],
      "metadata": {
        "id": "save"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download(filename)\n",
        "    files.download('clean_residual_validation.png')\n",
        "except ImportError:\n",
        "    print(\"Not in Colab\")"
      ],
      "metadata": {
        "id": "download"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final Summary\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CLEAN VALIDATION COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nüìÅ Files:\")\n",
        "print(f\"   ‚Ä¢ {filename}\")\n",
        "print(f\"   ‚Ä¢ clean_residual_validation.png\")\n",
        "print(f\"\\nüìä Key Finding:\")\n",
        "\n",
        "for name, r in results.items():\n",
        "    status = \"DAMPENING\" if r['is_dampening'] else \"EXPANSION\"\n",
        "    print(f\"   {name}: G = {r['last_layer_gain_mean']:.4f} ({status})\")"
      ],
      "metadata": {
        "id": "final"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
