% References for: Thermodynamic Constraints in Transformer Architectures
% v3.8 FINAL

@article{elhage2021mathematical,
  title={A Mathematical Framework for Transformer Circuits},
  author={Elhage, Nelson and Nanda, Neel and Olsson, Catherine and others},
  journal={Anthropic},
  year={2021},
  url={https://transformer-circuits.pub/2021/framework/index.html}
}

@article{olsson2022context,
  title={In-context Learning and Induction Heads},
  author={Olsson, Catherine and Elhage, Nelson and Nanda, Neel and others},
  journal={Anthropic},
  year={2022},
  url={https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html}
}

@inproceedings{bodnar2022neural,
  title={Neural Sheaf Diffusion: A Topological Perspective on Heterophily and Oversmoothing in {GNN}s},
  author={Bodnar, Cristian and Di Giovanni, Francesco and Chamberlain, Benjamin Paul and others},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}

@article{hansen2019toward,
  title={Toward a Spectral Theory of Cellular Sheaves},
  author={Hansen, Jakob and Ghrist, Robert},
  journal={Journal of Applied and Computational Topology},
  volume={3},
  number={4},
  pages={315--358},
  year={2019}
}

@article{ayzenberg2025sheaf,
  title={Sheaf Theory: From Deep Geometry to Deep Learning},
  author={Ayzenberg, Anton and Magai, German},
  journal={arXiv preprint arXiv:2501.XXXXX},
  year={2025}
}

@unpublished{gardner2024transformers,
  title={Transformers are Sheaves: A Unified Framework for Attention via Algebraic Topology},
  author={Gardner, R.},
  note={Unpublished manuscript},
  year={2024}
}

@misc{delia2025uniformity,
  title={Uniformity Asymmetry: An Exploratory Metric for Detecting Representational Preferences},
  author={D'Elia, Davide},
  year={2025},
  publisher={Zenodo},
  doi={10.5281/zenodo.18110161}
}

@misc{delia2026layerwise,
  title={Layer-wise Embedding-Output Dynamics Across {LLM} Families: Evidence for Phase-Structured Decision Commitment},
  author={D'Elia, Davide},
  year={2026},
  publisher={Zenodo},
  doi={10.5281/zenodo.18142454}
}

@misc{robinson2025geometric,
  title={The Geometric Structure of Reasoning in Large Language Models},
  author={Robinson, R.},
  year={2025},
  publisher={Zenodo},
  doi={10.5281/zenodo.18157610}
}

@inproceedings{ethayarajh2019contextual,
  title={How Contextual are Contextualized Word Representations? {C}omparing the Geometry of {BERT}, {ELM}o, and {GPT}-2 Embeddings},
  author={Ethayarajh, Kawin},
  booktitle={Proceedings of EMNLP-IJCNLP},
  year={2019}
}

@inproceedings{dong2021attention,
  title={Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth},
  author={Dong, Yihe and Cordonnier, Jean-Baptiste and Loukas, Andreas},
  booktitle={International Conference on Machine Learning},
  year={2021}
}

@inproceedings{voita2019analyzing,
  title={Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned},
  author={Voita, Elena and Talbot, David and Moiseev, Fedor and Sennrich, Rico and Titov, Ivan},
  booktitle={Proceedings of ACL},
  year={2019}
}

% Model references

@article{biderman2023pythia,
  title={Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling},
  author={Biderman, Stella and others},
  journal={arXiv preprint arXiv:2304.01373},
  year={2023}
}

@article{zhang2022opt,
  title={{OPT}: Open Pre-trained Transformer Language Models},
  author={Zhang, Susan and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}

@article{touvron2023llama,
  title={{LLaMA}: Open and Efficient Foundation Language Models},
  author={Touvron, Hugo and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{jiang2023mistral,
  title={Mistral 7{B}},
  author={Jiang, Albert Q. and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@article{team2024gemma,
  title={Gemma: Open Models Based on {G}emini Research and Technology},
  author={Gemma Team},
  journal={arXiv preprint arXiv:2403.08295},
  year={2024}
}

@article{workshop2023bloom,
  title={{BLOOM}: A 176{B}-Parameter Open-Access Multilingual Language Model},
  author={BigScience Workshop},
  journal={arXiv preprint arXiv:2211.05100},
  year={2023}
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal={OpenAI Blog},
  year={2019}
}
