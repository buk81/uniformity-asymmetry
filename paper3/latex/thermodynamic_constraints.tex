% Thermodynamic Constraints in Transformer Architectures: A Sheaf-Theoretic Perspective
% v3.8 FINAL - LaTeX Skeleton for NeurIPS/ICLR Submission
% Author: Davide D'Elia

\documentclass{article}

% ============================================
% CHOOSE YOUR VENUE (uncomment one):
% ============================================
% \usepackage{neurips_2024}        % NeurIPS 2024
% \usepackage{iclr2025_conference} % ICLR 2025
\usepackage[preprint]{neurips_2024} % Preprint mode (no anonymization)

% ============================================
% PACKAGES
% ============================================
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}
\usepackage{enumitem}

% ============================================
% MATH COMMANDS
% ============================================
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Tr}{\text{Tr}}
\newcommand{\norm}[1]{\|#1\|}
\newcommand{\sheaf}{\mathcal{F}}
\newcommand{\laplacian}{L_\sheaf}
\newcommand{\coboundary}{\delta}

\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}

% ============================================
% TITLE AND AUTHORS
% ============================================
\title{Thermodynamic Constraints in Transformer Architectures:\\A Sheaf-Theoretic Perspective}

\author{
  Davide D'Elia\thanks{This is an independent research project conducted in the author's personal capacity. The institutional affiliation is provided for identification purposes only; this work was not conducted under the auspices of, funded by, or otherwise affiliated with IU International University of Applied Sciences.}\\
  IU International University of Applied Sciences\\
  \texttt{davide.delia@iu-study.org}
}

\begin{document}

\maketitle

% ============================================
% ABSTRACT
% ============================================
\begin{abstract}
We present empirical evidence for thermodynamic-like constraints governing information flow in transformer architectures. Analysis of residual stream dynamics across 23+ models from 7 labs (2022--2024) reveals three scaling laws: (1) \textbf{Kleiber's Law for Transformers} ($r = -0.81$, $p = 0.014$; Pythia family)---maximum gain scales as $G_{\max} = 10^{1/L}$; (2) \textbf{Training Heritage Dominance} ($p < 0.001$)---training methodology determines thermodynamic behavior more than architecture (EleutherAI: 80\% dampening vs.\ Meta/OpenAI: 100\% expansion); (3) \textbf{Spectral Signature Correspondence}---$\norm{W_V}/\norm{W_O}$ predicts dampening/expansion with 10$\times$ magnitude differences between labs.

These regularities arise from sheaf-theoretic constraints on consistent information transport. We compute the \textbf{full-scale Sheaf Laplacian} via an $O(n^2 + d^2)$ algorithm, proving \textbf{multi-head block-diagonal structure}. GPT-2 exhibits \textbf{26$\times$ higher trace proxy} than OPT-125m, directly discriminating thermodynamic behavior. Three additional contributions: (4) \textbf{Dimensional Crowding}---head density $\rho = H/d_{\text{head}}$ mechanistically explains the Pythia anomaly; (5) \textbf{Thermodynamic Invariance}---RLHF modulates magnitude but cannot invert sign; (6) \textbf{Unified cross-architecture benchmark} (100 measurements) establishes the hierarchy Pythia (0.80) $<$ Mistral (1.11) $<$ LLaMA (1.48) $<$ Gemma (2.31).

\textbf{Core finding}: Thermodynamic character is determined by pretraining geometry ($\rho$, heritage) and cannot be overwritten by fine-tuning. The hierarchy is Heritage $>$ Geometry $>$ Scale.
\end{abstract}

% ============================================
% 1. INTRODUCTION
% ============================================
\section{Introduction}
\label{sec:intro}

The internal dynamics of transformer architectures remain incompletely understood despite their remarkable empirical success. While mechanistic interpretability has revealed individual circuit-level behaviors \citep{elhage2021mathematical,olsson2022context}, a unified framework explaining \emph{why} certain architectural choices lead to characteristic dynamical signatures has been lacking.

In this work, we report the discovery of thermodynamic-like constraints that appear to govern transformer behavior at the macro scale. These constraints manifest as predictable relationships between architectural parameters, training provenance, and the evolution of representation norms through the network.

\subsection{Motivating Observations}

Our investigation began with three puzzling empirical observations:

\begin{enumerate}[leftmargin=*]
    \item \textbf{Uniformity Asymmetry} \citep{delia2025uniformity}: Language models exhibit systematic differences in embedding uniformity when processing factual versus counterfactual statements.
    \item \textbf{Phase-Structured Dynamics} \citep{delia2026layerwise}: The embedding-output correlation follows a characteristic three-phase pattern across model families.
    \item \textbf{Architecture-Dependent Expansion}: Some model families consistently expand representation norms while others consistently dampen them.
\end{enumerate}

\subsection{Contributions}

We make the following contributions:

\begin{enumerate}[leftmargin=*]
    \item \textbf{Empirical Laws}: Three quantitative laws governing transformer thermodynamics, validated across 23 models from 7 labs (all $p < 0.05$).
    \item \textbf{Methodological Correction}: Identification of the final LayerNorm artifact.
    \item \textbf{Theoretical Framework}: Sheaf-theoretic interpretation of observed constraints.
    \item \textbf{Mechanistic Bridge}: Spectral properties predict macroscopic behavior.
    \item \textbf{Sheaf Laplacian Validation}: $O(n^2 + d^2)$ trace computation with 26$\times$ discriminating power.
    \item \textbf{Dimensional Crowding Theory}: Head density $\rho$ as mechanistic driver.
    \item \textbf{Thermodynamic Invariance}: RLHF cannot invert sign.
    \item \textbf{Unified Benchmark}: Definitive thermodynamic hierarchy.
\end{enumerate}

\subsection{Scope and Limitations}

We do not claim that transformers \emph{are} sheaf networks in any implementation sense. The sheaf framework is used as an explanatory lens and organizing principle; \textbf{none of the empirical claims depend on assuming transformers are explicitly implemented as sheaf networks}. We remain agnostic about deeper architectural implications.

% ============================================
% 2. BACKGROUND
% ============================================
\section{Background and Related Work}
\label{sec:background}

\subsection{Residual Stream Dynamics}

The residual stream framework \citep{elhage2021mathematical} models transformer computation as:
\begin{equation}
    x^{(\ell+1)} = x^{(\ell)} + \text{Attn}^{(\ell)}(x^{(\ell)}) + \text{FFN}^{(\ell)}(x^{(\ell)})
\end{equation}

\subsection{Sheaf Neural Networks}

Sheaf neural networks \citep{bodnar2022neural,hansen2019toward} generalize GNNs by replacing scalar edge weights with linear maps. The sheaf Laplacian $\laplacian = \coboundary^\top \coboundary$ measures local inconsistency.

\subsection{Categorical Perspectives on Transformers}

\citet{gardner2024transformers} proposes that ``Transformers are Sheaves.'' Our work provides the first empirical validation through direct Laplacian computation.

% ============================================
% 3. METHODS
% ============================================
\section{Methods}
\label{sec:methods}

\subsection{Residual Stream Measurement}

We define the \textbf{residual gain} $G$ at layer $\ell$ as:
\begin{equation}
    G^{(\ell)} = \frac{\norm{x^{(\ell)}}_2}{\norm{x^{(\ell-1)}}_2}
\end{equation}

\textbf{Critical}: We exclude the final LayerNorm from gain calculations.

\subsection{Spectral Analysis}

For each attention head, we extract $W_V$ (value projection) and $W_O$ (output projection), computing spectral norms $\norm{W}_2 = \sigma_{\max}(W)$.

\subsection{Model Selection}

We analyze models across 7 independent labs: EleutherAI (Pythia, GPT-Neo, GPT-J), Meta (OPT, LLaMA), BigScience (BLOOM), OpenAI (GPT-2), Google (Gemma), Mistral AI, TII (Falcon), StabilityAI (StableLM).

% ============================================
% 4. RESULTS
% ============================================
\section{Results}
\label{sec:results}

\subsection{Kleiber's Law for Transformers}

Maximum gain per layer scales as $G_{\max} = 10^{1/L}$, ensuring total network gain remains bounded:
\begin{equation}
    G_{\text{total}} = G_{\max}^L = 10
\end{equation}

\textbf{Results} (Pythia family, 8 models): $r = -0.81$, $p = 0.014$.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{Figures/fig1_kleiber_law.png}
    \caption{\textbf{Kleiber's Law for Transformers.} Mean residual gain vs.\ network depth (L) for the Pythia family. Dampeners ($\blacktriangledown$, $G < 1$) cluster below neutrality; expanders ($\blacktriangle$, $G > 1$) above. Dashed: theoretical bound $G_{\max} = 10^{1/L}$. Correlation $r = -0.81$ ($p = 0.014$).}
    \label{fig:kleiber}
\end{figure}

\subsection{Training Heritage Dominance}

EleutherAI models show 80\% dampening; all other labs show 100\% expansion. Fisher's exact test: $p < 0.001$.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{Figures/fig2_training_heritage.png}
    \caption{\textbf{Training Heritage Determines Thermodynamic Signature.} Mean residual gain by lab. EleutherAI is the only lab with mean $G < 1$. Error bars: $\pm 1$ SE (shown only for $n > 1$).}
    \label{fig:heritage}
\end{figure}

\subsection{Spectral Signature Correspondence}

10$\times$ difference in $\norm{W_V}_2$ between EleutherAI and Meta models.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{Figures/fig3_spectral_signature.png}
    \caption{\textbf{Spectral Signatures Predict Thermodynamic Behavior.} Mean $\norm{W_V}_2$ vs.\ $\norm{W_O}_2$ (log--log). EleutherAI clusters high; Meta/BigScience/OpenAI cluster low. *GPT-Neo-125M is an architectural outlier.}
    \label{fig:spectral}
\end{figure}

% ============================================
% 5. THEORETICAL FRAMEWORK
% ============================================
\section{Theoretical Framework: The Sheaf Perspective}
\label{sec:theory}

\subsection{Transformers as Implicit Sheaf Networks}

\begin{definition}[Transformer Sheaf]
For a transformer processing $N$ tokens:
\begin{itemize}[leftmargin=*]
    \item \textbf{Base space}: Complete graph $K_N$
    \item \textbf{Stalks}: $\sheaf_i = \R^d$
    \item \textbf{Restriction maps}: $\rho_{ij} = \sqrt{A_{ij}} \cdot W_V$
\end{itemize}
\end{definition}

\subsection{Full-Scale Sheaf Laplacian Validation}

\subsubsection{Efficient Trace Computation}

The trace can be computed directly:
\begin{equation}
    \Tr(\laplacian) = \left(\sum_{i,j} A_{ij} - n\right) \cdot \norm{W_V}_F^2
\end{equation}
reducing complexity to $O(n^2 + d^2)$.

\subsubsection{Multi-Head Integration}

\begin{proposition}[Block-Diagonal Structure]
For $H$ attention heads, the total Sheaf Laplacian is block-diagonal:
\begin{equation}
    \Delta_\sheaf^{\text{total}} = \text{diag}(\Delta_\sheaf^{(1)}, \ldots, \Delta_\sheaf^{(H)})
\end{equation}
\end{proposition}

\begin{corollary}
Traces sum across heads: $\Tr(\Delta_\sheaf^{\text{total}}) = \sum_{h=1}^{H} \Tr(\Delta_\sheaf^{(h)})$
\end{corollary}

\subsubsection{Results}

GPT-2: 62,696 mean trace. OPT-125m: 2,368. \textbf{26$\times$ difference}.

\subsection{The $L^*$ Prediction Formula}

Architecture-aware formula:
\begin{equation}
    L^* = L \times \left(0.11 + 0.012 \cdot L + \frac{4.9}{H}\right)
\end{equation}
Within-heritage MAPE: 4.8\% (vs.\ 25\% for naive $L/2$); cross-heritage validation yields 15.7\% (Appendix Fig.~5).

\subsection{Dimensional Crowding Theory}

Head density $\rho = H/d_{\text{head}}$ explains the Pythia anomaly:
\begin{itemize}[leftmargin=*]
    \item Pythia-6.9B ($\rho = 0.25$): DAMPEN
    \item GPT-J-6B ($\rho = 0.0625$): EXPAND
\end{itemize}
4$\times$ difference in $\rho$ produces opposite behavior despite identical heritage.

\subsection{Thermodynamic Invariance Under Fine-Tuning}

RLHF modulates magnitude (up to 50\%) but cannot invert thermodynamic sign. Gemma: $2.32 \to 1.15$ (still $G > 1$).

% ============================================
% 6. DISCUSSION
% ============================================
\section{Discussion}
\label{sec:discussion}

\subsection{Implications for Architecture Design}

Thermodynamic properties are largely determined at training time. Practitioners seeking specific dynamical behaviors should focus on training data and optimization schedules.

\subsection{Limitations}

\begin{enumerate}[leftmargin=*]
    \item \textbf{Causality}: Correlation, not causation established.
    \item \textbf{Scope}: Autoregressive LMs only.
    \item \textbf{ALiBi}: Excluded from $L^*$ formula scope.
    \item \textbf{Mechanistic Gap}: Specific training choices not isolated.
\end{enumerate}

% ============================================
% 7. CONCLUSION
% ============================================
\section{Conclusion}
\label{sec:conclusion}

We have presented evidence for thermodynamic-like constraints governing transformer architectures, validated across 23+ models from 7 labs. The hierarchy is \textbf{Heritage $>$ Geometry $>$ Scale}.

\textbf{For Practitioners}: Thermodynamic character cannot be overwritten by fine-tuning. When selecting base models, consider the thermodynamic signature as a fixed constraint.

\textbf{For Researchers}: The critical threshold $\rho_{\text{crit}} \approx 0.15$--$0.20$ separates expansion from dampening regimes.

The practical implication is clear: \emph{how} you train matters more than \emph{what} you build---but \emph{what} you build determines \emph{what} fine-tuning can achieve.

% ============================================
% ACKNOWLEDGMENTS (camera-ready only)
% ============================================
% \begin{ack}
% We thank...
% \end{ack}

% ============================================
% REFERENCES
% ============================================
\bibliographystyle{plainnat}
\bibliography{references}

% ============================================
% APPENDIX
% ============================================
\appendix

\section{OpenTimestamps Verification}
\label{app:timestamps}

All experiments were conducted with fixed random seeds (PYTHONHASHSEED=42). Experimental results are timestamped on the Bitcoin blockchain via OpenTimestamps.

\section{The Final LayerNorm Artifact}
\label{app:layernorm}

Computing gain as $\norm{x^{(L)}}/\norm{x^{(L-1)}}$ includes the final LayerNorm effect. We compute gain as $\norm{x^{(L-1)}}/\norm{x^{(L-2)}}$ instead. Validation accuracy improved from 43.75\% to 100\%.

\section{Anisotropy Profile and Five-Phase Structure}
\label{app:anisotropy}

Analysis of layer-wise anisotropy reveals a five-phase structure rather than three phases.

\section{Restriction Maps Extraction}
\label{app:restriction}

We validate the sheaf framework by extracting restriction maps $\rho_{ij} = \sqrt{A_{ij}} \cdot W_V$ from Pythia attention layers.

\section{$L^*$ Definition Clarification}
\label{app:lstar}

\textbf{Operational Definition}: $L^* = \arg\max_\ell \left| \frac{d}{d\ell} \Tr(\laplacian^{(\ell)}) \right|$

\section{Supplementary Figures}
\label{app:figures}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.95\linewidth]{Figures/fig_a1_layer_dynamics.png}
    \caption{\textbf{Layer-wise Spectral Dynamics.} (A) All models. (B) Non-EleutherAI only.}
    \label{fig:a1}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{Figures/fig_a2_lstar_validation.png}
    \caption{\textbf{$L^*$ Formula Validation.} Predicted vs.\ empirical $L^*$ across 6 models from 4 labs. MAPE = 15.7\%.}
    \label{fig:a2}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{Figures/fig_a3_pythia_scaling.png}
    \caption{\textbf{Pythia Family Scaling.} Error bars: $\pm 1$ SD across 25 prompts.}
    \label{fig:a3}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.95\linewidth]{Figures/fig_a4_input_robustness.png}
    \caption{\textbf{Input Robustness.} Residual gain across 25 prompts per model. Diamonds: mean (red = expander, gray = dampener). Error bars: $\pm 1$ SD. Outliers clipped at boundary.}
    \label{fig:a4}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{Figures/fig_a5_sheaf_trace.png}
    \caption{\textbf{Sheaf Laplacian Trace Proxy.} Layer-wise trace proxy ($\norm{W_V}^2_2$, log scale). EleutherAI exceeds others by $\sim$300$\times$ in late layers.}
    \label{fig:a5}
\end{figure}

\end{document}
