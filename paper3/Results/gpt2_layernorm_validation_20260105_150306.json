{
  "experiment": "GPT-2 LayerNorm Validation",
  "purpose": "Confirm LayerNorm dampening hypothesis (n=2)",
  "date": "20260105_150306",
  "n_models": 3,
  "n_prompts": 25,
  "n_measurements": 75,
  "models_tested": [
    "GPT2-XL",
    "GPT2-Large",
    "GPT2-Medium"
  ],
  "reference_models": {
    "Pythia-6.9B": {
      "gain": 0.8,
      "norm": "LayerNorm"
    },
    "Mistral-7B": {
      "gain": 1.11,
      "norm": "RMSNorm"
    },
    "LLaMA-3.1-8B": {
      "gain": 1.48,
      "norm": "RMSNorm"
    },
    "Gemma-7B": {
      "gain": 2.31,
      "norm": "RMSNorm"
    }
  },
  "base_levels": {
    "GPT2-Large": 1.0143503074804847,
    "GPT2-Medium": 1.162664923518424,
    "GPT2-XL": 1.0190977287373257
  },
  "validation_results": [
    {
      "Model": "GPT2-XL",
      "Mean_Gain": 1.0190977287373257,
      "Std": 0.024887101619302975,
      "Min": 0.9808754084925053,
      "Max": 1.0729531538416486,
      "t_stat": 3.8368728165824506,
      "p_value": 0.9996025520187249,
      "Is_Dampening": false,
      "Status": "REJECTED"
    },
    {
      "Model": "GPT2-Large",
      "Mean_Gain": 1.0143503074804847,
      "Std": 0.025269299814823072,
      "Min": 0.9812333220192438,
      "Max": 1.0695610422002755,
      "t_stat": 2.839474695706995,
      "p_value": 0.9954713537256051,
      "Is_Dampening": false,
      "Status": "REJECTED"
    },
    {
      "Model": "GPT2-Medium",
      "Mean_Gain": 1.162664923518424,
      "Std": 0.0769518520354483,
      "Min": 1.0653597179335865,
      "Max": 1.3984791743841452,
      "t_stat": 10.569266314960919,
      "p_value": 0.9999999999175416,
      "Is_Dampening": false,
      "Status": "REJECTED"
    }
  ],
  "bentov_law_results": [
    {
      "Model": "GPT2-XL",
      "Bentov_Correlation": 0.04464254260988337,
      "p_value": 0.8321926108678621,
      "Direction": "Positive"
    },
    {
      "Model": "GPT2-Large",
      "Bentov_Correlation": 0.1513573149825564,
      "p_value": 0.4701598286422459,
      "Direction": "Positive"
    },
    {
      "Model": "GPT2-Medium",
      "Bentov_Correlation": 0.3706579891008606,
      "p_value": 0.06815135892407825,
      "Direction": "Positive"
    }
  ],
  "hypothesis_status": {
    "layernorm_dampening": "REJECTED",
    "bentov_inversion": "REJECTED"
  },
  "all_results": [
    {
      "Model": "GPT2-XL",
      "Norm_Type": "LayerNorm",
      "Params": "1.5B",
      "Role": "LayerNorm Validation (Large)",
      "Category": "Factual",
      "Complexity": 1,
      "Prompt": "The capital city of France is",
      "Prompt_Short": "The capital city of France is...",
      "Entropy": 5.387718175195759,
      "Last_Gain": 1.004626809782079,
      "Total_Amp": 25.023097335620037,
      "Top_Token": " the",
      "Top_Prob": 0.09879097626511388,
      "N_Layers": 48
    },
    {
      "Model": "GPT2-XL",
      "Norm_Type": "LayerNorm",
      "Params": "1.5B",
      "Role": "LayerNorm Validation (Large)",
      "Category": "Factual",
      "Complexity": 1,
      "Prompt": "The atomic number of oxygen is",
      "Prompt_Short": "The atomic number of oxygen is...",
      "Entropy": 4.629468734603852,
      "Last_Gain": 1.069378718131835,
      "Total_Amp": 18.11920080271079,
      "Top_Token": " 1",
      "Top_Prob": 0.10680879028550755,
      "N_Layers": 48
    },
    {
      "Model": "GPT2-XL",
      "Norm_Type": "LayerNorm",
      "Params": "1.5B",
      "Role": "LayerNorm Validation (Large)",
      "Category": "Factual",
      "Complexity": 1,
      "Prompt": "Water boils at a temperature of",
      "Prompt_Short": "Water boils at a temperature of...",
      "Entropy": 4.613632411067274,
      "Last_Gain": 1.0519784873978095,
      "Total_Amp": 19.877897696956374,
      "Top_Token": " 212",
      "Top_Prob": 0.1785133960976489,
      "N_Layers": 48
    },
    {
      "Model": "GPT2-XL",
      "Norm_Type": "LayerNorm",
      "Params": "1.5B",
      "Role": "LayerNorm Validation (Large)",
      "Category": "Factual",
      "Complexity": 1,
      "Prompt": "The largest planet in our solar system is",
      "Prompt_Short": "The largest planet in our solar system i...",
      "Entropy": 5.26506822613082,
      "Last_Gain": 1.01807247050444,
      "Total_Amp": 21.823380822959212,
      "Top_Token": " a",
      "Top_Prob": 0.08452952575325509,
      "N_Layers": 48
    },
    {
      "Model": "GPT2-XL",
      "Norm_Type": "LayerNorm",
      "Params": "1.5B",
      "Role": "LayerNorm Validation (Large)",
      "Category": "Factual",
      "Complexity": 1,
      "Prompt": "The currency used in Japan is",
      "Prompt_Short": "The currency used in Japan is...",
      "Entropy": 2.033446615024385,
      "Last_Gain": 1.004205119821513,
      "Total_Amp": 28.161894856920874,
      "Top_Token": " the",
      "Top_Prob": 0.46280285437680463,
      "N_Layers": 48
    },
    {
      "Model": "GPT2-XL",
      "Norm_Type": "LayerNorm",
      "Params": "1.5B",
      "Role": "LayerNorm Validation (Large)",
      "Category": "Syntactic",
      "Complexity": 5,
      "Prompt": "The agreement, which, notwithstanding the fact that it was signed only yesterday, effectively binds all parties immediately, stipulates that",
      "Prompt_Short": "The agreement, which, notwithstanding th...",
      "Entropy": 4.789225179130913,
      "Last_Gain": 0.9977766561267903,
      "Total_Amp": 99.12653799197376,
      "Top_Token": " the",
      "Top_Prob": 0.2909488947650553,
      "N_Layers": 48
    },
    {
      "Model": "GPT2-XL",
      "Norm_Type": "LayerNorm",
      "Params": "1.5B",
      "Role": "LayerNorm Validation (Large)",
      "Category": "Syntactic",
      "Complexity": 5,
      "Prompt": "Although the weather was extremely cold, and despite the fact that they had no coats, the children decided to",
      "Prompt_Short": "Although the weather was extremely cold,...",
      "Entropy": 4.5915251896443445,
      "Last_Gain": 1.0729531538416486,
      "Total_Amp": 34.07886899594449,
      "Top_Token": " go",
      "Top_Prob": 0.14498827133098072,
      "N_Layers": 48
    },
    {
      "Model": "GPT2-XL",
      "Norm_Type": "LayerNorm",
      "Params": "1.5B",
      "Role": "LayerNorm Validation (Large)",
      "Category": "Syntactic",
      "Complexity": 5,
      "Prompt": "The professor, having reviewed the complex derivation multiple times without finding the error, finally realized that",
      "Prompt_Short": "The professor, having reviewed the compl...",
      "Entropy": 3.2200939163213143,
      "Last_Gain": 1.0030687526362623,
      "Total_Amp": 117.03711553016457,
      "Top_Token": " the",
      "Top_Prob": 0.31439032148619644,
      "N_Layers": 48
    },
    {
      "Model": "GPT2-XL",
      "Norm_Type": "LayerNorm",
      "Params": "1.5B",
      "Role": "LayerNorm Validation (Large)",
      "Category": "Syntactic",
      "Complexity": 5,
      "Prompt": "To imply that such a fundamental shift in policy could occur without significant public debate is to suggest that",
      "Prompt_Short": "To imply that such a fundamental shift i...",
      "Entropy": 4.58248566283646,
      "Last_Gain": 1.0063093563667902,
      "Total_Amp": 39.12730743774777,
      "Top_Token": " the",
      "Top_Prob": 0.26193862792116607,
      "N_Layers": 48
    },
    {
      "Model": "GPT2-XL",
      "Norm_Type": "LayerNorm",
      "Params": "1.5B",
      "Role": "LayerNorm Validation (Large)",
      "Category": "Syntactic",
      "Complexity": 5,
      "Prompt": "Not only did the experiment fail to yield the expected results, but it also demonstrated that the initial hypothesis was",
      "Prompt_Short": "Not only did the experiment fail to yiel...",
      "Entropy": 3.8594279622859102,
      "Last_Gain": 1.0410171004850348,
      "Total_Amp": 32.63720600845086,
      "Top_Token": " wrong",
      "Top_Prob": 0.23213126918181054,
      "N_Layers": 48
    },
    {
      "Model": "GPT2-XL",
      "Norm_Type": "LayerNorm",
      "Params": "1.5B",
      "Role": "LayerNorm Validation (Large)",
      "Category": "Cliche",
      "Complexity": 2,
      "Prompt": "The true meaning of happiness is often found in",
      "Prompt_Short": "The true meaning of happiness is often f...",
      "Entropy": 3.7163273517160995,
      "Last_Gain": 0.9922877102585759,
      "Total_Amp": 73.99107867021294,
      "Top_Token": " the",
      "Top_Prob": 0.4829740728197461,
      "N_Layers": 48
    },
    {
      "Model": "GPT2-XL",
      "Norm_Type": "LayerNorm",
      "Params": "1.5B",
      "Role": "LayerNorm Validation (Large)",
      "Category": "Cliche",
      "Complexity": 2,
      "Prompt": "Actions speak louder than",
      "Prompt_Short": "Actions speak louder than...",
      "Entropy": 0.5501162457172208,
      "Last_Gain": 1.032340370505601,
      "Total_Amp": 21.61275399536736,
      "Top_Token": " words",
      "Top_Prob": 0.9447099874451493,
      "N_Layers": 48
    },
    {
      "Model": "GPT2-XL",
      "Norm_Type": "LayerNorm",
      "Params": "1.5B",
      "Role": "LayerNorm Validation (Large)",
      "Category": "Cliche",
      "Complexity": 2,
      "Prompt": "It is what it is, and we must simply",
      "Prompt_Short": "It is what it is, and we must simply...",
      "Entropy": 3.824352664352574,
      "Last_Gain": 1.0663151044139891,
      "Total_Amp": 24.29998976234545,
      "Top_Token": " accept",
      "Top_Prob": 0.1995375817402378,
      "N_Layers": 48
    },
    {
      "Model": "GPT2-XL",
      "Norm_Type": "LayerNorm",
      "Params": "1.5B",
      "Role": "LayerNorm Validation (Large)",
      "Category": "Cliche",
      "Complexity": 2,
      "Prompt": "Time heals all",
      "Prompt_Short": "Time heals all...",
      "Entropy": 4.688300891807393,
      "Last_Gain": 1.0113682183552304,
      "Total_Amp": 15.32141497118411,
      "Top_Token": " wounds",
      "Top_Prob": 0.25788333085329157,
      "N_Layers": 48
    },
    {
      "Model": "GPT2-XL",
      "Norm_Type": "LayerNorm",
      "Params": "1.5B",
      "Role": "LayerNorm Validation (Large)",
      "Category": "Cliche",
      "Complexity": 2,
      "Prompt": "Life is a journey, not a",
      "Prompt_Short": "Life is a journey, not a...",
      "Entropy": 0.05307468644002415,
      "Last_Gain": 1.01181164751512,
      "Total_Amp": 15.152049960749096,
      "Top_Token": " destination",
      "Top_Prob": 0.9950234405313126,
      "N_Layers": 48
    },
    {
      "Model": "GPT2-XL",
      "Norm_Type": "LayerNorm",
      "Params": "1.5B",
      "Role": "LayerNorm Validation (Large)",
      "Category": "Novel",
      "Complexity": 4,
      "Prompt": "The epistemological implications of quantum decoherence suggest that the observer is",
      "Prompt_Short": "The epistemological implications of quan...",
      "Entropy": 4.251708788506583,
      "Last_Gain": 1.0050281438602506,
      "Total_Amp": 31.991065940556748,
      "Top_Token": " not",
      "Top_Prob": 0.2974606240055036,
      "N_Layers": 48
    },
    {
      "Model": "GPT2-XL",
      "Norm_Type": "LayerNorm",
      "Params": "1.5B",
      "Role": "LayerNorm Validation (Large)",
      "Category": "Novel",
      "Complexity": 4,
      "Prompt": "If consciousness creates reality, then the paradox of the unobserved electron implies",
      "Prompt_Short": "If consciousness creates reality, then t...",
      "Entropy": 1.1481849201148189,
      "Last_Gain": 1.0004166192559762,
      "Total_Amp": 82.00441453381335,
      "Top_Token": " that",
      "Top_Prob": 0.7801881372651642,
      "N_Layers": 48
    },
    {
      "Model": "GPT2-XL",
      "Norm_Type": "LayerNorm",
      "Params": "1.5B",
      "Role": "LayerNorm Validation (Large)",
      "Category": "Novel",
      "Complexity": 4,
      "Prompt": "The intersection of baroque architecture and cybernetic theory creates a space where",
      "Prompt_Short": "The intersection of baroque architecture...",
      "Entropy": 5.868348402786597,
      "Last_Gain": 1.0252388464052844,
      "Total_Amp": 36.54202024935589,
      "Top_Token": " the",
      "Top_Prob": 0.21157040863209473,
      "N_Layers": 48
    },
    {
      "Model": "GPT2-XL",
      "Norm_Type": "LayerNorm",
      "Params": "1.5B",
      "Role": "LayerNorm Validation (Large)",
      "Category": "Novel",
      "Complexity": 4,
      "Prompt": "Calculating the trajectory of a hyperspace jump requires factoring in the variability of",
      "Prompt_Short": "Calculating the trajectory of a hyperspa...",
      "Entropy": 3.869191454303958,
      "Last_Gain": 1.0151885010808734,
      "Total_Amp": 33.863992108745066,
      "Top_Token": " the",
      "Top_Prob": 0.4032034090468408,
      "N_Layers": 48
    },
    {
      "Model": "GPT2-XL",
      "Norm_Type": "LayerNorm",
      "Params": "1.5B",
      "Role": "LayerNorm Validation (Large)",
      "Category": "Novel",
      "Complexity": 4,
      "Prompt": "The symbiotic relationship between fungal mycelium and digital neural networks results in",
      "Prompt_Short": "The symbiotic relationship between funga...",
      "Entropy": 4.856134691358718,
      "Last_Gain": 1.0034837651663095,
      "Total_Amp": 71.20283638756538,
      "Top_Token": " the",
      "Top_Prob": 0.20019727120985456,
      "N_Layers": 48
    },
    {
      "Model": "GPT2-XL",
      "Norm_Type": "LayerNorm",
      "Params": "1.5B",
      "Role": "LayerNorm Validation (Large)",
      "Category": "Nonsense",
      "Complexity": 3,
      "Prompt": "Table sky run blue jump quickly under over",
      "Prompt_Short": "Table sky run blue jump quickly under ov...",
      "Entropy": 7.033677247034458,
      "Last_Gain": 1.0248634220607145,
      "Total_Amp": 20.27195495761905,
      "Top_Token": " jump",
      "Top_Prob": 0.04095270720929658,
      "N_Layers": 48
    },
    {
      "Model": "GPT2-XL",
      "Norm_Type": "LayerNorm",
      "Params": "1.5B",
      "Role": "LayerNorm Validation (Large)",
      "Category": "Nonsense",
      "Complexity": 3,
      "Prompt": "Purple idea furiously sleep colorless green",
      "Prompt_Short": "Purple idea furiously sleep colorless gr...",
      "Entropy": 7.394900273166593,
      "Last_Gain": 1.0365968038797981,
      "Total_Amp": 23.73472511889799,
      "Top_Token": " idea",
      "Top_Prob": 0.05834887994917714,
      "N_Layers": 48
    },
    {
      "Model": "GPT2-XL",
      "Norm_Type": "LayerNorm",
      "Params": "1.5B",
      "Role": "LayerNorm Validation (Large)",
      "Category": "Nonsense",
      "Complexity": 3,
      "Prompt": "Clock river dance potato seven fast",
      "Prompt_Short": "Clock river dance potato seven fast...",
      "Entropy": 7.1033128090434605,
      "Last_Gain": 0.9966256848402044,
      "Total_Amp": 16.726750951525222,
      "Top_Token": " seven",
      "Top_Prob": 0.03754149272305839,
      "N_Layers": 48
    },
    {
      "Model": "GPT2-XL",
      "Norm_Type": "LayerNorm",
      "Params": "1.5B",
      "Role": "LayerNorm Validation (Large)",
      "Category": "Nonsense",
      "Complexity": 3,
      "Prompt": "Window eat loud tomorrow yellow under",
      "Prompt_Short": "Window eat loud tomorrow yellow under...",
      "Entropy": 5.03880794444115,
      "Last_Gain": 0.9808754084925053,
      "Total_Amp": 16.046969133015732,
      "Top_Token": "pants",
      "Top_Prob": 0.3271946613535431,
      "N_Layers": 48
    },
    {
      "Model": "GPT2-XL",
      "Norm_Type": "LayerNorm",
      "Params": "1.5B",
      "Role": "LayerNorm Validation (Large)",
      "Category": "Nonsense",
      "Complexity": 3,
      "Prompt": "Fish bicycle logic cloud mountain swim",
      "Prompt_Short": "Fish bicycle logic cloud mountain swim...",
      "Entropy": 8.045099057116934,
      "Last_Gain": 1.0056163472485096,
      "Total_Amp": 20.815074320449618,
      "Top_Token": "\n",
      "Top_Prob": 0.04983696069000253,
      "N_Layers": 48
    },
    {
      "Model": "GPT2-Large",
      "Norm_Type": "LayerNorm",
      "Params": "774M",
      "Role": "LayerNorm Validation (Medium)",
      "Category": "Factual",
      "Complexity": 1,
      "Prompt": "The capital city of France is",
      "Prompt_Short": "The capital city of France is...",
      "Entropy": 5.509365215864677,
      "Last_Gain": 1.0162074853005587,
      "Total_Amp": 19.120291261959096,
      "Top_Token": " home",
      "Top_Prob": 0.0753666139637995,
      "N_Layers": 36
    },
    {
      "Model": "GPT2-Large",
      "Norm_Type": "LayerNorm",
      "Params": "774M",
      "Role": "LayerNorm Validation (Medium)",
      "Category": "Factual",
      "Complexity": 1,
      "Prompt": "The atomic number of oxygen is",
      "Prompt_Short": "The atomic number of oxygen is...",
      "Entropy": 4.942860366994637,
      "Last_Gain": 1.0413800947587415,
      "Total_Amp": 15.160706921891972,
      "Top_Token": " 2",
      "Top_Prob": 0.058314308622061654,
      "N_Layers": 36
    },
    {
      "Model": "GPT2-Large",
      "Norm_Type": "LayerNorm",
      "Params": "774M",
      "Role": "LayerNorm Validation (Medium)",
      "Category": "Factual",
      "Complexity": 1,
      "Prompt": "Water boils at a temperature of",
      "Prompt_Short": "Water boils at a temperature of...",
      "Entropy": 4.625160948193984,
      "Last_Gain": 1.0375611063410304,
      "Total_Amp": 19.83309933998523,
      "Top_Token": " about",
      "Top_Prob": 0.19265797091721296,
      "N_Layers": 36
    },
    {
      "Model": "GPT2-Large",
      "Norm_Type": "LayerNorm",
      "Params": "774M",
      "Role": "LayerNorm Validation (Medium)",
      "Category": "Factual",
      "Complexity": 1,
      "Prompt": "The largest planet in our solar system is",
      "Prompt_Short": "The largest planet in our solar system i...",
      "Entropy": 5.010766226383645,
      "Last_Gain": 0.9854104247537865,
      "Total_Amp": 19.160040023398658,
      "Top_Token": " the",
      "Top_Prob": 0.10063846039860284,
      "N_Layers": 36
    },
    {
      "Model": "GPT2-Large",
      "Norm_Type": "LayerNorm",
      "Params": "774M",
      "Role": "LayerNorm Validation (Medium)",
      "Category": "Factual",
      "Complexity": 1,
      "Prompt": "The currency used in Japan is",
      "Prompt_Short": "The currency used in Japan is...",
      "Entropy": 2.419671163680911,
      "Last_Gain": 1.0170377596764635,
      "Total_Amp": 16.756169869424465,
      "Top_Token": " the",
      "Top_Prob": 0.5050230724484182,
      "N_Layers": 36
    },
    {
      "Model": "GPT2-Large",
      "Norm_Type": "LayerNorm",
      "Params": "774M",
      "Role": "LayerNorm Validation (Medium)",
      "Category": "Syntactic",
      "Complexity": 5,
      "Prompt": "The agreement, which, notwithstanding the fact that it was signed only yesterday, effectively binds all parties immediately, stipulates that",
      "Prompt_Short": "The agreement, which, notwithstanding th...",
      "Entropy": 4.6032556550680175,
      "Last_Gain": 0.9918532551673512,
      "Total_Amp": 35.47133695442804,
      "Top_Token": " the",
      "Top_Prob": 0.2826944770220913,
      "N_Layers": 36
    },
    {
      "Model": "GPT2-Large",
      "Norm_Type": "LayerNorm",
      "Params": "774M",
      "Role": "LayerNorm Validation (Medium)",
      "Category": "Syntactic",
      "Complexity": 5,
      "Prompt": "Although the weather was extremely cold, and despite the fact that they had no coats, the children decided to",
      "Prompt_Short": "Although the weather was extremely cold,...",
      "Entropy": 4.808853698864136,
      "Last_Gain": 1.0595488130318789,
      "Total_Amp": 37.61827130093602,
      "Top_Token": " stay",
      "Top_Prob": 0.09443275753425222,
      "N_Layers": 36
    },
    {
      "Model": "GPT2-Large",
      "Norm_Type": "LayerNorm",
      "Params": "774M",
      "Role": "LayerNorm Validation (Medium)",
      "Category": "Syntactic",
      "Complexity": 5,
      "Prompt": "The professor, having reviewed the complex derivation multiple times without finding the error, finally realized that",
      "Prompt_Short": "The professor, having reviewed the compl...",
      "Entropy": 3.4109578389050443,
      "Last_Gain": 0.9882068804711721,
      "Total_Amp": 38.15111968893851,
      "Top_Token": " the",
      "Top_Prob": 0.3471255442723379,
      "N_Layers": 36
    },
    {
      "Model": "GPT2-Large",
      "Norm_Type": "LayerNorm",
      "Params": "774M",
      "Role": "LayerNorm Validation (Medium)",
      "Category": "Syntactic",
      "Complexity": 5,
      "Prompt": "To imply that such a fundamental shift in policy could occur without significant public debate is to suggest that",
      "Prompt_Short": "To imply that such a fundamental shift i...",
      "Entropy": 4.536418544926046,
      "Last_Gain": 0.984497780637196,
      "Total_Amp": 29.65258852913147,
      "Top_Token": " the",
      "Top_Prob": 0.3082019642454768,
      "N_Layers": 36
    },
    {
      "Model": "GPT2-Large",
      "Norm_Type": "LayerNorm",
      "Params": "774M",
      "Role": "LayerNorm Validation (Medium)",
      "Category": "Syntactic",
      "Complexity": 5,
      "Prompt": "Not only did the experiment fail to yield the expected results, but it also demonstrated that the initial hypothesis was",
      "Prompt_Short": "Not only did the experiment fail to yiel...",
      "Entropy": 3.6430422928408777,
      "Last_Gain": 1.0132483229876823,
      "Total_Amp": 20.946553662322867,
      "Top_Token": " incorrect",
      "Top_Prob": 0.2117088946143648,
      "N_Layers": 36
    },
    {
      "Model": "GPT2-Large",
      "Norm_Type": "LayerNorm",
      "Params": "774M",
      "Role": "LayerNorm Validation (Medium)",
      "Category": "Cliche",
      "Complexity": 2,
      "Prompt": "The true meaning of happiness is often found in",
      "Prompt_Short": "The true meaning of happiness is often f...",
      "Entropy": 4.1682856626880245,
      "Last_Gain": 0.9812333220192438,
      "Total_Amp": 24.645232350312238,
      "Top_Token": " the",
      "Top_Prob": 0.4367580560968875,
      "N_Layers": 36
    },
    {
      "Model": "GPT2-Large",
      "Norm_Type": "LayerNorm",
      "Params": "774M",
      "Role": "LayerNorm Validation (Medium)",
      "Category": "Cliche",
      "Complexity": 2,
      "Prompt": "Actions speak louder than",
      "Prompt_Short": "Actions speak louder than...",
      "Entropy": 0.28842626449377,
      "Last_Gain": 0.9938567089147734,
      "Total_Amp": 16.487459285122785,
      "Top_Token": " words",
      "Top_Prob": 0.9717777032833553,
      "N_Layers": 36
    },
    {
      "Model": "GPT2-Large",
      "Norm_Type": "LayerNorm",
      "Params": "774M",
      "Role": "LayerNorm Validation (Medium)",
      "Category": "Cliche",
      "Complexity": 2,
      "Prompt": "It is what it is, and we must simply",
      "Prompt_Short": "It is what it is, and we must simply...",
      "Entropy": 3.6089206148354247,
      "Last_Gain": 1.0695610422002755,
      "Total_Amp": 18.641333293027348,
      "Top_Token": " accept",
      "Top_Prob": 0.3508296084005729,
      "N_Layers": 36
    },
    {
      "Model": "GPT2-Large",
      "Norm_Type": "LayerNorm",
      "Params": "774M",
      "Role": "LayerNorm Validation (Medium)",
      "Category": "Cliche",
      "Complexity": 2,
      "Prompt": "Time heals all",
      "Prompt_Short": "Time heals all...",
      "Entropy": 5.725712697538125,
      "Last_Gain": 0.9995755306089298,
      "Total_Amp": 13.112530791493484,
      "Top_Token": " damage",
      "Top_Prob": 0.1226602274402274,
      "N_Layers": 36
    },
    {
      "Model": "GPT2-Large",
      "Norm_Type": "LayerNorm",
      "Params": "774M",
      "Role": "LayerNorm Validation (Medium)",
      "Category": "Cliche",
      "Complexity": 2,
      "Prompt": "Life is a journey, not a",
      "Prompt_Short": "Life is a journey, not a...",
      "Entropy": 0.07220746971096641,
      "Last_Gain": 1.0266399114672278,
      "Total_Amp": 13.698457901684282,
      "Top_Token": " destination",
      "Top_Prob": 0.9931079433227663,
      "N_Layers": 36
    },
    {
      "Model": "GPT2-Large",
      "Norm_Type": "LayerNorm",
      "Params": "774M",
      "Role": "LayerNorm Validation (Medium)",
      "Category": "Novel",
      "Complexity": 4,
      "Prompt": "The epistemological implications of quantum decoherence suggest that the observer is",
      "Prompt_Short": "The epistemological implications of quan...",
      "Entropy": 4.6452222803652115,
      "Last_Gain": 1.0056126443591549,
      "Total_Amp": 20.924292594595425,
      "Top_Token": " not",
      "Top_Prob": 0.28661050373836555,
      "N_Layers": 36
    },
    {
      "Model": "GPT2-Large",
      "Norm_Type": "LayerNorm",
      "Params": "774M",
      "Role": "LayerNorm Validation (Medium)",
      "Category": "Novel",
      "Complexity": 4,
      "Prompt": "If consciousness creates reality, then the paradox of the unobserved electron implies",
      "Prompt_Short": "If consciousness creates reality, then t...",
      "Entropy": 1.5717153795354701,
      "Last_Gain": 1.0078408010362905,
      "Total_Amp": 18.03227462066624,
      "Top_Token": " that",
      "Top_Prob": 0.6853368989195323,
      "N_Layers": 36
    },
    {
      "Model": "GPT2-Large",
      "Norm_Type": "LayerNorm",
      "Params": "774M",
      "Role": "LayerNorm Validation (Medium)",
      "Category": "Novel",
      "Complexity": 4,
      "Prompt": "The intersection of baroque architecture and cybernetic theory creates a space where",
      "Prompt_Short": "The intersection of baroque architecture...",
      "Entropy": 6.120353717582523,
      "Last_Gain": 0.9967217214859961,
      "Total_Amp": 25.254997833928183,
      "Top_Token": " the",
      "Top_Prob": 0.19019197387825212,
      "N_Layers": 36
    },
    {
      "Model": "GPT2-Large",
      "Norm_Type": "LayerNorm",
      "Params": "774M",
      "Role": "LayerNorm Validation (Medium)",
      "Category": "Novel",
      "Complexity": 4,
      "Prompt": "Calculating the trajectory of a hyperspace jump requires factoring in the variability of",
      "Prompt_Short": "Calculating the trajectory of a hyperspa...",
      "Entropy": 3.50489822757448,
      "Last_Gain": 1.0084256078923874,
      "Total_Amp": 27.628355728918443,
      "Top_Token": " the",
      "Top_Prob": 0.4891382148695207,
      "N_Layers": 36
    },
    {
      "Model": "GPT2-Large",
      "Norm_Type": "LayerNorm",
      "Params": "774M",
      "Role": "LayerNorm Validation (Medium)",
      "Category": "Novel",
      "Complexity": 4,
      "Prompt": "The symbiotic relationship between fungal mycelium and digital neural networks results in",
      "Prompt_Short": "The symbiotic relationship between funga...",
      "Entropy": 5.205685901556921,
      "Last_Gain": 0.9981784659428093,
      "Total_Amp": 31.09596774761462,
      "Top_Token": " the",
      "Top_Prob": 0.20348888681380167,
      "N_Layers": 36
    },
    {
      "Model": "GPT2-Large",
      "Norm_Type": "LayerNorm",
      "Params": "774M",
      "Role": "LayerNorm Validation (Medium)",
      "Category": "Nonsense",
      "Complexity": 3,
      "Prompt": "Table sky run blue jump quickly under over",
      "Prompt_Short": "Table sky run blue jump quickly under ov...",
      "Entropy": 3.6375233878976005,
      "Last_Gain": 1.0205697950704775,
      "Total_Amp": 16.338137774187754,
      "Top_Token": "hang",
      "Top_Prob": 0.2706461825126421,
      "N_Layers": 36
    },
    {
      "Model": "GPT2-Large",
      "Norm_Type": "LayerNorm",
      "Params": "774M",
      "Role": "LayerNorm Validation (Medium)",
      "Category": "Nonsense",
      "Complexity": 3,
      "Prompt": "Purple idea furiously sleep colorless green",
      "Prompt_Short": "Purple idea furiously sleep colorless gr...",
      "Entropy": 7.080576597369402,
      "Last_Gain": 1.0669562976258329,
      "Total_Amp": 24.394088175757506,
      "Top_Token": " idea",
      "Top_Prob": 0.10535833127002338,
      "N_Layers": 36
    },
    {
      "Model": "GPT2-Large",
      "Norm_Type": "LayerNorm",
      "Params": "774M",
      "Role": "LayerNorm Validation (Medium)",
      "Category": "Nonsense",
      "Complexity": 3,
      "Prompt": "Clock river dance potato seven fast",
      "Prompt_Short": "Clock river dance potato seven fast...",
      "Entropy": 7.279855464220374,
      "Last_Gain": 1.030323622176588,
      "Total_Amp": 15.56730118243258,
      "Top_Token": " and",
      "Top_Prob": 0.03172176152426017,
      "N_Layers": 36
    },
    {
      "Model": "GPT2-Large",
      "Norm_Type": "LayerNorm",
      "Params": "774M",
      "Role": "LayerNorm Validation (Medium)",
      "Category": "Nonsense",
      "Complexity": 3,
      "Prompt": "Window eat loud tomorrow yellow under",
      "Prompt_Short": "Window eat loud tomorrow yellow under...",
      "Entropy": 5.9114810778801745,
      "Last_Gain": 0.9967680503618603,
      "Total_Amp": 14.465664154986989,
      "Top_Token": "pants",
      "Top_Prob": 0.1324720196218288,
      "N_Layers": 36
    },
    {
      "Model": "GPT2-Large",
      "Norm_Type": "LayerNorm",
      "Params": "774M",
      "Role": "LayerNorm Validation (Medium)",
      "Category": "Nonsense",
      "Complexity": 3,
      "Prompt": "Fish bicycle logic cloud mountain swim",
      "Prompt_Short": "Fish bicycle logic cloud mountain swim...",
      "Entropy": 7.5095940280747335,
      "Last_Gain": 1.0215422427244103,
      "Total_Amp": 15.204502501767786,
      "Top_Token": "mer",
      "Top_Prob": 0.045315161830239946,
      "N_Layers": 36
    },
    {
      "Model": "GPT2-Medium",
      "Norm_Type": "LayerNorm",
      "Params": "355M",
      "Role": "LayerNorm Validation (Small)",
      "Category": "Factual",
      "Complexity": 1,
      "Prompt": "The capital city of France is",
      "Prompt_Short": "The capital city of France is...",
      "Entropy": 5.952868863595818,
      "Last_Gain": 1.0807882868313592,
      "Total_Amp": 7.403365829831194,
      "Top_Token": " the",
      "Top_Prob": 0.06975457911624777,
      "N_Layers": 24
    },
    {
      "Model": "GPT2-Medium",
      "Norm_Type": "LayerNorm",
      "Params": "355M",
      "Role": "LayerNorm Validation (Small)",
      "Category": "Factual",
      "Complexity": 1,
      "Prompt": "The atomic number of oxygen is",
      "Prompt_Short": "The atomic number of oxygen is...",
      "Entropy": 5.727411052519362,
      "Last_Gain": 1.169496321638779,
      "Total_Amp": 7.942761501908883,
      "Top_Token": " 1",
      "Top_Prob": 0.06726798859487966,
      "N_Layers": 24
    },
    {
      "Model": "GPT2-Medium",
      "Norm_Type": "LayerNorm",
      "Params": "355M",
      "Role": "LayerNorm Validation (Small)",
      "Category": "Factual",
      "Complexity": 1,
      "Prompt": "Water boils at a temperature of",
      "Prompt_Short": "Water boils at a temperature of...",
      "Entropy": 5.062001823424605,
      "Last_Gain": 1.173532064321473,
      "Total_Amp": 7.688581179612118,
      "Top_Token": " about",
      "Top_Prob": 0.12393665267241698,
      "N_Layers": 24
    },
    {
      "Model": "GPT2-Medium",
      "Norm_Type": "LayerNorm",
      "Params": "355M",
      "Role": "LayerNorm Validation (Small)",
      "Category": "Factual",
      "Complexity": 1,
      "Prompt": "The largest planet in our solar system is",
      "Prompt_Short": "The largest planet in our solar system i...",
      "Entropy": 5.64975183850016,
      "Last_Gain": 1.076784688388559,
      "Total_Amp": 7.115960361770947,
      "Top_Token": " the",
      "Top_Prob": 0.07249568770635563,
      "N_Layers": 24
    },
    {
      "Model": "GPT2-Medium",
      "Norm_Type": "LayerNorm",
      "Params": "355M",
      "Role": "LayerNorm Validation (Small)",
      "Category": "Factual",
      "Complexity": 1,
      "Prompt": "The currency used in Japan is",
      "Prompt_Short": "The currency used in Japan is...",
      "Entropy": 4.168624988124481,
      "Last_Gain": 1.0947067113892002,
      "Total_Amp": 6.573262501972408,
      "Top_Token": " called",
      "Top_Prob": 0.20017055847367562,
      "N_Layers": 24
    },
    {
      "Model": "GPT2-Medium",
      "Norm_Type": "LayerNorm",
      "Params": "355M",
      "Role": "LayerNorm Validation (Small)",
      "Category": "Syntactic",
      "Complexity": 5,
      "Prompt": "The agreement, which, notwithstanding the fact that it was signed only yesterday, effectively binds all parties immediately, stipulates that",
      "Prompt_Short": "The agreement, which, notwithstanding th...",
      "Entropy": 5.530827554647067,
      "Last_Gain": 1.1330799263635876,
      "Total_Amp": 9.679256292556877,
      "Top_Token": " the",
      "Top_Prob": 0.21614808413527983,
      "N_Layers": 24
    },
    {
      "Model": "GPT2-Medium",
      "Norm_Type": "LayerNorm",
      "Params": "355M",
      "Role": "LayerNorm Validation (Small)",
      "Category": "Syntactic",
      "Complexity": 5,
      "Prompt": "Although the weather was extremely cold, and despite the fact that they had no coats, the children decided to",
      "Prompt_Short": "Although the weather was extremely cold,...",
      "Entropy": 4.819445710957976,
      "Last_Gain": 1.2467498312158596,
      "Total_Amp": 9.097367129056078,
      "Top_Token": " wear",
      "Top_Prob": 0.09535403451021465,
      "N_Layers": 24
    },
    {
      "Model": "GPT2-Medium",
      "Norm_Type": "LayerNorm",
      "Params": "355M",
      "Role": "LayerNorm Validation (Small)",
      "Category": "Syntactic",
      "Complexity": 5,
      "Prompt": "The professor, having reviewed the complex derivation multiple times without finding the error, finally realized that",
      "Prompt_Short": "The professor, having reviewed the compl...",
      "Entropy": 3.759928932371195,
      "Last_Gain": 1.1635759328591198,
      "Total_Amp": 8.917658109555557,
      "Top_Token": " the",
      "Top_Prob": 0.3944238650544958,
      "N_Layers": 24
    },
    {
      "Model": "GPT2-Medium",
      "Norm_Type": "LayerNorm",
      "Params": "355M",
      "Role": "LayerNorm Validation (Small)",
      "Category": "Syntactic",
      "Complexity": 5,
      "Prompt": "To imply that such a fundamental shift in policy could occur without significant public debate is to suggest that",
      "Prompt_Short": "To imply that such a fundamental shift i...",
      "Entropy": 5.1221268067043555,
      "Last_Gain": 1.114384789840454,
      "Total_Amp": 8.670098019202264,
      "Top_Token": " the",
      "Top_Prob": 0.20812346348905236,
      "N_Layers": 24
    },
    {
      "Model": "GPT2-Medium",
      "Norm_Type": "LayerNorm",
      "Params": "355M",
      "Role": "LayerNorm Validation (Small)",
      "Category": "Syntactic",
      "Complexity": 5,
      "Prompt": "Not only did the experiment fail to yield the expected results, but it also demonstrated that the initial hypothesis was",
      "Prompt_Short": "Not only did the experiment fail to yiel...",
      "Entropy": 4.174481342853931,
      "Last_Gain": 1.1704929229237284,
      "Total_Amp": 8.589140163435523,
      "Top_Token": " not",
      "Top_Prob": 0.1910415671641669,
      "N_Layers": 24
    },
    {
      "Model": "GPT2-Medium",
      "Norm_Type": "LayerNorm",
      "Params": "355M",
      "Role": "LayerNorm Validation (Small)",
      "Category": "Cliche",
      "Complexity": 2,
      "Prompt": "The true meaning of happiness is often found in",
      "Prompt_Short": "The true meaning of happiness is often f...",
      "Entropy": 4.437849388218801,
      "Last_Gain": 1.0974531964769476,
      "Total_Amp": 7.443684494639407,
      "Top_Token": " the",
      "Top_Prob": 0.3722500900384875,
      "N_Layers": 24
    },
    {
      "Model": "GPT2-Medium",
      "Norm_Type": "LayerNorm",
      "Params": "355M",
      "Role": "LayerNorm Validation (Small)",
      "Category": "Cliche",
      "Complexity": 2,
      "Prompt": "Actions speak louder than",
      "Prompt_Short": "Actions speak louder than...",
      "Entropy": 0.4056136189472393,
      "Last_Gain": 1.0653597179335865,
      "Total_Amp": 5.912906857436899,
      "Top_Token": " words",
      "Top_Prob": 0.9591216804290755,
      "N_Layers": 24
    },
    {
      "Model": "GPT2-Medium",
      "Norm_Type": "LayerNorm",
      "Params": "355M",
      "Role": "LayerNorm Validation (Small)",
      "Category": "Cliche",
      "Complexity": 2,
      "Prompt": "It is what it is, and we must simply",
      "Prompt_Short": "It is what it is, and we must simply...",
      "Entropy": 4.617589523911223,
      "Last_Gain": 1.276053426262001,
      "Total_Amp": 8.50159538427005,
      "Top_Token": " accept",
      "Top_Prob": 0.18324992050392178,
      "N_Layers": 24
    },
    {
      "Model": "GPT2-Medium",
      "Norm_Type": "LayerNorm",
      "Params": "355M",
      "Role": "LayerNorm Validation (Small)",
      "Category": "Cliche",
      "Complexity": 2,
      "Prompt": "Time heals all",
      "Prompt_Short": "Time heals all...",
      "Entropy": 4.491224468988391,
      "Last_Gain": 1.095360010795854,
      "Total_Amp": 5.932700303404877,
      "Top_Token": " allies",
      "Top_Prob": 0.28590912406516383,
      "N_Layers": 24
    },
    {
      "Model": "GPT2-Medium",
      "Norm_Type": "LayerNorm",
      "Params": "355M",
      "Role": "LayerNorm Validation (Small)",
      "Category": "Cliche",
      "Complexity": 2,
      "Prompt": "Life is a journey, not a",
      "Prompt_Short": "Life is a journey, not a...",
      "Entropy": 0.1300684796835665,
      "Last_Gain": 1.0767632380045145,
      "Total_Amp": 5.326771316257524,
      "Top_Token": " destination",
      "Top_Prob": 0.987720155821749,
      "N_Layers": 24
    },
    {
      "Model": "GPT2-Medium",
      "Norm_Type": "LayerNorm",
      "Params": "355M",
      "Role": "LayerNorm Validation (Small)",
      "Category": "Novel",
      "Complexity": 4,
      "Prompt": "The epistemological implications of quantum decoherence suggest that the observer is",
      "Prompt_Short": "The epistemological implications of quan...",
      "Entropy": 5.243701621914382,
      "Last_Gain": 1.2036214523024373,
      "Total_Amp": 8.365618653842104,
      "Top_Token": " not",
      "Top_Prob": 0.1973827966661519,
      "N_Layers": 24
    },
    {
      "Model": "GPT2-Medium",
      "Norm_Type": "LayerNorm",
      "Params": "355M",
      "Role": "LayerNorm Validation (Small)",
      "Category": "Novel",
      "Complexity": 4,
      "Prompt": "If consciousness creates reality, then the paradox of the unobserved electron implies",
      "Prompt_Short": "If consciousness creates reality, then t...",
      "Entropy": 1.4177776041179724,
      "Last_Gain": 1.2440861888152257,
      "Total_Amp": 7.63816836932581,
      "Top_Token": " that",
      "Top_Prob": 0.7650926149497901,
      "N_Layers": 24
    },
    {
      "Model": "GPT2-Medium",
      "Norm_Type": "LayerNorm",
      "Params": "355M",
      "Role": "LayerNorm Validation (Small)",
      "Category": "Novel",
      "Complexity": 4,
      "Prompt": "The intersection of baroque architecture and cybernetic theory creates a space where",
      "Prompt_Short": "The intersection of baroque architecture...",
      "Entropy": 6.453261121126168,
      "Last_Gain": 1.164420744059241,
      "Total_Amp": 9.475022313610037,
      "Top_Token": " the",
      "Top_Prob": 0.15081536388126815,
      "N_Layers": 24
    },
    {
      "Model": "GPT2-Medium",
      "Norm_Type": "LayerNorm",
      "Params": "355M",
      "Role": "LayerNorm Validation (Small)",
      "Category": "Novel",
      "Complexity": 4,
      "Prompt": "Calculating the trajectory of a hyperspace jump requires factoring in the variability of",
      "Prompt_Short": "Calculating the trajectory of a hyperspa...",
      "Entropy": 4.681102556589254,
      "Last_Gain": 1.1550415298495023,
      "Total_Amp": 8.044121242710979,
      "Top_Token": " the",
      "Top_Prob": 0.3287280957401388,
      "N_Layers": 24
    },
    {
      "Model": "GPT2-Medium",
      "Norm_Type": "LayerNorm",
      "Params": "355M",
      "Role": "LayerNorm Validation (Small)",
      "Category": "Novel",
      "Complexity": 4,
      "Prompt": "The symbiotic relationship between fungal mycelium and digital neural networks results in",
      "Prompt_Short": "The symbiotic relationship between funga...",
      "Entropy": 5.274738278616679,
      "Last_Gain": 1.127484401945911,
      "Total_Amp": 7.956103884423601,
      "Top_Token": " the",
      "Top_Prob": 0.21092228431465002,
      "N_Layers": 24
    },
    {
      "Model": "GPT2-Medium",
      "Norm_Type": "LayerNorm",
      "Params": "355M",
      "Role": "LayerNorm Validation (Small)",
      "Category": "Nonsense",
      "Complexity": 3,
      "Prompt": "Table sky run blue jump quickly under over",
      "Prompt_Short": "Table sky run blue jump quickly under ov...",
      "Entropy": 5.17639813130245,
      "Last_Gain": 1.1922362537942106,
      "Total_Amp": 7.041310063858845,
      "Top_Token": "h",
      "Top_Prob": 0.11042965610720094,
      "N_Layers": 24
    },
    {
      "Model": "GPT2-Medium",
      "Norm_Type": "LayerNorm",
      "Params": "355M",
      "Role": "LayerNorm Validation (Small)",
      "Category": "Nonsense",
      "Complexity": 3,
      "Prompt": "Purple idea furiously sleep colorless green",
      "Prompt_Short": "Purple idea furiously sleep colorless gr...",
      "Entropy": 7.48423743951231,
      "Last_Gain": 1.3984791743841452,
      "Total_Amp": 9.34760315283998,
      "Top_Token": "\n",
      "Top_Prob": 0.06219777972266407,
      "N_Layers": 24
    },
    {
      "Model": "GPT2-Medium",
      "Norm_Type": "LayerNorm",
      "Params": "355M",
      "Role": "LayerNorm Validation (Small)",
      "Category": "Nonsense",
      "Complexity": 3,
      "Prompt": "Clock river dance potato seven fast",
      "Prompt_Short": "Clock river dance potato seven fast...",
      "Entropy": 7.571542411382321,
      "Last_Gain": 1.1645994766397405,
      "Total_Amp": 6.98258039355225,
      "Top_Token": "-",
      "Top_Prob": 0.025116170363595886,
      "N_Layers": 24
    },
    {
      "Model": "GPT2-Medium",
      "Norm_Type": "LayerNorm",
      "Params": "355M",
      "Role": "LayerNorm Validation (Small)",
      "Category": "Nonsense",
      "Complexity": 3,
      "Prompt": "Window eat loud tomorrow yellow under",
      "Prompt_Short": "Window eat loud tomorrow yellow under...",
      "Entropy": 6.051233245920015,
      "Last_Gain": 1.1330357693690292,
      "Total_Amp": 6.765565786249596,
      "Top_Token": "pants",
      "Top_Prob": 0.11288264768141795,
      "N_Layers": 24
    },
    {
      "Model": "GPT2-Medium",
      "Norm_Type": "LayerNorm",
      "Params": "355M",
      "Role": "LayerNorm Validation (Small)",
      "Category": "Nonsense",
      "Complexity": 3,
      "Prompt": "Fish bicycle logic cloud mountain swim",
      "Prompt_Short": "Fish bicycle logic cloud mountain swim...",
      "Entropy": 7.9511229029806,
      "Last_Gain": 1.249037031556135,
      "Total_Amp": 7.096448156579335,
      "Top_Token": "\n",
      "Top_Prob": 0.04284049038651625,
      "N_Layers": 24
    }
  ]
}