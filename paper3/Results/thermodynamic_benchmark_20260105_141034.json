{
  "experiment": "Grand Unified Thermodynamic Benchmark",
  "date": "20260105_141034",
  "n_models": 4,
  "n_prompts": 25,
  "n_measurements": 100,
  "models": [
    "Pythia-6.9B",
    "Gemma-7B",
    "Mistral-7B",
    "LLaMA-3.1-8B"
  ],
  "categories": [
    "Factual",
    "Syntactic",
    "Cliche",
    "Novel",
    "Nonsense"
  ],
  "base_levels": {
    "Gemma-7B": 2.31435905621165,
    "LLaMA-3.1-8B": 1.4845033765719124,
    "Mistral-7B": 1.1053303222054227,
    "Pythia-6.9B": 0.8047180176838018
  },
  "modulation_ranges": {
    "Pythia-6.9B": {
      "min": 0.6902460456942003,
      "max": 0.9719334719334719,
      "range": 0.28168742623927157
    },
    "Gemma-7B": {
      "min": 1.3211951447245565,
      "max": 2.9244186046511627,
      "range": 1.6032234599266062
    },
    "Mistral-7B": {
      "min": 0.9641068447412354,
      "max": 1.5578583765112262,
      "range": 0.5937515317699907
    },
    "LLaMA-3.1-8B": {
      "min": 1.1673239436619718,
      "max": 1.9281842818428185,
      "range": 0.7608603381808468
    }
  },
  "entropy_correlations": [
    {
      "Model": "Pythia-6.9B",
      "Spearman_r": 0.07923076923076922,
      "Spearman_p": 0.706571251745497,
      "Pearson_r": 0.19939950536289736,
      "Pearson_p": 0.33927523283804467
    },
    {
      "Model": "Gemma-7B",
      "Spearman_r": 0.6292307692307691,
      "Spearman_p": 0.0007525092916015074,
      "Pearson_r": 0.6921695591970106,
      "Pearson_p": 0.00012633989914345213
    },
    {
      "Model": "Mistral-7B",
      "Spearman_r": 0.13538461538461538,
      "Spearman_p": 0.518766195016132,
      "Pearson_r": 0.36705259350527,
      "Pearson_p": 0.07109007259816205
    },
    {
      "Model": "LLaMA-3.1-8B",
      "Spearman_r": 0.3853846153846154,
      "Spearman_p": 0.05710480980721742,
      "Pearson_r": 0.5935266077624924,
      "Pearson_p": 0.0017625688733215964
    }
  ],
  "complexity_correlations": [
    {
      "Model": "Pythia-6.9B",
      "Complexity_Spearman_r": 0.0823687767580373,
      "Complexity_Spearman_p": 0.6954829019678711
    },
    {
      "Model": "Gemma-7B",
      "Complexity_Spearman_r": 0.19611613513818404,
      "Complexity_Spearman_p": 0.3474502013308739
    },
    {
      "Model": "Mistral-7B",
      "Complexity_Spearman_r": -0.1608152308133109,
      "Complexity_Spearman_p": 0.4425293053539423
    },
    {
      "Model": "LLaMA-3.1-8B",
      "Complexity_Spearman_r": -0.2431840075713482,
      "Complexity_Spearman_p": 0.2414553368597104
    }
  ],
  "p3_platitude_tunnel": [
    {
      "Model": "Pythia-6.9B",
      "Cliche_Entropy_Mean": "2.2608685",
      "Novel_Entropy_Mean": "3.816464",
      "t_stat": -1.5318593297917262,
      "p_val": 0.1640933756515519,
      "cohens_d": -0.9688329235084417,
      "hypothesis": "CONFIRMED"
    },
    {
      "Model": "Gemma-7B",
      "Cliche_Entropy_Mean": "1.6581326",
      "Novel_Entropy_Mean": "3.809077",
      "t_stat": -2.1460779801734318,
      "p_val": 0.06416609840982011,
      "cohens_d": -1.3572989344960864,
      "hypothesis": "CONFIRMED"
    },
    {
      "Model": "Mistral-7B",
      "Cliche_Entropy_Mean": "1.7641042",
      "Novel_Entropy_Mean": "3.8226173",
      "t_stat": -2.0002608032085103,
      "p_val": 0.08048371272662024,
      "cohens_d": -1.2650759994733791,
      "hypothesis": "CONFIRMED"
    },
    {
      "Model": "LLaMA-3.1-8B",
      "Cliche_Entropy_Mean": "1.8657181",
      "Novel_Entropy_Mean": "3.9216926",
      "t_stat": -2.1843132786363295,
      "p_val": 0.060454597076852926,
      "cohens_d": -1.381481045162251,
      "hypothesis": "CONFIRMED"
    }
  ],
  "all_results": [
    {
      "Model": "Pythia-6.9B",
      "Norm_Type": "LayerNorm",
      "Role": "Control (LayerNorm)",
      "Category": "Factual",
      "Complexity": 1,
      "Prompt": "The capital city of France is",
      "Prompt_Short": "The capital city of France is...",
      "Entropy": "4.9601235",
      "Last_Gain": 0.7694174757281553,
      "Total_Amp": 5.4566971490048415,
      "Top_Token": " Paris",
      "Top_Prob": "0.1632",
      "N_Layers": 32
    },
    {
      "Model": "Pythia-6.9B",
      "Norm_Type": "LayerNorm",
      "Role": "Control (LayerNorm)",
      "Category": "Factual",
      "Complexity": 1,
      "Prompt": "The atomic number of oxygen is",
      "Prompt_Short": "The atomic number of oxygen is...",
      "Entropy": "1.263736",
      "Last_Gain": 0.6902460456942003,
      "Total_Amp": 3.389428263214671,
      "Top_Token": " 16",
      "Top_Prob": "0.7227",
      "N_Layers": 32
    },
    {
      "Model": "Pythia-6.9B",
      "Norm_Type": "LayerNorm",
      "Role": "Control (LayerNorm)",
      "Category": "Factual",
      "Complexity": 1,
      "Prompt": "Water boils at a temperature of",
      "Prompt_Short": "Water boils at a temperature of...",
      "Entropy": "5.165438",
      "Last_Gain": 0.9229756418696511,
      "Total_Amp": 6.301123595505618,
      "Top_Token": " about",
      "Top_Prob": "0.1213",
      "N_Layers": 32
    },
    {
      "Model": "Pythia-6.9B",
      "Norm_Type": "LayerNorm",
      "Role": "Control (LayerNorm)",
      "Category": "Factual",
      "Complexity": 1,
      "Prompt": "The largest planet in our solar system is",
      "Prompt_Short": "The largest planet in our solar system i...",
      "Entropy": "3.7027843",
      "Last_Gain": 0.7459222082810539,
      "Total_Amp": 5.1780076211213935,
      "Top_Token": " Jupiter",
      "Top_Prob": "0.3074",
      "N_Layers": 32
    },
    {
      "Model": "Pythia-6.9B",
      "Norm_Type": "LayerNorm",
      "Role": "Control (LayerNorm)",
      "Category": "Factual",
      "Complexity": 1,
      "Prompt": "The currency used in Japan is",
      "Prompt_Short": "The currency used in Japan is...",
      "Entropy": "2.807392",
      "Last_Gain": 0.803633822501747,
      "Total_Amp": 4.8936170212765955,
      "Top_Token": " the",
      "Top_Prob": "0.4678",
      "N_Layers": 32
    },
    {
      "Model": "Pythia-6.9B",
      "Norm_Type": "LayerNorm",
      "Role": "Control (LayerNorm)",
      "Category": "Syntactic",
      "Complexity": 5,
      "Prompt": "The agreement, which, notwithstanding the fact that it was signed only yesterday, effectively binds all parties immediately, stipulates that",
      "Prompt_Short": "The agreement, which, notwithstanding th...",
      "Entropy": "4.5576043",
      "Last_Gain": 0.8298068077276909,
      "Total_Amp": 9.62775183455637,
      "Top_Token": " the",
      "Top_Prob": "0.368",
      "N_Layers": 32
    },
    {
      "Model": "Pythia-6.9B",
      "Norm_Type": "LayerNorm",
      "Role": "Control (LayerNorm)",
      "Category": "Syntactic",
      "Complexity": 5,
      "Prompt": "Although the weather was extremely cold, and despite the fact that they had no coats, the children decided to",
      "Prompt_Short": "Although the weather was extremely cold,...",
      "Entropy": "4.322373",
      "Last_Gain": 0.8423035522066739,
      "Total_Amp": 8.551912568306012,
      "Top_Token": " go",
      "Top_Prob": "0.1682",
      "N_Layers": 32
    },
    {
      "Model": "Pythia-6.9B",
      "Norm_Type": "LayerNorm",
      "Role": "Control (LayerNorm)",
      "Category": "Syntactic",
      "Complexity": 5,
      "Prompt": "The professor, having reviewed the complex derivation multiple times without finding the error, finally realized that",
      "Prompt_Short": "The professor, having reviewed the compl...",
      "Entropy": "3.0718584",
      "Last_Gain": 0.8002819548872181,
      "Total_Amp": 8.705431309904153,
      "Top_Token": " the",
      "Top_Prob": "0.3289",
      "N_Layers": 32
    },
    {
      "Model": "Pythia-6.9B",
      "Norm_Type": "LayerNorm",
      "Role": "Control (LayerNorm)",
      "Category": "Syntactic",
      "Complexity": 5,
      "Prompt": "To imply that such a fundamental shift in policy could occur without significant public debate is to suggest that",
      "Prompt_Short": "To imply that such a fundamental shift i...",
      "Entropy": "4.3447914",
      "Last_Gain": 0.759,
      "Total_Amp": 7.377885783718105,
      "Top_Token": " the",
      "Top_Prob": "0.309",
      "N_Layers": 32
    },
    {
      "Model": "Pythia-6.9B",
      "Norm_Type": "LayerNorm",
      "Role": "Control (LayerNorm)",
      "Category": "Syntactic",
      "Complexity": 5,
      "Prompt": "Not only did the experiment fail to yield the expected results, but it also demonstrated that the initial hypothesis was",
      "Prompt_Short": "Not only did the experiment fail to yiel...",
      "Entropy": "3.2515066",
      "Last_Gain": 0.8487124463519313,
      "Total_Amp": 7.91,
      "Top_Token": " wrong",
      "Top_Prob": "0.2092",
      "N_Layers": 32
    },
    {
      "Model": "Pythia-6.9B",
      "Norm_Type": "LayerNorm",
      "Role": "Control (LayerNorm)",
      "Category": "Cliche",
      "Complexity": 2,
      "Prompt": "The true meaning of happiness is often found in",
      "Prompt_Short": "The true meaning of happiness is often f...",
      "Entropy": "3.9538033",
      "Last_Gain": 0.7802136031478358,
      "Total_Amp": 6.155210643015521,
      "Top_Token": " the",
      "Top_Prob": "0.41",
      "N_Layers": 32
    },
    {
      "Model": "Pythia-6.9B",
      "Norm_Type": "LayerNorm",
      "Role": "Control (LayerNorm)",
      "Category": "Cliche",
      "Complexity": 2,
      "Prompt": "Actions speak louder than",
      "Prompt_Short": "Actions speak louder than...",
      "Entropy": "1.4444913",
      "Last_Gain": 0.8189134808853119,
      "Total_Amp": 5.849101796407186,
      "Top_Token": " words",
      "Top_Prob": "0.833",
      "N_Layers": 32
    },
    {
      "Model": "Pythia-6.9B",
      "Norm_Type": "LayerNorm",
      "Role": "Control (LayerNorm)",
      "Category": "Cliche",
      "Complexity": 2,
      "Prompt": "It is what it is, and we must simply",
      "Prompt_Short": "It is what it is, and we must simply...",
      "Entropy": "3.8691475",
      "Last_Gain": 0.9128316188413643,
      "Total_Amp": 8.2294081757169,
      "Top_Token": " accept",
      "Top_Prob": "0.1708",
      "N_Layers": 32
    },
    {
      "Model": "Pythia-6.9B",
      "Norm_Type": "LayerNorm",
      "Role": "Control (LayerNorm)",
      "Category": "Cliche",
      "Complexity": 2,
      "Prompt": "Time heals all",
      "Prompt_Short": "Time heals all...",
      "Entropy": "1.7213478",
      "Last_Gain": 0.8206951026856241,
      "Total_Amp": 4.824143934997098,
      "Top_Token": " wounds",
      "Top_Prob": "0.7173",
      "N_Layers": 32
    },
    {
      "Model": "Pythia-6.9B",
      "Norm_Type": "LayerNorm",
      "Role": "Control (LayerNorm)",
      "Category": "Cliche",
      "Complexity": 2,
      "Prompt": "Life is a journey, not a",
      "Prompt_Short": "Life is a journey, not a...",
      "Entropy": "0.31555253",
      "Last_Gain": 0.8396764985727878,
      "Total_Amp": 3.295985060690943,
      "Top_Token": " destination",
      "Top_Prob": "0.9585",
      "N_Layers": 32
    },
    {
      "Model": "Pythia-6.9B",
      "Norm_Type": "LayerNorm",
      "Role": "Control (LayerNorm)",
      "Category": "Novel",
      "Complexity": 4,
      "Prompt": "The epistemological implications of quantum decoherence suggest that the observer is",
      "Prompt_Short": "The epistemological implications of quan...",
      "Entropy": "4.211759",
      "Last_Gain": 0.7647761194029851,
      "Total_Amp": 6.007033997655334,
      "Top_Token": " not",
      "Top_Prob": "0.274",
      "N_Layers": 32
    },
    {
      "Model": "Pythia-6.9B",
      "Norm_Type": "LayerNorm",
      "Role": "Control (LayerNorm)",
      "Category": "Novel",
      "Complexity": 4,
      "Prompt": "If consciousness creates reality, then the paradox of the unobserved electron implies",
      "Prompt_Short": "If consciousness creates reality, then t...",
      "Entropy": "1.303446",
      "Last_Gain": 0.6910880262438491,
      "Total_Amp": 3.036636636636637,
      "Top_Token": " that",
      "Top_Prob": "0.685",
      "N_Layers": 32
    },
    {
      "Model": "Pythia-6.9B",
      "Norm_Type": "LayerNorm",
      "Role": "Control (LayerNorm)",
      "Category": "Novel",
      "Complexity": 4,
      "Prompt": "The intersection of baroque architecture and cybernetic theory creates a space where",
      "Prompt_Short": "The intersection of baroque architecture...",
      "Entropy": "5.616253",
      "Last_Gain": 0.8373873873873874,
      "Total_Amp": 9.406704617330803,
      "Top_Token": " the",
      "Top_Prob": "0.2534",
      "N_Layers": 32
    },
    {
      "Model": "Pythia-6.9B",
      "Norm_Type": "LayerNorm",
      "Role": "Control (LayerNorm)",
      "Category": "Novel",
      "Complexity": 4,
      "Prompt": "Calculating the trajectory of a hyperspace jump requires factoring in the variability of",
      "Prompt_Short": "Calculating the trajectory of a hyperspa...",
      "Entropy": "3.3971133",
      "Last_Gain": 0.804437564499484,
      "Total_Amp": 7.623471882640587,
      "Top_Token": " the",
      "Top_Prob": "0.525",
      "N_Layers": 32
    },
    {
      "Model": "Pythia-6.9B",
      "Norm_Type": "LayerNorm",
      "Role": "Control (LayerNorm)",
      "Category": "Novel",
      "Complexity": 4,
      "Prompt": "The symbiotic relationship between fungal mycelium and digital neural networks results in",
      "Prompt_Short": "The symbiotic relationship between funga...",
      "Entropy": "4.5537477",
      "Last_Gain": 0.7801980198019802,
      "Total_Amp": 6.9734513274336285,
      "Top_Token": " the",
      "Top_Prob": "0.2527",
      "N_Layers": 32
    },
    {
      "Model": "Pythia-6.9B",
      "Norm_Type": "LayerNorm",
      "Role": "Control (LayerNorm)",
      "Category": "Nonsense",
      "Complexity": 3,
      "Prompt": "Table sky run blue jump quickly under over",
      "Prompt_Short": "Table sky run blue jump quickly under ov...",
      "Entropy": "5.9302077",
      "Last_Gain": 0.7622950819672131,
      "Total_Amp": 5.798561151079137,
      "Top_Token": "cast",
      "Top_Prob": "0.0694",
      "N_Layers": 32
    },
    {
      "Model": "Pythia-6.9B",
      "Norm_Type": "LayerNorm",
      "Role": "Control (LayerNorm)",
      "Category": "Nonsense",
      "Complexity": 3,
      "Prompt": "Purple idea furiously sleep colorless green",
      "Prompt_Short": "Purple idea furiously sleep colorless gr...",
      "Entropy": "5.9894505",
      "Last_Gain": 0.9719334719334719,
      "Total_Amp": 8.021447721179625,
      "Top_Token": " idea",
      "Top_Prob": "0.2148",
      "N_Layers": 32
    },
    {
      "Model": "Pythia-6.9B",
      "Norm_Type": "LayerNorm",
      "Role": "Control (LayerNorm)",
      "Category": "Nonsense",
      "Complexity": 3,
      "Prompt": "Clock river dance potato seven fast",
      "Prompt_Short": "Clock river dance potato seven fast...",
      "Entropy": "6.359151",
      "Last_Gain": 0.7263588979895755,
      "Total_Amp": 4.179967862881628,
      "Top_Token": " and",
      "Top_Prob": "0.0833",
      "N_Layers": 32
    },
    {
      "Model": "Pythia-6.9B",
      "Norm_Type": "LayerNorm",
      "Role": "Control (LayerNorm)",
      "Category": "Nonsense",
      "Complexity": 3,
      "Prompt": "Window eat loud tomorrow yellow under",
      "Prompt_Short": "Window eat loud tomorrow yellow under...",
      "Entropy": "6.421308",
      "Last_Gain": 0.7844190732034922,
      "Total_Amp": 5.002141327623126,
      "Top_Token": " the",
      "Top_Prob": "0.08777",
      "N_Layers": 32
    },
    {
      "Model": "Pythia-6.9B",
      "Norm_Type": "LayerNorm",
      "Role": "Control (LayerNorm)",
      "Category": "Nonsense",
      "Complexity": 3,
      "Prompt": "Fish bicycle logic cloud mountain swim",
      "Prompt_Short": "Fish bicycle logic cloud mountain swim...",
      "Entropy": "7.4498277",
      "Last_Gain": 0.8104265402843602,
      "Total_Amp": 5.40978744438952,
      "Top_Token": "\n",
      "Top_Prob": "0.0345",
      "N_Layers": 32
    },
    {
      "Model": "Gemma-7B",
      "Norm_Type": "RMSNorm",
      "Role": "Exploder",
      "Category": "Factual",
      "Complexity": 1,
      "Prompt": "The capital city of France is",
      "Prompt_Short": "The capital city of France is...",
      "Entropy": "4.2152014",
      "Last_Gain": 2.1458910433979685,
      "Total_Amp": 4.084358523725835,
      "Top_Token": " a",
      "Top_Prob": "0.1547",
      "N_Layers": 28
    },
    {
      "Model": "Gemma-7B",
      "Norm_Type": "RMSNorm",
      "Role": "Exploder",
      "Category": "Factual",
      "Complexity": 1,
      "Prompt": "The atomic number of oxygen is",
      "Prompt_Short": "The atomic number of oxygen is...",
      "Entropy": "0.77261233",
      "Last_Gain": 1.3211951447245565,
      "Total_Amp": 1.2466960352422907,
      "Top_Token": " $",
      "Top_Prob": "0.6875",
      "N_Layers": 28
    },
    {
      "Model": "Gemma-7B",
      "Norm_Type": "RMSNorm",
      "Role": "Exploder",
      "Category": "Factual",
      "Complexity": 1,
      "Prompt": "Water boils at a temperature of",
      "Prompt_Short": "Water boils at a temperature of...",
      "Entropy": "2.7387576",
      "Last_Gain": 2.034897713598075,
      "Total_Amp": 3.62486602357985,
      "Top_Token": " ",
      "Top_Prob": "0.396",
      "N_Layers": 28
    },
    {
      "Model": "Gemma-7B",
      "Norm_Type": "RMSNorm",
      "Role": "Exploder",
      "Category": "Factual",
      "Complexity": 1,
      "Prompt": "The largest planet in our solar system is",
      "Prompt_Short": "The largest planet in our solar system i...",
      "Entropy": "4.635408",
      "Last_Gain": 2.04,
      "Total_Amp": 3.0854092526690393,
      "Top_Token": " Jupiter",
      "Top_Prob": "0.165",
      "N_Layers": 28
    },
    {
      "Model": "Gemma-7B",
      "Norm_Type": "RMSNorm",
      "Role": "Exploder",
      "Category": "Factual",
      "Complexity": 1,
      "Prompt": "The currency used in Japan is",
      "Prompt_Short": "The currency used in Japan is...",
      "Entropy": "1.1235647",
      "Last_Gain": 2.251851851851852,
      "Total_Amp": 4.003511852502195,
      "Top_Token": " the",
      "Top_Prob": "0.7686",
      "N_Layers": 28
    },
    {
      "Model": "Gemma-7B",
      "Norm_Type": "RMSNorm",
      "Role": "Exploder",
      "Category": "Syntactic",
      "Complexity": 5,
      "Prompt": "The agreement, which, notwithstanding the fact that it was signed only yesterday, effectively binds all parties immediately, stipulates that",
      "Prompt_Short": "The agreement, which, notwithstanding th...",
      "Entropy": "4.7613444",
      "Last_Gain": 2.624889673433363,
      "Total_Amp": 5.363390441839495,
      "Top_Token": " the",
      "Top_Prob": "0.3896",
      "N_Layers": 28
    },
    {
      "Model": "Gemma-7B",
      "Norm_Type": "RMSNorm",
      "Role": "Exploder",
      "Category": "Syntactic",
      "Complexity": 5,
      "Prompt": "Although the weather was extremely cold, and despite the fact that they had no coats, the children decided to",
      "Prompt_Short": "Although the weather was extremely cold,...",
      "Entropy": "4.181109",
      "Last_Gain": 2.1734513274336282,
      "Total_Amp": 5.253475935828877,
      "Top_Token": " go",
      "Top_Prob": "0.2057",
      "N_Layers": 28
    },
    {
      "Model": "Gemma-7B",
      "Norm_Type": "RMSNorm",
      "Role": "Exploder",
      "Category": "Syntactic",
      "Complexity": 5,
      "Prompt": "The professor, having reviewed the complex derivation multiple times without finding the error, finally realized that",
      "Prompt_Short": "The professor, having reviewed the compl...",
      "Entropy": "1.5596079",
      "Last_Gain": 1.8681592039800996,
      "Total_Amp": 2.750915750915751,
      "Top_Token": " the",
      "Top_Prob": "0.7036",
      "N_Layers": 28
    },
    {
      "Model": "Gemma-7B",
      "Norm_Type": "RMSNorm",
      "Role": "Exploder",
      "Category": "Syntactic",
      "Complexity": 5,
      "Prompt": "To imply that such a fundamental shift in policy could occur without significant public debate is to suggest that",
      "Prompt_Short": "To imply that such a fundamental shift i...",
      "Entropy": "4.43663",
      "Last_Gain": 2.308675799086758,
      "Total_Amp": 4.613138686131387,
      "Top_Token": " the",
      "Top_Prob": "0.3567",
      "N_Layers": 28
    },
    {
      "Model": "Gemma-7B",
      "Norm_Type": "RMSNorm",
      "Role": "Exploder",
      "Category": "Syntactic",
      "Complexity": 5,
      "Prompt": "Not only did the experiment fail to yield the expected results, but it also demonstrated that the initial hypothesis was",
      "Prompt_Short": "Not only did the experiment fail to yiel...",
      "Entropy": "3.23374",
      "Last_Gain": 2.0980926430517712,
      "Total_Amp": 3.6377952755905514,
      "Top_Token": " incorrect",
      "Top_Prob": "0.298",
      "N_Layers": 28
    },
    {
      "Model": "Gemma-7B",
      "Norm_Type": "RMSNorm",
      "Role": "Exploder",
      "Category": "Cliche",
      "Complexity": 2,
      "Prompt": "The true meaning of happiness is often found in",
      "Prompt_Short": "The true meaning of happiness is often f...",
      "Entropy": "2.9217792",
      "Last_Gain": 2.1859617137648133,
      "Total_Amp": 4.440740740740741,
      "Top_Token": " the",
      "Top_Prob": "0.5513",
      "N_Layers": 28
    },
    {
      "Model": "Gemma-7B",
      "Norm_Type": "RMSNorm",
      "Role": "Exploder",
      "Category": "Cliche",
      "Complexity": 2,
      "Prompt": "Actions speak louder than",
      "Prompt_Short": "Actions speak louder than...",
      "Entropy": "0.15221682",
      "Last_Gain": 2.371732817037754,
      "Total_Amp": 3.4604519774011298,
      "Top_Token": " words",
      "Top_Prob": "0.986",
      "N_Layers": 28
    },
    {
      "Model": "Gemma-7B",
      "Norm_Type": "RMSNorm",
      "Role": "Exploder",
      "Category": "Cliche",
      "Complexity": 2,
      "Prompt": "It is what it is, and we must simply",
      "Prompt_Short": "It is what it is, and we must simply...",
      "Entropy": "3.8428404",
      "Last_Gain": 2.3321428571428573,
      "Total_Amp": 2.370235934664247,
      "Top_Token": " accept",
      "Top_Prob": "0.3228",
      "N_Layers": 28
    },
    {
      "Model": "Gemma-7B",
      "Norm_Type": "RMSNorm",
      "Role": "Exploder",
      "Category": "Cliche",
      "Complexity": 2,
      "Prompt": "Time heals all",
      "Prompt_Short": "Time heals all...",
      "Entropy": "0.72781086",
      "Last_Gain": 2.55017473789316,
      "Total_Amp": 3.714909090909091,
      "Top_Token": " wounds",
      "Top_Prob": "0.8877",
      "N_Layers": 28
    },
    {
      "Model": "Gemma-7B",
      "Norm_Type": "RMSNorm",
      "Role": "Exploder",
      "Category": "Cliche",
      "Complexity": 2,
      "Prompt": "Life is a journey, not a",
      "Prompt_Short": "Life is a journey, not a...",
      "Entropy": "0.6460156",
      "Last_Gain": 2.0111919418019024,
      "Total_Amp": 3.1225021720243267,
      "Top_Token": " destination",
      "Top_Prob": "0.908",
      "N_Layers": 28
    },
    {
      "Model": "Gemma-7B",
      "Norm_Type": "RMSNorm",
      "Role": "Exploder",
      "Category": "Novel",
      "Complexity": 4,
      "Prompt": "The epistemological implications of quantum decoherence suggest that the observer is",
      "Prompt_Short": "The epistemological implications of quan...",
      "Entropy": "3.8935215",
      "Last_Gain": 2.062788550323176,
      "Total_Amp": 3.915863277826468,
      "Top_Token": " an",
      "Top_Prob": "0.1846",
      "N_Layers": 28
    },
    {
      "Model": "Gemma-7B",
      "Norm_Type": "RMSNorm",
      "Role": "Exploder",
      "Category": "Novel",
      "Complexity": 4,
      "Prompt": "If consciousness creates reality, then the paradox of the unobserved electron implies",
      "Prompt_Short": "If consciousness creates reality, then t...",
      "Entropy": "1.4124408",
      "Last_Gain": 2.0821114369501466,
      "Total_Amp": 1.8298969072164948,
      "Top_Token": " that",
      "Top_Prob": "0.7583",
      "N_Layers": 28
    },
    {
      "Model": "Gemma-7B",
      "Norm_Type": "RMSNorm",
      "Role": "Exploder",
      "Category": "Novel",
      "Complexity": 4,
      "Prompt": "The intersection of baroque architecture and cybernetic theory creates a space where",
      "Prompt_Short": "The intersection of baroque architecture...",
      "Entropy": "5.730746",
      "Last_Gain": 2.5891472868217056,
      "Total_Amp": 4.137646249139711,
      "Top_Token": " the",
      "Top_Prob": "0.2527",
      "N_Layers": 28
    },
    {
      "Model": "Gemma-7B",
      "Norm_Type": "RMSNorm",
      "Role": "Exploder",
      "Category": "Novel",
      "Complexity": 4,
      "Prompt": "Calculating the trajectory of a hyperspace jump requires factoring in the variability of",
      "Prompt_Short": "Calculating the trajectory of a hyperspa...",
      "Entropy": "3.8571446",
      "Last_Gain": 2.274296094459582,
      "Total_Amp": 5.3590155163188875,
      "Top_Token": " the",
      "Top_Prob": "0.3474",
      "N_Layers": 28
    },
    {
      "Model": "Gemma-7B",
      "Norm_Type": "RMSNorm",
      "Role": "Exploder",
      "Category": "Novel",
      "Complexity": 4,
      "Prompt": "The symbiotic relationship between fungal mycelium and digital neural networks results in",
      "Prompt_Short": "The symbiotic relationship between funga...",
      "Entropy": "4.1515326",
      "Last_Gain": 2.611063829787234,
      "Total_Amp": 5.639705882352941,
      "Top_Token": " a",
      "Top_Prob": "0.339",
      "N_Layers": 28
    },
    {
      "Model": "Gemma-7B",
      "Norm_Type": "RMSNorm",
      "Role": "Exploder",
      "Category": "Nonsense",
      "Complexity": 3,
      "Prompt": "Table sky run blue jump quickly under over",
      "Prompt_Short": "Table sky run blue jump quickly under ov...",
      "Entropy": "6.6636214",
      "Last_Gain": 2.8817005545286505,
      "Total_Amp": 3.899937460913071,
      "Top_Token": " through",
      "Top_Prob": "0.069",
      "N_Layers": 28
    },
    {
      "Model": "Gemma-7B",
      "Norm_Type": "RMSNorm",
      "Role": "Exploder",
      "Category": "Nonsense",
      "Complexity": 3,
      "Prompt": "Purple idea furiously sleep colorless green",
      "Prompt_Short": "Purple idea furiously sleep colorless gr...",
      "Entropy": "7.268617",
      "Last_Gain": 2.5311812179016875,
      "Total_Amp": 1.3766959297685555,
      "Top_Token": " ideas",
      "Top_Prob": "0.0563",
      "N_Layers": 28
    },
    {
      "Model": "Gemma-7B",
      "Norm_Type": "RMSNorm",
      "Role": "Exploder",
      "Category": "Nonsense",
      "Complexity": 3,
      "Prompt": "Clock river dance potato seven fast",
      "Prompt_Short": "Clock river dance potato seven fast...",
      "Entropy": "7.682769",
      "Last_Gain": 2.8131655372700872,
      "Total_Amp": 2.3703099510603587,
      "Top_Token": " food",
      "Top_Prob": "0.03354",
      "N_Layers": 28
    },
    {
      "Model": "Gemma-7B",
      "Norm_Type": "RMSNorm",
      "Role": "Exploder",
      "Category": "Nonsense",
      "Complexity": 3,
      "Prompt": "Window eat loud tomorrow yellow under",
      "Prompt_Short": "Window eat loud tomorrow yellow under...",
      "Entropy": "7.7693",
      "Last_Gain": 2.9244186046511627,
      "Total_Amp": 3.404399323181049,
      "Top_Token": " the",
      "Top_Prob": "0.05966",
      "N_Layers": 28
    },
    {
      "Model": "Gemma-7B",
      "Norm_Type": "RMSNorm",
      "Role": "Exploder",
      "Category": "Nonsense",
      "Complexity": 3,
      "Prompt": "Fish bicycle logic cloud mountain swim",
      "Prompt_Short": "Fish bicycle logic cloud mountain swim...",
      "Entropy": "7.196358",
      "Last_Gain": 2.770794824399261,
      "Total_Amp": 2.1491039426523297,
      "Top_Token": "\n\n",
      "Top_Prob": "0.05225",
      "N_Layers": 28
    },
    {
      "Model": "Mistral-7B",
      "Norm_Type": "RMSNorm",
      "Role": "Inertia",
      "Category": "Factual",
      "Complexity": 1,
      "Prompt": "The capital city of France is",
      "Prompt_Short": "The capital city of France is...",
      "Entropy": "4.360517",
      "Last_Gain": 1.021243115656963,
      "Total_Amp": 180.1018970189702,
      "Top_Token": "a",
      "Top_Prob": "0.1288",
      "N_Layers": 32
    },
    {
      "Model": "Mistral-7B",
      "Norm_Type": "RMSNorm",
      "Role": "Inertia",
      "Category": "Factual",
      "Complexity": 1,
      "Prompt": "The atomic number of oxygen is",
      "Prompt_Short": "The atomic number of oxygen is...",
      "Entropy": "1.0504079",
      "Last_Gain": 1.1179078014184398,
      "Total_Amp": 177.56655665566558,
      "Top_Token": "",
      "Top_Prob": "0.804",
      "N_Layers": 32
    },
    {
      "Model": "Mistral-7B",
      "Norm_Type": "RMSNorm",
      "Role": "Inertia",
      "Category": "Factual",
      "Complexity": 1,
      "Prompt": "Water boils at a temperature of",
      "Prompt_Short": "Water boils at a temperature of...",
      "Entropy": "0.71609205",
      "Last_Gain": 1.0200873362445415,
      "Total_Amp": 206.4972375690608,
      "Top_Token": "",
      "Top_Prob": "0.8765",
      "N_Layers": 32
    },
    {
      "Model": "Mistral-7B",
      "Norm_Type": "RMSNorm",
      "Role": "Inertia",
      "Category": "Factual",
      "Complexity": 1,
      "Prompt": "The largest planet in our solar system is",
      "Prompt_Short": "The largest planet in our solar system i...",
      "Entropy": "4.803029",
      "Last_Gain": 1.0178571428571428,
      "Total_Amp": 173.24554776038855,
      "Top_Token": "J",
      "Top_Prob": "0.09875",
      "N_Layers": 32
    },
    {
      "Model": "Mistral-7B",
      "Norm_Type": "RMSNorm",
      "Role": "Inertia",
      "Category": "Factual",
      "Complexity": 1,
      "Prompt": "The currency used in Japan is",
      "Prompt_Short": "The currency used in Japan is...",
      "Entropy": "1.0820278",
      "Last_Gain": 1.0332225913621262,
      "Total_Amp": 173.0782608695652,
      "Top_Token": "the",
      "Top_Prob": "0.7827",
      "N_Layers": 32
    },
    {
      "Model": "Mistral-7B",
      "Norm_Type": "RMSNorm",
      "Role": "Inertia",
      "Category": "Syntactic",
      "Complexity": 5,
      "Prompt": "The agreement, which, notwithstanding the fact that it was signed only yesterday, effectively binds all parties immediately, stipulates that",
      "Prompt_Short": "The agreement, which, notwithstanding th...",
      "Entropy": "4.827191",
      "Last_Gain": 1.0863141524105755,
      "Total_Amp": 225.06733794839522,
      "Top_Token": "the",
      "Top_Prob": "0.3147",
      "N_Layers": 32
    },
    {
      "Model": "Mistral-7B",
      "Norm_Type": "RMSNorm",
      "Role": "Inertia",
      "Category": "Syntactic",
      "Complexity": 5,
      "Prompt": "Although the weather was extremely cold, and despite the fact that they had no coats, the children decided to",
      "Prompt_Short": "Although the weather was extremely cold,...",
      "Entropy": "3.5299141",
      "Last_Gain": 0.9741564967695621,
      "Total_Amp": 211.56638246041413,
      "Top_Token": "go",
      "Top_Prob": "0.3115",
      "N_Layers": 32
    },
    {
      "Model": "Mistral-7B",
      "Norm_Type": "RMSNorm",
      "Role": "Inertia",
      "Category": "Syntactic",
      "Complexity": 5,
      "Prompt": "The professor, having reviewed the complex derivation multiple times without finding the error, finally realized that",
      "Prompt_Short": "The professor, having reviewed the compl...",
      "Entropy": "2.6487303",
      "Last_Gain": 1.0282436010591351,
      "Total_Amp": 167.92792792792793,
      "Top_Token": "the",
      "Top_Prob": "0.3826",
      "N_Layers": 32
    },
    {
      "Model": "Mistral-7B",
      "Norm_Type": "RMSNorm",
      "Role": "Inertia",
      "Category": "Syntactic",
      "Complexity": 5,
      "Prompt": "To imply that such a fundamental shift in policy could occur without significant public debate is to suggest that",
      "Prompt_Short": "To imply that such a fundamental shift i...",
      "Entropy": "4.302432",
      "Last_Gain": 0.9641068447412354,
      "Total_Amp": 157.0260223048327,
      "Top_Token": "the",
      "Top_Prob": "0.3657",
      "N_Layers": 32
    },
    {
      "Model": "Mistral-7B",
      "Norm_Type": "RMSNorm",
      "Role": "Inertia",
      "Category": "Syntactic",
      "Complexity": 5,
      "Prompt": "Not only did the experiment fail to yield the expected results, but it also demonstrated that the initial hypothesis was",
      "Prompt_Short": "Not only did the experiment fail to yiel...",
      "Entropy": "2.4943848",
      "Last_Gain": 1.0438946528332003,
      "Total_Amp": 180.41379310344828,
      "Top_Token": "flaw",
      "Top_Prob": "0.3113",
      "N_Layers": 32
    },
    {
      "Model": "Mistral-7B",
      "Norm_Type": "RMSNorm",
      "Role": "Inertia",
      "Category": "Cliche",
      "Complexity": 2,
      "Prompt": "The true meaning of happiness is often found in",
      "Prompt_Short": "The true meaning of happiness is often f...",
      "Entropy": "3.5344467",
      "Last_Gain": 1.032846715328467,
      "Total_Amp": 158.61631089217295,
      "Top_Token": "the",
      "Top_Prob": "0.4663",
      "N_Layers": 32
    },
    {
      "Model": "Mistral-7B",
      "Norm_Type": "RMSNorm",
      "Role": "Inertia",
      "Category": "Cliche",
      "Complexity": 2,
      "Prompt": "Actions speak louder than",
      "Prompt_Short": "Actions speak louder than...",
      "Entropy": "0.33952677",
      "Last_Gain": 1.0861640430820216,
      "Total_Amp": 168.90588827377957,
      "Top_Token": "words",
      "Top_Prob": "0.968",
      "N_Layers": 32
    },
    {
      "Model": "Mistral-7B",
      "Norm_Type": "RMSNorm",
      "Role": "Inertia",
      "Category": "Cliche",
      "Complexity": 2,
      "Prompt": "It is what it is, and we must simply",
      "Prompt_Short": "It is what it is, and we must simply...",
      "Entropy": "3.4742146",
      "Last_Gain": 1.0145243282498184,
      "Total_Amp": 139.15642023346302,
      "Top_Token": "accept",
      "Top_Prob": "0.3577",
      "N_Layers": 32
    },
    {
      "Model": "Mistral-7B",
      "Norm_Type": "RMSNorm",
      "Role": "Inertia",
      "Category": "Cliche",
      "Complexity": 2,
      "Prompt": "Time heals all",
      "Prompt_Short": "Time heals all...",
      "Entropy": "0.70320654",
      "Last_Gain": 1.1323529411764706,
      "Total_Amp": 157.47368421052633,
      "Top_Token": "wounds",
      "Top_Prob": "0.903",
      "N_Layers": 32
    },
    {
      "Model": "Mistral-7B",
      "Norm_Type": "RMSNorm",
      "Role": "Inertia",
      "Category": "Cliche",
      "Complexity": 2,
      "Prompt": "Life is a journey, not a",
      "Prompt_Short": "Life is a journey, not a...",
      "Entropy": "0.7691265",
      "Last_Gain": 1.0521472392638036,
      "Total_Amp": 138.7172195892575,
      "Top_Token": "destination",
      "Top_Prob": "0.8975",
      "N_Layers": 32
    },
    {
      "Model": "Mistral-7B",
      "Norm_Type": "RMSNorm",
      "Role": "Inertia",
      "Category": "Novel",
      "Complexity": 4,
      "Prompt": "The epistemological implications of quantum decoherence suggest that the observer is",
      "Prompt_Short": "The epistemological implications of quan...",
      "Entropy": "4.351419",
      "Last_Gain": 0.9739837398373984,
      "Total_Amp": 175.95410212277682,
      "Top_Token": "not",
      "Top_Prob": "0.1368",
      "N_Layers": 32
    },
    {
      "Model": "Mistral-7B",
      "Norm_Type": "RMSNorm",
      "Role": "Inertia",
      "Category": "Novel",
      "Complexity": 4,
      "Prompt": "If consciousness creates reality, then the paradox of the unobserved electron implies",
      "Prompt_Short": "If consciousness creates reality, then t...",
      "Entropy": "1.1337633",
      "Last_Gain": 0.9853321829163072,
      "Total_Amp": 111.84085692425401,
      "Top_Token": "that",
      "Top_Prob": "0.8125",
      "N_Layers": 32
    },
    {
      "Model": "Mistral-7B",
      "Norm_Type": "RMSNorm",
      "Role": "Inertia",
      "Category": "Novel",
      "Complexity": 4,
      "Prompt": "The intersection of baroque architecture and cybernetic theory creates a space where",
      "Prompt_Short": "The intersection of baroque architecture...",
      "Entropy": "5.6709633",
      "Last_Gain": 1.0286599535243997,
      "Total_Amp": 160.51369216241739,
      "Top_Token": "the",
      "Top_Prob": "0.2306",
      "N_Layers": 32
    },
    {
      "Model": "Mistral-7B",
      "Norm_Type": "RMSNorm",
      "Role": "Inertia",
      "Category": "Novel",
      "Complexity": 4,
      "Prompt": "Calculating the trajectory of a hyperspace jump requires factoring in the variability of",
      "Prompt_Short": "Calculating the trajectory of a hyperspa...",
      "Entropy": "4.022235",
      "Last_Gain": 1.026122448979592,
      "Total_Amp": 218.75730795377294,
      "Top_Token": "the",
      "Top_Prob": "0.284",
      "N_Layers": 32
    },
    {
      "Model": "Mistral-7B",
      "Norm_Type": "RMSNorm",
      "Role": "Inertia",
      "Category": "Novel",
      "Complexity": 4,
      "Prompt": "The symbiotic relationship between fungal mycelium and digital neural networks results in",
      "Prompt_Short": "The symbiotic relationship between funga...",
      "Entropy": "3.9347076",
      "Last_Gain": 1.0700538876058507,
      "Total_Amp": 203.68631940469376,
      "Top_Token": "a",
      "Top_Prob": "0.3347",
      "N_Layers": 32
    },
    {
      "Model": "Mistral-7B",
      "Norm_Type": "RMSNorm",
      "Role": "Inertia",
      "Category": "Nonsense",
      "Complexity": 3,
      "Prompt": "Table sky run blue jump quickly under over",
      "Prompt_Short": "Table sky run blue jump quickly under ov...",
      "Entropy": "7.2789817",
      "Last_Gain": 1.5029527559055118,
      "Total_Amp": 178.6617915904936,
      "Top_Token": "\n",
      "Top_Prob": "0.0579",
      "N_Layers": 32
    },
    {
      "Model": "Mistral-7B",
      "Norm_Type": "RMSNorm",
      "Role": "Inertia",
      "Category": "Nonsense",
      "Complexity": 3,
      "Prompt": "Purple idea furiously sleep colorless green",
      "Prompt_Short": "Purple idea furiously sleep colorless gr...",
      "Entropy": "2.5529113",
      "Last_Gain": 1.5578583765112262,
      "Total_Amp": 129.92048012003002,
      "Top_Token": "ideas",
      "Top_Prob": "0.53",
      "N_Layers": 32
    },
    {
      "Model": "Mistral-7B",
      "Norm_Type": "RMSNorm",
      "Role": "Inertia",
      "Category": "Nonsense",
      "Complexity": 3,
      "Prompt": "Clock river dance potato seven fast",
      "Prompt_Short": "Clock river dance potato seven fast...",
      "Entropy": "6.8644223",
      "Last_Gain": 1.3239024390243903,
      "Total_Amp": 136.66089693154996,
      "Top_Token": "\n",
      "Top_Prob": "0.0824",
      "N_Layers": 32
    },
    {
      "Model": "Mistral-7B",
      "Norm_Type": "RMSNorm",
      "Role": "Inertia",
      "Category": "Nonsense",
      "Complexity": 3,
      "Prompt": "Window eat loud tomorrow yellow under",
      "Prompt_Short": "Window eat loud tomorrow yellow under...",
      "Entropy": "6.0541086",
      "Last_Gain": 1.279049295774648,
      "Total_Amp": 162.28970331588133,
      "Top_Token": "the",
      "Top_Prob": "0.10583",
      "N_Layers": 32
    },
    {
      "Model": "Mistral-7B",
      "Norm_Type": "RMSNorm",
      "Role": "Inertia",
      "Category": "Nonsense",
      "Complexity": 3,
      "Prompt": "Fish bicycle logic cloud mountain swim",
      "Prompt_Short": "Fish bicycle logic cloud mountain swim...",
      "Entropy": "5.763484",
      "Last_Gain": 1.2602739726027397,
      "Total_Amp": 138.21596244131456,
      "Top_Token": "\n",
      "Top_Prob": "0.1968",
      "N_Layers": 32
    },
    {
      "Model": "LLaMA-3.1-8B",
      "Norm_Type": "RMSNorm",
      "Role": "Dampener",
      "Category": "Factual",
      "Complexity": 1,
      "Prompt": "The capital city of France is",
      "Prompt_Short": "The capital city of France is...",
      "Entropy": "4.1027474",
      "Last_Gain": 1.4199584199584199,
      "Total_Amp": 100.48735632183909,
      "Top_Token": " a",
      "Top_Prob": "0.1658",
      "N_Layers": 32
    },
    {
      "Model": "LLaMA-3.1-8B",
      "Norm_Type": "RMSNorm",
      "Role": "Dampener",
      "Category": "Factual",
      "Complexity": 1,
      "Prompt": "The atomic number of oxygen is",
      "Prompt_Short": "The atomic number of oxygen is...",
      "Entropy": "1.1029315",
      "Last_Gain": 1.4140773620798985,
      "Total_Amp": 85.05363528009535,
      "Top_Token": " ",
      "Top_Prob": "0.8164",
      "N_Layers": 32
    },
    {
      "Model": "LLaMA-3.1-8B",
      "Norm_Type": "RMSNorm",
      "Role": "Dampener",
      "Category": "Factual",
      "Complexity": 1,
      "Prompt": "Water boils at a temperature of",
      "Prompt_Short": "Water boils at a temperature of...",
      "Entropy": "1.408613",
      "Last_Gain": 1.3708281829419036,
      "Total_Amp": 77.40021810250818,
      "Top_Token": " ",
      "Top_Prob": "0.775",
      "N_Layers": 32
    },
    {
      "Model": "LLaMA-3.1-8B",
      "Norm_Type": "RMSNorm",
      "Role": "Dampener",
      "Category": "Factual",
      "Complexity": 1,
      "Prompt": "The largest planet in our solar system is",
      "Prompt_Short": "The largest planet in our solar system i...",
      "Entropy": "3.1114092",
      "Last_Gain": 1.5102266445550028,
      "Total_Amp": 101.5377468060395,
      "Top_Token": " Jupiter",
      "Top_Prob": "0.514",
      "N_Layers": 32
    },
    {
      "Model": "LLaMA-3.1-8B",
      "Norm_Type": "RMSNorm",
      "Role": "Dampener",
      "Category": "Factual",
      "Complexity": 1,
      "Prompt": "The currency used in Japan is",
      "Prompt_Short": "The currency used in Japan is...",
      "Entropy": "1.1281254",
      "Last_Gain": 1.4405594405594406,
      "Total_Amp": 89.4841628959276,
      "Top_Token": " the",
      "Top_Prob": "0.7466",
      "N_Layers": 32
    },
    {
      "Model": "LLaMA-3.1-8B",
      "Norm_Type": "RMSNorm",
      "Role": "Dampener",
      "Category": "Syntactic",
      "Complexity": 5,
      "Prompt": "The agreement, which, notwithstanding the fact that it was signed only yesterday, effectively binds all parties immediately, stipulates that",
      "Prompt_Short": "The agreement, which, notwithstanding th...",
      "Entropy": "4.8474174",
      "Last_Gain": 1.4692348849652221,
      "Total_Amp": 106.77035236938032,
      "Top_Token": " the",
      "Top_Prob": "0.347",
      "N_Layers": 32
    },
    {
      "Model": "LLaMA-3.1-8B",
      "Norm_Type": "RMSNorm",
      "Role": "Dampener",
      "Category": "Syntactic",
      "Complexity": 5,
      "Prompt": "Although the weather was extremely cold, and despite the fact that they had no coats, the children decided to",
      "Prompt_Short": "Although the weather was extremely cold,...",
      "Entropy": "3.9584036",
      "Last_Gain": 1.378509196515005,
      "Total_Amp": 96.79872543813065,
      "Top_Token": " go",
      "Top_Prob": "0.2612",
      "N_Layers": 32
    },
    {
      "Model": "LLaMA-3.1-8B",
      "Norm_Type": "RMSNorm",
      "Role": "Dampener",
      "Category": "Syntactic",
      "Complexity": 5,
      "Prompt": "The professor, having reviewed the complex derivation multiple times without finding the error, finally realized that",
      "Prompt_Short": "The professor, having reviewed the compl...",
      "Entropy": "2.8732069",
      "Last_Gain": 1.3904982618771726,
      "Total_Amp": 88.68360277136259,
      "Top_Token": " the",
      "Top_Prob": "0.359",
      "N_Layers": 32
    },
    {
      "Model": "LLaMA-3.1-8B",
      "Norm_Type": "RMSNorm",
      "Role": "Dampener",
      "Category": "Syntactic",
      "Complexity": 5,
      "Prompt": "To imply that such a fundamental shift in policy could occur without significant public debate is to suggest that",
      "Prompt_Short": "To imply that such a fundamental shift i...",
      "Entropy": "4.4665513",
      "Last_Gain": 1.3428258488499452,
      "Total_Amp": 87.18222222222222,
      "Top_Token": " the",
      "Top_Prob": "0.34",
      "N_Layers": 32
    },
    {
      "Model": "LLaMA-3.1-8B",
      "Norm_Type": "RMSNorm",
      "Role": "Dampener",
      "Category": "Syntactic",
      "Complexity": 5,
      "Prompt": "Not only did the experiment fail to yield the expected results, but it also demonstrated that the initial hypothesis was",
      "Prompt_Short": "Not only did the experiment fail to yiel...",
      "Entropy": "1.7609602",
      "Last_Gain": 1.1673239436619718,
      "Total_Amp": 75.04697226938313,
      "Top_Token": " incorrect",
      "Top_Prob": "0.563",
      "N_Layers": 32
    },
    {
      "Model": "LLaMA-3.1-8B",
      "Norm_Type": "RMSNorm",
      "Role": "Dampener",
      "Category": "Cliche",
      "Complexity": 2,
      "Prompt": "The true meaning of happiness is often found in",
      "Prompt_Short": "The true meaning of happiness is often f...",
      "Entropy": "3.0032327",
      "Last_Gain": 1.3957219251336899,
      "Total_Amp": 93.42281879194631,
      "Top_Token": " the",
      "Top_Prob": "0.5366",
      "N_Layers": 32
    },
    {
      "Model": "LLaMA-3.1-8B",
      "Norm_Type": "RMSNorm",
      "Role": "Dampener",
      "Category": "Cliche",
      "Complexity": 2,
      "Prompt": "Actions speak louder than",
      "Prompt_Short": "Actions speak louder than...",
      "Entropy": "0.88808966",
      "Last_Gain": 1.5201793721973094,
      "Total_Amp": 80.50463821892393,
      "Top_Token": " words",
      "Top_Prob": "0.9087",
      "N_Layers": 32
    },
    {
      "Model": "LLaMA-3.1-8B",
      "Norm_Type": "RMSNorm",
      "Role": "Dampener",
      "Category": "Cliche",
      "Complexity": 2,
      "Prompt": "It is what it is, and we must simply",
      "Prompt_Short": "It is what it is, and we must simply...",
      "Entropy": "3.6972508",
      "Last_Gain": 1.4410876132930515,
      "Total_Amp": 75.68925619834711,
      "Top_Token": " accept",
      "Top_Prob": "0.342",
      "N_Layers": 32
    },
    {
      "Model": "LLaMA-3.1-8B",
      "Norm_Type": "RMSNorm",
      "Role": "Dampener",
      "Category": "Cliche",
      "Complexity": 2,
      "Prompt": "Time heals all",
      "Prompt_Short": "Time heals all...",
      "Entropy": "1.0810019",
      "Last_Gain": 1.5294117647058822,
      "Total_Amp": 83.28164867517174,
      "Top_Token": " wounds",
      "Top_Prob": "0.8564",
      "N_Layers": 32
    },
    {
      "Model": "LLaMA-3.1-8B",
      "Norm_Type": "RMSNorm",
      "Role": "Dampener",
      "Category": "Cliche",
      "Complexity": 2,
      "Prompt": "Life is a journey, not a",
      "Prompt_Short": "Life is a journey, not a...",
      "Entropy": "0.65901476",
      "Last_Gain": 1.3437978560490047,
      "Total_Amp": 64.25629290617849,
      "Top_Token": " destination",
      "Top_Prob": "0.9126",
      "N_Layers": 32
    },
    {
      "Model": "LLaMA-3.1-8B",
      "Norm_Type": "RMSNorm",
      "Role": "Dampener",
      "Category": "Novel",
      "Complexity": 4,
      "Prompt": "The epistemological implications of quantum decoherence suggest that the observer is",
      "Prompt_Short": "The epistemological implications of quan...",
      "Entropy": "4.383278",
      "Last_Gain": 1.3355119825708062,
      "Total_Amp": 92.58289085545722,
      "Top_Token": " not",
      "Top_Prob": "0.1318",
      "N_Layers": 32
    },
    {
      "Model": "LLaMA-3.1-8B",
      "Norm_Type": "RMSNorm",
      "Role": "Dampener",
      "Category": "Novel",
      "Complexity": 4,
      "Prompt": "If consciousness creates reality, then the paradox of the unobserved electron implies",
      "Prompt_Short": "If consciousness creates reality, then t...",
      "Entropy": "1.33682",
      "Last_Gain": 1.3059154929577466,
      "Total_Amp": 66.94584837545126,
      "Top_Token": " that",
      "Top_Prob": "0.775",
      "N_Layers": 32
    },
    {
      "Model": "LLaMA-3.1-8B",
      "Norm_Type": "RMSNorm",
      "Role": "Dampener",
      "Category": "Novel",
      "Complexity": 4,
      "Prompt": "The intersection of baroque architecture and cybernetic theory creates a space where",
      "Prompt_Short": "The intersection of baroque architecture...",
      "Entropy": "5.651355",
      "Last_Gain": 1.473135106937924,
      "Total_Amp": 89.60634605850272,
      "Top_Token": " the",
      "Top_Prob": "0.2344",
      "N_Layers": 32
    },
    {
      "Model": "LLaMA-3.1-8B",
      "Norm_Type": "RMSNorm",
      "Role": "Dampener",
      "Category": "Novel",
      "Complexity": 4,
      "Prompt": "Calculating the trajectory of a hyperspace jump requires factoring in the variability of",
      "Prompt_Short": "Calculating the trajectory of a hyperspa...",
      "Entropy": "4.3171268",
      "Last_Gain": 1.3741573033707866,
      "Total_Amp": 85.58993985784582,
      "Top_Token": " the",
      "Top_Prob": "0.266",
      "N_Layers": 32
    },
    {
      "Model": "LLaMA-3.1-8B",
      "Norm_Type": "RMSNorm",
      "Role": "Dampener",
      "Category": "Novel",
      "Complexity": 4,
      "Prompt": "The symbiotic relationship between fungal mycelium and digital neural networks results in",
      "Prompt_Short": "The symbiotic relationship between funga...",
      "Entropy": "3.9198837",
      "Last_Gain": 1.5379098360655739,
      "Total_Amp": 102.74224598930482,
      "Top_Token": " a",
      "Top_Prob": "0.343",
      "N_Layers": 32
    },
    {
      "Model": "LLaMA-3.1-8B",
      "Norm_Type": "RMSNorm",
      "Role": "Dampener",
      "Category": "Nonsense",
      "Complexity": 3,
      "Prompt": "Table sky run blue jump quickly under over",
      "Prompt_Short": "Table sky run blue jump quickly under ov...",
      "Entropy": "7.3447576",
      "Last_Gain": 1.795020266357846,
      "Total_Amp": 101.17287098419173,
      "Top_Token": " the",
      "Top_Prob": "0.05872",
      "N_Layers": 32
    },
    {
      "Model": "LLaMA-3.1-8B",
      "Norm_Type": "RMSNorm",
      "Role": "Dampener",
      "Category": "Nonsense",
      "Complexity": 3,
      "Prompt": "Purple idea furiously sleep colorless green",
      "Prompt_Short": "Purple idea furiously sleep colorless gr...",
      "Entropy": "3.91662",
      "Last_Gain": 1.9281842818428185,
      "Total_Amp": 71.71023622047244,
      "Top_Token": " ideas",
      "Top_Prob": "0.4067",
      "N_Layers": 32
    },
    {
      "Model": "LLaMA-3.1-8B",
      "Norm_Type": "RMSNorm",
      "Role": "Dampener",
      "Category": "Nonsense",
      "Complexity": 3,
      "Prompt": "Clock river dance potato seven fast",
      "Prompt_Short": "Clock river dance potato seven fast...",
      "Entropy": "7.065116",
      "Last_Gain": 1.6367041198501873,
      "Total_Amp": 64.74074074074075,
      "Top_Token": " food",
      "Top_Prob": "0.07825",
      "N_Layers": 32
    },
    {
      "Model": "LLaMA-3.1-8B",
      "Norm_Type": "RMSNorm",
      "Role": "Dampener",
      "Category": "Nonsense",
      "Complexity": 3,
      "Prompt": "Window eat loud tomorrow yellow under",
      "Prompt_Short": "Window eat loud tomorrow yellow under...",
      "Entropy": "7.1442666",
      "Last_Gain": 1.7707462686567164,
      "Total_Amp": 90.39238095238095,
      "Top_Token": " the",
      "Top_Prob": "0.0962",
      "N_Layers": 32
    },
    {
      "Model": "LLaMA-3.1-8B",
      "Norm_Type": "RMSNorm",
      "Role": "Dampener",
      "Category": "Nonsense",
      "Complexity": 3,
      "Prompt": "Fish bicycle logic cloud mountain swim",
      "Prompt_Short": "Fish bicycle logic cloud mountain swim...",
      "Entropy": "7.175366",
      "Last_Gain": 1.8210590383444918,
      "Total_Amp": 77.65125709651257,
      "Top_Token": "\n",
      "Top_Prob": "0.09283",
      "N_Layers": 32
    }
  ]
}