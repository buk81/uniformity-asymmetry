{
  "experiment": "Pythia Scaling Law Validation",
  "date": "2026-01-05T00:47:00.250097",
  "n_models": 8,
  "scaling_law": {
    "exponent_alpha": 0.26500476896304215,
    "exponent_std_err": 0.07884574800568511,
    "intercept": -1.8906036936519606,
    "r_squared": 0.6531126188430292,
    "p_value": 0.015209792158816016,
    "formula": "Final_MLP_Gain = 10^-1.891 \u00d7 Params^0.265"
  },
  "hypothesis_test": {
    "hypothesized_alpha": 0.35,
    "measured_alpha": 0.26500476896304215,
    "difference": 0.08499523103695783,
    "within_1_std_err": "False"
  },
  "models": [
    {
      "model": "pythia-70m",
      "n_params": 70426624,
      "n_layers": 6,
      "hidden_dim": 512,
      "attn_gains": [
        0.22540982528643505,
        0.18982222450358655,
        0.27028285963873366,
        0.11265321745845332,
        0.05088045828274881,
        0.15613828907593985
      ],
      "mlp_gains": [
        0.31178355070496216,
        0.2138347042666027,
        0.6796221870028768,
        0.3570697926304953,
        0.6092761054105461,
        1.4961128889324515
      ],
      "last_layer_attn_gain": 0.15613828907593985,
      "last_layer_mlp_gain": 1.4961128889324515,
      "attn_contracting_pct": 100.0,
      "mlp_contracting_pct": 83.33333333333333,
      "max_mlp_gain": 1.4961128889324515,
      "max_mlp_layer": 5
    },
    {
      "model": "pythia-160m",
      "n_params": 162322944,
      "n_layers": 12,
      "hidden_dim": 768,
      "attn_gains": [
        0.26247404157090626,
        0.18296756561698702,
        0.1885481717976251,
        0.23974058802810255,
        0.11471341322552958,
        0.10773298318711089,
        0.09708478667311772,
        0.09634272412284521,
        0.07516764590799441,
        0.037779740935668944,
        0.02036685726373848,
        0.13328925810649578
      ],
      "mlp_gains": [
        0.4495728565748446,
        0.19232871773029533,
        0.3734188324654015,
        1.0749962622944027,
        0.2968736175088426,
        0.3343987671254933,
        0.349995766545522,
        0.45253880919430856,
        0.5013983638618812,
        0.7352927531237916,
        1.0272723558262309,
        2.824326222335742
      ],
      "last_layer_attn_gain": 0.13328925810649578,
      "last_layer_mlp_gain": 2.824326222335742,
      "attn_contracting_pct": 100.0,
      "mlp_contracting_pct": 75.0,
      "max_mlp_gain": 2.824326222335742,
      "max_mlp_layer": 11
    },
    {
      "model": "pythia-410m",
      "n_params": 405334016,
      "n_layers": 24,
      "hidden_dim": 1024,
      "attn_gains": [
        0.23863536083105485,
        0.18941521720383517,
        0.11215140843627129,
        0.19525831403416605,
        0.16288391471564703,
        0.2584965770093978,
        0.13222194818117203,
        0.13116309759503217,
        0.110306545341199,
        0.12108810411556287,
        0.11624431816110813,
        0.08312840180489148,
        0.1124852063026867,
        0.11848424721778304,
        0.07192798087906965,
        0.10426163957803052,
        0.1456413262906329,
        0.12003648630863463,
        0.054904408086881434,
        0.12972168746870733,
        0.10508782257014276,
        0.08537322628167042,
        0.08705334657312985,
        0.10200925442484468
      ],
      "mlp_gains": [
        0.6965302086961298,
        0.2112655102935438,
        0.22512831145980233,
        0.2526511992493203,
        0.2470093014530449,
        1.4872327238371967,
        0.9272955289201343,
        0.5132450423003887,
        0.38255478729402304,
        0.25634151060363697,
        0.2668482980187653,
        0.2914872963811968,
        0.28072604349116304,
        0.3001579091949173,
        0.28153968366549253,
        0.29829308310857416,
        0.44792594865397317,
        0.4079539990159056,
        0.4602637015686791,
        0.4415251093348913,
        0.4474189476463008,
        0.6566786979100029,
        1.4061053031988495,
        1.779249307610584
      ],
      "last_layer_attn_gain": 0.10200925442484468,
      "last_layer_mlp_gain": 1.779249307610584,
      "attn_contracting_pct": 100.0,
      "mlp_contracting_pct": 87.5,
      "max_mlp_gain": 1.779249307610584,
      "max_mlp_layer": 23
    },
    {
      "model": "pythia-1b",
      "n_params": 1011781632,
      "n_layers": 16,
      "hidden_dim": 2048,
      "attn_gains": [
        0.7300094372246103,
        0.2945672083282746,
        0.2624087666926981,
        0.37147926767679507,
        0.20197647241504577,
        0.2268333411014975,
        0.27197077112998536,
        0.20541193514385625,
        0.2103049103563603,
        0.308932834568457,
        0.22954915728051356,
        0.2950556988029236,
        0.2267666142956145,
        0.2893629132391301,
        0.3975461582244994,
        0.3917449364341775
      ],
      "mlp_gains": [
        1.018065333619525,
        0.3188150422321525,
        0.35565266211705465,
        3.078234493279356,
        0.9422633553609803,
        0.8807302946329137,
        0.5424692341153521,
        0.49921784626858484,
        0.5251439736504865,
        0.640327313969211,
        0.6318202203218791,
        0.8286137156119716,
        0.8091426483255355,
        1.1595265163929027,
        1.8542410754231506,
        3.719352964845835
      ],
      "last_layer_attn_gain": 0.3917449364341775,
      "last_layer_mlp_gain": 3.719352964845835,
      "attn_contracting_pct": 100.0,
      "mlp_contracting_pct": 68.75,
      "max_mlp_gain": 3.719352964845835,
      "max_mlp_layer": 15
    },
    {
      "model": "pythia-1.4b",
      "n_params": 1414647808,
      "n_layers": 24,
      "hidden_dim": 2048,
      "attn_gains": [
        0.5311680762542179,
        0.31783764585780067,
        0.20926703979480307,
        0.30322756820636815,
        0.2315538226074993,
        0.220000266057264,
        0.2339324572603736,
        0.20038592209624975,
        0.28976436299263064,
        0.2393258566465082,
        0.2111074273939632,
        0.22716616662306363,
        0.2545335692875917,
        0.2204223451736917,
        0.2474619559298108,
        0.1878370520056033,
        0.19218163940477026,
        0.17384091137199364,
        0.16737091931028208,
        0.31728126836712717,
        0.08622757616748526,
        0.09068000272610201,
        0.09597085702830316,
        0.3689073543501241
      ],
      "mlp_gains": [
        0.8925172249000719,
        0.24923698955295798,
        0.38738673141171087,
        1.8821339165274211,
        0.9053816095904909,
        0.4504936122042132,
        0.5635303164635996,
        0.43236805945235735,
        0.47549251780803126,
        0.43420427426611263,
        0.4891284726016785,
        0.525757180188684,
        0.5818714752331683,
        0.6110695506773944,
        0.652694410334417,
        0.6431798968829239,
        0.6420756577233122,
        0.6632737781887799,
        0.7571811720202014,
        0.6912158733201819,
        0.6905787514390254,
        0.6231078359757329,
        0.9065141560788054,
        3.5206754973999654
      ],
      "last_layer_attn_gain": 0.3689073543501241,
      "last_layer_mlp_gain": 3.5206754973999654,
      "attn_contracting_pct": 100.0,
      "mlp_contracting_pct": 91.66666666666667,
      "max_mlp_gain": 3.5206754973999654,
      "max_mlp_layer": 23
    },
    {
      "model": "pythia-2.8b",
      "n_params": 2775208960,
      "n_layers": 32,
      "hidden_dim": 2560,
      "attn_gains": [
        0.6182989588428497,
        0.3244968608127774,
        0.30801088505069246,
        0.19912275915066505,
        0.268398765955588,
        0.256755967694519,
        0.18548109896346668,
        0.23156197670441997,
        0.276847448478837,
        0.2423429069311318,
        0.3204465520868225,
        0.21902699938881576,
        0.22794511237330986,
        0.2509965670604894,
        0.2954366575357111,
        0.22971321377834913,
        0.2829563874780396,
        0.27206853327134406,
        0.2695693396143176,
        0.25652890528569716,
        0.25370191468798153,
        0.21100552973205317,
        0.2854680895746011,
        0.14440961690378962,
        0.08969579278482115,
        0.09100737110838955,
        0.06152806209949554,
        0.07450737190312684,
        0.08212785011757673,
        0.05934203338650417,
        0.16189417928037403,
        0.24070811171880804
      ],
      "mlp_gains": [
        1.205014388752464,
        0.29349916448314906,
        1.3779771482921048,
        0.5143271892576775,
        1.1627819684256104,
        0.6571210906705596,
        0.5528792274341809,
        0.49210296987009794,
        0.48990827431403317,
        0.4417282645429179,
        0.4082735882604977,
        0.41897538292189157,
        0.4615998937393074,
        0.46669105602987665,
        0.5578354032413553,
        0.554110711310176,
        0.6159394578862535,
        0.7428133334909218,
        0.8512290922550471,
        0.9590037370373872,
        1.16071971833338,
        1.2835734607042302,
        1.2807441750966013,
        0.8902073629557041,
        0.7134115542384688,
        0.6374168150539434,
        0.6090130458344889,
        0.5785689181335598,
        0.5993514576678526,
        0.7814381614231711,
        2.428566299679557,
        2.100075294565454
      ],
      "last_layer_attn_gain": 0.24070811171880804,
      "last_layer_mlp_gain": 2.100075294565454,
      "attn_contracting_pct": 100.0,
      "mlp_contracting_pct": 75.0,
      "max_mlp_gain": 2.428566299679557,
      "max_mlp_layer": 30
    },
    {
      "model": "pythia-6.9b",
      "n_params": 6857302016,
      "n_layers": 32,
      "hidden_dim": 4096,
      "attn_gains": [
        1.011783941190762,
        0.32811210672719726,
        0.2688075737781485,
        0.391208610893142,
        0.3159299191122098,
        0.3134418621226363,
        0.3149963060640789,
        0.2793297878194724,
        0.2871530071669576,
        0.3565647535503592,
        0.4179476241194876,
        0.28747991763252245,
        0.35685410930153655,
        0.42925198986701,
        0.3178207819263104,
        0.34538033372289245,
        0.3506709045289983,
        0.16837679290850688,
        0.23210408189017498,
        0.20047575090916464,
        0.20441377486185003,
        0.1293071435045908,
        0.09372536687418113,
        0.08502287319727181,
        0.08732265765485003,
        0.07445124664101134,
        0.0963921821852286,
        0.11854895905214527,
        0.09919405129073514,
        0.10490134355158111,
        0.11939067235115386,
        0.2711165722114885
      ],
      "mlp_gains": [
        1.2678795816195172,
        0.2565270204688789,
        0.41298992496092213,
        1.5727581370147654,
        2.1261358198526747,
        1.478362492941966,
        1.100531798430901,
        0.6366874211060798,
        0.7048810027195563,
        0.8388120233842242,
        1.0345638914103685,
        0.9332780541385265,
        0.949759922484739,
        1.098824121112917,
        0.917935534121197,
        1.0726426845574972,
        1.039887647686503,
        1.0259363886601538,
        0.9966145978083489,
        1.1137661374806114,
        1.0589964145065167,
        0.963900138319577,
        0.9881008407367693,
        0.9395632839735217,
        0.8937085916014195,
        0.9100494006271919,
        0.8821764841174118,
        0.8567193379587489,
        0.8589657454980797,
        0.8996710138592652,
        1.2045704577900833,
        6.295790931251437
      ],
      "last_layer_attn_gain": 0.2711165722114885,
      "last_layer_mlp_gain": 6.295790931251437,
      "attn_contracting_pct": 96.875,
      "mlp_contracting_pct": 56.25,
      "max_mlp_gain": 6.295790931251437,
      "max_mlp_layer": 31
    },
    {
      "model": "pythia-12b",
      "n_params": 11846072320,
      "n_layers": 36,
      "hidden_dim": 5120,
      "attn_gains": [
        1.6111145723291889,
        0.46502841577937487,
        0.43529974728987164,
        0.5315222968182856,
        0.3418876308834055,
        0.32247488304530075,
        0.34916754517629967,
        0.3657196170360969,
        0.3290791587046116,
        0.31124788708904516,
        0.3252259133016917,
        0.3175857972390443,
        0.371922624876041,
        0.43139404419649313,
        0.4169950932751052,
        0.3425475543272688,
        0.3503818530954016,
        0.22214235736556537,
        0.1814574172501006,
        0.23167466746522997,
        0.13506363685763095,
        0.08580735741520491,
        0.1380526073560724,
        0.10766790228956731,
        0.09966924527011391,
        0.10458070778279582,
        0.08680672869305713,
        0.07896880895048676,
        0.10430606408378149,
        0.10222270469017702,
        0.12230044536877847,
        0.15366866222754155,
        0.12464584295338946,
        0.25135201219601017,
        0.30261486504027646,
        0.2751407555890711
      ],
      "mlp_gains": [
        1.8417947667751131,
        0.3500385902853326,
        0.7582262883510476,
        3.945338502947003,
        2.7133728135284283,
        1.5816118767482954,
        1.2416845604625033,
        1.228271428616985,
        1.1859320628178835,
        1.0662113462218903,
        1.099518692522961,
        1.0349206186983013,
        1.243689420005866,
        1.4188642466059882,
        1.7698936089881023,
        1.7419650481090732,
        1.9032662490196466,
        1.7180309458999061,
        1.7297191190142311,
        1.7016123038662234,
        1.5154514234865475,
        1.4003301164309694,
        1.3823881860910556,
        1.2850244426728408,
        1.2898157589620274,
        1.2028441869695337,
        1.2050398657727532,
        1.1986696089668487,
        1.1760273622164275,
        1.105283974654462,
        1.1315746156496462,
        1.0776191164072633,
        1.0408790568965014,
        1.1562081331892275,
        1.8158625414601564,
        7.714583158856591
      ],
      "last_layer_attn_gain": 0.2751407555890711,
      "last_layer_mlp_gain": 7.714583158856591,
      "attn_contracting_pct": 97.22222222222223,
      "mlp_contracting_pct": 5.555555555555555,
      "max_mlp_gain": 7.714583158856591,
      "max_mlp_layer": 35
    }
  ],
  "universal_findings": {
    "attention_always_contracts": true,
    "last_layer_always_expands": true,
    "mlp_contraction_decreases_with_size": true
  }
}