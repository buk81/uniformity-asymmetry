{
  "experiment": "Twin Test: Base vs Instruct",
  "purpose": "Test Nature vs Nurture hypothesis - is thermodynamic profile genetic?",
  "date": "20260105_160237",
  "n_families": 3,
  "n_prompts_per_model": 25,
  "n_total_measurements": 150,
  "twin_pairs_tested": [
    "Mistral-7B",
    "Gemma-7B",
    "LLaMA-3.1-8B"
  ],
  "twin_summary": {
    "Mistral-7B": {
      "base": {
        "mean_gain": 1.1060267134438262,
        "std_gain": 0.15569978669037726,
        "n_samples": 25
      },
      "instruct": {
        "mean_gain": 1.1352367278227393,
        "std_gain": 0.15042661052003423,
        "n_samples": 25
      }
    },
    "Gemma-7B": {
      "base": {
        "mean_gain": 2.3158992258981477,
        "std_gain": 0.35306834387628194,
        "n_samples": 25
      },
      "instruct": {
        "mean_gain": 1.153106714521428,
        "std_gain": 0.019692972039959768,
        "n_samples": 25
      }
    },
    "LLaMA-3.1-8B": {
      "base": {
        "mean_gain": 1.483777591456048,
        "std_gain": 0.17747106526602524,
        "n_samples": 25
      },
      "instruct": {
        "mean_gain": 1.5759476614500867,
        "std_gain": 0.34824578268208217,
        "n_samples": 25
      }
    }
  },
  "hypothesis_results": [
    {
      "Family": "Mistral-7B",
      "Base_Gain": 1.1060267134438262,
      "Instruct_Gain": 1.1352367278227393,
      "Delta": 0.02921001437891313,
      "Ratio": 1.0264098633639347,
      "Same_Sign": true,
      "RLHF_Effect": "NEUTRAL"
    },
    {
      "Family": "Gemma-7B",
      "Base_Gain": 2.3158992258981477,
      "Instruct_Gain": 1.153106714521428,
      "Delta": -1.1627925113767197,
      "Ratio": 0.4979088475122368,
      "Same_Sign": true,
      "RLHF_Effect": "DAMPENS"
    },
    {
      "Family": "LLaMA-3.1-8B",
      "Base_Gain": 1.483777591456048,
      "Instruct_Gain": 1.5759476614500867,
      "Delta": 0.09217006999403865,
      "Ratio": 1.0621185213503535,
      "Same_Sign": true,
      "RLHF_Effect": "AMPLIFIES"
    }
  ],
  "verdict": "NATURE_DOMINATES",
  "interpretation": "Thermodynamic profile (Gain) is an architectural invariant that persists through fine-tuning",
  "reference_gains": {
    "Pythia-6.9B": 0.8,
    "GPT-J-6B": 1.065,
    "GPT2-XL": 1.02,
    "Mistral-7B": 1.11,
    "LLaMA-3.1-8B": 1.48,
    "Gemma-7B": 2.31
  },
  "all_results": [
    {
      "Family": "Mistral-7B",
      "Model": "Mistral-7B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Factual",
      "Prompt": "The capital city of France is",
      "Entropy": 4.374516046557726,
      "Last_Gain": 1.0188679245283019,
      "Total_Amp": 179.53246753246754,
      "Top_Token": "a",
      "Top_Prob": 0.12446268810101101
    },
    {
      "Family": "Mistral-7B",
      "Model": "Mistral-7B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Factual",
      "Prompt": "The atomic number of oxygen is",
      "Entropy": 1.0455481863603917,
      "Last_Gain": 1.1134751773049645,
      "Total_Amp": 177.05726872246697,
      "Top_Token": "",
      "Top_Prob": 0.8056846423217128
    },
    {
      "Family": "Mistral-7B",
      "Model": "Mistral-7B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Factual",
      "Prompt": "Water boils at a temperature of",
      "Entropy": 0.6950839395162407,
      "Last_Gain": 1.020979020979021,
      "Total_Amp": 206.4972375690608,
      "Top_Token": "",
      "Top_Prob": 0.880459685257234
    },
    {
      "Family": "Mistral-7B",
      "Model": "Mistral-7B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Factual",
      "Prompt": "The largest planet in our solar system is",
      "Entropy": 4.784833540010156,
      "Last_Gain": 1.0129870129870129,
      "Total_Amp": 172.13793103448276,
      "Top_Token": "J",
      "Top_Prob": 0.09938024898798743
    },
    {
      "Family": "Mistral-7B",
      "Model": "Mistral-7B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Factual",
      "Prompt": "The currency used in Japan is",
      "Entropy": 1.0506463667207726,
      "Last_Gain": 1.04,
      "Total_Amp": 173.63478260869564,
      "Top_Token": "the",
      "Top_Prob": 0.7902495528890974
    },
    {
      "Family": "Mistral-7B",
      "Model": "Mistral-7B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Syntactic",
      "Prompt": "The agreement, which, notwithstanding the fact that it was signed only yesterday, effectively binds all parties immediately, stipulates that",
      "Entropy": 4.846352205951243,
      "Last_Gain": 1.0869565217391304,
      "Total_Amp": 225.12562814070353,
      "Top_Token": "the",
      "Top_Prob": 0.3107723075833866
    },
    {
      "Family": "Mistral-7B",
      "Model": "Mistral-7B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Syntactic",
      "Prompt": "Although the weather was extremely cold, and despite the fact that they had no coats, the children decided to",
      "Entropy": 3.542749798406073,
      "Last_Gain": 0.9770114942528736,
      "Total_Amp": 211.2621359223301,
      "Top_Token": "go",
      "Top_Prob": 0.30604041289411277
    },
    {
      "Family": "Mistral-7B",
      "Model": "Mistral-7B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Syntactic",
      "Prompt": "The professor, having reviewed the complex derivation multiple times without finding the error, finally realized that",
      "Entropy": 2.6522577515030643,
      "Last_Gain": 1.028169014084507,
      "Total_Amp": 168.36036036036037,
      "Top_Token": "the",
      "Top_Prob": 0.38171580748970857
    },
    {
      "Family": "Mistral-7B",
      "Model": "Mistral-7B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Syntactic",
      "Prompt": "To imply that such a fundamental shift in policy could occur without significant public debate is to suggest that",
      "Entropy": 4.33533403356005,
      "Last_Gain": 0.96,
      "Total_Amp": 156.86808510638298,
      "Top_Token": "the",
      "Top_Prob": 0.3595046747490684
    },
    {
      "Family": "Mistral-7B",
      "Model": "Mistral-7B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Syntactic",
      "Prompt": "Not only did the experiment fail to yield the expected results, but it also demonstrated that the initial hypothesis was",
      "Entropy": 2.4920854101742207,
      "Last_Gain": 1.044871794871795,
      "Total_Amp": 179.86206896551724,
      "Top_Token": "flaw",
      "Top_Prob": 0.31935752478010687
    },
    {
      "Family": "Mistral-7B",
      "Model": "Mistral-7B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Cliche",
      "Prompt": "The true meaning of happiness is often found in",
      "Entropy": 3.5379045857117886,
      "Last_Gain": 1.036764705882353,
      "Total_Amp": 158.31578947368422,
      "Top_Token": "the",
      "Top_Prob": 0.4662009651837324
    },
    {
      "Family": "Mistral-7B",
      "Model": "Mistral-7B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Cliche",
      "Prompt": "Actions speak louder than",
      "Entropy": 0.3661268200717271,
      "Last_Gain": 1.086092715231788,
      "Total_Amp": 169.29032258064515,
      "Top_Token": "words",
      "Top_Prob": 0.9649666268643148
    },
    {
      "Family": "Mistral-7B",
      "Model": "Mistral-7B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Cliche",
      "Prompt": "It is what it is, and we must simply",
      "Entropy": 3.47139840186372,
      "Last_Gain": 1.0116279069767442,
      "Total_Amp": 138.33540372670808,
      "Top_Token": "accept",
      "Top_Prob": 0.35942686230455206
    },
    {
      "Family": "Mistral-7B",
      "Model": "Mistral-7B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Cliche",
      "Prompt": "Time heals all",
      "Entropy": 0.6730058464452137,
      "Last_Gain": 1.1319444444444444,
      "Total_Amp": 156.87218045112783,
      "Top_Token": "wounds",
      "Top_Prob": 0.9076049930227119
    },
    {
      "Family": "Mistral-7B",
      "Model": "Mistral-7B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Cliche",
      "Prompt": "Life is a journey, not a",
      "Entropy": 0.7681758139566945,
      "Last_Gain": 1.0530612244897959,
      "Total_Amp": 139.34177215189874,
      "Top_Token": "destination",
      "Top_Prob": 0.8985289807540062
    },
    {
      "Family": "Mistral-7B",
      "Model": "Mistral-7B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Novel",
      "Prompt": "The epistemological implications of quantum decoherence suggest that the observer is",
      "Entropy": 4.368493168963666,
      "Last_Gain": 0.9803921568627451,
      "Total_Amp": 176.14678899082568,
      "Top_Token": "not",
      "Top_Prob": 0.13456354945259155
    },
    {
      "Family": "Mistral-7B",
      "Model": "Mistral-7B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Novel",
      "Prompt": "If consciousness creates reality, then the paradox of the unobserved electron implies",
      "Entropy": 1.139476375898541,
      "Last_Gain": 0.9862068965517241,
      "Total_Amp": 111.60975609756098,
      "Top_Token": "that",
      "Top_Prob": 0.8121816744367839
    },
    {
      "Family": "Mistral-7B",
      "Model": "Mistral-7B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Novel",
      "Prompt": "The intersection of baroque architecture and cybernetic theory creates a space where",
      "Entropy": 5.670915915101772,
      "Last_Gain": 1.031055900621118,
      "Total_Amp": 160.96969696969697,
      "Top_Token": "the",
      "Top_Prob": 0.22923223759143288
    },
    {
      "Family": "Mistral-7B",
      "Model": "Mistral-7B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Novel",
      "Prompt": "Calculating the trajectory of a hyperspace jump requires factoring in the variability of",
      "Entropy": 4.000098724412581,
      "Last_Gain": 1.026143790849673,
      "Total_Amp": 218.43478260869566,
      "Top_Token": "the",
      "Top_Prob": 0.2872095542399571
    },
    {
      "Family": "Mistral-7B",
      "Model": "Mistral-7B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Novel",
      "Prompt": "The symbiotic relationship between fungal mycelium and digital neural networks results in",
      "Entropy": 3.96918413813918,
      "Last_Gain": 1.0679012345679013,
      "Total_Amp": 203.1559633027523,
      "Top_Token": "a",
      "Top_Prob": 0.32945225071202494
    },
    {
      "Family": "Mistral-7B",
      "Model": "Mistral-7B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Nonsense",
      "Prompt": "Table sky run blue jump quickly under over",
      "Entropy": 7.283986094880858,
      "Last_Gain": 1.5039370078740157,
      "Total_Amp": 178.45255474452554,
      "Top_Token": "\n",
      "Top_Prob": 0.056578738569712246
    },
    {
      "Family": "Mistral-7B",
      "Model": "Mistral-7B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Nonsense",
      "Prompt": "Purple idea furiously sleep colorless green",
      "Entropy": 2.7124615165140478,
      "Last_Gain": 1.5596330275229358,
      "Total_Amp": 130.2994011976048,
      "Top_Token": "ideas",
      "Top_Prob": 0.5086527020216506
    },
    {
      "Family": "Mistral-7B",
      "Model": "Mistral-7B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Nonsense",
      "Prompt": "Clock river dance potato seven fast",
      "Entropy": 6.868483030012733,
      "Last_Gain": 1.328125,
      "Total_Amp": 136.85534591194968,
      "Top_Token": "\n",
      "Top_Prob": 0.08317111501639564
    },
    {
      "Family": "Mistral-7B",
      "Model": "Mistral-7B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Nonsense",
      "Prompt": "Window eat loud tomorrow yellow under",
      "Entropy": 6.030499896332759,
      "Last_Gain": 1.2816901408450705,
      "Total_Amp": 162.9090909090909,
      "Top_Token": "the",
      "Top_Prob": 0.10850464534153118
    },
    {
      "Family": "Mistral-7B",
      "Model": "Mistral-7B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Nonsense",
      "Prompt": "Fish bicycle logic cloud mountain swim",
      "Entropy": 5.792553161294037,
      "Last_Gain": 1.2627737226277371,
      "Total_Amp": 138.4,
      "Top_Token": "\n",
      "Top_Prob": 0.19671868063782744
    },
    {
      "Family": "Mistral-7B",
      "Model": "Mistral-7B-Instruct",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Factual",
      "Prompt": "The capital city of France is",
      "Entropy": 3.357691619643726,
      "Last_Gain": 1.0579710144927537,
      "Total_Amp": 152.55510204081634,
      "Top_Token": "Paris",
      "Top_Prob": 0.24947832188973673
    },
    {
      "Family": "Mistral-7B",
      "Model": "Mistral-7B-Instruct",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Factual",
      "Prompt": "The atomic number of oxygen is",
      "Entropy": 0.2310669645756797,
      "Last_Gain": 1.0597609561752988,
      "Total_Amp": 136.192,
      "Top_Token": "",
      "Top_Prob": 0.9583893592052363
    },
    {
      "Family": "Mistral-7B",
      "Model": "Mistral-7B-Instruct",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Factual",
      "Prompt": "Water boils at a temperature of",
      "Entropy": 0.6088333322314786,
      "Last_Gain": 1.0465116279069768,
      "Total_Amp": 176.3265306122449,
      "Top_Token": "",
      "Top_Prob": 0.8789461503961687
    },
    {
      "Family": "Mistral-7B",
      "Model": "Mistral-7B-Instruct",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Factual",
      "Prompt": "The largest planet in our solar system is",
      "Entropy": 2.7630053509939083,
      "Last_Gain": 1.0465116279069768,
      "Total_Amp": 136.600790513834,
      "Top_Token": "J",
      "Top_Prob": 0.5276930278502421
    },
    {
      "Family": "Mistral-7B",
      "Model": "Mistral-7B-Instruct",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Factual",
      "Prompt": "The currency used in Japan is",
      "Entropy": 0.28989537158499207,
      "Last_Gain": 1.0303030303030303,
      "Total_Amp": 139.82329317269077,
      "Top_Token": "the",
      "Top_Prob": 0.9411010916940133
    },
    {
      "Family": "Mistral-7B",
      "Model": "Mistral-7B-Instruct",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Syntactic",
      "Prompt": "The agreement, which, notwithstanding the fact that it was signed only yesterday, effectively binds all parties immediately, stipulates that",
      "Entropy": 4.922523581089308,
      "Last_Gain": 1.1513157894736843,
      "Total_Amp": 217.4757281553398,
      "Top_Token": "the",
      "Top_Prob": 0.3104153899665972
    },
    {
      "Family": "Mistral-7B",
      "Model": "Mistral-7B-Instruct",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Syntactic",
      "Prompt": "Although the weather was extremely cold, and despite the fact that they had no coats, the children decided to",
      "Entropy": 2.874711014854782,
      "Last_Gain": 1.0316455696202531,
      "Total_Amp": 192.29493087557603,
      "Top_Token": "go",
      "Top_Prob": 0.4475153041419134
    },
    {
      "Family": "Mistral-7B",
      "Model": "Mistral-7B-Instruct",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Syntactic",
      "Prompt": "The professor, having reviewed the complex derivation multiple times without finding the error, finally realized that",
      "Entropy": 1.9412649255925543,
      "Last_Gain": 1.0161943319838056,
      "Total_Amp": 140.29694323144105,
      "Top_Token": "the",
      "Top_Prob": 0.5276052976491413
    },
    {
      "Family": "Mistral-7B",
      "Model": "Mistral-7B-Instruct",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Syntactic",
      "Prompt": "To imply that such a fundamental shift in policy could occur without significant public debate is to suggest that",
      "Entropy": 3.4790166074820466,
      "Last_Gain": 0.9926470588235294,
      "Total_Amp": 139.91902834008098,
      "Top_Token": "the",
      "Top_Prob": 0.38819869804760754
    },
    {
      "Family": "Mistral-7B",
      "Model": "Mistral-7B-Instruct",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Syntactic",
      "Prompt": "Not only did the experiment fail to yield the expected results, but it also demonstrated that the initial hypothesis was",
      "Entropy": 1.835121496508826,
      "Last_Gain": 1.1119402985074627,
      "Total_Amp": 151.96812749003985,
      "Top_Token": "flaw",
      "Top_Prob": 0.456134094680186
    },
    {
      "Family": "Mistral-7B",
      "Model": "Mistral-7B-Instruct",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Cliche",
      "Prompt": "The true meaning of happiness is often found in",
      "Entropy": 1.9038933401833573,
      "Last_Gain": 1.0144927536231885,
      "Total_Amp": 146.28571428571428,
      "Top_Token": "the",
      "Top_Prob": 0.7015372211622771
    },
    {
      "Family": "Mistral-7B",
      "Model": "Mistral-7B-Instruct",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Cliche",
      "Prompt": "Actions speak louder than",
      "Entropy": 0.07303150150051856,
      "Last_Gain": 1.118881118881119,
      "Total_Amp": 161.25984251968504,
      "Top_Token": "words",
      "Top_Prob": 0.9935265922046786
    },
    {
      "Family": "Mistral-7B",
      "Model": "Mistral-7B-Instruct",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Cliche",
      "Prompt": "It is what it is, and we must simply",
      "Entropy": 2.9199246533796295,
      "Last_Gain": 1.1139240506329113,
      "Total_Amp": 135.710843373494,
      "Top_Token": "accept",
      "Top_Prob": 0.4210597236075788
    },
    {
      "Family": "Mistral-7B",
      "Model": "Mistral-7B-Instruct",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Cliche",
      "Prompt": "Time heals all",
      "Entropy": 0.08772479790862797,
      "Last_Gain": 1.1764705882352942,
      "Total_Amp": 146.28571428571428,
      "Top_Token": "wounds",
      "Top_Prob": 0.9905241025500571
    },
    {
      "Family": "Mistral-7B",
      "Model": "Mistral-7B-Instruct",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Cliche",
      "Prompt": "Life is a journey, not a",
      "Entropy": 0.35909635069325635,
      "Last_Gain": 1.0560344827586208,
      "Total_Amp": 122.5,
      "Top_Token": "destination",
      "Top_Prob": 0.942179806070728
    },
    {
      "Family": "Mistral-7B",
      "Model": "Mistral-7B-Instruct",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Novel",
      "Prompt": "The epistemological implications of quantum decoherence suggest that the observer is",
      "Entropy": 3.5482945402639356,
      "Last_Gain": 1.0416666666666667,
      "Total_Amp": 160.6694560669456,
      "Top_Token": "an",
      "Top_Prob": 0.21060187246531512
    },
    {
      "Family": "Mistral-7B",
      "Model": "Mistral-7B-Instruct",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Novel",
      "Prompt": "If consciousness creates reality, then the paradox of the unobserved electron implies",
      "Entropy": 0.8945072857319557,
      "Last_Gain": 1.0236220472440944,
      "Total_Amp": 97.30994152046783,
      "Top_Token": "that",
      "Top_Prob": 0.8452332631906977
    },
    {
      "Family": "Mistral-7B",
      "Model": "Mistral-7B-Instruct",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Novel",
      "Prompt": "The intersection of baroque architecture and cybernetic theory creates a space where",
      "Entropy": 4.0969738574656125,
      "Last_Gain": 1.06993006993007,
      "Total_Amp": 136.95104895104896,
      "Top_Token": "the",
      "Top_Prob": 0.41287256248656407
    },
    {
      "Family": "Mistral-7B",
      "Model": "Mistral-7B-Instruct",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Novel",
      "Prompt": "Calculating the trajectory of a hyperspace jump requires factoring in the variability of",
      "Entropy": 3.5927721679596263,
      "Last_Gain": 1.0797101449275361,
      "Total_Amp": 189.77114427860695,
      "Top_Token": "the",
      "Top_Prob": 0.25135240321064245
    },
    {
      "Family": "Mistral-7B",
      "Model": "Mistral-7B-Instruct",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Novel",
      "Prompt": "The symbiotic relationship between fungal mycelium and digital neural networks results in",
      "Entropy": 3.3914487446978154,
      "Last_Gain": 1.1224489795918366,
      "Total_Amp": 184.45414847161572,
      "Top_Token": "a",
      "Top_Prob": 0.3227114194110401
    },
    {
      "Family": "Mistral-7B",
      "Model": "Mistral-7B-Instruct",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Nonsense",
      "Prompt": "Table sky run blue jump quickly under over",
      "Entropy": 7.155579842841306,
      "Last_Gain": 1.5895196506550218,
      "Total_Amp": 158.47619047619048,
      "Top_Token": "\n",
      "Top_Prob": 0.06758620518620281
    },
    {
      "Family": "Mistral-7B",
      "Model": "Mistral-7B-Instruct",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Nonsense",
      "Prompt": "Purple idea furiously sleep colorless green",
      "Entropy": 4.149331658280537,
      "Last_Gain": 1.4926829268292683,
      "Total_Amp": 115.2,
      "Top_Token": "idea",
      "Top_Prob": 0.26160869965797373
    },
    {
      "Family": "Mistral-7B",
      "Model": "Mistral-7B-Instruct",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Nonsense",
      "Prompt": "Clock river dance potato seven fast",
      "Entropy": 6.160564228692705,
      "Last_Gain": 1.3391304347826087,
      "Total_Amp": 118.03592814371258,
      "Top_Token": "\n",
      "Top_Prob": 0.15133530661720915
    },
    {
      "Family": "Mistral-7B",
      "Model": "Mistral-7B-Instruct",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Nonsense",
      "Prompt": "Window eat loud tomorrow yellow under",
      "Entropy": 5.725648751807588,
      "Last_Gain": 1.3025210084033614,
      "Total_Amp": 128.83116883116884,
      "Top_Token": "wear",
      "Top_Prob": 0.17812368282981988
    },
    {
      "Family": "Mistral-7B",
      "Model": "Mistral-7B-Instruct",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Nonsense",
      "Prompt": "Fish bicycle logic cloud mountain swim",
      "Entropy": 4.7455825908532,
      "Last_Gain": 1.2950819672131149,
      "Total_Amp": 121.83132530120481,
      "Top_Token": "\n",
      "Top_Prob": 0.33962777367086416
    },
    {
      "Family": "Gemma-7B",
      "Model": "Gemma-7B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Factual",
      "Prompt": "The capital city of France is",
      "Entropy": 4.385413323924485,
      "Last_Gain": 2.1481481481481484,
      "Total_Amp": 4.084507042253521,
      "Top_Token": " a",
      "Top_Prob": 0.12896944261248153
    },
    {
      "Family": "Gemma-7B",
      "Model": "Gemma-7B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Factual",
      "Prompt": "The atomic number of oxygen is",
      "Entropy": 0.7581442353328433,
      "Last_Gain": 1.362962962962963,
      "Total_Amp": 1.3049645390070923,
      "Top_Token": " $",
      "Top_Prob": 0.712415801101477
    },
    {
      "Family": "Gemma-7B",
      "Model": "Gemma-7B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Factual",
      "Prompt": "Water boils at a temperature of",
      "Entropy": 2.8855165793215565,
      "Last_Gain": 2.0386473429951693,
      "Total_Amp": 3.606837606837607,
      "Top_Token": " $",
      "Top_Prob": 0.28348720909481595
    },
    {
      "Family": "Gemma-7B",
      "Model": "Gemma-7B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Factual",
      "Prompt": "The largest planet in our solar system is",
      "Entropy": 4.755843108938536,
      "Last_Gain": 2.0473933649289098,
      "Total_Amp": 3.085714285714286,
      "Top_Token": " Jupiter",
      "Top_Prob": 0.16873053314219327
    },
    {
      "Family": "Gemma-7B",
      "Model": "Gemma-7B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Factual",
      "Prompt": "The currency used in Japan is",
      "Entropy": 1.6511879335782471,
      "Last_Gain": 2.260869565217391,
      "Total_Amp": 4.0,
      "Top_Token": " the",
      "Top_Prob": 0.6330396363862869
    },
    {
      "Family": "Gemma-7B",
      "Model": "Gemma-7B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Syntactic",
      "Prompt": "The agreement, which, notwithstanding the fact that it was signed only yesterday, effectively binds all parties immediately, stipulates that",
      "Entropy": 5.708814352709386,
      "Last_Gain": 2.6338028169014085,
      "Total_Amp": 5.381294964028777,
      "Top_Token": " the",
      "Top_Prob": 0.25402554991845805
    },
    {
      "Family": "Gemma-7B",
      "Model": "Gemma-7B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Syntactic",
      "Prompt": "Although the weather was extremely cold, and despite the fact that they had no coats, the children decided to",
      "Entropy": 4.167120916435578,
      "Last_Gain": 2.1702127659574466,
      "Total_Amp": 5.253218884120171,
      "Top_Token": " go",
      "Top_Prob": 0.17994900928611487
    },
    {
      "Family": "Gemma-7B",
      "Model": "Gemma-7B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Syntactic",
      "Prompt": "The professor, having reviewed the complex derivation multiple times without finding the error, finally realized that",
      "Entropy": 1.5666589370759356,
      "Last_Gain": 1.8613861386138615,
      "Total_Amp": 2.7445255474452557,
      "Top_Token": " the",
      "Top_Prob": 0.7022310653811612
    },
    {
      "Family": "Gemma-7B",
      "Model": "Gemma-7B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Syntactic",
      "Prompt": "To imply that such a fundamental shift in policy could occur without significant public debate is to suggest that",
      "Entropy": 3.95152895301869,
      "Last_Gain": 2.3065693430656933,
      "Total_Amp": 4.613138686131387,
      "Top_Token": " the",
      "Top_Prob": 0.43972180734629845
    },
    {
      "Family": "Gemma-7B",
      "Model": "Gemma-7B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Syntactic",
      "Prompt": "Not only did the experiment fail to yield the expected results, but it also demonstrated that the initial hypothesis was",
      "Entropy": 3.225116474587177,
      "Last_Gain": 2.102189781021898,
      "Total_Amp": 3.6,
      "Top_Token": " incorrect",
      "Top_Prob": 0.30434724501503535
    },
    {
      "Family": "Gemma-7B",
      "Model": "Gemma-7B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Cliche",
      "Prompt": "The true meaning of happiness is often found in",
      "Entropy": 3.1108930837652133,
      "Last_Gain": 2.18978102189781,
      "Total_Amp": 4.411764705882353,
      "Top_Token": " the",
      "Top_Prob": 0.5071814405109274
    },
    {
      "Family": "Gemma-7B",
      "Model": "Gemma-7B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Cliche",
      "Prompt": "Actions speak louder than",
      "Entropy": 0.3005610409487192,
      "Last_Gain": 2.372093023255814,
      "Total_Amp": 3.457627118644068,
      "Top_Token": " words",
      "Top_Prob": 0.9703904778117125
    },
    {
      "Family": "Gemma-7B",
      "Model": "Gemma-7B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Cliche",
      "Prompt": "It is what it is, and we must simply",
      "Entropy": 3.8587207972395623,
      "Last_Gain": 2.3285714285714287,
      "Total_Amp": 2.36231884057971,
      "Top_Token": " accept",
      "Top_Prob": 0.3030365004853763
    },
    {
      "Family": "Gemma-7B",
      "Model": "Gemma-7B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Cliche",
      "Prompt": "Time heals all",
      "Entropy": 0.5605989886609205,
      "Last_Gain": 2.544,
      "Total_Amp": 3.697674418604651,
      "Top_Token": " wounds",
      "Top_Prob": 0.9171837089156983
    },
    {
      "Family": "Gemma-7B",
      "Model": "Gemma-7B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Cliche",
      "Prompt": "Life is a journey, not a",
      "Entropy": 0.6561399979587592,
      "Last_Gain": 2.0089285714285716,
      "Total_Amp": 3.125,
      "Top_Token": " destination",
      "Top_Prob": 0.9076277229746845
    },
    {
      "Family": "Gemma-7B",
      "Model": "Gemma-7B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Novel",
      "Prompt": "The epistemological implications of quantum decoherence suggest that the observer is",
      "Entropy": 3.753392646181187,
      "Last_Gain": 2.074074074074074,
      "Total_Amp": 3.943661971830986,
      "Top_Token": " an",
      "Top_Prob": 0.1867316874264177
    },
    {
      "Family": "Gemma-7B",
      "Model": "Gemma-7B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Novel",
      "Prompt": "If consciousness creates reality, then the paradox of the unobserved electron implies",
      "Entropy": 1.724221601759987,
      "Last_Gain": 2.078125,
      "Total_Amp": 1.821917808219178,
      "Top_Token": " that",
      "Top_Prob": 0.6757583374329997
    },
    {
      "Family": "Gemma-7B",
      "Model": "Gemma-7B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Novel",
      "Prompt": "The intersection of baroque architecture and cybernetic theory creates a space where",
      "Entropy": 5.166580772946132,
      "Last_Gain": 2.593103448275862,
      "Total_Amp": 4.131868131868132,
      "Top_Token": " the",
      "Top_Prob": 0.2929139306928519
    },
    {
      "Family": "Gemma-7B",
      "Model": "Gemma-7B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Novel",
      "Prompt": "Calculating the trajectory of a hyperspace jump requires factoring in the variability of",
      "Entropy": 3.8131658279733234,
      "Last_Gain": 2.2753623188405796,
      "Total_Amp": 5.367521367521367,
      "Top_Token": " the",
      "Top_Prob": 0.35079797400862683
    },
    {
      "Family": "Gemma-7B",
      "Model": "Gemma-7B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Novel",
      "Prompt": "The symbiotic relationship between fungal mycelium and digital neural networks results in",
      "Entropy": 4.119281525485163,
      "Last_Gain": 2.6122448979591835,
      "Total_Amp": 5.605839416058394,
      "Top_Token": " a",
      "Top_Prob": 0.36228393417179255
    },
    {
      "Family": "Gemma-7B",
      "Model": "Gemma-7B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Nonsense",
      "Prompt": "Table sky run blue jump quickly under over",
      "Entropy": 6.451957069883283,
      "Last_Gain": 2.8970588235294117,
      "Total_Amp": 3.94,
      "Top_Token": "\n\n",
      "Top_Prob": 0.03446013695220566
    },
    {
      "Family": "Gemma-7B",
      "Model": "Gemma-7B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Nonsense",
      "Prompt": "Purple idea furiously sleep colorless green",
      "Entropy": 7.183961870328457,
      "Last_Gain": 2.4606060606060605,
      "Total_Amp": 1.2929936305732483,
      "Top_Token": " idea",
      "Top_Prob": 0.06039142923535729
    },
    {
      "Family": "Gemma-7B",
      "Model": "Gemma-7B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Nonsense",
      "Prompt": "Clock river dance potato seven fast",
      "Entropy": 7.585874574844152,
      "Last_Gain": 2.8217054263565893,
      "Total_Amp": 2.3790849673202614,
      "Top_Token": "\n",
      "Top_Prob": 0.017011223589319505
    },
    {
      "Family": "Gemma-7B",
      "Model": "Gemma-7B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Nonsense",
      "Prompt": "Window eat loud tomorrow yellow under",
      "Entropy": 7.8412544420678065,
      "Last_Gain": 2.9302325581395348,
      "Total_Amp": 3.4054054054054053,
      "Top_Token": "\n",
      "Top_Prob": 0.024633110236751
    },
    {
      "Family": "Gemma-7B",
      "Model": "Gemma-7B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Nonsense",
      "Prompt": "Fish bicycle logic cloud mountain swim",
      "Entropy": 6.945213725853388,
      "Last_Gain": 2.7794117647058822,
      "Total_Amp": 2.16,
      "Top_Token": "\n",
      "Top_Prob": 0.04885234345400056
    },
    {
      "Family": "Gemma-7B",
      "Model": "Gemma-7B-IT",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Factual",
      "Prompt": "The capital city of France is",
      "Entropy": 2.0018786276224936,
      "Last_Gain": 1.1534090909090908,
      "Total_Amp": 1.592156862745098,
      "Top_Token": " what",
      "Top_Prob": 0.3888341791396241
    },
    {
      "Family": "Gemma-7B",
      "Model": "Gemma-7B-IT",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Factual",
      "Prompt": "The atomic number of oxygen is",
      "Entropy": 0.015875062846330258,
      "Last_Gain": 1.1502890173410405,
      "Total_Amp": 1.5983935742971886,
      "Top_Token": " ",
      "Top_Prob": 0.9982762689501303
    },
    {
      "Family": "Gemma-7B",
      "Model": "Gemma-7B-IT",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Factual",
      "Prompt": "Water boils at a temperature of",
      "Entropy": 0.1463093210271066,
      "Last_Gain": 1.1336898395721926,
      "Total_Amp": 1.8197424892703862,
      "Top_Token": " ",
      "Top_Prob": 0.9789613666052231
    },
    {
      "Family": "Gemma-7B",
      "Model": "Gemma-7B-IT",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Factual",
      "Prompt": "The largest planet in our solar system is",
      "Entropy": 1.809282870060733,
      "Last_Gain": 1.1290322580645162,
      "Total_Amp": 1.6666666666666667,
      "Top_Token": "...",
      "Top_Prob": 0.5451918421201837
    },
    {
      "Family": "Gemma-7B",
      "Model": "Gemma-7B-IT",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Factual",
      "Prompt": "The currency used in Japan is",
      "Entropy": 0.04833903209190974,
      "Last_Gain": 1.138728323699422,
      "Total_Amp": 1.5634920634920635,
      "Top_Token": " the",
      "Top_Prob": 0.9940700480917001
    },
    {
      "Family": "Gemma-7B",
      "Model": "Gemma-7B-IT",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Syntactic",
      "Prompt": "The agreement, which, notwithstanding the fact that it was signed only yesterday, effectively binds all parties immediately, stipulates that",
      "Entropy": 1.490934230752382,
      "Last_Gain": 1.1761363636363635,
      "Total_Amp": 1.7178423236514522,
      "Top_Token": " the",
      "Top_Prob": 0.6322610355460565
    },
    {
      "Family": "Gemma-7B",
      "Model": "Gemma-7B-IT",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Syntactic",
      "Prompt": "Although the weather was extremely cold, and despite the fact that they had no coats, the children decided to",
      "Entropy": 0.11823930501646918,
      "Last_Gain": 1.1261682242990654,
      "Total_Amp": 2.0598290598290596,
      "Top_Token": " play",
      "Top_Prob": 0.9787477428869276
    },
    {
      "Family": "Gemma-7B",
      "Model": "Gemma-7B-IT",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Syntactic",
      "Prompt": "The professor, having reviewed the complex derivation multiple times without finding the error, finally realized that",
      "Entropy": 0.06160160097816747,
      "Last_Gain": 1.1538461538461537,
      "Total_Amp": 1.8181818181818181,
      "Top_Token": " the",
      "Top_Prob": 0.9909915399747043
    },
    {
      "Family": "Gemma-7B",
      "Model": "Gemma-7B-IT",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Syntactic",
      "Prompt": "To imply that such a fundamental shift in policy could occur without significant public debate is to suggest that",
      "Entropy": 1.2181841459976466,
      "Last_Gain": 1.1849710982658959,
      "Total_Amp": 1.6734693877551021,
      "Top_Token": " the",
      "Top_Prob": 0.7272910560414267
    },
    {
      "Family": "Gemma-7B",
      "Model": "Gemma-7B-IT",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Syntactic",
      "Prompt": "Not only did the experiment fail to yield the expected results, but it also demonstrated that the initial hypothesis was",
      "Entropy": 1.0732551415403164,
      "Last_Gain": 1.1196172248803828,
      "Total_Amp": 1.7727272727272727,
      "Top_Token": " incorrect",
      "Top_Prob": 0.5674734776939792
    },
    {
      "Family": "Gemma-7B",
      "Model": "Gemma-7B-IT",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Cliche",
      "Prompt": "The true meaning of happiness is often found in",
      "Entropy": 0.4881128254843238,
      "Last_Gain": 1.1516587677725119,
      "Total_Amp": 1.9836734693877551,
      "Top_Token": " the",
      "Top_Prob": 0.9085552955537698
    },
    {
      "Family": "Gemma-7B",
      "Model": "Gemma-7B-IT",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Cliche",
      "Prompt": "Actions speak louder than",
      "Entropy": 0.002899925381425015,
      "Last_Gain": 1.1695906432748537,
      "Total_Amp": 1.5267175572519085,
      "Top_Token": " words",
      "Top_Prob": 0.9997650467630048
    },
    {
      "Family": "Gemma-7B",
      "Model": "Gemma-7B-IT",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Cliche",
      "Prompt": "It is what it is, and we must simply",
      "Entropy": 0.3307364627354479,
      "Last_Gain": 1.1625615763546797,
      "Total_Amp": 1.2688172043010753,
      "Top_Token": " accept",
      "Top_Prob": 0.9440609760686325
    },
    {
      "Family": "Gemma-7B",
      "Model": "Gemma-7B-IT",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Cliche",
      "Prompt": "Time heals all",
      "Entropy": 0.05026243698686992,
      "Last_Gain": 1.1494845360824741,
      "Total_Amp": 1.5594405594405594,
      "Top_Token": " wounds",
      "Top_Prob": 0.9921827931080021
    },
    {
      "Family": "Gemma-7B",
      "Model": "Gemma-7B-IT",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Cliche",
      "Prompt": "Life is a journey, not a",
      "Entropy": 0.0006712713392633637,
      "Last_Gain": 1.1458333333333333,
      "Total_Amp": 1.7741935483870968,
      "Top_Token": " destination",
      "Top_Prob": 0.999970598122453
    },
    {
      "Family": "Gemma-7B",
      "Model": "Gemma-7B-IT",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Novel",
      "Prompt": "The epistemological implications of quantum decoherence suggest that the observer is",
      "Entropy": 1.5577538026736317,
      "Last_Gain": 1.1352657004830917,
      "Total_Amp": 1.902834008097166,
      "Top_Token": " an",
      "Top_Prob": 0.3882567240787827
    },
    {
      "Family": "Gemma-7B",
      "Model": "Gemma-7B-IT",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Novel",
      "Prompt": "If consciousness creates reality, then the paradox of the unobserved electron implies",
      "Entropy": 0.428655130651343,
      "Last_Gain": 1.1564245810055866,
      "Total_Amp": 1.1761363636363635,
      "Top_Token": " that",
      "Top_Prob": 0.8726787462594215
    },
    {
      "Family": "Gemma-7B",
      "Model": "Gemma-7B-IT",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Novel",
      "Prompt": "The intersection of baroque architecture and cybernetic theory creates a space where",
      "Entropy": 2.534145045926141,
      "Last_Gain": 1.1714285714285715,
      "Total_Amp": 1.7826086956521738,
      "Top_Token": " the",
      "Top_Prob": 0.37589416537964426
    },
    {
      "Family": "Gemma-7B",
      "Model": "Gemma-7B-IT",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Novel",
      "Prompt": "Calculating the trajectory of a hyperspace jump requires factoring in the variability of",
      "Entropy": 0.5747164798581802,
      "Last_Gain": 1.13,
      "Total_Amp": 1.9152542372881356,
      "Top_Token": " worm",
      "Top_Prob": 0.8877073532480775
    },
    {
      "Family": "Gemma-7B",
      "Model": "Gemma-7B-IT",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Novel",
      "Prompt": "The symbiotic relationship between fungal mycelium and digital neural networks results in",
      "Entropy": 2.434082689158963,
      "Last_Gain": 1.1659192825112108,
      "Total_Amp": 2.08,
      "Top_Token": " a",
      "Top_Prob": 0.3640507854180566
    },
    {
      "Family": "Gemma-7B",
      "Model": "Gemma-7B-IT",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Nonsense",
      "Prompt": "Table sky run blue jump quickly under over",
      "Entropy": 2.151344761965035,
      "Last_Gain": 1.13125,
      "Total_Amp": 1.1753246753246753,
      "Top_Token": ".",
      "Top_Prob": 0.45884884287844824
    },
    {
      "Family": "Gemma-7B",
      "Model": "Gemma-7B-IT",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Nonsense",
      "Prompt": "Purple idea furiously sleep colorless green",
      "Entropy": 1.6198180357247303,
      "Last_Gain": 1.1746031746031746,
      "Total_Amp": 1.2613636363636365,
      "Top_Token": ".",
      "Top_Prob": 0.6752283840171669
    },
    {
      "Family": "Gemma-7B",
      "Model": "Gemma-7B-IT",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Nonsense",
      "Prompt": "Clock river dance potato seven fast",
      "Entropy": 2.412219011948136,
      "Last_Gain": 1.177914110429448,
      "Total_Amp": 1.0607734806629834,
      "Top_Token": " food",
      "Top_Prob": 0.45398633069781275
    },
    {
      "Family": "Gemma-7B",
      "Model": "Gemma-7B-IT",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Nonsense",
      "Prompt": "Window eat loud tomorrow yellow under",
      "Entropy": 2.094895109532422,
      "Last_Gain": 1.1452513966480447,
      "Total_Amp": 1.2893081761006289,
      "Top_Token": " the",
      "Top_Prob": 0.4401321413242116
    },
    {
      "Family": "Gemma-7B",
      "Model": "Gemma-7B-IT",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Nonsense",
      "Prompt": "Fish bicycle logic cloud mountain swim",
      "Entropy": 2.7740093434508055,
      "Last_Gain": 1.1945945945945946,
      "Total_Amp": 1.1510416666666667,
      "Top_Token": "\n\n",
      "Top_Prob": 0.2720587410148179
    },
    {
      "Family": "LLaMA-3.1-8B",
      "Model": "LLaMA-3.1-8B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Factual",
      "Prompt": "The capital city of France is",
      "Entropy": 4.112517870072319,
      "Last_Gain": 1.425,
      "Total_Amp": 100.86635944700461,
      "Top_Token": " a",
      "Top_Prob": 0.16234250356727536
    },
    {
      "Family": "LLaMA-3.1-8B",
      "Model": "LLaMA-3.1-8B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Factual",
      "Prompt": "The atomic number of oxygen is",
      "Entropy": 1.0904856317898213,
      "Last_Gain": 1.4111675126903553,
      "Total_Amp": 84.72380952380952,
      "Top_Token": " ",
      "Top_Prob": 0.8192446488553171
    },
    {
      "Family": "LLaMA-3.1-8B",
      "Model": "LLaMA-3.1-8B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Factual",
      "Prompt": "Water boils at a temperature of",
      "Entropy": 1.3952371704204511,
      "Last_Gain": 1.3694581280788178,
      "Total_Amp": 77.6943231441048,
      "Top_Token": " ",
      "Top_Prob": 0.7793046270580966
    },
    {
      "Family": "LLaMA-3.1-8B",
      "Model": "LLaMA-3.1-8B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Factual",
      "Prompt": "The largest planet in our solar system is",
      "Entropy": 3.1849956863373916,
      "Last_Gain": 1.5132743362831858,
      "Total_Amp": 101.80465116279069,
      "Top_Token": " Jupiter",
      "Top_Prob": 0.5003174008465833
    },
    {
      "Family": "LLaMA-3.1-8B",
      "Model": "LLaMA-3.1-8B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Factual",
      "Prompt": "The currency used in Japan is",
      "Entropy": 1.1008497027831454,
      "Last_Gain": 1.439252336448598,
      "Total_Amp": 89.1945701357466,
      "Top_Token": " the",
      "Top_Prob": 0.7576539904400745
    },
    {
      "Family": "LLaMA-3.1-8B",
      "Model": "LLaMA-3.1-8B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Syntactic",
      "Prompt": "The agreement, which, notwithstanding the fact that it was signed only yesterday, effectively binds all parties immediately, stipulates that",
      "Entropy": 4.936545589899381,
      "Last_Gain": 1.4700854700854702,
      "Total_Amp": 106.87378640776699,
      "Top_Token": " the",
      "Top_Prob": 0.3356903805982811
    },
    {
      "Family": "LLaMA-3.1-8B",
      "Model": "LLaMA-3.1-8B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Syntactic",
      "Prompt": "Although the weather was extremely cold, and despite the fact that they had no coats, the children decided to",
      "Entropy": 4.000147418402857,
      "Last_Gain": 1.37984496124031,
      "Total_Amp": 96.54237288135593,
      "Top_Token": " go",
      "Top_Prob": 0.2503498424286564
    },
    {
      "Family": "LLaMA-3.1-8B",
      "Model": "LLaMA-3.1-8B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Syntactic",
      "Prompt": "The professor, having reviewed the complex derivation multiple times without finding the error, finally realized that",
      "Entropy": 2.885349649741727,
      "Last_Gain": 1.3981481481481481,
      "Total_Amp": 89.06912442396313,
      "Top_Token": " the",
      "Top_Prob": 0.3637412069930268
    },
    {
      "Family": "LLaMA-3.1-8B",
      "Model": "LLaMA-3.1-8B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Syntactic",
      "Prompt": "To imply that such a fundamental shift in policy could occur without significant public debate is to suggest that",
      "Entropy": 4.438380327437504,
      "Last_Gain": 1.3362445414847162,
      "Total_Amp": 87.04,
      "Top_Token": " the",
      "Top_Prob": 0.34270395383823776
    },
    {
      "Family": "LLaMA-3.1-8B",
      "Model": "LLaMA-3.1-8B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Syntactic",
      "Prompt": "Not only did the experiment fail to yield the expected results, but it also demonstrated that the initial hypothesis was",
      "Entropy": 1.7650455514584196,
      "Last_Gain": 1.162162162162162,
      "Total_Amp": 74.71493212669684,
      "Top_Token": " incorrect",
      "Top_Prob": 0.5656390526171962
    },
    {
      "Family": "LLaMA-3.1-8B",
      "Model": "LLaMA-3.1-8B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Cliche",
      "Prompt": "The true meaning of happiness is often found in",
      "Entropy": 2.917939602596336,
      "Last_Gain": 1.393162393162393,
      "Total_Amp": 93.56053811659193,
      "Top_Token": " the",
      "Top_Prob": 0.5525339851676785
    },
    {
      "Family": "LLaMA-3.1-8B",
      "Model": "LLaMA-3.1-8B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Cliche",
      "Prompt": "Actions speak louder than",
      "Entropy": 0.8854178807266727,
      "Last_Gain": 1.5246636771300448,
      "Total_Amp": 80.5925925925926,
      "Top_Token": " words",
      "Top_Prob": 0.9094549827604964
    },
    {
      "Family": "LLaMA-3.1-8B",
      "Model": "LLaMA-3.1-8B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Cliche",
      "Prompt": "It is what it is, and we must simply",
      "Entropy": 3.7094170043179138,
      "Last_Gain": 1.4435483870967742,
      "Total_Amp": 75.86754966887418,
      "Top_Token": " accept",
      "Top_Prob": 0.3401256898278587
    },
    {
      "Family": "LLaMA-3.1-8B",
      "Model": "LLaMA-3.1-8B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Cliche",
      "Prompt": "Time heals all",
      "Entropy": 1.0316151629337815,
      "Last_Gain": 1.5207373271889402,
      "Total_Amp": 82.82352941176471,
      "Top_Token": " wounds",
      "Top_Prob": 0.865028779014302
    },
    {
      "Family": "LLaMA-3.1-8B",
      "Model": "LLaMA-3.1-8B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Cliche",
      "Prompt": "Life is a journey, not a",
      "Entropy": 0.6495191780483114,
      "Last_Gain": 1.343558282208589,
      "Total_Amp": 64.0,
      "Top_Token": " destination",
      "Top_Prob": 0.9135130873422835
    },
    {
      "Family": "LLaMA-3.1-8B",
      "Model": "LLaMA-3.1-8B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Novel",
      "Prompt": "The epistemological implications of quantum decoherence suggest that the observer is",
      "Entropy": 4.361623953295844,
      "Last_Gain": 1.3304347826086957,
      "Total_Amp": 92.37735849056604,
      "Top_Token": " not",
      "Top_Prob": 0.1387174276681881
    },
    {
      "Family": "LLaMA-3.1-8B",
      "Model": "LLaMA-3.1-8B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Novel",
      "Prompt": "If consciousness creates reality, then the paradox of the unobserved electron implies",
      "Entropy": 1.3217356652027004,
      "Last_Gain": 1.3063063063063063,
      "Total_Amp": 67.2463768115942,
      "Top_Token": " that",
      "Top_Prob": 0.7774533749035221
    },
    {
      "Family": "LLaMA-3.1-8B",
      "Model": "LLaMA-3.1-8B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Novel",
      "Prompt": "The intersection of baroque architecture and cybernetic theory creates a space where",
      "Entropy": 5.648225530447404,
      "Last_Gain": 1.475,
      "Total_Amp": 89.9047619047619,
      "Top_Token": " the",
      "Top_Prob": 0.2360828551491647
    },
    {
      "Family": "LLaMA-3.1-8B",
      "Model": "LLaMA-3.1-8B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Novel",
      "Prompt": "Calculating the trajectory of a hyperspace jump requires factoring in the variability of",
      "Entropy": 4.355209122788051,
      "Last_Gain": 1.3693693693693694,
      "Total_Amp": 85.33333333333333,
      "Top_Token": " the",
      "Top_Prob": 0.25552953363127273
    },
    {
      "Family": "LLaMA-3.1-8B",
      "Model": "LLaMA-3.1-8B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Novel",
      "Prompt": "The symbiotic relationship between fungal mycelium and digital neural networks results in",
      "Entropy": 3.8909681356560117,
      "Last_Gain": 1.5327868852459017,
      "Total_Amp": 102.2905982905983,
      "Top_Token": " a",
      "Top_Prob": 0.3494660453024787
    },
    {
      "Family": "LLaMA-3.1-8B",
      "Model": "LLaMA-3.1-8B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Nonsense",
      "Prompt": "Table sky run blue jump quickly under over",
      "Entropy": 7.34580109581521,
      "Last_Gain": 1.7962962962962963,
      "Total_Amp": 101.35510204081632,
      "Top_Token": " the",
      "Top_Prob": 0.058682656586005584
    },
    {
      "Family": "LLaMA-3.1-8B",
      "Model": "LLaMA-3.1-8B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Nonsense",
      "Prompt": "Purple idea furiously sleep colorless green",
      "Entropy": 3.957291863778078,
      "Last_Gain": 1.9351351351351351,
      "Total_Amp": 72.50632911392405,
      "Top_Token": " ideas",
      "Top_Prob": 0.39123201376537636
    },
    {
      "Family": "LLaMA-3.1-8B",
      "Model": "LLaMA-3.1-8B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Nonsense",
      "Prompt": "Clock river dance potato seven fast",
      "Entropy": 7.0592667320236595,
      "Last_Gain": 1.6318407960199004,
      "Total_Amp": 64.79012345679013,
      "Top_Token": " food",
      "Top_Prob": 0.08075256674320712
    },
    {
      "Family": "LLaMA-3.1-8B",
      "Model": "LLaMA-3.1-8B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Nonsense",
      "Prompt": "Window eat loud tomorrow yellow under",
      "Entropy": 7.150132438820021,
      "Last_Gain": 1.7714285714285714,
      "Total_Amp": 90.87022900763358,
      "Top_Token": " the",
      "Top_Prob": 0.0943189666950142
    },
    {
      "Family": "LLaMA-3.1-8B",
      "Model": "LLaMA-3.1-8B-Base",
      "Variant": "base",
      "Norm_Type": "RMSNorm",
      "Category": "Nonsense",
      "Prompt": "Fish bicycle logic cloud mountain swim",
      "Entropy": 7.1491266476309105,
      "Last_Gain": 1.8155339805825244,
      "Total_Amp": 77.71428571428571,
      "Top_Token": "\n",
      "Top_Prob": 0.0954633633479599
    },
    {
      "Family": "LLaMA-3.1-8B",
      "Model": "LLaMA-3.1-8B-Instruct",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Factual",
      "Prompt": "The capital city of France is",
      "Entropy": 3.627665206708651,
      "Last_Gain": 1.413953488372093,
      "Total_Amp": 91.77358490566037,
      "Top_Token": " Paris",
      "Top_Prob": 0.17397481064247786
    },
    {
      "Family": "LLaMA-3.1-8B",
      "Model": "LLaMA-3.1-8B-Instruct",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Factual",
      "Prompt": "The atomic number of oxygen is",
      "Entropy": 0.8120792917422592,
      "Last_Gain": 1.3829787234042554,
      "Total_Amp": 80.0,
      "Top_Token": " ",
      "Top_Prob": 0.8798036215775107
    },
    {
      "Family": "LLaMA-3.1-8B",
      "Model": "LLaMA-3.1-8B-Instruct",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Factual",
      "Prompt": "Water boils at a temperature of",
      "Entropy": 0.6545175508321296,
      "Last_Gain": 1.3471502590673574,
      "Total_Amp": 73.95555555555555,
      "Top_Token": " ",
      "Top_Prob": 0.9038466497986516
    },
    {
      "Family": "LLaMA-3.1-8B",
      "Model": "LLaMA-3.1-8B-Instruct",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Factual",
      "Prompt": "The largest planet in our solar system is",
      "Entropy": 1.8803872640081154,
      "Last_Gain": 1.4356435643564356,
      "Total_Amp": 88.38095238095238,
      "Top_Token": " Jupiter",
      "Top_Prob": 0.7130305522114877
    },
    {
      "Family": "LLaMA-3.1-8B",
      "Model": "LLaMA-3.1-8B-Instruct",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Factual",
      "Prompt": "The currency used in Japan is",
      "Entropy": 0.41977481961546476,
      "Last_Gain": 1.3471502590673574,
      "Total_Amp": 76.68202764976958,
      "Top_Token": " the",
      "Top_Prob": 0.9181426308783356
    },
    {
      "Family": "LLaMA-3.1-8B",
      "Model": "LLaMA-3.1-8B-Instruct",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Syntactic",
      "Prompt": "The agreement, which, notwithstanding the fact that it was signed only yesterday, effectively binds all parties immediately, stipulates that",
      "Entropy": 4.971041625012856,
      "Last_Gain": 1.4672897196261683,
      "Total_Amp": 103.05641025641026,
      "Top_Token": " the",
      "Top_Prob": 0.3112512360604348
    },
    {
      "Family": "LLaMA-3.1-8B",
      "Model": "LLaMA-3.1-8B-Instruct",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Syntactic",
      "Prompt": "Although the weather was extremely cold, and despite the fact that they had no coats, the children decided to",
      "Entropy": 3.1273814860606644,
      "Last_Gain": 1.3805309734513274,
      "Total_Amp": 86.81739130434782,
      "Top_Token": " go",
      "Top_Prob": 0.369122673337032
    },
    {
      "Family": "LLaMA-3.1-8B",
      "Model": "LLaMA-3.1-8B-Instruct",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Syntactic",
      "Prompt": "The professor, having reviewed the complex derivation multiple times without finding the error, finally realized that",
      "Entropy": 2.0779128908129443,
      "Last_Gain": 1.4371859296482412,
      "Total_Amp": 85.13488372093023,
      "Top_Token": " the",
      "Top_Prob": 0.5434066032378397
    },
    {
      "Family": "LLaMA-3.1-8B",
      "Model": "LLaMA-3.1-8B-Instruct",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Syntactic",
      "Prompt": "To imply that such a fundamental shift in policy could occur without significant public debate is to suggest that",
      "Entropy": 3.9881480417822397,
      "Last_Gain": 1.3719806763285025,
      "Total_Amp": 82.61818181818182,
      "Top_Token": " the",
      "Top_Prob": 0.38981036225241544
    },
    {
      "Family": "LLaMA-3.1-8B",
      "Model": "LLaMA-3.1-8B-Instruct",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Syntactic",
      "Prompt": "Not only did the experiment fail to yield the expected results, but it also demonstrated that the initial hypothesis was",
      "Entropy": 2.757374429741723,
      "Last_Gain": 1.2890995260663507,
      "Total_Amp": 81.34579439252336,
      "Top_Token": " incorrect",
      "Top_Prob": 0.283918613257739
    },
    {
      "Family": "LLaMA-3.1-8B",
      "Model": "LLaMA-3.1-8B-Instruct",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Cliche",
      "Prompt": "The true meaning of happiness is often found in",
      "Entropy": 2.1433910112078878,
      "Last_Gain": 1.4150943396226414,
      "Total_Amp": 86.48648648648648,
      "Top_Token": " the",
      "Top_Prob": 0.6421916517540094
    },
    {
      "Family": "LLaMA-3.1-8B",
      "Model": "LLaMA-3.1-8B-Instruct",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Cliche",
      "Prompt": "Actions speak louder than",
      "Entropy": 0.60489722235364,
      "Last_Gain": 1.4711538461538463,
      "Total_Amp": 75.32307692307693,
      "Top_Token": " words",
      "Top_Prob": 0.9383374759351008
    },
    {
      "Family": "LLaMA-3.1-8B",
      "Model": "LLaMA-3.1-8B-Instruct",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Cliche",
      "Prompt": "It is what it is, and we must simply",
      "Entropy": 2.630777102631308,
      "Last_Gain": 1.4887892376681615,
      "Total_Amp": 72.27210884353741,
      "Top_Token": " accept",
      "Top_Prob": 0.5618410176525981
    },
    {
      "Family": "LLaMA-3.1-8B",
      "Model": "LLaMA-3.1-8B-Instruct",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Cliche",
      "Prompt": "Time heals all",
      "Entropy": 0.33303161985740515,
      "Last_Gain": 1.4554455445544554,
      "Total_Amp": 76.8,
      "Top_Token": " wounds",
      "Top_Prob": 0.9624169737175817
    },
    {
      "Family": "LLaMA-3.1-8B",
      "Model": "LLaMA-3.1-8B-Instruct",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Cliche",
      "Prompt": "Life is a journey, not a",
      "Entropy": 0.2938508823438273,
      "Last_Gain": 1.287292817679558,
      "Total_Amp": 68.71889400921658,
      "Top_Token": " destination",
      "Top_Prob": 0.9693946402051714
    },
    {
      "Family": "LLaMA-3.1-8B",
      "Model": "LLaMA-3.1-8B-Instruct",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Novel",
      "Prompt": "The epistemological implications of quantum decoherence suggest that the observer is",
      "Entropy": 3.227644479934013,
      "Last_Gain": 1.3170731707317074,
      "Total_Amp": 83.88349514563107,
      "Top_Token": " not",
      "Top_Prob": 0.3707279639664631
    },
    {
      "Family": "LLaMA-3.1-8B",
      "Model": "LLaMA-3.1-8B-Instruct",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Novel",
      "Prompt": "If consciousness creates reality, then the paradox of the unobserved electron implies",
      "Entropy": 0.6165607829429617,
      "Last_Gain": 1.32,
      "Total_Amp": 60.34285714285714,
      "Top_Token": " that",
      "Top_Prob": 0.8926493567583063
    },
    {
      "Family": "LLaMA-3.1-8B",
      "Model": "LLaMA-3.1-8B-Instruct",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Novel",
      "Prompt": "The intersection of baroque architecture and cybernetic theory creates a space where",
      "Entropy": 5.029249383510826,
      "Last_Gain": 1.4857142857142858,
      "Total_Amp": 78.92490118577075,
      "Top_Token": " the",
      "Top_Prob": 0.2897259677153462
    },
    {
      "Family": "LLaMA-3.1-8B",
      "Model": "LLaMA-3.1-8B-Instruct",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Novel",
      "Prompt": "Calculating the trajectory of a hyperspace jump requires factoring in the variability of",
      "Entropy": 4.158043857186328,
      "Last_Gain": 1.4366197183098592,
      "Total_Amp": 84.77922077922078,
      "Top_Token": " the",
      "Top_Prob": 0.14575429246343338
    },
    {
      "Family": "LLaMA-3.1-8B",
      "Model": "LLaMA-3.1-8B-Instruct",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Novel",
      "Prompt": "The symbiotic relationship between fungal mycelium and digital neural networks results in",
      "Entropy": 3.619439320703934,
      "Last_Gain": 1.6090909090909091,
      "Total_Amp": 93.6198347107438,
      "Top_Token": " a",
      "Top_Prob": 0.32607931087859765
    },
    {
      "Family": "LLaMA-3.1-8B",
      "Model": "LLaMA-3.1-8B-Instruct",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Nonsense",
      "Prompt": "Table sky run blue jump quickly under over",
      "Entropy": 8.42808952878076,
      "Last_Gain": 2.199004975124378,
      "Total_Amp": 117.37759336099585,
      "Top_Token": " the",
      "Top_Prob": 0.022295988639297354
    },
    {
      "Family": "LLaMA-3.1-8B",
      "Model": "LLaMA-3.1-8B-Instruct",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Nonsense",
      "Prompt": "Purple idea furiously sleep colorless green",
      "Entropy": 8.01963059380356,
      "Last_Gain": 2.3657142857142857,
      "Total_Amp": 83.84810126582279,
      "Top_Token": " ideas",
      "Top_Prob": 0.055229860370102225
    },
    {
      "Family": "LLaMA-3.1-8B",
      "Model": "LLaMA-3.1-8B-Instruct",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Nonsense",
      "Prompt": "Clock river dance potato seven fast",
      "Entropy": 8.893966359084597,
      "Last_Gain": 2.085561497326203,
      "Total_Amp": 80.51612903225806,
      "Top_Token": " food",
      "Top_Prob": 0.022042649627948312
    },
    {
      "Family": "LLaMA-3.1-8B",
      "Model": "LLaMA-3.1-8B-Instruct",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Nonsense",
      "Prompt": "Window eat loud tomorrow yellow under",
      "Entropy": 9.257812490228966,
      "Last_Gain": 2.4410256410256412,
      "Total_Amp": 118.07751937984496,
      "Top_Token": ".",
      "Top_Prob": 0.009808502644626155
    },
    {
      "Family": "LLaMA-3.1-8B",
      "Model": "LLaMA-3.1-8B-Instruct",
      "Variant": "instruct",
      "Norm_Type": "RMSNorm",
      "Category": "Nonsense",
      "Prompt": "Fish bicycle logic cloud mountain swim",
      "Entropy": 8.231469629566176,
      "Last_Gain": 2.1481481481481484,
      "Total_Amp": 84.36363636363636,
      "Top_Token": "\n",
      "Top_Prob": 0.06951443180812994
    }
  ]
}